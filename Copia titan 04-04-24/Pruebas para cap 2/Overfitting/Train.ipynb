{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02364ba-31a7-4ede-b3c5-9ff2617bf233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodel\u001b[0m/  Train.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bd1d87-7d84-4055-aee7-0f07f69c38de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------ epoch 001 (0 steps) ------------------------------------\n",
      "Max loss: 1.0087573528289795\n",
      "Min loss: 1.0018398761749268\n",
      "Mean loss: 1.00500883658727\n",
      "Std loss: 0.0025105518383904154\n",
      "Total Loss: 6.030053019523621\n",
      "saved model at ./weights/model_001.pth\n",
      "------------------------------------ epoch 002 (6 steps) ------------------------------------\n",
      "Max loss: 1.0038663148880005\n",
      "Min loss: 0.9962489008903503\n",
      "Mean loss: 1.0012801984945934\n",
      "Std loss: 0.0025586870554932853\n",
      "Total Loss: 6.00768119096756\n",
      "------------------------------------ epoch 003 (12 steps) ------------------------------------\n",
      "Max loss: 1.0057692527770996\n",
      "Min loss: 0.999032199382782\n",
      "Mean loss: 1.0021658837795258\n",
      "Std loss: 0.0023014581623905465\n",
      "Total Loss: 6.0129953026771545\n",
      "------------------------------------ epoch 004 (18 steps) ------------------------------------\n",
      "Max loss: 1.0050029754638672\n",
      "Min loss: 0.9959608316421509\n",
      "Mean loss: 1.001015881697337\n",
      "Std loss: 0.0036870280404473947\n",
      "Total Loss: 6.006095290184021\n",
      "------------------------------------ epoch 005 (24 steps) ------------------------------------\n",
      "Max loss: 1.0052098035812378\n",
      "Min loss: 0.9993879199028015\n",
      "Mean loss: 1.0018095473448436\n",
      "Std loss: 0.0019096932549027728\n",
      "Total Loss: 6.010857284069061\n",
      "------------------------------------ epoch 006 (30 steps) ------------------------------------\n",
      "Max loss: 1.0064706802368164\n",
      "Min loss: 0.9947521686553955\n",
      "Mean loss: 1.0009759763876598\n",
      "Std loss: 0.003786320290363804\n",
      "Total Loss: 6.005855858325958\n",
      "------------------------------------ epoch 007 (36 steps) ------------------------------------\n",
      "Max loss: 1.0014266967773438\n",
      "Min loss: 0.996660590171814\n",
      "Mean loss: 0.9995283683141073\n",
      "Std loss: 0.001695378095041137\n",
      "Total Loss: 5.9971702098846436\n",
      "------------------------------------ epoch 008 (42 steps) ------------------------------------\n",
      "Max loss: 1.0045589208602905\n",
      "Min loss: 0.9946029186248779\n",
      "Mean loss: 1.0005227625370026\n",
      "Std loss: 0.0035564436469920444\n",
      "Total Loss: 6.003136575222015\n",
      "------------------------------------ epoch 009 (48 steps) ------------------------------------\n",
      "Max loss: 1.0000836849212646\n",
      "Min loss: 0.9911816120147705\n",
      "Mean loss: 0.9970421691735586\n",
      "Std loss: 0.0029986514288727474\n",
      "Total Loss: 5.982253015041351\n",
      "------------------------------------ epoch 010 (54 steps) ------------------------------------\n",
      "Max loss: 0.9968841075897217\n",
      "Min loss: 0.983573853969574\n",
      "Mean loss: 0.99005988240242\n",
      "Std loss: 0.004625955555105061\n",
      "Total Loss: 5.94035929441452\n",
      "------------------------------------ epoch 011 (60 steps) ------------------------------------\n",
      "Max loss: 0.9925177097320557\n",
      "Min loss: 0.967742919921875\n",
      "Mean loss: 0.9819665551185608\n",
      "Std loss: 0.008060245801126737\n",
      "Total Loss: 5.891799330711365\n",
      "------------------------------------ epoch 012 (66 steps) ------------------------------------\n",
      "Max loss: 0.975890040397644\n",
      "Min loss: 0.9532608389854431\n",
      "Mean loss: 0.9636420408884684\n",
      "Std loss: 0.008318609183525822\n",
      "Total Loss: 5.7818522453308105\n",
      "------------------------------------ epoch 013 (72 steps) ------------------------------------\n",
      "Max loss: 0.9439791440963745\n",
      "Min loss: 0.9272516965866089\n",
      "Mean loss: 0.9367346862951914\n",
      "Std loss: 0.006149361192225365\n",
      "Total Loss: 5.620408117771149\n",
      "------------------------------------ epoch 014 (78 steps) ------------------------------------\n",
      "Max loss: 0.9263636469841003\n",
      "Min loss: 0.8875738978385925\n",
      "Mean loss: 0.9084263245264689\n",
      "Std loss: 0.011280630240443764\n",
      "Total Loss: 5.4505579471588135\n",
      "------------------------------------ epoch 015 (84 steps) ------------------------------------\n",
      "Max loss: 0.8950659036636353\n",
      "Min loss: 0.872491717338562\n",
      "Mean loss: 0.8827135662237803\n",
      "Std loss: 0.009260218030427954\n",
      "Total Loss: 5.296281397342682\n",
      "------------------------------------ epoch 016 (90 steps) ------------------------------------\n",
      "Max loss: 0.8748736381530762\n",
      "Min loss: 0.8450753688812256\n",
      "Mean loss: 0.8604598542054495\n",
      "Std loss: 0.00891040637416755\n",
      "Total Loss: 5.1627591252326965\n",
      "------------------------------------ epoch 017 (96 steps) ------------------------------------\n",
      "Max loss: 0.8473517298698425\n",
      "Min loss: 0.8266138434410095\n",
      "Mean loss: 0.8390739560127258\n",
      "Std loss: 0.007039641980286348\n",
      "Total Loss: 5.034443736076355\n",
      "------------------------------------ epoch 018 (102 steps) ------------------------------------\n",
      "Max loss: 0.8266350626945496\n",
      "Min loss: 0.811367392539978\n",
      "Mean loss: 0.8194269637266794\n",
      "Std loss: 0.004493933817923987\n",
      "Total Loss: 4.916561782360077\n",
      "------------------------------------ epoch 019 (108 steps) ------------------------------------\n",
      "Max loss: 0.8131734728813171\n",
      "Min loss: 0.7921460866928101\n",
      "Mean loss: 0.8069538176059723\n",
      "Std loss: 0.007154034556792359\n",
      "Total Loss: 4.841722905635834\n",
      "------------------------------------ epoch 020 (114 steps) ------------------------------------\n",
      "Max loss: 0.8011525869369507\n",
      "Min loss: 0.7692243456840515\n",
      "Mean loss: 0.7890441517035166\n",
      "Std loss: 0.011742201293187076\n",
      "Total Loss: 4.7342649102211\n",
      "------------------------------------ epoch 021 (120 steps) ------------------------------------\n",
      "Max loss: 0.7792031764984131\n",
      "Min loss: 0.7609919905662537\n",
      "Mean loss: 0.7711744606494904\n",
      "Std loss: 0.007189669729726084\n",
      "Total Loss: 4.627046763896942\n",
      "------------------------------------ epoch 022 (126 steps) ------------------------------------\n",
      "Max loss: 0.776546835899353\n",
      "Min loss: 0.7464596033096313\n",
      "Mean loss: 0.7609889010588328\n",
      "Std loss: 0.01210233541735223\n",
      "Total Loss: 4.565933406352997\n",
      "------------------------------------ epoch 023 (132 steps) ------------------------------------\n",
      "Max loss: 0.7491343021392822\n",
      "Min loss: 0.7218310236930847\n",
      "Mean loss: 0.7340925931930542\n",
      "Std loss: 0.00976944410495509\n",
      "Total Loss: 4.404555559158325\n",
      "------------------------------------ epoch 024 (138 steps) ------------------------------------\n",
      "Max loss: 0.7444909811019897\n",
      "Min loss: 0.716058611869812\n",
      "Mean loss: 0.726759801308314\n",
      "Std loss: 0.008948215603959232\n",
      "Total Loss: 4.360558807849884\n",
      "------------------------------------ epoch 025 (144 steps) ------------------------------------\n",
      "Max loss: 0.7149405479431152\n",
      "Min loss: 0.6987216472625732\n",
      "Mean loss: 0.7070196668306986\n",
      "Std loss: 0.005825620989352347\n",
      "Total Loss: 4.242118000984192\n",
      "------------------------------------ epoch 026 (150 steps) ------------------------------------\n",
      "Max loss: 0.705969512462616\n",
      "Min loss: 0.6749527454376221\n",
      "Mean loss: 0.6879160702228546\n",
      "Std loss: 0.009584351427553024\n",
      "Total Loss: 4.127496421337128\n",
      "------------------------------------ epoch 027 (156 steps) ------------------------------------\n",
      "Max loss: 0.6816508173942566\n",
      "Min loss: 0.6595103740692139\n",
      "Mean loss: 0.6752104957898458\n",
      "Std loss: 0.007719299390611851\n",
      "Total Loss: 4.051262974739075\n",
      "------------------------------------ epoch 028 (162 steps) ------------------------------------\n",
      "Max loss: 0.668846607208252\n",
      "Min loss: 0.6468935012817383\n",
      "Mean loss: 0.6620658536752065\n",
      "Std loss: 0.007789305157621297\n",
      "Total Loss: 3.972395122051239\n",
      "------------------------------------ epoch 029 (168 steps) ------------------------------------\n",
      "Max loss: 0.6727527379989624\n",
      "Min loss: 0.6357357501983643\n",
      "Mean loss: 0.64882493019104\n",
      "Std loss: 0.013851001349026141\n",
      "Total Loss: 3.8929495811462402\n",
      "------------------------------------ epoch 030 (174 steps) ------------------------------------\n",
      "Max loss: 0.6452240943908691\n",
      "Min loss: 0.6154953837394714\n",
      "Mean loss: 0.633578360080719\n",
      "Std loss: 0.009335166278955675\n",
      "Total Loss: 3.801470160484314\n",
      "------------------------------------ epoch 031 (180 steps) ------------------------------------\n",
      "Max loss: 0.6382779479026794\n",
      "Min loss: 0.6036885380744934\n",
      "Mean loss: 0.6213933726151785\n",
      "Std loss: 0.011318833690748072\n",
      "Total Loss: 3.7283602356910706\n",
      "------------------------------------ epoch 032 (186 steps) ------------------------------------\n",
      "Max loss: 0.6223306655883789\n",
      "Min loss: 0.6009520292282104\n",
      "Mean loss: 0.6134205758571625\n",
      "Std loss: 0.007822322133388473\n",
      "Total Loss: 3.680523455142975\n",
      "------------------------------------ epoch 033 (192 steps) ------------------------------------\n",
      "Max loss: 0.6131811738014221\n",
      "Min loss: 0.5735355615615845\n",
      "Mean loss: 0.5925321181615194\n",
      "Std loss: 0.01323545875970157\n",
      "Total Loss: 3.555192708969116\n",
      "------------------------------------ epoch 034 (198 steps) ------------------------------------\n",
      "Max loss: 0.5942977666854858\n",
      "Min loss: 0.5627965331077576\n",
      "Mean loss: 0.5789705912272135\n",
      "Std loss: 0.011175775171382116\n",
      "Total Loss: 3.4738235473632812\n",
      "------------------------------------ epoch 035 (204 steps) ------------------------------------\n",
      "Max loss: 0.5798234939575195\n",
      "Min loss: 0.5501842498779297\n",
      "Mean loss: 0.5626014967759451\n",
      "Std loss: 0.010972929804847256\n",
      "Total Loss: 3.37560898065567\n",
      "------------------------------------ epoch 036 (210 steps) ------------------------------------\n",
      "Max loss: 0.5946370363235474\n",
      "Min loss: 0.5501976013183594\n",
      "Mean loss: 0.5731188456217448\n",
      "Std loss: 0.013426552468934089\n",
      "Total Loss: 3.4387130737304688\n",
      "------------------------------------ epoch 037 (216 steps) ------------------------------------\n",
      "Max loss: 0.5890232920646667\n",
      "Min loss: 0.5531651377677917\n",
      "Mean loss: 0.5708840688069662\n",
      "Std loss: 0.014199163167780032\n",
      "Total Loss: 3.425304412841797\n",
      "------------------------------------ epoch 038 (222 steps) ------------------------------------\n",
      "Max loss: 0.5592856407165527\n",
      "Min loss: 0.5362555384635925\n",
      "Mean loss: 0.5467373728752136\n",
      "Std loss: 0.008317863220780342\n",
      "Total Loss: 3.2804242372512817\n",
      "------------------------------------ epoch 039 (228 steps) ------------------------------------\n",
      "Max loss: 0.57280033826828\n",
      "Min loss: 0.5345854759216309\n",
      "Mean loss: 0.5553822716077169\n",
      "Std loss: 0.015165608567685207\n",
      "Total Loss: 3.3322936296463013\n",
      "------------------------------------ epoch 040 (234 steps) ------------------------------------\n",
      "Max loss: 0.5440350770950317\n",
      "Min loss: 0.513018012046814\n",
      "Mean loss: 0.5302135944366455\n",
      "Std loss: 0.011824284842150659\n",
      "Total Loss: 3.181281566619873\n",
      "------------------------------------ epoch 041 (240 steps) ------------------------------------\n",
      "Max loss: 0.5423060655593872\n",
      "Min loss: 0.498195618391037\n",
      "Mean loss: 0.5150047292311987\n",
      "Std loss: 0.014270985344835008\n",
      "Total Loss: 3.0900283753871918\n",
      "------------------------------------ epoch 042 (246 steps) ------------------------------------\n",
      "Max loss: 0.5128191113471985\n",
      "Min loss: 0.4957601726055145\n",
      "Mean loss: 0.5019672363996506\n",
      "Std loss: 0.006420986195906151\n",
      "Total Loss: 3.0118034183979034\n",
      "------------------------------------ epoch 043 (252 steps) ------------------------------------\n",
      "Max loss: 0.5300086736679077\n",
      "Min loss: 0.490628182888031\n",
      "Mean loss: 0.5075164685646693\n",
      "Std loss: 0.014846197345618692\n",
      "Total Loss: 3.0450988113880157\n",
      "------------------------------------ epoch 044 (258 steps) ------------------------------------\n",
      "Max loss: 0.5217850804328918\n",
      "Min loss: 0.4834747314453125\n",
      "Mean loss: 0.49534620841344196\n",
      "Std loss: 0.013713616279770384\n",
      "Total Loss: 2.972077250480652\n",
      "------------------------------------ epoch 045 (264 steps) ------------------------------------\n",
      "Max loss: 0.5069266557693481\n",
      "Min loss: 0.4774016737937927\n",
      "Mean loss: 0.49536284307638806\n",
      "Std loss: 0.012751805011766267\n",
      "Total Loss: 2.9721770584583282\n",
      "------------------------------------ epoch 046 (270 steps) ------------------------------------\n",
      "Max loss: 0.5241779685020447\n",
      "Min loss: 0.481173574924469\n",
      "Mean loss: 0.49429049094518024\n",
      "Std loss: 0.014236205140309627\n",
      "Total Loss: 2.9657429456710815\n",
      "------------------------------------ epoch 047 (276 steps) ------------------------------------\n",
      "Max loss: 0.507506251335144\n",
      "Min loss: 0.4642671048641205\n",
      "Mean loss: 0.4841340035200119\n",
      "Std loss: 0.014471710307756982\n",
      "Total Loss: 2.9048040211200714\n",
      "------------------------------------ epoch 048 (282 steps) ------------------------------------\n",
      "Max loss: 0.4794139862060547\n",
      "Min loss: 0.45348021388053894\n",
      "Mean loss: 0.4692622472842534\n",
      "Std loss: 0.00853158414133203\n",
      "Total Loss: 2.8155734837055206\n",
      "------------------------------------ epoch 049 (288 steps) ------------------------------------\n",
      "Max loss: 0.5004479289054871\n",
      "Min loss: 0.45331066846847534\n",
      "Mean loss: 0.46873478094736737\n",
      "Std loss: 0.016063800324833212\n",
      "Total Loss: 2.812408685684204\n",
      "------------------------------------ epoch 050 (294 steps) ------------------------------------\n",
      "Max loss: 0.5096821188926697\n",
      "Min loss: 0.4443945586681366\n",
      "Mean loss: 0.47193993131319684\n",
      "Std loss: 0.02038478575902957\n",
      "Total Loss: 2.831639587879181\n",
      "------------------------------------ epoch 051 (300 steps) ------------------------------------\n",
      "Max loss: 0.48746582865715027\n",
      "Min loss: 0.45450401306152344\n",
      "Mean loss: 0.4681981901327769\n",
      "Std loss: 0.011787658188021495\n",
      "Total Loss: 2.8091891407966614\n",
      "------------------------------------ epoch 052 (306 steps) ------------------------------------\n",
      "Max loss: 0.46853241324424744\n",
      "Min loss: 0.43938350677490234\n",
      "Mean loss: 0.45613181094328564\n",
      "Std loss: 0.009740667813494785\n",
      "Total Loss: 2.7367908656597137\n",
      "------------------------------------ epoch 053 (312 steps) ------------------------------------\n",
      "Max loss: 0.4595068395137787\n",
      "Min loss: 0.4370752274990082\n",
      "Mean loss: 0.4501744856437047\n",
      "Std loss: 0.008080156036000185\n",
      "Total Loss: 2.7010469138622284\n",
      "------------------------------------ epoch 054 (318 steps) ------------------------------------\n",
      "Max loss: 0.46465227007865906\n",
      "Min loss: 0.44205814599990845\n",
      "Mean loss: 0.4523823410272598\n",
      "Std loss: 0.0074982214438632285\n",
      "Total Loss: 2.714294046163559\n",
      "------------------------------------ epoch 055 (324 steps) ------------------------------------\n",
      "Max loss: 0.49027806520462036\n",
      "Min loss: 0.43130359053611755\n",
      "Mean loss: 0.44716258347034454\n",
      "Std loss: 0.019910264771622416\n",
      "Total Loss: 2.6829755008220673\n",
      "------------------------------------ epoch 056 (330 steps) ------------------------------------\n",
      "Max loss: 0.4639894366264343\n",
      "Min loss: 0.42999905347824097\n",
      "Mean loss: 0.44632983207702637\n",
      "Std loss: 0.013903268834147182\n",
      "Total Loss: 2.677978992462158\n",
      "------------------------------------ epoch 057 (336 steps) ------------------------------------\n",
      "Max loss: 0.4747169017791748\n",
      "Min loss: 0.42901310324668884\n",
      "Mean loss: 0.4476148337125778\n",
      "Std loss: 0.01730089514231592\n",
      "Total Loss: 2.685689002275467\n",
      "------------------------------------ epoch 058 (342 steps) ------------------------------------\n",
      "Max loss: 0.46092769503593445\n",
      "Min loss: 0.42282089591026306\n",
      "Mean loss: 0.4323960493008296\n",
      "Std loss: 0.013219865998484586\n",
      "Total Loss: 2.5943762958049774\n",
      "------------------------------------ epoch 059 (348 steps) ------------------------------------\n",
      "Max loss: 0.4503115117549896\n",
      "Min loss: 0.41978833079338074\n",
      "Mean loss: 0.4311379740635554\n",
      "Std loss: 0.009494913766740333\n",
      "Total Loss: 2.5868278443813324\n",
      "------------------------------------ epoch 060 (354 steps) ------------------------------------\n",
      "Max loss: 0.4586646556854248\n",
      "Min loss: 0.4250373840332031\n",
      "Mean loss: 0.4404561122258504\n",
      "Std loss: 0.010859135643681106\n",
      "Total Loss: 2.6427366733551025\n",
      "------------------------------------ epoch 061 (360 steps) ------------------------------------\n",
      "Max loss: 0.41990041732788086\n",
      "Min loss: 0.4092680811882019\n",
      "Mean loss: 0.4159774531920751\n",
      "Std loss: 0.0036752315014833025\n",
      "Total Loss: 2.4958647191524506\n",
      "------------------------------------ epoch 062 (366 steps) ------------------------------------\n",
      "Max loss: 0.4248453974723816\n",
      "Min loss: 0.40481656789779663\n",
      "Mean loss: 0.41463925937811535\n",
      "Std loss: 0.007414267680292909\n",
      "Total Loss: 2.487835556268692\n",
      "------------------------------------ epoch 063 (372 steps) ------------------------------------\n",
      "Max loss: 0.4285362958908081\n",
      "Min loss: 0.40778160095214844\n",
      "Mean loss: 0.417564516266187\n",
      "Std loss: 0.006357013796929093\n",
      "Total Loss: 2.505387097597122\n",
      "------------------------------------ epoch 064 (378 steps) ------------------------------------\n",
      "Max loss: 0.44106537103652954\n",
      "Min loss: 0.4071083068847656\n",
      "Mean loss: 0.41848555703957874\n",
      "Std loss: 0.014448029853390307\n",
      "Total Loss: 2.5109133422374725\n",
      "------------------------------------ epoch 065 (384 steps) ------------------------------------\n",
      "Max loss: 0.43950915336608887\n",
      "Min loss: 0.40595826506614685\n",
      "Mean loss: 0.42512450615564984\n",
      "Std loss: 0.011439237228426246\n",
      "Total Loss: 2.550747036933899\n",
      "------------------------------------ epoch 066 (390 steps) ------------------------------------\n",
      "Max loss: 0.4492194652557373\n",
      "Min loss: 0.41636794805526733\n",
      "Mean loss: 0.4318961054086685\n",
      "Std loss: 0.012188483070785735\n",
      "Total Loss: 2.591376632452011\n",
      "------------------------------------ epoch 067 (396 steps) ------------------------------------\n",
      "Max loss: 0.44111770391464233\n",
      "Min loss: 0.4083659052848816\n",
      "Mean loss: 0.4200766732295354\n",
      "Std loss: 0.0104865949922021\n",
      "Total Loss: 2.5204600393772125\n",
      "------------------------------------ epoch 068 (402 steps) ------------------------------------\n",
      "Max loss: 0.4419229030609131\n",
      "Min loss: 0.4017122685909271\n",
      "Mean loss: 0.4154277443885803\n",
      "Std loss: 0.015085954681148454\n",
      "Total Loss: 2.492566466331482\n",
      "------------------------------------ epoch 069 (408 steps) ------------------------------------\n",
      "Max loss: 0.4220370948314667\n",
      "Min loss: 0.39280813932418823\n",
      "Mean loss: 0.4063773403565089\n",
      "Std loss: 0.010947369868869794\n",
      "Total Loss: 2.4382640421390533\n",
      "------------------------------------ epoch 070 (414 steps) ------------------------------------\n",
      "Max loss: 0.436252236366272\n",
      "Min loss: 0.40214109420776367\n",
      "Mean loss: 0.41412463287512463\n",
      "Std loss: 0.01320621478223534\n",
      "Total Loss: 2.4847477972507477\n",
      "------------------------------------ epoch 071 (420 steps) ------------------------------------\n",
      "Max loss: 0.4708753824234009\n",
      "Min loss: 0.39128196239471436\n",
      "Mean loss: 0.4206846207380295\n",
      "Std loss: 0.026317764316467353\n",
      "Total Loss: 2.524107724428177\n",
      "------------------------------------ epoch 072 (426 steps) ------------------------------------\n",
      "Max loss: 0.4281180799007416\n",
      "Min loss: 0.3996361494064331\n",
      "Mean loss: 0.40972913801670074\n",
      "Std loss: 0.010021570492622558\n",
      "Total Loss: 2.4583748281002045\n",
      "------------------------------------ epoch 073 (432 steps) ------------------------------------\n",
      "Max loss: 0.44755885004997253\n",
      "Min loss: 0.3916301727294922\n",
      "Mean loss: 0.4056820919116338\n",
      "Std loss: 0.01982350927409668\n",
      "Total Loss: 2.434092551469803\n",
      "------------------------------------ epoch 074 (438 steps) ------------------------------------\n",
      "Max loss: 0.4204876124858856\n",
      "Min loss: 0.38815343379974365\n",
      "Mean loss: 0.4058504104614258\n",
      "Std loss: 0.012875462355462077\n",
      "Total Loss: 2.4351024627685547\n",
      "------------------------------------ epoch 075 (444 steps) ------------------------------------\n",
      "Max loss: 0.4404153823852539\n",
      "Min loss: 0.38623449206352234\n",
      "Mean loss: 0.4039386063814163\n",
      "Std loss: 0.01808015728346051\n",
      "Total Loss: 2.423631638288498\n",
      "------------------------------------ epoch 076 (450 steps) ------------------------------------\n",
      "Max loss: 0.4234030842781067\n",
      "Min loss: 0.392002671957016\n",
      "Mean loss: 0.40544896324475604\n",
      "Std loss: 0.00968289482369416\n",
      "Total Loss: 2.4326937794685364\n",
      "------------------------------------ epoch 077 (456 steps) ------------------------------------\n",
      "Max loss: 0.4206508994102478\n",
      "Min loss: 0.3819217085838318\n",
      "Mean loss: 0.4043505787849426\n",
      "Std loss: 0.015157809213489777\n",
      "Total Loss: 2.4261034727096558\n",
      "------------------------------------ epoch 078 (462 steps) ------------------------------------\n",
      "Max loss: 0.4109664559364319\n",
      "Min loss: 0.38359534740448\n",
      "Mean loss: 0.39471643169720966\n",
      "Std loss: 0.008764131120869443\n",
      "Total Loss: 2.368298590183258\n",
      "------------------------------------ epoch 079 (468 steps) ------------------------------------\n",
      "Max loss: 0.41126999258995056\n",
      "Min loss: 0.3816244602203369\n",
      "Mean loss: 0.39325230320294696\n",
      "Std loss: 0.009341672139916603\n",
      "Total Loss: 2.359513819217682\n",
      "------------------------------------ epoch 080 (474 steps) ------------------------------------\n",
      "Max loss: 0.433266818523407\n",
      "Min loss: 0.37995609641075134\n",
      "Mean loss: 0.3994688391685486\n",
      "Std loss: 0.0168440446929667\n",
      "Total Loss: 2.3968130350112915\n",
      "------------------------------------ epoch 081 (480 steps) ------------------------------------\n",
      "Max loss: 0.40981656312942505\n",
      "Min loss: 0.37787172198295593\n",
      "Mean loss: 0.3939954737822215\n",
      "Std loss: 0.009907608001184356\n",
      "Total Loss: 2.363972842693329\n",
      "------------------------------------ epoch 082 (486 steps) ------------------------------------\n",
      "Max loss: 0.4057541489601135\n",
      "Min loss: 0.37735089659690857\n",
      "Mean loss: 0.3934115419785182\n",
      "Std loss: 0.010674378958339135\n",
      "Total Loss: 2.360469251871109\n",
      "------------------------------------ epoch 083 (492 steps) ------------------------------------\n",
      "Max loss: 0.4111735224723816\n",
      "Min loss: 0.3769378662109375\n",
      "Mean loss: 0.3906979461510976\n",
      "Std loss: 0.010765744952387456\n",
      "Total Loss: 2.3441876769065857\n",
      "------------------------------------ epoch 084 (498 steps) ------------------------------------\n",
      "Max loss: 0.4022277593612671\n",
      "Min loss: 0.3761715292930603\n",
      "Mean loss: 0.38938933114210766\n",
      "Std loss: 0.009364830546377753\n",
      "Total Loss: 2.336335986852646\n",
      "------------------------------------ epoch 085 (504 steps) ------------------------------------\n",
      "Max loss: 0.42672449350357056\n",
      "Min loss: 0.3725915849208832\n",
      "Mean loss: 0.3889017850160599\n",
      "Std loss: 0.018001432560456308\n",
      "Total Loss: 2.3334107100963593\n",
      "------------------------------------ epoch 086 (510 steps) ------------------------------------\n",
      "Max loss: 0.3997792899608612\n",
      "Min loss: 0.37722843885421753\n",
      "Mean loss: 0.38538410266240436\n",
      "Std loss: 0.007752849300562467\n",
      "Total Loss: 2.3123046159744263\n",
      "------------------------------------ epoch 087 (516 steps) ------------------------------------\n",
      "Max loss: 0.41256746649742126\n",
      "Min loss: 0.37701940536499023\n",
      "Mean loss: 0.3887563496828079\n",
      "Std loss: 0.014316939890378062\n",
      "Total Loss: 2.3325380980968475\n",
      "------------------------------------ epoch 088 (522 steps) ------------------------------------\n",
      "Max loss: 0.4174518585205078\n",
      "Min loss: 0.372904896736145\n",
      "Mean loss: 0.3887932350238164\n",
      "Std loss: 0.016144699465493215\n",
      "Total Loss: 2.3327594101428986\n",
      "------------------------------------ epoch 089 (528 steps) ------------------------------------\n",
      "Max loss: 0.38603681325912476\n",
      "Min loss: 0.37473902106285095\n",
      "Mean loss: 0.380877286195755\n",
      "Std loss: 0.004128248086349972\n",
      "Total Loss: 2.28526371717453\n",
      "------------------------------------ epoch 090 (534 steps) ------------------------------------\n",
      "Max loss: 0.40822696685791016\n",
      "Min loss: 0.3776155710220337\n",
      "Mean loss: 0.39363918205102283\n",
      "Std loss: 0.010648812654191424\n",
      "Total Loss: 2.361835092306137\n",
      "------------------------------------ epoch 091 (540 steps) ------------------------------------\n",
      "Max loss: 0.3922765254974365\n",
      "Min loss: 0.37257659435272217\n",
      "Mean loss: 0.3807585487763087\n",
      "Std loss: 0.00626399795714816\n",
      "Total Loss: 2.284551292657852\n",
      "------------------------------------ epoch 092 (546 steps) ------------------------------------\n",
      "Max loss: 0.4105980396270752\n",
      "Min loss: 0.3761889338493347\n",
      "Mean loss: 0.3880893389383952\n",
      "Std loss: 0.010666532737393417\n",
      "Total Loss: 2.328536033630371\n",
      "------------------------------------ epoch 093 (552 steps) ------------------------------------\n",
      "Max loss: 0.41956472396850586\n",
      "Min loss: 0.3664141297340393\n",
      "Mean loss: 0.393345649043719\n",
      "Std loss: 0.02261046392262147\n",
      "Total Loss: 2.360073894262314\n",
      "------------------------------------ epoch 094 (558 steps) ------------------------------------\n",
      "Max loss: 0.41153907775878906\n",
      "Min loss: 0.37978166341781616\n",
      "Mean loss: 0.39353827635447186\n",
      "Std loss: 0.011226013423311665\n",
      "Total Loss: 2.361229658126831\n",
      "------------------------------------ epoch 095 (564 steps) ------------------------------------\n",
      "Max loss: 0.43856367468833923\n",
      "Min loss: 0.3688964545726776\n",
      "Mean loss: 0.38954878350098926\n",
      "Std loss: 0.023754231213278514\n",
      "Total Loss: 2.3372927010059357\n",
      "------------------------------------ epoch 096 (570 steps) ------------------------------------\n",
      "Max loss: 0.44253700971603394\n",
      "Min loss: 0.37290212512016296\n",
      "Mean loss: 0.3877175698677699\n",
      "Std loss: 0.024756800866664663\n",
      "Total Loss: 2.3263054192066193\n",
      "------------------------------------ epoch 097 (576 steps) ------------------------------------\n",
      "Max loss: 0.4212522506713867\n",
      "Min loss: 0.3741256594657898\n",
      "Mean loss: 0.3939908941586812\n",
      "Std loss: 0.017003379011584566\n",
      "Total Loss: 2.3639453649520874\n",
      "------------------------------------ epoch 098 (582 steps) ------------------------------------\n",
      "Max loss: 0.39944109320640564\n",
      "Min loss: 0.3750770092010498\n",
      "Mean loss: 0.38585784037907916\n",
      "Std loss: 0.009504075310298486\n",
      "Total Loss: 2.315147042274475\n",
      "------------------------------------ epoch 099 (588 steps) ------------------------------------\n",
      "Max loss: 0.40391552448272705\n",
      "Min loss: 0.3705437183380127\n",
      "Mean loss: 0.37909916043281555\n",
      "Std loss: 0.011317807665266987\n",
      "Total Loss: 2.2745949625968933\n",
      "------------------------------------ epoch 100 (594 steps) ------------------------------------\n",
      "Max loss: 0.3906451463699341\n",
      "Min loss: 0.372026652097702\n",
      "Mean loss: 0.383538285891215\n",
      "Std loss: 0.006947636744115154\n",
      "Total Loss: 2.30122971534729\n",
      "------------------------------------ epoch 101 (600 steps) ------------------------------------\n",
      "Max loss: 0.42377957701683044\n",
      "Min loss: 0.3755269944667816\n",
      "Mean loss: 0.40192073583602905\n",
      "Std loss: 0.0160091662247665\n",
      "Total Loss: 2.4115244150161743\n",
      "saved model at ./weights/model_101.pth\n",
      "------------------------------------ epoch 102 (606 steps) ------------------------------------\n",
      "Max loss: 0.41013428568840027\n",
      "Min loss: 0.36813387274742126\n",
      "Mean loss: 0.38593747715155285\n",
      "Std loss: 0.012955394087138293\n",
      "Total Loss: 2.315624862909317\n",
      "------------------------------------ epoch 103 (612 steps) ------------------------------------\n",
      "Max loss: 0.39131149649620056\n",
      "Min loss: 0.3697400987148285\n",
      "Mean loss: 0.3789827922979991\n",
      "Std loss: 0.007195522808721672\n",
      "Total Loss: 2.2738967537879944\n",
      "------------------------------------ epoch 104 (618 steps) ------------------------------------\n",
      "Max loss: 0.40017208456993103\n",
      "Min loss: 0.3701763153076172\n",
      "Mean loss: 0.38004305958747864\n",
      "Std loss: 0.01052663429233778\n",
      "Total Loss: 2.280258357524872\n",
      "------------------------------------ epoch 105 (624 steps) ------------------------------------\n",
      "Max loss: 0.3965296149253845\n",
      "Min loss: 0.36181068420410156\n",
      "Mean loss: 0.3813691834608714\n",
      "Std loss: 0.012908013511588613\n",
      "Total Loss: 2.2882151007652283\n",
      "------------------------------------ epoch 106 (630 steps) ------------------------------------\n",
      "Max loss: 0.37653279304504395\n",
      "Min loss: 0.3597337603569031\n",
      "Mean loss: 0.36989126602808636\n",
      "Std loss: 0.005318860812926124\n",
      "Total Loss: 2.219347596168518\n",
      "------------------------------------ epoch 107 (636 steps) ------------------------------------\n",
      "Max loss: 0.39775344729423523\n",
      "Min loss: 0.36320924758911133\n",
      "Mean loss: 0.37818801403045654\n",
      "Std loss: 0.010736897032769659\n",
      "Total Loss: 2.2691280841827393\n",
      "------------------------------------ epoch 108 (642 steps) ------------------------------------\n",
      "Max loss: 0.3827459216117859\n",
      "Min loss: 0.36413872241973877\n",
      "Mean loss: 0.3732985258102417\n",
      "Std loss: 0.006559681009795261\n",
      "Total Loss: 2.23979115486145\n",
      "------------------------------------ epoch 109 (648 steps) ------------------------------------\n",
      "Max loss: 0.3915509581565857\n",
      "Min loss: 0.36442333459854126\n",
      "Mean loss: 0.37626534700393677\n",
      "Std loss: 0.00848171057531476\n",
      "Total Loss: 2.2575920820236206\n",
      "------------------------------------ epoch 110 (654 steps) ------------------------------------\n",
      "Max loss: 0.37803351879119873\n",
      "Min loss: 0.3630714416503906\n",
      "Mean loss: 0.36823679010073346\n",
      "Std loss: 0.005230190821483625\n",
      "Total Loss: 2.2094207406044006\n",
      "------------------------------------ epoch 111 (660 steps) ------------------------------------\n",
      "Max loss: 0.3747284412384033\n",
      "Min loss: 0.3578053414821625\n",
      "Mean loss: 0.3650227536757787\n",
      "Std loss: 0.006565333883819765\n",
      "Total Loss: 2.1901365220546722\n",
      "------------------------------------ epoch 112 (666 steps) ------------------------------------\n",
      "Max loss: 0.38572508096694946\n",
      "Min loss: 0.359823077917099\n",
      "Mean loss: 0.3733675479888916\n",
      "Std loss: 0.009494865440628913\n",
      "Total Loss: 2.2402052879333496\n",
      "------------------------------------ epoch 113 (672 steps) ------------------------------------\n",
      "Max loss: 0.36285340785980225\n",
      "Min loss: 0.34915977716445923\n",
      "Mean loss: 0.35705122848351795\n",
      "Std loss: 0.005016622583607197\n",
      "Total Loss: 2.142307370901108\n",
      "------------------------------------ epoch 114 (678 steps) ------------------------------------\n",
      "Max loss: 0.39447635412216187\n",
      "Min loss: 0.35435956716537476\n",
      "Mean loss: 0.3637012441953023\n",
      "Std loss: 0.013840032118493679\n",
      "Total Loss: 2.182207465171814\n",
      "------------------------------------ epoch 115 (684 steps) ------------------------------------\n",
      "Max loss: 0.37703239917755127\n",
      "Min loss: 0.3505387306213379\n",
      "Mean loss: 0.3613365441560745\n",
      "Std loss: 0.009280695392834699\n",
      "Total Loss: 2.168019264936447\n",
      "------------------------------------ epoch 116 (690 steps) ------------------------------------\n",
      "Max loss: 0.3895634114742279\n",
      "Min loss: 0.3397704064846039\n",
      "Mean loss: 0.35536834100882214\n",
      "Std loss: 0.016873821044195644\n",
      "Total Loss: 2.1322100460529327\n",
      "------------------------------------ epoch 117 (696 steps) ------------------------------------\n",
      "Max loss: 0.3467272222042084\n",
      "Min loss: 0.3323889374732971\n",
      "Mean loss: 0.3391995032628377\n",
      "Std loss: 0.0049596567351309975\n",
      "Total Loss: 2.0351970195770264\n",
      "------------------------------------ epoch 118 (702 steps) ------------------------------------\n",
      "Max loss: 0.37529420852661133\n",
      "Min loss: 0.33274853229522705\n",
      "Mean loss: 0.3472512513399124\n",
      "Std loss: 0.015304309357534065\n",
      "Total Loss: 2.0835075080394745\n",
      "------------------------------------ epoch 119 (708 steps) ------------------------------------\n",
      "Max loss: 0.37480276823043823\n",
      "Min loss: 0.3274596035480499\n",
      "Mean loss: 0.34547894696394604\n",
      "Std loss: 0.014849261746719591\n",
      "Total Loss: 2.072873681783676\n",
      "------------------------------------ epoch 120 (714 steps) ------------------------------------\n",
      "Max loss: 0.3515698313713074\n",
      "Min loss: 0.31469589471817017\n",
      "Mean loss: 0.3330039829015732\n",
      "Std loss: 0.012849623090751683\n",
      "Total Loss: 1.998023897409439\n",
      "------------------------------------ epoch 121 (720 steps) ------------------------------------\n",
      "Max loss: 0.3527410924434662\n",
      "Min loss: 0.31375089287757874\n",
      "Mean loss: 0.3336349775393804\n",
      "Std loss: 0.012772063668752576\n",
      "Total Loss: 2.0018098652362823\n",
      "------------------------------------ epoch 122 (726 steps) ------------------------------------\n",
      "Max loss: 0.352701872587204\n",
      "Min loss: 0.2931005358695984\n",
      "Mean loss: 0.322140375773112\n",
      "Std loss: 0.021550984068499375\n",
      "Total Loss: 1.9328422546386719\n",
      "------------------------------------ epoch 123 (732 steps) ------------------------------------\n",
      "Max loss: 0.32878589630126953\n",
      "Min loss: 0.29657381772994995\n",
      "Mean loss: 0.3131493330001831\n",
      "Std loss: 0.011519056324357435\n",
      "Total Loss: 1.8788959980010986\n",
      "------------------------------------ epoch 124 (738 steps) ------------------------------------\n",
      "Max loss: 0.3159230649471283\n",
      "Min loss: 0.2898818850517273\n",
      "Mean loss: 0.30405164261658985\n",
      "Std loss: 0.009809840512517442\n",
      "Total Loss: 1.8243098556995392\n",
      "------------------------------------ epoch 125 (744 steps) ------------------------------------\n",
      "Max loss: 0.30652934312820435\n",
      "Min loss: 0.26620471477508545\n",
      "Mean loss: 0.29034710427125293\n",
      "Std loss: 0.01462889849286651\n",
      "Total Loss: 1.7420826256275177\n",
      "------------------------------------ epoch 126 (750 steps) ------------------------------------\n",
      "Max loss: 0.3096851110458374\n",
      "Min loss: 0.2633790671825409\n",
      "Mean loss: 0.28012242913246155\n",
      "Std loss: 0.014404626667096115\n",
      "Total Loss: 1.6807345747947693\n",
      "------------------------------------ epoch 127 (756 steps) ------------------------------------\n",
      "Max loss: 0.3003982901573181\n",
      "Min loss: 0.24488727748394012\n",
      "Mean loss: 0.26772021502256393\n",
      "Std loss: 0.019099659272247833\n",
      "Total Loss: 1.6063212901353836\n",
      "------------------------------------ epoch 128 (762 steps) ------------------------------------\n",
      "Max loss: 0.3027175962924957\n",
      "Min loss: 0.2424505203962326\n",
      "Mean loss: 0.2608928506573041\n",
      "Std loss: 0.020547001634383284\n",
      "Total Loss: 1.5653571039438248\n",
      "------------------------------------ epoch 129 (768 steps) ------------------------------------\n",
      "Max loss: 0.2855823338031769\n",
      "Min loss: 0.23581688106060028\n",
      "Mean loss: 0.25563197831312817\n",
      "Std loss: 0.01701283199796525\n",
      "Total Loss: 1.533791869878769\n",
      "------------------------------------ epoch 130 (774 steps) ------------------------------------\n",
      "Max loss: 0.25485652685165405\n",
      "Min loss: 0.2218581885099411\n",
      "Mean loss: 0.24224762866894403\n",
      "Std loss: 0.014164323625580772\n",
      "Total Loss: 1.4534857720136642\n",
      "------------------------------------ epoch 131 (780 steps) ------------------------------------\n",
      "Max loss: 0.25412505865097046\n",
      "Min loss: 0.23462393879890442\n",
      "Mean loss: 0.24338710804780325\n",
      "Std loss: 0.0065662847209973976\n",
      "Total Loss: 1.4603226482868195\n",
      "------------------------------------ epoch 132 (786 steps) ------------------------------------\n",
      "Max loss: 0.2577439248561859\n",
      "Min loss: 0.2141025960445404\n",
      "Mean loss: 0.22921929508447647\n",
      "Std loss: 0.0173121701042452\n",
      "Total Loss: 1.3753157705068588\n",
      "------------------------------------ epoch 133 (792 steps) ------------------------------------\n",
      "Max loss: 0.2657400965690613\n",
      "Min loss: 0.19533175230026245\n",
      "Mean loss: 0.22759636988242468\n",
      "Std loss: 0.02472624355341868\n",
      "Total Loss: 1.365578219294548\n",
      "------------------------------------ epoch 134 (798 steps) ------------------------------------\n",
      "Max loss: 0.2700124979019165\n",
      "Min loss: 0.20612791180610657\n",
      "Mean loss: 0.22638258834679922\n",
      "Std loss: 0.021195887843240807\n",
      "Total Loss: 1.3582955300807953\n",
      "------------------------------------ epoch 135 (804 steps) ------------------------------------\n",
      "Max loss: 0.23581677675247192\n",
      "Min loss: 0.18096141517162323\n",
      "Mean loss: 0.20855273058017096\n",
      "Std loss: 0.01918918901815214\n",
      "Total Loss: 1.2513163834810257\n",
      "------------------------------------ epoch 136 (810 steps) ------------------------------------\n",
      "Max loss: 0.2131282538175583\n",
      "Min loss: 0.1758849024772644\n",
      "Mean loss: 0.19163562605778375\n",
      "Std loss: 0.013354414491204173\n",
      "Total Loss: 1.1498137563467026\n",
      "------------------------------------ epoch 137 (816 steps) ------------------------------------\n",
      "Max loss: 0.22106370329856873\n",
      "Min loss: 0.1632283329963684\n",
      "Mean loss: 0.17979739854733148\n",
      "Std loss: 0.01943520493508836\n",
      "Total Loss: 1.078784391283989\n",
      "------------------------------------ epoch 138 (822 steps) ------------------------------------\n",
      "Max loss: 0.24911019206047058\n",
      "Min loss: 0.17132829129695892\n",
      "Mean loss: 0.2009094829360644\n",
      "Std loss: 0.031596003108677946\n",
      "Total Loss: 1.2054568976163864\n",
      "------------------------------------ epoch 139 (828 steps) ------------------------------------\n",
      "Max loss: 0.25054940581321716\n",
      "Min loss: 0.1734556406736374\n",
      "Mean loss: 0.1987506424387296\n",
      "Std loss: 0.025067319132391876\n",
      "Total Loss: 1.1925038546323776\n",
      "------------------------------------ epoch 140 (834 steps) ------------------------------------\n",
      "Max loss: 0.20218157768249512\n",
      "Min loss: 0.16568659245967865\n",
      "Mean loss: 0.18448077390591303\n",
      "Std loss: 0.012912156262017412\n",
      "Total Loss: 1.1068846434354782\n",
      "------------------------------------ epoch 141 (840 steps) ------------------------------------\n",
      "Max loss: 0.2103598564863205\n",
      "Min loss: 0.16965150833129883\n",
      "Mean loss: 0.18042343854904175\n",
      "Std loss: 0.014079468364526065\n",
      "Total Loss: 1.0825406312942505\n",
      "------------------------------------ epoch 142 (846 steps) ------------------------------------\n",
      "Max loss: 0.19849161803722382\n",
      "Min loss: 0.15343527495861053\n",
      "Mean loss: 0.17692205558220545\n",
      "Std loss: 0.014223820706279283\n",
      "Total Loss: 1.0615323334932327\n",
      "------------------------------------ epoch 143 (852 steps) ------------------------------------\n",
      "Max loss: 0.1891501396894455\n",
      "Min loss: 0.1401660442352295\n",
      "Mean loss: 0.16745392481486002\n",
      "Std loss: 0.016961623015311962\n",
      "Total Loss: 1.0047235488891602\n",
      "------------------------------------ epoch 144 (858 steps) ------------------------------------\n",
      "Max loss: 0.206763356924057\n",
      "Min loss: 0.14142918586730957\n",
      "Mean loss: 0.15913212796052298\n",
      "Std loss: 0.022000422565720115\n",
      "Total Loss: 0.9547927677631378\n",
      "------------------------------------ epoch 145 (864 steps) ------------------------------------\n",
      "Max loss: 0.17162680625915527\n",
      "Min loss: 0.12681961059570312\n",
      "Mean loss: 0.1475947896639506\n",
      "Std loss: 0.014073046273310462\n",
      "Total Loss: 0.8855687379837036\n",
      "------------------------------------ epoch 146 (870 steps) ------------------------------------\n",
      "Max loss: 0.17886614799499512\n",
      "Min loss: 0.1204877719283104\n",
      "Mean loss: 0.13938542579611143\n",
      "Std loss: 0.019063070097390646\n",
      "Total Loss: 0.8363125547766685\n",
      "------------------------------------ epoch 147 (876 steps) ------------------------------------\n",
      "Max loss: 0.20909743010997772\n",
      "Min loss: 0.10783233493566513\n",
      "Mean loss: 0.15478548283378282\n",
      "Std loss: 0.032224542761444694\n",
      "Total Loss: 0.928712897002697\n",
      "------------------------------------ epoch 148 (882 steps) ------------------------------------\n",
      "Max loss: 0.158744215965271\n",
      "Min loss: 0.1009703278541565\n",
      "Mean loss: 0.12901745984951654\n",
      "Std loss: 0.018728104348329593\n",
      "Total Loss: 0.7741047590970993\n",
      "------------------------------------ epoch 149 (888 steps) ------------------------------------\n",
      "Max loss: 0.14625763893127441\n",
      "Min loss: 0.10038469731807709\n",
      "Mean loss: 0.12281243254741032\n",
      "Std loss: 0.016617836324558957\n",
      "Total Loss: 0.736874595284462\n",
      "------------------------------------ epoch 150 (894 steps) ------------------------------------\n",
      "Max loss: 0.15476155281066895\n",
      "Min loss: 0.09180088341236115\n",
      "Mean loss: 0.1365527535478274\n",
      "Std loss: 0.02166411970472653\n",
      "Total Loss: 0.8193165212869644\n",
      "------------------------------------ epoch 151 (900 steps) ------------------------------------\n",
      "Max loss: 0.1382133513689041\n",
      "Min loss: 0.08801780641078949\n",
      "Mean loss: 0.12062442675232887\n",
      "Std loss: 0.01645542759007238\n",
      "Total Loss: 0.7237465605139732\n",
      "------------------------------------ epoch 152 (906 steps) ------------------------------------\n",
      "Max loss: 0.1614837795495987\n",
      "Min loss: 0.09227167814970016\n",
      "Mean loss: 0.12415544067819913\n",
      "Std loss: 0.024776386905891762\n",
      "Total Loss: 0.7449326440691948\n",
      "------------------------------------ epoch 153 (912 steps) ------------------------------------\n",
      "Max loss: 0.12011447548866272\n",
      "Min loss: 0.09743531793355942\n",
      "Mean loss: 0.11141757170359294\n",
      "Std loss: 0.007480433788746471\n",
      "Total Loss: 0.6685054302215576\n",
      "------------------------------------ epoch 154 (918 steps) ------------------------------------\n",
      "Max loss: 0.12382113933563232\n",
      "Min loss: 0.08053287863731384\n",
      "Mean loss: 0.10285585125287373\n",
      "Std loss: 0.014446341264225917\n",
      "Total Loss: 0.6171351075172424\n",
      "------------------------------------ epoch 155 (924 steps) ------------------------------------\n",
      "Max loss: 0.12054318189620972\n",
      "Min loss: 0.07930271327495575\n",
      "Mean loss: 0.09567721684773763\n",
      "Std loss: 0.013433955538064037\n",
      "Total Loss: 0.5740633010864258\n",
      "------------------------------------ epoch 156 (930 steps) ------------------------------------\n",
      "Max loss: 0.13624045252799988\n",
      "Min loss: 0.0873064398765564\n",
      "Mean loss: 0.10479918246467908\n",
      "Std loss: 0.016598261819847283\n",
      "Total Loss: 0.6287950947880745\n",
      "------------------------------------ epoch 157 (936 steps) ------------------------------------\n",
      "Max loss: 0.11131738126277924\n",
      "Min loss: 0.07848842442035675\n",
      "Mean loss: 0.09685677289962769\n",
      "Std loss: 0.01162617939872964\n",
      "Total Loss: 0.5811406373977661\n",
      "------------------------------------ epoch 158 (942 steps) ------------------------------------\n",
      "Max loss: 0.1394559144973755\n",
      "Min loss: 0.10562354326248169\n",
      "Mean loss: 0.11803202703595161\n",
      "Std loss: 0.012361260869781022\n",
      "Total Loss: 0.7081921622157097\n",
      "------------------------------------ epoch 159 (948 steps) ------------------------------------\n",
      "Max loss: 0.13363081216812134\n",
      "Min loss: 0.0724838525056839\n",
      "Mean loss: 0.10089713955918948\n",
      "Std loss: 0.021603193579061597\n",
      "Total Loss: 0.6053828373551369\n",
      "------------------------------------ epoch 160 (954 steps) ------------------------------------\n",
      "Max loss: 0.11523690074682236\n",
      "Min loss: 0.07786489278078079\n",
      "Mean loss: 0.09448931862910588\n",
      "Std loss: 0.01374419911278196\n",
      "Total Loss: 0.5669359117746353\n",
      "------------------------------------ epoch 161 (960 steps) ------------------------------------\n",
      "Max loss: 0.14931371808052063\n",
      "Min loss: 0.07501378655433655\n",
      "Mean loss: 0.098672720293204\n",
      "Std loss: 0.028000848292689243\n",
      "Total Loss: 0.5920363217592239\n",
      "------------------------------------ epoch 162 (966 steps) ------------------------------------\n",
      "Max loss: 0.1469079852104187\n",
      "Min loss: 0.07727482914924622\n",
      "Mean loss: 0.1098462591568629\n",
      "Std loss: 0.02395476346600039\n",
      "Total Loss: 0.6590775549411774\n",
      "------------------------------------ epoch 163 (972 steps) ------------------------------------\n",
      "Max loss: 0.10298492759466171\n",
      "Min loss: 0.07848873734474182\n",
      "Mean loss: 0.08864591891566913\n",
      "Std loss: 0.008249877282236655\n",
      "Total Loss: 0.5318755134940147\n",
      "------------------------------------ epoch 164 (978 steps) ------------------------------------\n",
      "Max loss: 0.18071836233139038\n",
      "Min loss: 0.07847557961940765\n",
      "Mean loss: 0.10726784418026607\n",
      "Std loss: 0.0392230172754697\n",
      "Total Loss: 0.6436070650815964\n",
      "------------------------------------ epoch 165 (984 steps) ------------------------------------\n",
      "Max loss: 0.12706002593040466\n",
      "Min loss: 0.07461080700159073\n",
      "Mean loss: 0.09609760095675786\n",
      "Std loss: 0.019401691708051238\n",
      "Total Loss: 0.5765856057405472\n",
      "------------------------------------ epoch 166 (990 steps) ------------------------------------\n",
      "Max loss: 0.10543650388717651\n",
      "Min loss: 0.07290694117546082\n",
      "Mean loss: 0.09014262010653813\n",
      "Std loss: 0.012495350840103817\n",
      "Total Loss: 0.5408557206392288\n",
      "------------------------------------ epoch 167 (996 steps) ------------------------------------\n",
      "Max loss: 0.11143536865711212\n",
      "Min loss: 0.0765993595123291\n",
      "Mean loss: 0.09151492764552434\n",
      "Std loss: 0.013818469843274193\n",
      "Total Loss: 0.5490895658731461\n",
      "------------------------------------ epoch 168 (1002 steps) ------------------------------------\n",
      "Max loss: 0.13582518696784973\n",
      "Min loss: 0.08902256190776825\n",
      "Mean loss: 0.10517925148208936\n",
      "Std loss: 0.015662347841344005\n",
      "Total Loss: 0.6310755088925362\n",
      "------------------------------------ epoch 169 (1008 steps) ------------------------------------\n",
      "Max loss: 0.1495080590248108\n",
      "Min loss: 0.07001620531082153\n",
      "Mean loss: 0.10982031375169754\n",
      "Std loss: 0.025259138190089517\n",
      "Total Loss: 0.6589218825101852\n",
      "------------------------------------ epoch 170 (1014 steps) ------------------------------------\n",
      "Max loss: 0.09258858859539032\n",
      "Min loss: 0.07505380362272263\n",
      "Mean loss: 0.08291130761305492\n",
      "Std loss: 0.005295436883023181\n",
      "Total Loss: 0.49746784567832947\n",
      "------------------------------------ epoch 171 (1020 steps) ------------------------------------\n",
      "Max loss: 0.14438092708587646\n",
      "Min loss: 0.057680726051330566\n",
      "Mean loss: 0.10166446616252263\n",
      "Std loss: 0.02750946114614578\n",
      "Total Loss: 0.6099867969751358\n",
      "------------------------------------ epoch 172 (1026 steps) ------------------------------------\n",
      "Max loss: 0.10273642838001251\n",
      "Min loss: 0.07103413343429565\n",
      "Mean loss: 0.08525549247860909\n",
      "Std loss: 0.010513374548254273\n",
      "Total Loss: 0.5115329548716545\n",
      "------------------------------------ epoch 173 (1032 steps) ------------------------------------\n",
      "Max loss: 0.10817401111125946\n",
      "Min loss: 0.06623949855566025\n",
      "Mean loss: 0.08214439079165459\n",
      "Std loss: 0.014319767761454766\n",
      "Total Loss: 0.4928663447499275\n",
      "------------------------------------ epoch 174 (1038 steps) ------------------------------------\n",
      "Max loss: 0.1262371838092804\n",
      "Min loss: 0.08371315896511078\n",
      "Mean loss: 0.10775941858688991\n",
      "Std loss: 0.015178998523077242\n",
      "Total Loss: 0.6465565115213394\n",
      "------------------------------------ epoch 175 (1044 steps) ------------------------------------\n",
      "Max loss: 0.16149640083312988\n",
      "Min loss: 0.06878666579723358\n",
      "Mean loss: 0.09406031544009845\n",
      "Std loss: 0.030704961603733847\n",
      "Total Loss: 0.5643618926405907\n",
      "------------------------------------ epoch 176 (1050 steps) ------------------------------------\n",
      "Max loss: 0.0938524454832077\n",
      "Min loss: 0.06708012521266937\n",
      "Mean loss: 0.0835542380809784\n",
      "Std loss: 0.009577586019517822\n",
      "Total Loss: 0.5013254284858704\n",
      "------------------------------------ epoch 177 (1056 steps) ------------------------------------\n",
      "Max loss: 0.1324402540922165\n",
      "Min loss: 0.0830061286687851\n",
      "Mean loss: 0.11139840260148048\n",
      "Std loss: 0.018529410602994536\n",
      "Total Loss: 0.6683904156088829\n",
      "------------------------------------ epoch 178 (1062 steps) ------------------------------------\n",
      "Max loss: 0.141321063041687\n",
      "Min loss: 0.06578730046749115\n",
      "Mean loss: 0.09419864291946094\n",
      "Std loss: 0.022947480196262374\n",
      "Total Loss: 0.5651918575167656\n",
      "------------------------------------ epoch 179 (1068 steps) ------------------------------------\n",
      "Max loss: 0.12535510957241058\n",
      "Min loss: 0.0693783313035965\n",
      "Mean loss: 0.08102652803063393\n",
      "Std loss: 0.01993555025720571\n",
      "Total Loss: 0.48615916818380356\n",
      "------------------------------------ epoch 180 (1074 steps) ------------------------------------\n",
      "Max loss: 0.1146882027387619\n",
      "Min loss: 0.06586293876171112\n",
      "Mean loss: 0.0930508350332578\n",
      "Std loss: 0.01670566154216356\n",
      "Total Loss: 0.5583050101995468\n",
      "------------------------------------ epoch 181 (1080 steps) ------------------------------------\n",
      "Max loss: 0.11147534847259521\n",
      "Min loss: 0.06352704018354416\n",
      "Mean loss: 0.08780264978607495\n",
      "Std loss: 0.01866169469007482\n",
      "Total Loss: 0.5268158987164497\n",
      "------------------------------------ epoch 182 (1086 steps) ------------------------------------\n",
      "Max loss: 0.14957749843597412\n",
      "Min loss: 0.07313374429941177\n",
      "Mean loss: 0.10004905114571254\n",
      "Std loss: 0.02385747187220879\n",
      "Total Loss: 0.6002943068742752\n",
      "------------------------------------ epoch 183 (1092 steps) ------------------------------------\n",
      "Max loss: 0.11705907434225082\n",
      "Min loss: 0.06762033700942993\n",
      "Mean loss: 0.08191584423184395\n",
      "Std loss: 0.016280355489326832\n",
      "Total Loss: 0.4914950653910637\n",
      "------------------------------------ epoch 184 (1098 steps) ------------------------------------\n",
      "Max loss: 0.11185767501592636\n",
      "Min loss: 0.056079745292663574\n",
      "Mean loss: 0.08560691773891449\n",
      "Std loss: 0.02013695006218135\n",
      "Total Loss: 0.5136415064334869\n",
      "------------------------------------ epoch 185 (1104 steps) ------------------------------------\n",
      "Max loss: 0.16452141106128693\n",
      "Min loss: 0.05773717164993286\n",
      "Mean loss: 0.08965346962213516\n",
      "Std loss: 0.03551543131211565\n",
      "Total Loss: 0.537920817732811\n",
      "------------------------------------ epoch 186 (1110 steps) ------------------------------------\n",
      "Max loss: 0.0916273221373558\n",
      "Min loss: 0.05989934131503105\n",
      "Mean loss: 0.07567334733903408\n",
      "Std loss: 0.012284187415906929\n",
      "Total Loss: 0.4540400840342045\n",
      "------------------------------------ epoch 187 (1116 steps) ------------------------------------\n",
      "Max loss: 0.1026015505194664\n",
      "Min loss: 0.05454402416944504\n",
      "Mean loss: 0.07640019431710243\n",
      "Std loss: 0.015918892444579887\n",
      "Total Loss: 0.4584011659026146\n",
      "------------------------------------ epoch 188 (1122 steps) ------------------------------------\n",
      "Max loss: 0.13143068552017212\n",
      "Min loss: 0.05791091546416283\n",
      "Mean loss: 0.0902984378238519\n",
      "Std loss: 0.027846961733579503\n",
      "Total Loss: 0.5417906269431114\n",
      "------------------------------------ epoch 189 (1128 steps) ------------------------------------\n",
      "Max loss: 0.17607370018959045\n",
      "Min loss: 0.0730176791548729\n",
      "Mean loss: 0.11310151343544324\n",
      "Std loss: 0.036597234093833204\n",
      "Total Loss: 0.6786090806126595\n",
      "------------------------------------ epoch 190 (1134 steps) ------------------------------------\n",
      "Max loss: 0.1470164656639099\n",
      "Min loss: 0.06447836756706238\n",
      "Mean loss: 0.10852580020825069\n",
      "Std loss: 0.026959970776952995\n",
      "Total Loss: 0.6511548012495041\n",
      "------------------------------------ epoch 191 (1140 steps) ------------------------------------\n",
      "Max loss: 0.1300916075706482\n",
      "Min loss: 0.060674529522657394\n",
      "Mean loss: 0.09115943250556786\n",
      "Std loss: 0.021271276250124364\n",
      "Total Loss: 0.5469565950334072\n",
      "------------------------------------ epoch 192 (1146 steps) ------------------------------------\n",
      "Max loss: 0.10561495274305344\n",
      "Min loss: 0.05191488191485405\n",
      "Mean loss: 0.07870684005320072\n",
      "Std loss: 0.019513620985204365\n",
      "Total Loss: 0.47224104031920433\n",
      "------------------------------------ epoch 193 (1152 steps) ------------------------------------\n",
      "Max loss: 0.11017338931560516\n",
      "Min loss: 0.06492449343204498\n",
      "Mean loss: 0.08037662257750829\n",
      "Std loss: 0.01577494403230747\n",
      "Total Loss: 0.48225973546504974\n",
      "------------------------------------ epoch 194 (1158 steps) ------------------------------------\n",
      "Max loss: 0.12947481870651245\n",
      "Min loss: 0.061304401606321335\n",
      "Mean loss: 0.09573412997027238\n",
      "Std loss: 0.02256518644402212\n",
      "Total Loss: 0.5744047798216343\n",
      "------------------------------------ epoch 195 (1164 steps) ------------------------------------\n",
      "Max loss: 0.11875712126493454\n",
      "Min loss: 0.05872265249490738\n",
      "Mean loss: 0.08346416180332501\n",
      "Std loss: 0.019757144478891694\n",
      "Total Loss: 0.5007849708199501\n",
      "------------------------------------ epoch 196 (1170 steps) ------------------------------------\n",
      "Max loss: 0.09864585101604462\n",
      "Min loss: 0.0628218948841095\n",
      "Mean loss: 0.08020991583665212\n",
      "Std loss: 0.013608070227577904\n",
      "Total Loss: 0.4812594950199127\n",
      "------------------------------------ epoch 197 (1176 steps) ------------------------------------\n",
      "Max loss: 0.10039857029914856\n",
      "Min loss: 0.06651154160499573\n",
      "Mean loss: 0.07732968653241794\n",
      "Std loss: 0.011962750299368904\n",
      "Total Loss: 0.4639781191945076\n",
      "------------------------------------ epoch 198 (1182 steps) ------------------------------------\n",
      "Max loss: 0.0881974920630455\n",
      "Min loss: 0.06575601547956467\n",
      "Mean loss: 0.07455137992898624\n",
      "Std loss: 0.00738097986090231\n",
      "Total Loss: 0.4473082795739174\n",
      "------------------------------------ epoch 199 (1188 steps) ------------------------------------\n",
      "Max loss: 0.0968998372554779\n",
      "Min loss: 0.05738039314746857\n",
      "Mean loss: 0.07938336084286372\n",
      "Std loss: 0.012704315295389491\n",
      "Total Loss: 0.4763001650571823\n",
      "------------------------------------ epoch 200 (1194 steps) ------------------------------------\n",
      "Max loss: 0.1494354009628296\n",
      "Min loss: 0.050094954669475555\n",
      "Mean loss: 0.09164145216345787\n",
      "Std loss: 0.03412582905567952\n",
      "Total Loss: 0.5498487129807472\n",
      "------------------------------------ epoch 201 (1200 steps) ------------------------------------\n",
      "Max loss: 0.14451445639133453\n",
      "Min loss: 0.058146845549345016\n",
      "Mean loss: 0.08537999354302883\n",
      "Std loss: 0.031120196457882363\n",
      "Total Loss: 0.512279961258173\n",
      "saved model at ./weights/model_201.pth\n",
      "------------------------------------ epoch 202 (1206 steps) ------------------------------------\n",
      "Max loss: 0.08818332850933075\n",
      "Min loss: 0.05006420612335205\n",
      "Mean loss: 0.0742734968662262\n",
      "Std loss: 0.01202784878098095\n",
      "Total Loss: 0.4456409811973572\n",
      "------------------------------------ epoch 203 (1212 steps) ------------------------------------\n",
      "Max loss: 0.10693001002073288\n",
      "Min loss: 0.05459681153297424\n",
      "Mean loss: 0.07364959580202897\n",
      "Std loss: 0.02063907754448789\n",
      "Total Loss: 0.44189757481217384\n",
      "------------------------------------ epoch 204 (1218 steps) ------------------------------------\n",
      "Max loss: 0.13007287681102753\n",
      "Min loss: 0.04645867273211479\n",
      "Mean loss: 0.08823791705071926\n",
      "Std loss: 0.033304748277921965\n",
      "Total Loss: 0.5294275023043156\n",
      "------------------------------------ epoch 205 (1224 steps) ------------------------------------\n",
      "Max loss: 0.14388246834278107\n",
      "Min loss: 0.05115145444869995\n",
      "Mean loss: 0.07893534501393636\n",
      "Std loss: 0.0302703276088959\n",
      "Total Loss: 0.47361207008361816\n",
      "------------------------------------ epoch 206 (1230 steps) ------------------------------------\n",
      "Max loss: 0.08309277892112732\n",
      "Min loss: 0.056017857044935226\n",
      "Mean loss: 0.06573653655747573\n",
      "Std loss: 0.009209378241601819\n",
      "Total Loss: 0.39441921934485435\n",
      "------------------------------------ epoch 207 (1236 steps) ------------------------------------\n",
      "Max loss: 0.08293082565069199\n",
      "Min loss: 0.06381358206272125\n",
      "Mean loss: 0.07225121806065242\n",
      "Std loss: 0.006096792390055253\n",
      "Total Loss: 0.4335073083639145\n",
      "------------------------------------ epoch 208 (1242 steps) ------------------------------------\n",
      "Max loss: 0.1142679899930954\n",
      "Min loss: 0.05584125220775604\n",
      "Mean loss: 0.07549817735950153\n",
      "Std loss: 0.0203601610004096\n",
      "Total Loss: 0.4529890641570091\n",
      "------------------------------------ epoch 209 (1248 steps) ------------------------------------\n",
      "Max loss: 0.11356433480978012\n",
      "Min loss: 0.06323840469121933\n",
      "Mean loss: 0.07794324432810147\n",
      "Std loss: 0.017093317493023175\n",
      "Total Loss: 0.46765946596860886\n",
      "------------------------------------ epoch 210 (1254 steps) ------------------------------------\n",
      "Max loss: 0.09358503669500351\n",
      "Min loss: 0.0708700567483902\n",
      "Mean loss: 0.0824452315767606\n",
      "Std loss: 0.0093111808592495\n",
      "Total Loss: 0.49467138946056366\n",
      "------------------------------------ epoch 211 (1260 steps) ------------------------------------\n",
      "Max loss: 0.10617315024137497\n",
      "Min loss: 0.05744726583361626\n",
      "Mean loss: 0.07642461359500885\n",
      "Std loss: 0.0176431465316684\n",
      "Total Loss: 0.4585476815700531\n",
      "------------------------------------ epoch 212 (1266 steps) ------------------------------------\n",
      "Max loss: 0.1164209172129631\n",
      "Min loss: 0.0675220787525177\n",
      "Mean loss: 0.08561602731545766\n",
      "Std loss: 0.01626012726487772\n",
      "Total Loss: 0.513696163892746\n",
      "------------------------------------ epoch 213 (1272 steps) ------------------------------------\n",
      "Max loss: 0.1275467872619629\n",
      "Min loss: 0.04824722930788994\n",
      "Mean loss: 0.0859376098960638\n",
      "Std loss: 0.025533597928872373\n",
      "Total Loss: 0.5156256593763828\n",
      "------------------------------------ epoch 214 (1278 steps) ------------------------------------\n",
      "Max loss: 0.08423081040382385\n",
      "Min loss: 0.046717070043087006\n",
      "Mean loss: 0.06193994854887327\n",
      "Std loss: 0.012566082827779\n",
      "Total Loss: 0.3716396912932396\n",
      "------------------------------------ epoch 215 (1284 steps) ------------------------------------\n",
      "Max loss: 0.08835854381322861\n",
      "Min loss: 0.04331299662590027\n",
      "Mean loss: 0.07397476583719254\n",
      "Std loss: 0.015026992971217342\n",
      "Total Loss: 0.4438485950231552\n",
      "------------------------------------ epoch 216 (1290 steps) ------------------------------------\n",
      "Max loss: 0.0933404266834259\n",
      "Min loss: 0.0575866661965847\n",
      "Mean loss: 0.07222930528223515\n",
      "Std loss: 0.011131770073673396\n",
      "Total Loss: 0.4333758316934109\n",
      "------------------------------------ epoch 217 (1296 steps) ------------------------------------\n",
      "Max loss: 0.09687237441539764\n",
      "Min loss: 0.062469761818647385\n",
      "Mean loss: 0.08037899248301983\n",
      "Std loss: 0.013461686961220137\n",
      "Total Loss: 0.482273954898119\n",
      "------------------------------------ epoch 218 (1302 steps) ------------------------------------\n",
      "Max loss: 0.09403704851865768\n",
      "Min loss: 0.049730148166418076\n",
      "Mean loss: 0.06850200270613034\n",
      "Std loss: 0.015248267755707938\n",
      "Total Loss: 0.4110120162367821\n",
      "------------------------------------ epoch 219 (1308 steps) ------------------------------------\n",
      "Max loss: 0.08871837705373764\n",
      "Min loss: 0.05322747677564621\n",
      "Mean loss: 0.0757020798822244\n",
      "Std loss: 0.015654282256027004\n",
      "Total Loss: 0.4542124792933464\n",
      "------------------------------------ epoch 220 (1314 steps) ------------------------------------\n",
      "Max loss: 0.10116845369338989\n",
      "Min loss: 0.056012384593486786\n",
      "Mean loss: 0.07436452185114224\n",
      "Std loss: 0.015620184131920562\n",
      "Total Loss: 0.4461871311068535\n",
      "------------------------------------ epoch 221 (1320 steps) ------------------------------------\n",
      "Max loss: 0.13546288013458252\n",
      "Min loss: 0.05575805902481079\n",
      "Mean loss: 0.09790565942724545\n",
      "Std loss: 0.027253484212960425\n",
      "Total Loss: 0.5874339565634727\n",
      "------------------------------------ epoch 222 (1326 steps) ------------------------------------\n",
      "Max loss: 0.1238318532705307\n",
      "Min loss: 0.05561302229762077\n",
      "Mean loss: 0.08420555231471856\n",
      "Std loss: 0.022703810860743977\n",
      "Total Loss: 0.5052333138883114\n",
      "------------------------------------ epoch 223 (1332 steps) ------------------------------------\n",
      "Max loss: 0.09119793027639389\n",
      "Min loss: 0.054386138916015625\n",
      "Mean loss: 0.07995180909832318\n",
      "Std loss: 0.012092813697744354\n",
      "Total Loss: 0.4797108545899391\n",
      "------------------------------------ epoch 224 (1338 steps) ------------------------------------\n",
      "Max loss: 0.08877243846654892\n",
      "Min loss: 0.04599557816982269\n",
      "Mean loss: 0.06605163713296254\n",
      "Std loss: 0.013865936258484224\n",
      "Total Loss: 0.39630982279777527\n",
      "------------------------------------ epoch 225 (1344 steps) ------------------------------------\n",
      "Max loss: 0.09603603929281235\n",
      "Min loss: 0.061258815228939056\n",
      "Mean loss: 0.0764705240726471\n",
      "Std loss: 0.013677620303354576\n",
      "Total Loss: 0.45882314443588257\n",
      "------------------------------------ epoch 226 (1350 steps) ------------------------------------\n",
      "Max loss: 0.11346881091594696\n",
      "Min loss: 0.044046007096767426\n",
      "Mean loss: 0.07063451843957107\n",
      "Std loss: 0.021025662913132535\n",
      "Total Loss: 0.4238071106374264\n",
      "------------------------------------ epoch 227 (1356 steps) ------------------------------------\n",
      "Max loss: 0.11161216348409653\n",
      "Min loss: 0.04723173379898071\n",
      "Mean loss: 0.07080969959497452\n",
      "Std loss: 0.020599158250235346\n",
      "Total Loss: 0.4248581975698471\n",
      "------------------------------------ epoch 228 (1362 steps) ------------------------------------\n",
      "Max loss: 0.11880811303853989\n",
      "Min loss: 0.05596452206373215\n",
      "Mean loss: 0.0831114761531353\n",
      "Std loss: 0.021134796579895854\n",
      "Total Loss: 0.4986688569188118\n",
      "------------------------------------ epoch 229 (1368 steps) ------------------------------------\n",
      "Max loss: 0.07230714708566666\n",
      "Min loss: 0.04727737605571747\n",
      "Mean loss: 0.060021368165810905\n",
      "Std loss: 0.008424727901936777\n",
      "Total Loss: 0.3601282089948654\n",
      "------------------------------------ epoch 230 (1374 steps) ------------------------------------\n",
      "Max loss: 0.14911824464797974\n",
      "Min loss: 0.05350164324045181\n",
      "Mean loss: 0.08271016925573349\n",
      "Std loss: 0.03163939209617894\n",
      "Total Loss: 0.49626101553440094\n",
      "------------------------------------ epoch 231 (1380 steps) ------------------------------------\n",
      "Max loss: 0.08867071568965912\n",
      "Min loss: 0.05001796782016754\n",
      "Mean loss: 0.07362808162967364\n",
      "Std loss: 0.016364019531962752\n",
      "Total Loss: 0.44176848977804184\n",
      "------------------------------------ epoch 232 (1386 steps) ------------------------------------\n",
      "Max loss: 0.13742151856422424\n",
      "Min loss: 0.05399877950549126\n",
      "Mean loss: 0.07932175820072492\n",
      "Std loss: 0.028258272483501108\n",
      "Total Loss: 0.4759305492043495\n",
      "------------------------------------ epoch 233 (1392 steps) ------------------------------------\n",
      "Max loss: 0.10169872641563416\n",
      "Min loss: 0.0665731132030487\n",
      "Mean loss: 0.08150922258694966\n",
      "Std loss: 0.01387416272142953\n",
      "Total Loss: 0.489055335521698\n",
      "------------------------------------ epoch 234 (1398 steps) ------------------------------------\n",
      "Max loss: 0.11895128339529037\n",
      "Min loss: 0.04729596525430679\n",
      "Mean loss: 0.0781975748638312\n",
      "Std loss: 0.0218483471141657\n",
      "Total Loss: 0.4691854491829872\n",
      "------------------------------------ epoch 235 (1404 steps) ------------------------------------\n",
      "Max loss: 0.1173657774925232\n",
      "Min loss: 0.0688701644539833\n",
      "Mean loss: 0.08447808648149173\n",
      "Std loss: 0.01633440520668235\n",
      "Total Loss: 0.5068685188889503\n",
      "------------------------------------ epoch 236 (1410 steps) ------------------------------------\n",
      "Max loss: 0.11312159895896912\n",
      "Min loss: 0.05180651694536209\n",
      "Mean loss: 0.07641080829004447\n",
      "Std loss: 0.019025228439947472\n",
      "Total Loss: 0.4584648497402668\n",
      "------------------------------------ epoch 237 (1416 steps) ------------------------------------\n",
      "Max loss: 0.12085773795843124\n",
      "Min loss: 0.059330519288778305\n",
      "Mean loss: 0.08731481619179249\n",
      "Std loss: 0.020997908390312268\n",
      "Total Loss: 0.5238888971507549\n",
      "------------------------------------ epoch 238 (1422 steps) ------------------------------------\n",
      "Max loss: 0.07551237940788269\n",
      "Min loss: 0.056388936936855316\n",
      "Mean loss: 0.06529206906755765\n",
      "Std loss: 0.007772102449885094\n",
      "Total Loss: 0.3917524144053459\n",
      "------------------------------------ epoch 239 (1428 steps) ------------------------------------\n",
      "Max loss: 0.1492370069026947\n",
      "Min loss: 0.04099138826131821\n",
      "Mean loss: 0.08343040694793065\n",
      "Std loss: 0.034635427732147915\n",
      "Total Loss: 0.5005824416875839\n",
      "------------------------------------ epoch 240 (1434 steps) ------------------------------------\n",
      "Max loss: 0.101803719997406\n",
      "Min loss: 0.05050129443407059\n",
      "Mean loss: 0.06689334474503994\n",
      "Std loss: 0.017725954369204242\n",
      "Total Loss: 0.40136006847023964\n",
      "------------------------------------ epoch 241 (1440 steps) ------------------------------------\n",
      "Max loss: 0.11862989515066147\n",
      "Min loss: 0.052838366478681564\n",
      "Mean loss: 0.07384329165021579\n",
      "Std loss: 0.022760252398442673\n",
      "Total Loss: 0.4430597499012947\n",
      "------------------------------------ epoch 242 (1446 steps) ------------------------------------\n",
      "Max loss: 0.09750647842884064\n",
      "Min loss: 0.06049254164099693\n",
      "Mean loss: 0.07070428940157096\n",
      "Std loss: 0.01220002234067078\n",
      "Total Loss: 0.42422573640942574\n",
      "------------------------------------ epoch 243 (1452 steps) ------------------------------------\n",
      "Max loss: 0.08219562470912933\n",
      "Min loss: 0.05056251212954521\n",
      "Mean loss: 0.06497919807831447\n",
      "Std loss: 0.011536041113658661\n",
      "Total Loss: 0.3898751884698868\n",
      "------------------------------------ epoch 244 (1458 steps) ------------------------------------\n",
      "Max loss: 0.11623403429985046\n",
      "Min loss: 0.04178152233362198\n",
      "Mean loss: 0.0691315575192372\n",
      "Std loss: 0.026571239328875177\n",
      "Total Loss: 0.4147893451154232\n",
      "------------------------------------ epoch 245 (1464 steps) ------------------------------------\n",
      "Max loss: 0.0946970209479332\n",
      "Min loss: 0.045716967433691025\n",
      "Mean loss: 0.07157323757807414\n",
      "Std loss: 0.017509997954558907\n",
      "Total Loss: 0.4294394254684448\n",
      "------------------------------------ epoch 246 (1470 steps) ------------------------------------\n",
      "Max loss: 0.10649313032627106\n",
      "Min loss: 0.044285692274570465\n",
      "Mean loss: 0.06468686833977699\n",
      "Std loss: 0.01985658291271973\n",
      "Total Loss: 0.38812121003866196\n",
      "------------------------------------ epoch 247 (1476 steps) ------------------------------------\n",
      "Max loss: 0.1110367700457573\n",
      "Min loss: 0.05906372889876366\n",
      "Mean loss: 0.07250932728250821\n",
      "Std loss: 0.017836128916664373\n",
      "Total Loss: 0.4350559636950493\n",
      "------------------------------------ epoch 248 (1482 steps) ------------------------------------\n",
      "Max loss: 0.09948970377445221\n",
      "Min loss: 0.04381522536277771\n",
      "Mean loss: 0.07263842100898425\n",
      "Std loss: 0.018136274640131005\n",
      "Total Loss: 0.4358305260539055\n",
      "------------------------------------ epoch 249 (1488 steps) ------------------------------------\n",
      "Max loss: 0.07617443054914474\n",
      "Min loss: 0.041714832186698914\n",
      "Mean loss: 0.058543158074220024\n",
      "Std loss: 0.011120445983276413\n",
      "Total Loss: 0.35125894844532013\n",
      "------------------------------------ epoch 250 (1494 steps) ------------------------------------\n",
      "Max loss: 0.12545883655548096\n",
      "Min loss: 0.04703082889318466\n",
      "Mean loss: 0.07063265269001325\n",
      "Std loss: 0.02594267225973418\n",
      "Total Loss: 0.4237959161400795\n",
      "------------------------------------ epoch 251 (1500 steps) ------------------------------------\n",
      "Max loss: 0.10291068255901337\n",
      "Min loss: 0.0536966398358345\n",
      "Mean loss: 0.0699624481300513\n",
      "Std loss: 0.0175322709192839\n",
      "Total Loss: 0.41977468878030777\n",
      "------------------------------------ epoch 252 (1506 steps) ------------------------------------\n",
      "Max loss: 0.10446394979953766\n",
      "Min loss: 0.04899192601442337\n",
      "Mean loss: 0.07568580284714699\n",
      "Std loss: 0.02005689002501079\n",
      "Total Loss: 0.4541148170828819\n",
      "------------------------------------ epoch 253 (1512 steps) ------------------------------------\n",
      "Max loss: 0.07267463207244873\n",
      "Min loss: 0.049240440130233765\n",
      "Mean loss: 0.06027090052763621\n",
      "Std loss: 0.008679243404554435\n",
      "Total Loss: 0.36162540316581726\n",
      "------------------------------------ epoch 254 (1518 steps) ------------------------------------\n",
      "Max loss: 0.08248603343963623\n",
      "Min loss: 0.04576935991644859\n",
      "Mean loss: 0.06007846258580685\n",
      "Std loss: 0.012680514566295435\n",
      "Total Loss: 0.3604707755148411\n",
      "------------------------------------ epoch 255 (1524 steps) ------------------------------------\n",
      "Max loss: 0.1257464587688446\n",
      "Min loss: 0.06475953012704849\n",
      "Mean loss: 0.08760374411940575\n",
      "Std loss: 0.021870714608691614\n",
      "Total Loss: 0.5256224647164345\n",
      "------------------------------------ epoch 256 (1530 steps) ------------------------------------\n",
      "Max loss: 0.08440165966749191\n",
      "Min loss: 0.06121867522597313\n",
      "Mean loss: 0.07450350932776928\n",
      "Std loss: 0.009510914256319556\n",
      "Total Loss: 0.4470210559666157\n",
      "------------------------------------ epoch 257 (1536 steps) ------------------------------------\n",
      "Max loss: 0.07210104912519455\n",
      "Min loss: 0.037141814827919006\n",
      "Mean loss: 0.05726724242170652\n",
      "Std loss: 0.014397893308471663\n",
      "Total Loss: 0.3436034545302391\n",
      "------------------------------------ epoch 258 (1542 steps) ------------------------------------\n",
      "Max loss: 0.0827760323882103\n",
      "Min loss: 0.05916934832930565\n",
      "Mean loss: 0.07230585378905137\n",
      "Std loss: 0.007525989760131949\n",
      "Total Loss: 0.43383512273430824\n",
      "------------------------------------ epoch 259 (1548 steps) ------------------------------------\n",
      "Max loss: 0.07163706421852112\n",
      "Min loss: 0.04645403474569321\n",
      "Mean loss: 0.0596828181296587\n",
      "Std loss: 0.009197421731551672\n",
      "Total Loss: 0.3580969087779522\n",
      "------------------------------------ epoch 260 (1554 steps) ------------------------------------\n",
      "Max loss: 0.07648493349552155\n",
      "Min loss: 0.0413736030459404\n",
      "Mean loss: 0.059307519967357315\n",
      "Std loss: 0.012478992402623849\n",
      "Total Loss: 0.3558451198041439\n",
      "------------------------------------ epoch 261 (1560 steps) ------------------------------------\n",
      "Max loss: 0.10444708913564682\n",
      "Min loss: 0.045115046203136444\n",
      "Mean loss: 0.0647010716299216\n",
      "Std loss: 0.019193870586663175\n",
      "Total Loss: 0.38820642977952957\n",
      "------------------------------------ epoch 262 (1566 steps) ------------------------------------\n",
      "Max loss: 0.09593137353658676\n",
      "Min loss: 0.04250836372375488\n",
      "Mean loss: 0.0640122735251983\n",
      "Std loss: 0.019091327225580115\n",
      "Total Loss: 0.3840736411511898\n",
      "------------------------------------ epoch 263 (1572 steps) ------------------------------------\n",
      "Max loss: 0.09381403774023056\n",
      "Min loss: 0.03991686552762985\n",
      "Mean loss: 0.06867317482829094\n",
      "Std loss: 0.01922242513321286\n",
      "Total Loss: 0.41203904896974564\n",
      "------------------------------------ epoch 264 (1578 steps) ------------------------------------\n",
      "Max loss: 0.10699109733104706\n",
      "Min loss: 0.03961168974637985\n",
      "Mean loss: 0.07731133575240771\n",
      "Std loss: 0.025329206648265927\n",
      "Total Loss: 0.46386801451444626\n",
      "------------------------------------ epoch 265 (1584 steps) ------------------------------------\n",
      "Max loss: 0.07627435028553009\n",
      "Min loss: 0.045912109315395355\n",
      "Mean loss: 0.05818731213609377\n",
      "Std loss: 0.010963169139908791\n",
      "Total Loss: 0.34912387281656265\n",
      "------------------------------------ epoch 266 (1590 steps) ------------------------------------\n",
      "Max loss: 0.06608787178993225\n",
      "Min loss: 0.05569882318377495\n",
      "Mean loss: 0.05908281418184439\n",
      "Std loss: 0.0037960410993092574\n",
      "Total Loss: 0.35449688509106636\n",
      "------------------------------------ epoch 267 (1596 steps) ------------------------------------\n",
      "Max loss: 0.08384998887777328\n",
      "Min loss: 0.05489405244588852\n",
      "Mean loss: 0.06999290610353152\n",
      "Std loss: 0.009325376647994321\n",
      "Total Loss: 0.4199574366211891\n",
      "------------------------------------ epoch 268 (1602 steps) ------------------------------------\n",
      "Max loss: 0.07233540713787079\n",
      "Min loss: 0.05463973432779312\n",
      "Mean loss: 0.06620261693994205\n",
      "Std loss: 0.005870183698594872\n",
      "Total Loss: 0.39721570163965225\n",
      "------------------------------------ epoch 269 (1608 steps) ------------------------------------\n",
      "Max loss: 0.09335365891456604\n",
      "Min loss: 0.04644288122653961\n",
      "Mean loss: 0.05910467108090719\n",
      "Std loss: 0.01558575055050486\n",
      "Total Loss: 0.3546280264854431\n",
      "------------------------------------ epoch 270 (1614 steps) ------------------------------------\n",
      "Max loss: 0.07715241611003876\n",
      "Min loss: 0.04258955642580986\n",
      "Mean loss: 0.059204076106349625\n",
      "Std loss: 0.013478284720095159\n",
      "Total Loss: 0.35522445663809776\n",
      "------------------------------------ epoch 271 (1620 steps) ------------------------------------\n",
      "Max loss: 0.08218434453010559\n",
      "Min loss: 0.04333055764436722\n",
      "Mean loss: 0.06621640361845493\n",
      "Std loss: 0.014783428372195978\n",
      "Total Loss: 0.3972984217107296\n",
      "------------------------------------ epoch 272 (1626 steps) ------------------------------------\n",
      "Max loss: 0.09881289303302765\n",
      "Min loss: 0.04688755422830582\n",
      "Mean loss: 0.07911386340856552\n",
      "Std loss: 0.01932437132193844\n",
      "Total Loss: 0.4746831804513931\n",
      "------------------------------------ epoch 273 (1632 steps) ------------------------------------\n",
      "Max loss: 0.0853809043765068\n",
      "Min loss: 0.04352770000696182\n",
      "Mean loss: 0.058318297689159714\n",
      "Std loss: 0.01540383731199776\n",
      "Total Loss: 0.34990978613495827\n",
      "------------------------------------ epoch 274 (1638 steps) ------------------------------------\n",
      "Max loss: 0.11992573738098145\n",
      "Min loss: 0.045367531478405\n",
      "Mean loss: 0.07561275611321132\n",
      "Std loss: 0.027738127281440546\n",
      "Total Loss: 0.4536765366792679\n",
      "------------------------------------ epoch 275 (1644 steps) ------------------------------------\n",
      "Max loss: 0.08466356247663498\n",
      "Min loss: 0.04833962768316269\n",
      "Mean loss: 0.06456581875681877\n",
      "Std loss: 0.01230100062326129\n",
      "Total Loss: 0.38739491254091263\n",
      "------------------------------------ epoch 276 (1650 steps) ------------------------------------\n",
      "Max loss: 0.10246589779853821\n",
      "Min loss: 0.05114017054438591\n",
      "Mean loss: 0.07560143806040287\n",
      "Std loss: 0.01683125605595472\n",
      "Total Loss: 0.4536086283624172\n",
      "------------------------------------ epoch 277 (1656 steps) ------------------------------------\n",
      "Max loss: 0.0841507613658905\n",
      "Min loss: 0.03470339626073837\n",
      "Mean loss: 0.06390343109766643\n",
      "Std loss: 0.01866330505838701\n",
      "Total Loss: 0.38342058658599854\n",
      "------------------------------------ epoch 278 (1662 steps) ------------------------------------\n",
      "Max loss: 0.08853597939014435\n",
      "Min loss: 0.04282499849796295\n",
      "Mean loss: 0.06701074416438739\n",
      "Std loss: 0.015862697950644756\n",
      "Total Loss: 0.4020644649863243\n",
      "------------------------------------ epoch 279 (1668 steps) ------------------------------------\n",
      "Max loss: 0.09085532277822495\n",
      "Min loss: 0.04211340844631195\n",
      "Mean loss: 0.06624490084747474\n",
      "Std loss: 0.015258655966068695\n",
      "Total Loss: 0.3974694050848484\n",
      "------------------------------------ epoch 280 (1674 steps) ------------------------------------\n",
      "Max loss: 0.13315537571907043\n",
      "Min loss: 0.05729009956121445\n",
      "Mean loss: 0.08376628160476685\n",
      "Std loss: 0.025566796215172147\n",
      "Total Loss: 0.5025976896286011\n",
      "------------------------------------ epoch 281 (1680 steps) ------------------------------------\n",
      "Max loss: 0.10665720701217651\n",
      "Min loss: 0.0567350797355175\n",
      "Mean loss: 0.07545272509256999\n",
      "Std loss: 0.016809067228183927\n",
      "Total Loss: 0.4527163505554199\n",
      "------------------------------------ epoch 282 (1686 steps) ------------------------------------\n",
      "Max loss: 0.10022157430648804\n",
      "Min loss: 0.054847490042448044\n",
      "Mean loss: 0.06921198529501756\n",
      "Std loss: 0.015027191404429341\n",
      "Total Loss: 0.41527191177010536\n",
      "------------------------------------ epoch 283 (1692 steps) ------------------------------------\n",
      "Max loss: 0.10826988518238068\n",
      "Min loss: 0.042356960475444794\n",
      "Mean loss: 0.0717795081436634\n",
      "Std loss: 0.024555887104506274\n",
      "Total Loss: 0.43067704886198044\n",
      "------------------------------------ epoch 284 (1698 steps) ------------------------------------\n",
      "Max loss: 0.08609260618686676\n",
      "Min loss: 0.04604720324277878\n",
      "Mean loss: 0.06440063007175922\n",
      "Std loss: 0.012626421761999242\n",
      "Total Loss: 0.38640378043055534\n",
      "------------------------------------ epoch 285 (1704 steps) ------------------------------------\n",
      "Max loss: 0.06838981807231903\n",
      "Min loss: 0.04751601070165634\n",
      "Mean loss: 0.05982647277414799\n",
      "Std loss: 0.007347109835740007\n",
      "Total Loss: 0.3589588366448879\n",
      "------------------------------------ epoch 286 (1710 steps) ------------------------------------\n",
      "Max loss: 0.1062469631433487\n",
      "Min loss: 0.04704279452562332\n",
      "Mean loss: 0.07449579859773318\n",
      "Std loss: 0.021707805955287693\n",
      "Total Loss: 0.4469747915863991\n",
      "------------------------------------ epoch 287 (1716 steps) ------------------------------------\n",
      "Max loss: 0.09397139400243759\n",
      "Min loss: 0.053424298763275146\n",
      "Mean loss: 0.06728251340488593\n",
      "Std loss: 0.014464571154363146\n",
      "Total Loss: 0.40369508042931557\n",
      "------------------------------------ epoch 288 (1722 steps) ------------------------------------\n",
      "Max loss: 0.11517433822154999\n",
      "Min loss: 0.07126960158348083\n",
      "Mean loss: 0.08219759166240692\n",
      "Std loss: 0.015005895333474933\n",
      "Total Loss: 0.49318554997444153\n",
      "------------------------------------ epoch 289 (1728 steps) ------------------------------------\n",
      "Max loss: 0.09872692823410034\n",
      "Min loss: 0.04749831184744835\n",
      "Mean loss: 0.06614167802035809\n",
      "Std loss: 0.017959009435328802\n",
      "Total Loss: 0.3968500681221485\n",
      "------------------------------------ epoch 290 (1734 steps) ------------------------------------\n",
      "Max loss: 0.07317293435335159\n",
      "Min loss: 0.044307880103588104\n",
      "Mean loss: 0.06136102912326654\n",
      "Std loss: 0.011051617888582062\n",
      "Total Loss: 0.36816617473959923\n",
      "------------------------------------ epoch 291 (1740 steps) ------------------------------------\n",
      "Max loss: 0.12294507026672363\n",
      "Min loss: 0.049109965562820435\n",
      "Mean loss: 0.07991970454653104\n",
      "Std loss: 0.025608126575237124\n",
      "Total Loss: 0.47951822727918625\n",
      "------------------------------------ epoch 292 (1746 steps) ------------------------------------\n",
      "Max loss: 0.07217184454202652\n",
      "Min loss: 0.041719332337379456\n",
      "Mean loss: 0.054817638670404754\n",
      "Std loss: 0.010424668811693917\n",
      "Total Loss: 0.3289058320224285\n",
      "------------------------------------ epoch 293 (1752 steps) ------------------------------------\n",
      "Max loss: 0.08225779980421066\n",
      "Min loss: 0.047658637166023254\n",
      "Mean loss: 0.06307164952158928\n",
      "Std loss: 0.010945615532041075\n",
      "Total Loss: 0.3784298971295357\n",
      "------------------------------------ epoch 294 (1758 steps) ------------------------------------\n",
      "Max loss: 0.06824298202991486\n",
      "Min loss: 0.04340384528040886\n",
      "Mean loss: 0.0558533159395059\n",
      "Std loss: 0.009478173164140687\n",
      "Total Loss: 0.33511989563703537\n",
      "------------------------------------ epoch 295 (1764 steps) ------------------------------------\n",
      "Max loss: 0.12740302085876465\n",
      "Min loss: 0.0574776828289032\n",
      "Mean loss: 0.08317076166470845\n",
      "Std loss: 0.024729641060211527\n",
      "Total Loss: 0.49902456998825073\n",
      "------------------------------------ epoch 296 (1770 steps) ------------------------------------\n",
      "Max loss: 0.08227142691612244\n",
      "Min loss: 0.04442738741636276\n",
      "Mean loss: 0.05880844406783581\n",
      "Std loss: 0.013182655604251622\n",
      "Total Loss: 0.35285066440701485\n",
      "------------------------------------ epoch 297 (1776 steps) ------------------------------------\n",
      "Max loss: 0.06045961007475853\n",
      "Min loss: 0.04091642424464226\n",
      "Mean loss: 0.05263966073592504\n",
      "Std loss: 0.006553509645916788\n",
      "Total Loss: 0.31583796441555023\n",
      "------------------------------------ epoch 298 (1782 steps) ------------------------------------\n",
      "Max loss: 0.09107454866170883\n",
      "Min loss: 0.03699924051761627\n",
      "Mean loss: 0.06595445424318314\n",
      "Std loss: 0.01774457263949039\n",
      "Total Loss: 0.3957267254590988\n",
      "------------------------------------ epoch 299 (1788 steps) ------------------------------------\n",
      "Max loss: 0.08181684464216232\n",
      "Min loss: 0.032118625938892365\n",
      "Mean loss: 0.048282732566197716\n",
      "Std loss: 0.017429627184077003\n",
      "Total Loss: 0.2896963953971863\n",
      "------------------------------------ epoch 300 (1794 steps) ------------------------------------\n",
      "Max loss: 0.17699001729488373\n",
      "Min loss: 0.052748002111911774\n",
      "Mean loss: 0.0837667907277743\n",
      "Std loss: 0.04292782888529316\n",
      "Total Loss: 0.5026007443666458\n",
      "------------------------------------ epoch 301 (1800 steps) ------------------------------------\n",
      "Max loss: 0.0879850834608078\n",
      "Min loss: 0.05719929188489914\n",
      "Mean loss: 0.06893586677809556\n",
      "Std loss: 0.010948870853960367\n",
      "Total Loss: 0.4136152006685734\n",
      "saved model at ./weights/model_301.pth\n",
      "------------------------------------ epoch 302 (1806 steps) ------------------------------------\n",
      "Max loss: 0.0753452330827713\n",
      "Min loss: 0.04526551812887192\n",
      "Mean loss: 0.060920714711149536\n",
      "Std loss: 0.010733036435345067\n",
      "Total Loss: 0.3655242882668972\n",
      "------------------------------------ epoch 303 (1812 steps) ------------------------------------\n",
      "Max loss: 0.1302170753479004\n",
      "Min loss: 0.04419413208961487\n",
      "Mean loss: 0.0749570628007253\n",
      "Std loss: 0.032873294192442774\n",
      "Total Loss: 0.4497423768043518\n",
      "------------------------------------ epoch 304 (1818 steps) ------------------------------------\n",
      "Max loss: 0.0743153840303421\n",
      "Min loss: 0.04550959914922714\n",
      "Mean loss: 0.05518519009153048\n",
      "Std loss: 0.009404041161033124\n",
      "Total Loss: 0.3311111405491829\n",
      "------------------------------------ epoch 305 (1824 steps) ------------------------------------\n",
      "Max loss: 0.11258666217327118\n",
      "Min loss: 0.04140787571668625\n",
      "Mean loss: 0.07352414665122826\n",
      "Std loss: 0.023966011704873526\n",
      "Total Loss: 0.4411448799073696\n",
      "------------------------------------ epoch 306 (1830 steps) ------------------------------------\n",
      "Max loss: 0.11451657116413116\n",
      "Min loss: 0.050011664628982544\n",
      "Mean loss: 0.07705748329559962\n",
      "Std loss: 0.021824615670112544\n",
      "Total Loss: 0.4623448997735977\n",
      "------------------------------------ epoch 307 (1836 steps) ------------------------------------\n",
      "Max loss: 0.1240147054195404\n",
      "Min loss: 0.03524152934551239\n",
      "Mean loss: 0.06920806939403217\n",
      "Std loss: 0.02714161578357318\n",
      "Total Loss: 0.41524841636419296\n",
      "------------------------------------ epoch 308 (1842 steps) ------------------------------------\n",
      "Max loss: 0.08423066139221191\n",
      "Min loss: 0.029652930796146393\n",
      "Mean loss: 0.056598203256726265\n",
      "Std loss: 0.019064759163464264\n",
      "Total Loss: 0.3395892195403576\n",
      "------------------------------------ epoch 309 (1848 steps) ------------------------------------\n",
      "Max loss: 0.08125604689121246\n",
      "Min loss: 0.0447949543595314\n",
      "Mean loss: 0.06670978044470151\n",
      "Std loss: 0.01149943916874589\n",
      "Total Loss: 0.4002586826682091\n",
      "------------------------------------ epoch 310 (1854 steps) ------------------------------------\n",
      "Max loss: 0.07675552368164062\n",
      "Min loss: 0.039390746504068375\n",
      "Mean loss: 0.054878202577432\n",
      "Std loss: 0.013648683843388638\n",
      "Total Loss: 0.329269215464592\n",
      "------------------------------------ epoch 311 (1860 steps) ------------------------------------\n",
      "Max loss: 0.08731919527053833\n",
      "Min loss: 0.04542424902319908\n",
      "Mean loss: 0.058661273370186486\n",
      "Std loss: 0.014352679133091005\n",
      "Total Loss: 0.3519676402211189\n",
      "------------------------------------ epoch 312 (1866 steps) ------------------------------------\n",
      "Max loss: 0.09460809826850891\n",
      "Min loss: 0.03500404581427574\n",
      "Mean loss: 0.05251286240915457\n",
      "Std loss: 0.019822959291534856\n",
      "Total Loss: 0.31507717445492744\n",
      "------------------------------------ epoch 313 (1872 steps) ------------------------------------\n",
      "Max loss: 0.10531236231327057\n",
      "Min loss: 0.035354431718587875\n",
      "Mean loss: 0.06793315584460895\n",
      "Std loss: 0.027695361598382057\n",
      "Total Loss: 0.40759893506765366\n",
      "------------------------------------ epoch 314 (1878 steps) ------------------------------------\n",
      "Max loss: 0.14199870824813843\n",
      "Min loss: 0.03833336383104324\n",
      "Mean loss: 0.08181315970917542\n",
      "Std loss: 0.04284802579303439\n",
      "Total Loss: 0.49087895825505257\n",
      "------------------------------------ epoch 315 (1884 steps) ------------------------------------\n",
      "Max loss: 0.1083841547369957\n",
      "Min loss: 0.05023934692144394\n",
      "Mean loss: 0.08194251110156377\n",
      "Std loss: 0.017293657221423994\n",
      "Total Loss: 0.49165506660938263\n",
      "------------------------------------ epoch 316 (1890 steps) ------------------------------------\n",
      "Max loss: 0.08417391777038574\n",
      "Min loss: 0.0550069622695446\n",
      "Mean loss: 0.07260771778722604\n",
      "Std loss: 0.01046462805936807\n",
      "Total Loss: 0.43564630672335625\n",
      "------------------------------------ epoch 317 (1896 steps) ------------------------------------\n",
      "Max loss: 0.10816343873739243\n",
      "Min loss: 0.05888543650507927\n",
      "Mean loss: 0.0743910080442826\n",
      "Std loss: 0.01652244515483085\n",
      "Total Loss: 0.44634604826569557\n",
      "------------------------------------ epoch 318 (1902 steps) ------------------------------------\n",
      "Max loss: 0.10397594422101974\n",
      "Min loss: 0.048926666378974915\n",
      "Mean loss: 0.07899026572704315\n",
      "Std loss: 0.018956400215738366\n",
      "Total Loss: 0.4739415943622589\n",
      "------------------------------------ epoch 319 (1908 steps) ------------------------------------\n",
      "Max loss: 0.07114814221858978\n",
      "Min loss: 0.043276071548461914\n",
      "Mean loss: 0.054959528148174286\n",
      "Std loss: 0.012050247495699367\n",
      "Total Loss: 0.3297571688890457\n",
      "------------------------------------ epoch 320 (1914 steps) ------------------------------------\n",
      "Max loss: 0.09658007323741913\n",
      "Min loss: 0.044978972524404526\n",
      "Mean loss: 0.062087973579764366\n",
      "Std loss: 0.019558445113853243\n",
      "Total Loss: 0.3725278414785862\n",
      "------------------------------------ epoch 321 (1920 steps) ------------------------------------\n",
      "Max loss: 0.08778271079063416\n",
      "Min loss: 0.04216141998767853\n",
      "Mean loss: 0.05906703509390354\n",
      "Std loss: 0.01557765853626997\n",
      "Total Loss: 0.35440221056342125\n",
      "------------------------------------ epoch 322 (1926 steps) ------------------------------------\n",
      "Max loss: 0.08811774104833603\n",
      "Min loss: 0.046245276927948\n",
      "Mean loss: 0.06306464473406474\n",
      "Std loss: 0.012825559628895228\n",
      "Total Loss: 0.3783878684043884\n",
      "------------------------------------ epoch 323 (1932 steps) ------------------------------------\n",
      "Max loss: 0.16191940009593964\n",
      "Min loss: 0.05723088979721069\n",
      "Mean loss: 0.08207198604941368\n",
      "Std loss: 0.03609635493355687\n",
      "Total Loss: 0.4924319162964821\n",
      "------------------------------------ epoch 324 (1938 steps) ------------------------------------\n",
      "Max loss: 0.08956917375326157\n",
      "Min loss: 0.04647725820541382\n",
      "Mean loss: 0.06016954345007738\n",
      "Std loss: 0.014588885301668181\n",
      "Total Loss: 0.36101726070046425\n",
      "------------------------------------ epoch 325 (1944 steps) ------------------------------------\n",
      "Max loss: 0.12085676193237305\n",
      "Min loss: 0.0460386797785759\n",
      "Mean loss: 0.06742649773756663\n",
      "Std loss: 0.026271762452070312\n",
      "Total Loss: 0.4045589864253998\n",
      "------------------------------------ epoch 326 (1950 steps) ------------------------------------\n",
      "Max loss: 0.10575567930936813\n",
      "Min loss: 0.038388416171073914\n",
      "Mean loss: 0.06457493640482426\n",
      "Std loss: 0.021068759012854682\n",
      "Total Loss: 0.38744961842894554\n",
      "------------------------------------ epoch 327 (1956 steps) ------------------------------------\n",
      "Max loss: 0.0884317085146904\n",
      "Min loss: 0.03877270966768265\n",
      "Mean loss: 0.055859545866648354\n",
      "Std loss: 0.01567091994825176\n",
      "Total Loss: 0.33515727519989014\n",
      "------------------------------------ epoch 328 (1962 steps) ------------------------------------\n",
      "Max loss: 0.09348424524068832\n",
      "Min loss: 0.03894030302762985\n",
      "Mean loss: 0.061440534268816314\n",
      "Std loss: 0.0190082125375893\n",
      "Total Loss: 0.3686432056128979\n",
      "------------------------------------ epoch 329 (1968 steps) ------------------------------------\n",
      "Max loss: 0.10520815849304199\n",
      "Min loss: 0.0435161218047142\n",
      "Mean loss: 0.07140974948803584\n",
      "Std loss: 0.02004340837995589\n",
      "Total Loss: 0.428458496928215\n",
      "------------------------------------ epoch 330 (1974 steps) ------------------------------------\n",
      "Max loss: 0.10897736251354218\n",
      "Min loss: 0.04036606848239899\n",
      "Mean loss: 0.07313663139939308\n",
      "Std loss: 0.02162434960069623\n",
      "Total Loss: 0.4388197883963585\n",
      "------------------------------------ epoch 331 (1980 steps) ------------------------------------\n",
      "Max loss: 0.10992907732725143\n",
      "Min loss: 0.04905053973197937\n",
      "Mean loss: 0.06773779727518559\n",
      "Std loss: 0.020961974769995256\n",
      "Total Loss: 0.4064267836511135\n",
      "------------------------------------ epoch 332 (1986 steps) ------------------------------------\n",
      "Max loss: 0.08062475919723511\n",
      "Min loss: 0.03751682490110397\n",
      "Mean loss: 0.0603702204922835\n",
      "Std loss: 0.016793184261322383\n",
      "Total Loss: 0.362221322953701\n",
      "------------------------------------ epoch 333 (1992 steps) ------------------------------------\n",
      "Max loss: 0.12342207133769989\n",
      "Min loss: 0.04711129516363144\n",
      "Mean loss: 0.07416020147502422\n",
      "Std loss: 0.027616659487354447\n",
      "Total Loss: 0.44496120885014534\n",
      "------------------------------------ epoch 334 (1998 steps) ------------------------------------\n",
      "Max loss: 0.08199797570705414\n",
      "Min loss: 0.03045797348022461\n",
      "Mean loss: 0.0532069057226181\n",
      "Std loss: 0.015378092023436206\n",
      "Total Loss: 0.3192414343357086\n",
      "------------------------------------ epoch 335 (2004 steps) ------------------------------------\n",
      "Max loss: 0.08543674647808075\n",
      "Min loss: 0.03190650790929794\n",
      "Mean loss: 0.05569791421294212\n",
      "Std loss: 0.017988673675951517\n",
      "Total Loss: 0.33418748527765274\n",
      "------------------------------------ epoch 336 (2010 steps) ------------------------------------\n",
      "Max loss: 0.0771879106760025\n",
      "Min loss: 0.03810657560825348\n",
      "Mean loss: 0.05354821992417177\n",
      "Std loss: 0.014159798394363229\n",
      "Total Loss: 0.3212893195450306\n",
      "------------------------------------ epoch 337 (2016 steps) ------------------------------------\n",
      "Max loss: 0.08532534539699554\n",
      "Min loss: 0.03475331887602806\n",
      "Mean loss: 0.05741082690656185\n",
      "Std loss: 0.01716463159948394\n",
      "Total Loss: 0.3444649614393711\n",
      "------------------------------------ epoch 338 (2022 steps) ------------------------------------\n",
      "Max loss: 0.09933769702911377\n",
      "Min loss: 0.03314920514822006\n",
      "Mean loss: 0.056981866558392845\n",
      "Std loss: 0.02466341810336625\n",
      "Total Loss: 0.34189119935035706\n",
      "------------------------------------ epoch 339 (2028 steps) ------------------------------------\n",
      "Max loss: 0.10249879211187363\n",
      "Min loss: 0.042082976549863815\n",
      "Mean loss: 0.06792388794322808\n",
      "Std loss: 0.020062688478422318\n",
      "Total Loss: 0.4075433276593685\n",
      "------------------------------------ epoch 340 (2034 steps) ------------------------------------\n",
      "Max loss: 0.09652199596166611\n",
      "Min loss: 0.03534810245037079\n",
      "Mean loss: 0.0683723861972491\n",
      "Std loss: 0.021625326975640773\n",
      "Total Loss: 0.41023431718349457\n",
      "------------------------------------ epoch 341 (2040 steps) ------------------------------------\n",
      "Max loss: 0.07429370284080505\n",
      "Min loss: 0.04812689125537872\n",
      "Mean loss: 0.058759430423378944\n",
      "Std loss: 0.008975464863597194\n",
      "Total Loss: 0.35255658254027367\n",
      "------------------------------------ epoch 342 (2046 steps) ------------------------------------\n",
      "Max loss: 0.0771646499633789\n",
      "Min loss: 0.03317926079034805\n",
      "Mean loss: 0.053490870321790375\n",
      "Std loss: 0.013511883087635708\n",
      "Total Loss: 0.32094522193074226\n",
      "------------------------------------ epoch 343 (2052 steps) ------------------------------------\n",
      "Max loss: 0.08793970197439194\n",
      "Min loss: 0.0370415560901165\n",
      "Mean loss: 0.06059045592943827\n",
      "Std loss: 0.015421022036262713\n",
      "Total Loss: 0.36354273557662964\n",
      "------------------------------------ epoch 344 (2058 steps) ------------------------------------\n",
      "Max loss: 0.0913434773683548\n",
      "Min loss: 0.035739924758672714\n",
      "Mean loss: 0.05562549022336801\n",
      "Std loss: 0.018913279306259564\n",
      "Total Loss: 0.33375294134020805\n",
      "------------------------------------ epoch 345 (2064 steps) ------------------------------------\n",
      "Max loss: 0.07902508974075317\n",
      "Min loss: 0.056638263165950775\n",
      "Mean loss: 0.06571959642072518\n",
      "Std loss: 0.008425657500062847\n",
      "Total Loss: 0.3943175785243511\n",
      "------------------------------------ epoch 346 (2070 steps) ------------------------------------\n",
      "Max loss: 0.062263693660497665\n",
      "Min loss: 0.041827619075775146\n",
      "Mean loss: 0.054537529746691384\n",
      "Std loss: 0.0063628450180315335\n",
      "Total Loss: 0.3272251784801483\n",
      "------------------------------------ epoch 347 (2076 steps) ------------------------------------\n",
      "Max loss: 0.09606705605983734\n",
      "Min loss: 0.060012951493263245\n",
      "Mean loss: 0.07935570925474167\n",
      "Std loss: 0.01471788376949145\n",
      "Total Loss: 0.47613425552845\n",
      "------------------------------------ epoch 348 (2082 steps) ------------------------------------\n",
      "Max loss: 0.06870563328266144\n",
      "Min loss: 0.03818752244114876\n",
      "Mean loss: 0.057074482863148056\n",
      "Std loss: 0.010630816396402499\n",
      "Total Loss: 0.3424468971788883\n",
      "------------------------------------ epoch 349 (2088 steps) ------------------------------------\n",
      "Max loss: 0.07059605419635773\n",
      "Min loss: 0.036831602454185486\n",
      "Mean loss: 0.05744503624737263\n",
      "Std loss: 0.010584603427267015\n",
      "Total Loss: 0.34467021748423576\n",
      "------------------------------------ epoch 350 (2094 steps) ------------------------------------\n",
      "Max loss: 0.08838255703449249\n",
      "Min loss: 0.041227757930755615\n",
      "Mean loss: 0.056205726539095245\n",
      "Std loss: 0.015461649340765802\n",
      "Total Loss: 0.33723435923457146\n",
      "------------------------------------ epoch 351 (2100 steps) ------------------------------------\n",
      "Max loss: 0.07969425618648529\n",
      "Min loss: 0.04005682095885277\n",
      "Mean loss: 0.05863148719072342\n",
      "Std loss: 0.01757853605099924\n",
      "Total Loss: 0.3517889231443405\n",
      "------------------------------------ epoch 352 (2106 steps) ------------------------------------\n",
      "Max loss: 0.0842340812087059\n",
      "Min loss: 0.05181276053190231\n",
      "Mean loss: 0.060551428546508156\n",
      "Std loss: 0.011260796986481985\n",
      "Total Loss: 0.3633085712790489\n",
      "------------------------------------ epoch 353 (2112 steps) ------------------------------------\n",
      "Max loss: 0.08109734952449799\n",
      "Min loss: 0.030283933505415916\n",
      "Mean loss: 0.05757379997521639\n",
      "Std loss: 0.018399245620289918\n",
      "Total Loss: 0.34544279985129833\n",
      "------------------------------------ epoch 354 (2118 steps) ------------------------------------\n",
      "Max loss: 0.06375782191753387\n",
      "Min loss: 0.03488475829362869\n",
      "Mean loss: 0.04946032601098219\n",
      "Std loss: 0.010520935692808639\n",
      "Total Loss: 0.2967619560658932\n",
      "------------------------------------ epoch 355 (2124 steps) ------------------------------------\n",
      "Max loss: 0.07967563718557358\n",
      "Min loss: 0.0440024696290493\n",
      "Mean loss: 0.06020474495987097\n",
      "Std loss: 0.012752247507174441\n",
      "Total Loss: 0.36122846975922585\n",
      "------------------------------------ epoch 356 (2130 steps) ------------------------------------\n",
      "Max loss: 0.07918091863393784\n",
      "Min loss: 0.03561185300350189\n",
      "Mean loss: 0.04965559641520182\n",
      "Std loss: 0.014397429031004171\n",
      "Total Loss: 0.29793357849121094\n",
      "------------------------------------ epoch 357 (2136 steps) ------------------------------------\n",
      "Max loss: 0.06422555446624756\n",
      "Min loss: 0.036223482340574265\n",
      "Mean loss: 0.05183870283265909\n",
      "Std loss: 0.009068193349633246\n",
      "Total Loss: 0.3110322169959545\n",
      "------------------------------------ epoch 358 (2142 steps) ------------------------------------\n",
      "Max loss: 0.06726963073015213\n",
      "Min loss: 0.043322935700416565\n",
      "Mean loss: 0.05417852786680063\n",
      "Std loss: 0.007885197579854451\n",
      "Total Loss: 0.32507116720080376\n",
      "------------------------------------ epoch 359 (2148 steps) ------------------------------------\n",
      "Max loss: 0.07442881911993027\n",
      "Min loss: 0.03997839614748955\n",
      "Mean loss: 0.05663664204378923\n",
      "Std loss: 0.011775784461097998\n",
      "Total Loss: 0.33981985226273537\n",
      "------------------------------------ epoch 360 (2154 steps) ------------------------------------\n",
      "Max loss: 0.08517436683177948\n",
      "Min loss: 0.03044193610548973\n",
      "Mean loss: 0.048496209705869354\n",
      "Std loss: 0.01784295049581878\n",
      "Total Loss: 0.29097725823521614\n",
      "------------------------------------ epoch 361 (2160 steps) ------------------------------------\n",
      "Max loss: 0.06782066822052002\n",
      "Min loss: 0.03968428447842598\n",
      "Mean loss: 0.05260808703800043\n",
      "Std loss: 0.011087058282383416\n",
      "Total Loss: 0.31564852222800255\n",
      "------------------------------------ epoch 362 (2166 steps) ------------------------------------\n",
      "Max loss: 0.0920637845993042\n",
      "Min loss: 0.03917340561747551\n",
      "Mean loss: 0.062355258191625275\n",
      "Std loss: 0.0203419175000208\n",
      "Total Loss: 0.37413154914975166\n",
      "------------------------------------ epoch 363 (2172 steps) ------------------------------------\n",
      "Max loss: 0.08579003065824509\n",
      "Min loss: 0.045282892882823944\n",
      "Mean loss: 0.05971297745903333\n",
      "Std loss: 0.013973261369228612\n",
      "Total Loss: 0.3582778647542\n",
      "------------------------------------ epoch 364 (2178 steps) ------------------------------------\n",
      "Max loss: 0.0890754908323288\n",
      "Min loss: 0.03557845577597618\n",
      "Mean loss: 0.053902032474676766\n",
      "Std loss: 0.01820342939656164\n",
      "Total Loss: 0.3234121948480606\n",
      "------------------------------------ epoch 365 (2184 steps) ------------------------------------\n",
      "Max loss: 0.08100979030132294\n",
      "Min loss: 0.04693020507693291\n",
      "Mean loss: 0.06279546394944191\n",
      "Std loss: 0.014942681157729135\n",
      "Total Loss: 0.37677278369665146\n",
      "------------------------------------ epoch 366 (2190 steps) ------------------------------------\n",
      "Max loss: 0.09094011038541794\n",
      "Min loss: 0.03651080280542374\n",
      "Mean loss: 0.055176458631952606\n",
      "Std loss: 0.017538048333090403\n",
      "Total Loss: 0.3310587517917156\n",
      "------------------------------------ epoch 367 (2196 steps) ------------------------------------\n",
      "Max loss: 0.09943512082099915\n",
      "Min loss: 0.04456750303506851\n",
      "Mean loss: 0.06324830651283264\n",
      "Std loss: 0.019531401514449036\n",
      "Total Loss: 0.37948983907699585\n",
      "------------------------------------ epoch 368 (2202 steps) ------------------------------------\n",
      "Max loss: 0.06741383671760559\n",
      "Min loss: 0.036063846200704575\n",
      "Mean loss: 0.04970606478552023\n",
      "Std loss: 0.010512569010498457\n",
      "Total Loss: 0.2982363887131214\n",
      "------------------------------------ epoch 369 (2208 steps) ------------------------------------\n",
      "Max loss: 0.04684000462293625\n",
      "Min loss: 0.03386879712343216\n",
      "Mean loss: 0.04159058257937431\n",
      "Std loss: 0.004080401757501097\n",
      "Total Loss: 0.24954349547624588\n",
      "------------------------------------ epoch 370 (2214 steps) ------------------------------------\n",
      "Max loss: 0.08071178197860718\n",
      "Min loss: 0.035345420241355896\n",
      "Mean loss: 0.05754919039706389\n",
      "Std loss: 0.015358561756812177\n",
      "Total Loss: 0.34529514238238335\n",
      "------------------------------------ epoch 371 (2220 steps) ------------------------------------\n",
      "Max loss: 0.09286205470561981\n",
      "Min loss: 0.034662019461393356\n",
      "Mean loss: 0.05664778749148051\n",
      "Std loss: 0.020449822685307754\n",
      "Total Loss: 0.33988672494888306\n",
      "------------------------------------ epoch 372 (2226 steps) ------------------------------------\n",
      "Max loss: 0.09210662543773651\n",
      "Min loss: 0.04506376013159752\n",
      "Mean loss: 0.0625154593338569\n",
      "Std loss: 0.018413451018002992\n",
      "Total Loss: 0.3750927560031414\n",
      "------------------------------------ epoch 373 (2232 steps) ------------------------------------\n",
      "Max loss: 0.05604243278503418\n",
      "Min loss: 0.03371712565422058\n",
      "Mean loss: 0.047331844146052994\n",
      "Std loss: 0.00855423979600742\n",
      "Total Loss: 0.283991064876318\n",
      "------------------------------------ epoch 374 (2238 steps) ------------------------------------\n",
      "Max loss: 0.10215165466070175\n",
      "Min loss: 0.038379475474357605\n",
      "Mean loss: 0.0596604502449433\n",
      "Std loss: 0.02221165125941995\n",
      "Total Loss: 0.3579627014696598\n",
      "------------------------------------ epoch 375 (2244 steps) ------------------------------------\n",
      "Max loss: 0.07060389220714569\n",
      "Min loss: 0.03949753940105438\n",
      "Mean loss: 0.05671395485599836\n",
      "Std loss: 0.01170191811494562\n",
      "Total Loss: 0.34028372913599014\n",
      "------------------------------------ epoch 376 (2250 steps) ------------------------------------\n",
      "Max loss: 0.07704601436853409\n",
      "Min loss: 0.036189913749694824\n",
      "Mean loss: 0.046053690214951835\n",
      "Std loss: 0.014208272373705193\n",
      "Total Loss: 0.276322141289711\n",
      "------------------------------------ epoch 377 (2256 steps) ------------------------------------\n",
      "Max loss: 0.06983721256256104\n",
      "Min loss: 0.04649189114570618\n",
      "Mean loss: 0.05846219199399153\n",
      "Std loss: 0.009124736248324336\n",
      "Total Loss: 0.3507731519639492\n",
      "------------------------------------ epoch 378 (2262 steps) ------------------------------------\n",
      "Max loss: 0.11576560139656067\n",
      "Min loss: 0.03909163177013397\n",
      "Mean loss: 0.06476772079865138\n",
      "Std loss: 0.028105964083286267\n",
      "Total Loss: 0.38860632479190826\n",
      "------------------------------------ epoch 379 (2268 steps) ------------------------------------\n",
      "Max loss: 0.06186704337596893\n",
      "Min loss: 0.034020356833934784\n",
      "Mean loss: 0.05442231769363085\n",
      "Std loss: 0.00965144073366988\n",
      "Total Loss: 0.3265339061617851\n",
      "------------------------------------ epoch 380 (2274 steps) ------------------------------------\n",
      "Max loss: 0.060524679720401764\n",
      "Min loss: 0.0383579358458519\n",
      "Mean loss: 0.05030315803984801\n",
      "Std loss: 0.00789989667277664\n",
      "Total Loss: 0.30181894823908806\n",
      "------------------------------------ epoch 381 (2280 steps) ------------------------------------\n",
      "Max loss: 0.06773566454648972\n",
      "Min loss: 0.03422432392835617\n",
      "Mean loss: 0.04889585946997007\n",
      "Std loss: 0.013502721982302084\n",
      "Total Loss: 0.2933751568198204\n",
      "------------------------------------ epoch 382 (2286 steps) ------------------------------------\n",
      "Max loss: 0.10268297046422958\n",
      "Min loss: 0.03392615169286728\n",
      "Mean loss: 0.06014254130423069\n",
      "Std loss: 0.02654161548060312\n",
      "Total Loss: 0.36085524782538414\n",
      "------------------------------------ epoch 383 (2292 steps) ------------------------------------\n",
      "Max loss: 0.08298584818840027\n",
      "Min loss: 0.036798425018787384\n",
      "Mean loss: 0.06509410900374253\n",
      "Std loss: 0.015679821319973704\n",
      "Total Loss: 0.3905646540224552\n",
      "------------------------------------ epoch 384 (2298 steps) ------------------------------------\n",
      "Max loss: 0.10254915803670883\n",
      "Min loss: 0.03434049338102341\n",
      "Mean loss: 0.06521931228538354\n",
      "Std loss: 0.022406620550555156\n",
      "Total Loss: 0.39131587371230125\n",
      "------------------------------------ epoch 385 (2304 steps) ------------------------------------\n",
      "Max loss: 0.07340815663337708\n",
      "Min loss: 0.038707513362169266\n",
      "Mean loss: 0.054465011383096375\n",
      "Std loss: 0.011283651766311366\n",
      "Total Loss: 0.32679006829857826\n",
      "------------------------------------ epoch 386 (2310 steps) ------------------------------------\n",
      "Max loss: 0.06648404896259308\n",
      "Min loss: 0.039413005113601685\n",
      "Mean loss: 0.051057279109954834\n",
      "Std loss: 0.009694740083427623\n",
      "Total Loss: 0.306343674659729\n",
      "------------------------------------ epoch 387 (2316 steps) ------------------------------------\n",
      "Max loss: 0.08456581085920334\n",
      "Min loss: 0.03896597400307655\n",
      "Mean loss: 0.06201251596212387\n",
      "Std loss: 0.017892266811657155\n",
      "Total Loss: 0.3720750957727432\n",
      "------------------------------------ epoch 388 (2322 steps) ------------------------------------\n",
      "Max loss: 0.1148902177810669\n",
      "Min loss: 0.03634604439139366\n",
      "Mean loss: 0.07347403652966022\n",
      "Std loss: 0.027116429506956932\n",
      "Total Loss: 0.44084421917796135\n",
      "------------------------------------ epoch 389 (2328 steps) ------------------------------------\n",
      "Max loss: 0.07180299609899521\n",
      "Min loss: 0.03863082081079483\n",
      "Mean loss: 0.05313179331521193\n",
      "Std loss: 0.011813909350477814\n",
      "Total Loss: 0.3187907598912716\n",
      "------------------------------------ epoch 390 (2334 steps) ------------------------------------\n",
      "Max loss: 0.10533258318901062\n",
      "Min loss: 0.04369945451617241\n",
      "Mean loss: 0.06181517740090688\n",
      "Std loss: 0.020822980678781865\n",
      "Total Loss: 0.3708910644054413\n",
      "------------------------------------ epoch 391 (2340 steps) ------------------------------------\n",
      "Max loss: 0.07018764317035675\n",
      "Min loss: 0.03880390152335167\n",
      "Mean loss: 0.05996235584219297\n",
      "Std loss: 0.011005519583186215\n",
      "Total Loss: 0.3597741350531578\n",
      "------------------------------------ epoch 392 (2346 steps) ------------------------------------\n",
      "Max loss: 0.062140092253685\n",
      "Min loss: 0.040425606071949005\n",
      "Mean loss: 0.05043026556571325\n",
      "Std loss: 0.007848835661688314\n",
      "Total Loss: 0.3025815933942795\n",
      "------------------------------------ epoch 393 (2352 steps) ------------------------------------\n",
      "Max loss: 0.06662829220294952\n",
      "Min loss: 0.03173653036355972\n",
      "Mean loss: 0.059706129133701324\n",
      "Std loss: 0.012577779460867477\n",
      "Total Loss: 0.35823677480220795\n",
      "------------------------------------ epoch 394 (2358 steps) ------------------------------------\n",
      "Max loss: 0.07199446856975555\n",
      "Min loss: 0.04050048440694809\n",
      "Mean loss: 0.057334622368216515\n",
      "Std loss: 0.012488671632812816\n",
      "Total Loss: 0.3440077342092991\n",
      "------------------------------------ epoch 395 (2364 steps) ------------------------------------\n",
      "Max loss: 0.09408905357122421\n",
      "Min loss: 0.037033967673778534\n",
      "Mean loss: 0.06756679837902387\n",
      "Std loss: 0.01889308354399676\n",
      "Total Loss: 0.4054007902741432\n",
      "------------------------------------ epoch 396 (2370 steps) ------------------------------------\n",
      "Max loss: 0.06544113159179688\n",
      "Min loss: 0.0346754752099514\n",
      "Mean loss: 0.0451714980105559\n",
      "Std loss: 0.009662844817775235\n",
      "Total Loss: 0.2710289880633354\n",
      "------------------------------------ epoch 397 (2376 steps) ------------------------------------\n",
      "Max loss: 0.07281620800495148\n",
      "Min loss: 0.03173521161079407\n",
      "Mean loss: 0.049252825478712715\n",
      "Std loss: 0.013581926316381887\n",
      "Total Loss: 0.2955169528722763\n",
      "------------------------------------ epoch 398 (2382 steps) ------------------------------------\n",
      "Max loss: 0.07135701924562454\n",
      "Min loss: 0.03640519827604294\n",
      "Mean loss: 0.05210386961698532\n",
      "Std loss: 0.012993026872397461\n",
      "Total Loss: 0.3126232177019119\n",
      "------------------------------------ epoch 399 (2388 steps) ------------------------------------\n",
      "Max loss: 0.09463058412075043\n",
      "Min loss: 0.03720957413315773\n",
      "Mean loss: 0.05229809756080309\n",
      "Std loss: 0.019235850897130976\n",
      "Total Loss: 0.3137885853648186\n",
      "------------------------------------ epoch 400 (2394 steps) ------------------------------------\n",
      "Max loss: 0.064395472407341\n",
      "Min loss: 0.02807135134935379\n",
      "Mean loss: 0.046531438206632934\n",
      "Std loss: 0.014977887880129591\n",
      "Total Loss: 0.2791886292397976\n",
      "------------------------------------ epoch 401 (2400 steps) ------------------------------------\n",
      "Max loss: 0.07688641548156738\n",
      "Min loss: 0.03340322896838188\n",
      "Mean loss: 0.04870262866218885\n",
      "Std loss: 0.016259424549398216\n",
      "Total Loss: 0.2922157719731331\n",
      "saved model at ./weights/model_401.pth\n",
      "------------------------------------ epoch 402 (2406 steps) ------------------------------------\n",
      "Max loss: 0.08666118234395981\n",
      "Min loss: 0.03655995428562164\n",
      "Mean loss: 0.05946583300828934\n",
      "Std loss: 0.01970855383039597\n",
      "Total Loss: 0.356794998049736\n",
      "------------------------------------ epoch 403 (2412 steps) ------------------------------------\n",
      "Max loss: 0.10607969015836716\n",
      "Min loss: 0.05178634449839592\n",
      "Mean loss: 0.06514131339887778\n",
      "Std loss: 0.018958554811193545\n",
      "Total Loss: 0.3908478803932667\n",
      "------------------------------------ epoch 404 (2418 steps) ------------------------------------\n",
      "Max loss: 0.07314641773700714\n",
      "Min loss: 0.04351246356964111\n",
      "Mean loss: 0.06211357253293196\n",
      "Std loss: 0.012755424704588438\n",
      "Total Loss: 0.3726814351975918\n",
      "------------------------------------ epoch 405 (2424 steps) ------------------------------------\n",
      "Max loss: 0.0727742537856102\n",
      "Min loss: 0.039245087653398514\n",
      "Mean loss: 0.05301470806201299\n",
      "Std loss: 0.011422428809409921\n",
      "Total Loss: 0.31808824837207794\n",
      "------------------------------------ epoch 406 (2430 steps) ------------------------------------\n",
      "Max loss: 0.10970461368560791\n",
      "Min loss: 0.04238051921129227\n",
      "Mean loss: 0.07977437848846118\n",
      "Std loss: 0.023891044127622846\n",
      "Total Loss: 0.47864627093076706\n",
      "------------------------------------ epoch 407 (2436 steps) ------------------------------------\n",
      "Max loss: 0.08481281250715256\n",
      "Min loss: 0.038225483149290085\n",
      "Mean loss: 0.054374875500798225\n",
      "Std loss: 0.016159037834743643\n",
      "Total Loss: 0.32624925300478935\n",
      "------------------------------------ epoch 408 (2442 steps) ------------------------------------\n",
      "Max loss: 0.11545878648757935\n",
      "Min loss: 0.03777812421321869\n",
      "Mean loss: 0.05921213080485662\n",
      "Std loss: 0.02569703431032081\n",
      "Total Loss: 0.3552727848291397\n",
      "------------------------------------ epoch 409 (2448 steps) ------------------------------------\n",
      "Max loss: 0.06636135280132294\n",
      "Min loss: 0.030463039875030518\n",
      "Mean loss: 0.04687446914613247\n",
      "Std loss: 0.011769453784448705\n",
      "Total Loss: 0.2812468148767948\n",
      "------------------------------------ epoch 410 (2454 steps) ------------------------------------\n",
      "Max loss: 0.08186028897762299\n",
      "Min loss: 0.04244309291243553\n",
      "Mean loss: 0.05670497256020705\n",
      "Std loss: 0.013860030848683142\n",
      "Total Loss: 0.3402298353612423\n",
      "------------------------------------ epoch 411 (2460 steps) ------------------------------------\n",
      "Max loss: 0.05369642376899719\n",
      "Min loss: 0.04882574826478958\n",
      "Mean loss: 0.050864268094301224\n",
      "Std loss: 0.001721272835107751\n",
      "Total Loss: 0.30518560856580734\n",
      "------------------------------------ epoch 412 (2466 steps) ------------------------------------\n",
      "Max loss: 0.06953057646751404\n",
      "Min loss: 0.03506704419851303\n",
      "Mean loss: 0.051265438397725425\n",
      "Std loss: 0.013350761269907013\n",
      "Total Loss: 0.30759263038635254\n",
      "------------------------------------ epoch 413 (2472 steps) ------------------------------------\n",
      "Max loss: 0.09242868423461914\n",
      "Min loss: 0.030628569424152374\n",
      "Mean loss: 0.06063980360825857\n",
      "Std loss: 0.023241537313038907\n",
      "Total Loss: 0.3638388216495514\n",
      "------------------------------------ epoch 414 (2478 steps) ------------------------------------\n",
      "Max loss: 0.05794186890125275\n",
      "Min loss: 0.036470457911491394\n",
      "Mean loss: 0.05131772222618262\n",
      "Std loss: 0.0071256303444454305\n",
      "Total Loss: 0.3079063333570957\n",
      "------------------------------------ epoch 415 (2484 steps) ------------------------------------\n",
      "Max loss: 0.07191386073827744\n",
      "Min loss: 0.044555675238370895\n",
      "Mean loss: 0.05807895896335443\n",
      "Std loss: 0.010027543006419769\n",
      "Total Loss: 0.34847375378012657\n",
      "------------------------------------ epoch 416 (2490 steps) ------------------------------------\n",
      "Max loss: 0.10759960860013962\n",
      "Min loss: 0.035718150436878204\n",
      "Mean loss: 0.06751076566676299\n",
      "Std loss: 0.02665744034100894\n",
      "Total Loss: 0.4050645940005779\n",
      "------------------------------------ epoch 417 (2496 steps) ------------------------------------\n",
      "Max loss: 0.09098988771438599\n",
      "Min loss: 0.04167400300502777\n",
      "Mean loss: 0.05914433921376864\n",
      "Std loss: 0.016363794804298424\n",
      "Total Loss: 0.35486603528261185\n",
      "------------------------------------ epoch 418 (2502 steps) ------------------------------------\n",
      "Max loss: 0.09358759224414825\n",
      "Min loss: 0.026202954351902008\n",
      "Mean loss: 0.05403872517247995\n",
      "Std loss: 0.020485250167012997\n",
      "Total Loss: 0.3242323510348797\n",
      "------------------------------------ epoch 419 (2508 steps) ------------------------------------\n",
      "Max loss: 0.07998327910900116\n",
      "Min loss: 0.03446315973997116\n",
      "Mean loss: 0.056710963447888695\n",
      "Std loss: 0.01571133045217081\n",
      "Total Loss: 0.34026578068733215\n",
      "------------------------------------ epoch 420 (2514 steps) ------------------------------------\n",
      "Max loss: 0.12785957753658295\n",
      "Min loss: 0.042896077036857605\n",
      "Mean loss: 0.06493430025875568\n",
      "Std loss: 0.029713337862245916\n",
      "Total Loss: 0.3896058015525341\n",
      "------------------------------------ epoch 421 (2520 steps) ------------------------------------\n",
      "Max loss: 0.08756934106349945\n",
      "Min loss: 0.03743970766663551\n",
      "Mean loss: 0.050885076324145\n",
      "Std loss: 0.01699102085195413\n",
      "Total Loss: 0.30531045794487\n",
      "------------------------------------ epoch 422 (2526 steps) ------------------------------------\n",
      "Max loss: 0.06683385372161865\n",
      "Min loss: 0.027503248304128647\n",
      "Mean loss: 0.04377262977262338\n",
      "Std loss: 0.012393765808616501\n",
      "Total Loss: 0.2626357786357403\n",
      "------------------------------------ epoch 423 (2532 steps) ------------------------------------\n",
      "Max loss: 0.06622259318828583\n",
      "Min loss: 0.03888826072216034\n",
      "Mean loss: 0.05006949231028557\n",
      "Std loss: 0.011252217089687313\n",
      "Total Loss: 0.3004169538617134\n",
      "------------------------------------ epoch 424 (2538 steps) ------------------------------------\n",
      "Max loss: 0.08297751843929291\n",
      "Min loss: 0.023957809433341026\n",
      "Mean loss: 0.05095062187562386\n",
      "Std loss: 0.019062970450146745\n",
      "Total Loss: 0.30570373125374317\n",
      "------------------------------------ epoch 425 (2544 steps) ------------------------------------\n",
      "Max loss: 0.11407528817653656\n",
      "Min loss: 0.032617997378110886\n",
      "Mean loss: 0.06300845680137475\n",
      "Std loss: 0.025508234259805675\n",
      "Total Loss: 0.3780507408082485\n",
      "------------------------------------ epoch 426 (2550 steps) ------------------------------------\n",
      "Max loss: 0.08375101536512375\n",
      "Min loss: 0.0406351238489151\n",
      "Mean loss: 0.05922067227462927\n",
      "Std loss: 0.017100094383790523\n",
      "Total Loss: 0.35532403364777565\n",
      "------------------------------------ epoch 427 (2556 steps) ------------------------------------\n",
      "Max loss: 0.09994738548994064\n",
      "Min loss: 0.03129112347960472\n",
      "Mean loss: 0.050981332237521805\n",
      "Std loss: 0.02364069304473827\n",
      "Total Loss: 0.30588799342513084\n",
      "------------------------------------ epoch 428 (2562 steps) ------------------------------------\n",
      "Max loss: 0.07963711023330688\n",
      "Min loss: 0.0491744689643383\n",
      "Mean loss: 0.06211434925595919\n",
      "Std loss: 0.010233565498711737\n",
      "Total Loss: 0.37268609553575516\n",
      "------------------------------------ epoch 429 (2568 steps) ------------------------------------\n",
      "Max loss: 0.0696556568145752\n",
      "Min loss: 0.02813621237874031\n",
      "Mean loss: 0.043317592392365135\n",
      "Std loss: 0.013130229199085604\n",
      "Total Loss: 0.2599055543541908\n",
      "------------------------------------ epoch 430 (2574 steps) ------------------------------------\n",
      "Max loss: 0.12392166256904602\n",
      "Min loss: 0.034973062574863434\n",
      "Mean loss: 0.0626459289342165\n",
      "Std loss: 0.03189369875858356\n",
      "Total Loss: 0.375875573605299\n",
      "------------------------------------ epoch 431 (2580 steps) ------------------------------------\n",
      "Max loss: 0.06837745010852814\n",
      "Min loss: 0.029678575694561005\n",
      "Mean loss: 0.04379760039349397\n",
      "Std loss: 0.01212977274127189\n",
      "Total Loss: 0.2627856023609638\n",
      "------------------------------------ epoch 432 (2586 steps) ------------------------------------\n",
      "Max loss: 0.07499128580093384\n",
      "Min loss: 0.02415158599615097\n",
      "Mean loss: 0.048395562916994095\n",
      "Std loss: 0.016984703648665268\n",
      "Total Loss: 0.29037337750196457\n",
      "------------------------------------ epoch 433 (2592 steps) ------------------------------------\n",
      "Max loss: 0.06633555889129639\n",
      "Min loss: 0.03197326883673668\n",
      "Mean loss: 0.0458552942921718\n",
      "Std loss: 0.011294774045896907\n",
      "Total Loss: 0.2751317657530308\n",
      "------------------------------------ epoch 434 (2598 steps) ------------------------------------\n",
      "Max loss: 0.06453868001699448\n",
      "Min loss: 0.0336952731013298\n",
      "Mean loss: 0.04559815737108389\n",
      "Std loss: 0.010428552826073311\n",
      "Total Loss: 0.27358894422650337\n",
      "------------------------------------ epoch 435 (2604 steps) ------------------------------------\n",
      "Max loss: 0.05087965354323387\n",
      "Min loss: 0.026192203164100647\n",
      "Mean loss: 0.04090429035325845\n",
      "Std loss: 0.007624557953070841\n",
      "Total Loss: 0.2454257421195507\n",
      "------------------------------------ epoch 436 (2610 steps) ------------------------------------\n",
      "Max loss: 0.05108501762151718\n",
      "Min loss: 0.035061948001384735\n",
      "Mean loss: 0.04119469287494818\n",
      "Std loss: 0.005993038445089269\n",
      "Total Loss: 0.2471681572496891\n",
      "------------------------------------ epoch 437 (2616 steps) ------------------------------------\n",
      "Max loss: 0.06772857904434204\n",
      "Min loss: 0.027470527216792107\n",
      "Mean loss: 0.040274049155414104\n",
      "Std loss: 0.013613238611311902\n",
      "Total Loss: 0.24164429493248463\n",
      "------------------------------------ epoch 438 (2622 steps) ------------------------------------\n",
      "Max loss: 0.04086586833000183\n",
      "Min loss: 0.035180360078811646\n",
      "Mean loss: 0.0389924750973781\n",
      "Std loss: 0.0018359193728960577\n",
      "Total Loss: 0.23395485058426857\n",
      "------------------------------------ epoch 439 (2628 steps) ------------------------------------\n",
      "Max loss: 0.0743468850851059\n",
      "Min loss: 0.03822857886552811\n",
      "Mean loss: 0.052928260217110314\n",
      "Std loss: 0.014970298940985956\n",
      "Total Loss: 0.3175695613026619\n",
      "------------------------------------ epoch 440 (2634 steps) ------------------------------------\n",
      "Max loss: 0.08034086227416992\n",
      "Min loss: 0.03428390249609947\n",
      "Mean loss: 0.052050858115156494\n",
      "Std loss: 0.017806105448422028\n",
      "Total Loss: 0.31230514869093895\n",
      "------------------------------------ epoch 441 (2640 steps) ------------------------------------\n",
      "Max loss: 0.08029665797948837\n",
      "Min loss: 0.037611041218042374\n",
      "Mean loss: 0.052955991898973785\n",
      "Std loss: 0.01767125565967754\n",
      "Total Loss: 0.3177359513938427\n",
      "------------------------------------ epoch 442 (2646 steps) ------------------------------------\n",
      "Max loss: 0.29042574763298035\n",
      "Min loss: 0.06462597846984863\n",
      "Mean loss: 0.135004044820865\n",
      "Std loss: 0.07947966017846333\n",
      "Total Loss: 0.81002426892519\n",
      "------------------------------------ epoch 443 (2652 steps) ------------------------------------\n",
      "Max loss: 0.9891473650932312\n",
      "Min loss: 0.16481468081474304\n",
      "Mean loss: 0.5744459430376688\n",
      "Std loss: 0.2797293922643884\n",
      "Total Loss: 3.446675658226013\n",
      "------------------------------------ epoch 444 (2658 steps) ------------------------------------\n",
      "Max loss: 1.1328080892562866\n",
      "Min loss: 1.005847692489624\n",
      "Mean loss: 1.0610915025075276\n",
      "Std loss: 0.042022287548295366\n",
      "Total Loss: 6.366549015045166\n",
      "------------------------------------ epoch 445 (2664 steps) ------------------------------------\n",
      "Max loss: 1.0748122930526733\n",
      "Min loss: 1.0316123962402344\n",
      "Mean loss: 1.0460058649381\n",
      "Std loss: 0.014227174798651173\n",
      "Total Loss: 6.276035189628601\n",
      "------------------------------------ epoch 446 (2670 steps) ------------------------------------\n",
      "Max loss: 1.025614619255066\n",
      "Min loss: 1.0068418979644775\n",
      "Mean loss: 1.018120805422465\n",
      "Std loss: 0.006086256049888471\n",
      "Total Loss: 6.10872483253479\n",
      "------------------------------------ epoch 447 (2676 steps) ------------------------------------\n",
      "Max loss: 1.0107661485671997\n",
      "Min loss: 0.9990338087081909\n",
      "Mean loss: 1.0057931939760845\n",
      "Std loss: 0.0042976882826554486\n",
      "Total Loss: 6.034759163856506\n",
      "------------------------------------ epoch 448 (2682 steps) ------------------------------------\n",
      "Max loss: 1.0071358680725098\n",
      "Min loss: 0.9918949007987976\n",
      "Mean loss: 1.0006667872269948\n",
      "Std loss: 0.005660693800123729\n",
      "Total Loss: 6.004000723361969\n",
      "------------------------------------ epoch 449 (2688 steps) ------------------------------------\n",
      "Max loss: 0.998110294342041\n",
      "Min loss: 0.9832195043563843\n",
      "Mean loss: 0.9916977882385254\n",
      "Std loss: 0.005971703376599025\n",
      "Total Loss: 5.950186729431152\n",
      "------------------------------------ epoch 450 (2694 steps) ------------------------------------\n",
      "Max loss: 1.0344213247299194\n",
      "Min loss: 0.9830542802810669\n",
      "Mean loss: 1.0007123251756032\n",
      "Std loss: 0.016314400769274782\n",
      "Total Loss: 6.004273951053619\n",
      "------------------------------------ epoch 451 (2700 steps) ------------------------------------\n",
      "Max loss: 0.9996250867843628\n",
      "Min loss: 0.9946236610412598\n",
      "Mean loss: 0.9976380268732706\n",
      "Std loss: 0.0017868858164538605\n",
      "Total Loss: 5.985828161239624\n",
      "------------------------------------ epoch 452 (2706 steps) ------------------------------------\n",
      "Max loss: 1.0008691549301147\n",
      "Min loss: 0.9932241439819336\n",
      "Mean loss: 0.9970234533150991\n",
      "Std loss: 0.0025809557646929437\n",
      "Total Loss: 5.9821407198905945\n",
      "------------------------------------ epoch 453 (2712 steps) ------------------------------------\n",
      "Max loss: 0.9983420372009277\n",
      "Min loss: 0.9918333292007446\n",
      "Mean loss: 0.9949163794517517\n",
      "Std loss: 0.0019717430374155166\n",
      "Total Loss: 5.96949827671051\n",
      "------------------------------------ epoch 454 (2718 steps) ------------------------------------\n",
      "Max loss: 1.000335454940796\n",
      "Min loss: 0.9938364028930664\n",
      "Mean loss: 0.997778981924057\n",
      "Std loss: 0.0019941085983827826\n",
      "Total Loss: 5.986673891544342\n",
      "------------------------------------ epoch 455 (2724 steps) ------------------------------------\n",
      "Max loss: 1.0047008991241455\n",
      "Min loss: 0.9931914806365967\n",
      "Mean loss: 0.9987837374210358\n",
      "Std loss: 0.004048131465155632\n",
      "Total Loss: 5.992702424526215\n",
      "------------------------------------ epoch 456 (2730 steps) ------------------------------------\n",
      "Max loss: 1.0015019178390503\n",
      "Min loss: 0.993614137172699\n",
      "Mean loss: 0.9972215890884399\n",
      "Std loss: 0.0024014844687516262\n",
      "Total Loss: 5.98332953453064\n",
      "------------------------------------ epoch 457 (2736 steps) ------------------------------------\n",
      "Max loss: 0.9965296983718872\n",
      "Min loss: 0.9901301860809326\n",
      "Mean loss: 0.9929763674736023\n",
      "Std loss: 0.0022205896181170756\n",
      "Total Loss: 5.957858204841614\n",
      "------------------------------------ epoch 458 (2742 steps) ------------------------------------\n",
      "Max loss: 0.9936841726303101\n",
      "Min loss: 0.986985445022583\n",
      "Mean loss: 0.9895250201225281\n",
      "Std loss: 0.002040193134474273\n",
      "Total Loss: 5.9371501207351685\n",
      "------------------------------------ epoch 459 (2748 steps) ------------------------------------\n",
      "Max loss: 0.9947338104248047\n",
      "Min loss: 0.9791814684867859\n",
      "Mean loss: 0.9858664671579996\n",
      "Std loss: 0.005149788284293998\n",
      "Total Loss: 5.915198802947998\n",
      "------------------------------------ epoch 460 (2754 steps) ------------------------------------\n",
      "Max loss: 0.983069896697998\n",
      "Min loss: 0.9577484130859375\n",
      "Mean loss: 0.970238188902537\n",
      "Std loss: 0.009312425653846105\n",
      "Total Loss: 5.821429133415222\n",
      "------------------------------------ epoch 461 (2760 steps) ------------------------------------\n",
      "Max loss: 0.9810808897018433\n",
      "Min loss: 0.913984477519989\n",
      "Mean loss: 0.9529452323913574\n",
      "Std loss: 0.02334148271469257\n",
      "Total Loss: 5.7176713943481445\n",
      "------------------------------------ epoch 462 (2766 steps) ------------------------------------\n",
      "Max loss: 0.9168359041213989\n",
      "Min loss: 0.8629767298698425\n",
      "Mean loss: 0.8926183382670084\n",
      "Std loss: 0.020256671036536\n",
      "Total Loss: 5.355710029602051\n",
      "------------------------------------ epoch 463 (2772 steps) ------------------------------------\n",
      "Max loss: 0.9126483798027039\n",
      "Min loss: 0.8390552401542664\n",
      "Mean loss: 0.8804222842057546\n",
      "Std loss: 0.027693435481802153\n",
      "Total Loss: 5.282533705234528\n",
      "------------------------------------ epoch 464 (2778 steps) ------------------------------------\n",
      "Max loss: 0.8327971696853638\n",
      "Min loss: 0.8121339082717896\n",
      "Mean loss: 0.8223794003327688\n",
      "Std loss: 0.007467562855617753\n",
      "Total Loss: 4.9342764019966125\n",
      "------------------------------------ epoch 465 (2784 steps) ------------------------------------\n",
      "Max loss: 0.7860928773880005\n",
      "Min loss: 0.7354269027709961\n",
      "Mean loss: 0.7672581672668457\n",
      "Std loss: 0.02197446882041136\n",
      "Total Loss: 4.603549003601074\n",
      "------------------------------------ epoch 466 (2790 steps) ------------------------------------\n",
      "Max loss: 0.8698136210441589\n",
      "Min loss: 0.6862322688102722\n",
      "Mean loss: 0.7849894265333811\n",
      "Std loss: 0.07778999102448578\n",
      "Total Loss: 4.709936559200287\n",
      "------------------------------------ epoch 467 (2796 steps) ------------------------------------\n",
      "Max loss: 0.8164924383163452\n",
      "Min loss: 0.6648398041725159\n",
      "Mean loss: 0.7200721700986227\n",
      "Std loss: 0.05631638656152097\n",
      "Total Loss: 4.320433020591736\n",
      "------------------------------------ epoch 468 (2802 steps) ------------------------------------\n",
      "Max loss: 0.6607795357704163\n",
      "Min loss: 0.5591680407524109\n",
      "Mean loss: 0.6065617303053538\n",
      "Std loss: 0.03612626161520382\n",
      "Total Loss: 3.639370381832123\n",
      "------------------------------------ epoch 469 (2808 steps) ------------------------------------\n",
      "Max loss: 0.6702258586883545\n",
      "Min loss: 0.5738798379898071\n",
      "Mean loss: 0.6034482717514038\n",
      "Std loss: 0.03288616835183061\n",
      "Total Loss: 3.620689630508423\n",
      "------------------------------------ epoch 470 (2814 steps) ------------------------------------\n",
      "Max loss: 0.5516630411148071\n",
      "Min loss: 0.46912068128585815\n",
      "Mean loss: 0.5153807699680328\n",
      "Std loss: 0.02504212272606705\n",
      "Total Loss: 3.092284619808197\n",
      "------------------------------------ epoch 471 (2820 steps) ------------------------------------\n",
      "Max loss: 0.49720725417137146\n",
      "Min loss: 0.41736385226249695\n",
      "Mean loss: 0.4505081425110499\n",
      "Std loss: 0.027660964877026063\n",
      "Total Loss: 2.7030488550662994\n",
      "------------------------------------ epoch 472 (2826 steps) ------------------------------------\n",
      "Max loss: 0.4515363872051239\n",
      "Min loss: 0.4068656861782074\n",
      "Mean loss: 0.4311094135046005\n",
      "Std loss: 0.01652582247319631\n",
      "Total Loss: 2.586656481027603\n",
      "------------------------------------ epoch 473 (2832 steps) ------------------------------------\n",
      "Max loss: 0.4177014231681824\n",
      "Min loss: 0.3411023020744324\n",
      "Mean loss: 0.3617323438326518\n",
      "Std loss: 0.025655397093852214\n",
      "Total Loss: 2.1703940629959106\n",
      "------------------------------------ epoch 474 (2838 steps) ------------------------------------\n",
      "Max loss: 0.37155383825302124\n",
      "Min loss: 0.3253251314163208\n",
      "Mean loss: 0.34149521092573804\n",
      "Std loss: 0.016208149711337553\n",
      "Total Loss: 2.048971265554428\n",
      "------------------------------------ epoch 475 (2844 steps) ------------------------------------\n",
      "Max loss: 0.3149373531341553\n",
      "Min loss: 0.27960291504859924\n",
      "Mean loss: 0.2998257875442505\n",
      "Std loss: 0.013387849293119515\n",
      "Total Loss: 1.798954725265503\n",
      "------------------------------------ epoch 476 (2850 steps) ------------------------------------\n",
      "Max loss: 0.28415828943252563\n",
      "Min loss: 0.24964556097984314\n",
      "Mean loss: 0.2684546758731206\n",
      "Std loss: 0.011330796960086965\n",
      "Total Loss: 1.6107280552387238\n",
      "------------------------------------ epoch 477 (2856 steps) ------------------------------------\n",
      "Max loss: 0.3240382671356201\n",
      "Min loss: 0.24067974090576172\n",
      "Mean loss: 0.2713165233532588\n",
      "Std loss: 0.02743566541237943\n",
      "Total Loss: 1.6278991401195526\n",
      "------------------------------------ epoch 478 (2862 steps) ------------------------------------\n",
      "Max loss: 0.2927199602127075\n",
      "Min loss: 0.24160945415496826\n",
      "Mean loss: 0.2603411128123601\n",
      "Std loss: 0.018916704364840182\n",
      "Total Loss: 1.5620466768741608\n",
      "------------------------------------ epoch 479 (2868 steps) ------------------------------------\n",
      "Max loss: 0.26568207144737244\n",
      "Min loss: 0.2384144514799118\n",
      "Mean loss: 0.2469280684987704\n",
      "Std loss: 0.009024365289786871\n",
      "Total Loss: 1.4815684109926224\n",
      "------------------------------------ epoch 480 (2874 steps) ------------------------------------\n",
      "Max loss: 0.290723592042923\n",
      "Min loss: 0.20694105327129364\n",
      "Mean loss: 0.22968226422866186\n",
      "Std loss: 0.028355153503840456\n",
      "Total Loss: 1.3780935853719711\n",
      "------------------------------------ epoch 481 (2880 steps) ------------------------------------\n",
      "Max loss: 0.26100683212280273\n",
      "Min loss: 0.20697669684886932\n",
      "Mean loss: 0.23119538774092993\n",
      "Std loss: 0.019182207149061662\n",
      "Total Loss: 1.3871723264455795\n",
      "------------------------------------ epoch 482 (2886 steps) ------------------------------------\n",
      "Max loss: 0.26096874475479126\n",
      "Min loss: 0.18714243173599243\n",
      "Mean loss: 0.2210779810945193\n",
      "Std loss: 0.02293244816790443\n",
      "Total Loss: 1.3264678865671158\n",
      "------------------------------------ epoch 483 (2892 steps) ------------------------------------\n",
      "Max loss: 0.2235107719898224\n",
      "Min loss: 0.18332557380199432\n",
      "Mean loss: 0.20884142319361368\n",
      "Std loss: 0.012591050867261824\n",
      "Total Loss: 1.2530485391616821\n",
      "------------------------------------ epoch 484 (2898 steps) ------------------------------------\n",
      "Max loss: 0.25607678294181824\n",
      "Min loss: 0.17609903216362\n",
      "Mean loss: 0.20620761315027872\n",
      "Std loss: 0.029224005365885996\n",
      "Total Loss: 1.2372456789016724\n",
      "------------------------------------ epoch 485 (2904 steps) ------------------------------------\n",
      "Max loss: 0.2092042863368988\n",
      "Min loss: 0.1608937829732895\n",
      "Mean loss: 0.18032380441824594\n",
      "Std loss: 0.017226353952583137\n",
      "Total Loss: 1.0819428265094757\n",
      "------------------------------------ epoch 486 (2910 steps) ------------------------------------\n",
      "Max loss: 0.21147780120372772\n",
      "Min loss: 0.14312666654586792\n",
      "Mean loss: 0.18300881485144296\n",
      "Std loss: 0.021961695582029848\n",
      "Total Loss: 1.0980528891086578\n",
      "------------------------------------ epoch 487 (2916 steps) ------------------------------------\n",
      "Max loss: 0.17412123084068298\n",
      "Min loss: 0.14420288801193237\n",
      "Mean loss: 0.15885639687379202\n",
      "Std loss: 0.012625725311064326\n",
      "Total Loss: 0.9531383812427521\n",
      "------------------------------------ epoch 488 (2922 steps) ------------------------------------\n",
      "Max loss: 0.1830609291791916\n",
      "Min loss: 0.14851534366607666\n",
      "Mean loss: 0.16398726652065912\n",
      "Std loss: 0.013503662149526542\n",
      "Total Loss: 0.9839235991239548\n",
      "------------------------------------ epoch 489 (2928 steps) ------------------------------------\n",
      "Max loss: 0.2062036395072937\n",
      "Min loss: 0.1274976134300232\n",
      "Mean loss: 0.15995745857556662\n",
      "Std loss: 0.03127403760879399\n",
      "Total Loss: 0.9597447514533997\n",
      "------------------------------------ epoch 490 (2934 steps) ------------------------------------\n",
      "Max loss: 0.1868331879377365\n",
      "Min loss: 0.11871873587369919\n",
      "Mean loss: 0.13952761267622313\n",
      "Std loss: 0.023053194763090604\n",
      "Total Loss: 0.8371656760573387\n",
      "------------------------------------ epoch 491 (2940 steps) ------------------------------------\n",
      "Max loss: 0.14738577604293823\n",
      "Min loss: 0.113287054002285\n",
      "Mean loss: 0.13644810890158018\n",
      "Std loss: 0.012888907457588483\n",
      "Total Loss: 0.818688653409481\n",
      "------------------------------------ epoch 492 (2946 steps) ------------------------------------\n",
      "Max loss: 0.14054471254348755\n",
      "Min loss: 0.11195781826972961\n",
      "Mean loss: 0.12287810693184535\n",
      "Std loss: 0.009385449416368711\n",
      "Total Loss: 0.7372686415910721\n",
      "------------------------------------ epoch 493 (2952 steps) ------------------------------------\n",
      "Max loss: 0.15530657768249512\n",
      "Min loss: 0.12258569896221161\n",
      "Mean loss: 0.13635715345541635\n",
      "Std loss: 0.011362728327675372\n",
      "Total Loss: 0.8181429207324982\n",
      "------------------------------------ epoch 494 (2958 steps) ------------------------------------\n",
      "Max loss: 0.14600971341133118\n",
      "Min loss: 0.09231066703796387\n",
      "Mean loss: 0.12820812066396078\n",
      "Std loss: 0.017340648598180904\n",
      "Total Loss: 0.7692487239837646\n",
      "------------------------------------ epoch 495 (2964 steps) ------------------------------------\n",
      "Max loss: 0.14700840413570404\n",
      "Min loss: 0.10081379860639572\n",
      "Mean loss: 0.13349530970056853\n",
      "Std loss: 0.01639927692731086\n",
      "Total Loss: 0.8009718582034111\n",
      "------------------------------------ epoch 496 (2970 steps) ------------------------------------\n",
      "Max loss: 0.11866860091686249\n",
      "Min loss: 0.09288370609283447\n",
      "Mean loss: 0.10329179093241692\n",
      "Std loss: 0.009381007297289821\n",
      "Total Loss: 0.6197507455945015\n",
      "------------------------------------ epoch 497 (2976 steps) ------------------------------------\n",
      "Max loss: 0.16011318564414978\n",
      "Min loss: 0.08637438714504242\n",
      "Mean loss: 0.11920203889409701\n",
      "Std loss: 0.023188420267292585\n",
      "Total Loss: 0.7152122333645821\n",
      "------------------------------------ epoch 498 (2982 steps) ------------------------------------\n",
      "Max loss: 0.13596829771995544\n",
      "Min loss: 0.10098974406719208\n",
      "Mean loss: 0.11397782961527507\n",
      "Std loss: 0.011659508605850792\n",
      "Total Loss: 0.6838669776916504\n",
      "------------------------------------ epoch 499 (2988 steps) ------------------------------------\n",
      "Max loss: 0.16556507349014282\n",
      "Min loss: 0.08854570239782333\n",
      "Mean loss: 0.12016455456614494\n",
      "Std loss: 0.02487302109257737\n",
      "Total Loss: 0.7209873273968697\n",
      "------------------------------------ epoch 500 (2994 steps) ------------------------------------\n",
      "Max loss: 0.12218020856380463\n",
      "Min loss: 0.07902076840400696\n",
      "Mean loss: 0.10337497914830844\n",
      "Std loss: 0.013687183686819952\n",
      "Total Loss: 0.6202498748898506\n",
      "------------------------------------ epoch 501 (3000 steps) ------------------------------------\n",
      "Max loss: 0.14307095110416412\n",
      "Min loss: 0.08432863652706146\n",
      "Mean loss: 0.11263298243284225\n",
      "Std loss: 0.01956832626752891\n",
      "Total Loss: 0.6757978945970535\n",
      "saved model at ./weights/model_501.pth\n",
      "------------------------------------ epoch 502 (3006 steps) ------------------------------------\n",
      "Max loss: 0.11509033292531967\n",
      "Min loss: 0.08695028722286224\n",
      "Mean loss: 0.10255432377258937\n",
      "Std loss: 0.009151416914639339\n",
      "Total Loss: 0.6153259426355362\n",
      "------------------------------------ epoch 503 (3012 steps) ------------------------------------\n",
      "Max loss: 0.18476776778697968\n",
      "Min loss: 0.07503393292427063\n",
      "Mean loss: 0.10959514478842418\n",
      "Std loss: 0.04187157861142022\n",
      "Total Loss: 0.657570868730545\n",
      "------------------------------------ epoch 504 (3018 steps) ------------------------------------\n",
      "Max loss: 0.15254592895507812\n",
      "Min loss: 0.07840234041213989\n",
      "Mean loss: 0.0995326650639375\n",
      "Std loss: 0.025656055579703838\n",
      "Total Loss: 0.597195990383625\n",
      "------------------------------------ epoch 505 (3024 steps) ------------------------------------\n",
      "Max loss: 0.12451037019491196\n",
      "Min loss: 0.08169995993375778\n",
      "Mean loss: 0.09841050580143929\n",
      "Std loss: 0.014355830178673673\n",
      "Total Loss: 0.5904630348086357\n",
      "------------------------------------ epoch 506 (3030 steps) ------------------------------------\n",
      "Max loss: 0.13368871808052063\n",
      "Min loss: 0.08651557564735413\n",
      "Mean loss: 0.1070785050590833\n",
      "Std loss: 0.01714947153068987\n",
      "Total Loss: 0.6424710303544998\n",
      "------------------------------------ epoch 507 (3036 steps) ------------------------------------\n",
      "Max loss: 0.11158531904220581\n",
      "Min loss: 0.06915070116519928\n",
      "Mean loss: 0.09355615948637326\n",
      "Std loss: 0.013880987382360054\n",
      "Total Loss: 0.5613369569182396\n",
      "------------------------------------ epoch 508 (3042 steps) ------------------------------------\n",
      "Max loss: 0.1316853165626526\n",
      "Min loss: 0.08366440236568451\n",
      "Mean loss: 0.09833978364864986\n",
      "Std loss: 0.018963159684836\n",
      "Total Loss: 0.5900387018918991\n",
      "------------------------------------ epoch 509 (3048 steps) ------------------------------------\n",
      "Max loss: 0.13255193829536438\n",
      "Min loss: 0.07604050636291504\n",
      "Mean loss: 0.09844456861416499\n",
      "Std loss: 0.019873296711981472\n",
      "Total Loss: 0.5906674116849899\n",
      "------------------------------------ epoch 510 (3054 steps) ------------------------------------\n",
      "Max loss: 0.10432472079992294\n",
      "Min loss: 0.08195384591817856\n",
      "Mean loss: 0.0963335062066714\n",
      "Std loss: 0.0076698810380643\n",
      "Total Loss: 0.5780010372400284\n",
      "------------------------------------ epoch 511 (3060 steps) ------------------------------------\n",
      "Max loss: 0.12074070423841476\n",
      "Min loss: 0.08588796854019165\n",
      "Mean loss: 0.09848449255029361\n",
      "Std loss: 0.01182248339297172\n",
      "Total Loss: 0.5909069553017616\n",
      "------------------------------------ epoch 512 (3066 steps) ------------------------------------\n",
      "Max loss: 0.1261748969554901\n",
      "Min loss: 0.060109883546829224\n",
      "Mean loss: 0.09912861262758572\n",
      "Std loss: 0.023783951769700912\n",
      "Total Loss: 0.5947716757655144\n",
      "------------------------------------ epoch 513 (3072 steps) ------------------------------------\n",
      "Max loss: 0.10453969240188599\n",
      "Min loss: 0.07576900720596313\n",
      "Mean loss: 0.09158666059374809\n",
      "Std loss: 0.010304631489685407\n",
      "Total Loss: 0.5495199635624886\n",
      "------------------------------------ epoch 514 (3078 steps) ------------------------------------\n",
      "Max loss: 0.130179762840271\n",
      "Min loss: 0.07054531574249268\n",
      "Mean loss: 0.08903250719110171\n",
      "Std loss: 0.021646262520274868\n",
      "Total Loss: 0.5341950431466103\n",
      "------------------------------------ epoch 515 (3084 steps) ------------------------------------\n",
      "Max loss: 0.160004124045372\n",
      "Min loss: 0.06268927454948425\n",
      "Mean loss: 0.09973116715749104\n",
      "Std loss: 0.034384251032589734\n",
      "Total Loss: 0.5983870029449463\n",
      "------------------------------------ epoch 516 (3090 steps) ------------------------------------\n",
      "Max loss: 0.10466927289962769\n",
      "Min loss: 0.07835473865270615\n",
      "Mean loss: 0.08569401626785596\n",
      "Std loss: 0.008843907976422663\n",
      "Total Loss: 0.5141640976071358\n",
      "------------------------------------ epoch 517 (3096 steps) ------------------------------------\n",
      "Max loss: 0.11529417335987091\n",
      "Min loss: 0.06103391945362091\n",
      "Mean loss: 0.08764144281546275\n",
      "Std loss: 0.016613785916539302\n",
      "Total Loss: 0.5258486568927765\n",
      "------------------------------------ epoch 518 (3102 steps) ------------------------------------\n",
      "Max loss: 0.11067366600036621\n",
      "Min loss: 0.06188663840293884\n",
      "Mean loss: 0.0849996991455555\n",
      "Std loss: 0.016712460001409985\n",
      "Total Loss: 0.509998194873333\n",
      "------------------------------------ epoch 519 (3108 steps) ------------------------------------\n",
      "Max loss: 0.13636699318885803\n",
      "Min loss: 0.07534700632095337\n",
      "Mean loss: 0.09302229434251785\n",
      "Std loss: 0.02153369331844808\n",
      "Total Loss: 0.5581337660551071\n",
      "------------------------------------ epoch 520 (3114 steps) ------------------------------------\n",
      "Max loss: 0.09590411186218262\n",
      "Min loss: 0.06832561641931534\n",
      "Mean loss: 0.08436055605610211\n",
      "Std loss: 0.01110753266042725\n",
      "Total Loss: 0.5061633363366127\n",
      "------------------------------------ epoch 521 (3120 steps) ------------------------------------\n",
      "Max loss: 0.1358988732099533\n",
      "Min loss: 0.07624617964029312\n",
      "Mean loss: 0.10639769956469536\n",
      "Std loss: 0.023147631750397964\n",
      "Total Loss: 0.6383861973881721\n",
      "------------------------------------ epoch 522 (3126 steps) ------------------------------------\n",
      "Max loss: 0.08966070413589478\n",
      "Min loss: 0.06473580747842789\n",
      "Mean loss: 0.07483305657903354\n",
      "Std loss: 0.00870842399178468\n",
      "Total Loss: 0.4489983394742012\n",
      "------------------------------------ epoch 523 (3132 steps) ------------------------------------\n",
      "Max loss: 0.11031156033277512\n",
      "Min loss: 0.07050162553787231\n",
      "Mean loss: 0.08542250096797943\n",
      "Std loss: 0.016599702202369903\n",
      "Total Loss: 0.5125350058078766\n",
      "------------------------------------ epoch 524 (3138 steps) ------------------------------------\n",
      "Max loss: 0.1291864812374115\n",
      "Min loss: 0.08215557783842087\n",
      "Mean loss: 0.09838864579796791\n",
      "Std loss: 0.015167662358719176\n",
      "Total Loss: 0.5903318747878075\n",
      "------------------------------------ epoch 525 (3144 steps) ------------------------------------\n",
      "Max loss: 0.10318905115127563\n",
      "Min loss: 0.06201061233878136\n",
      "Mean loss: 0.08282650200029214\n",
      "Std loss: 0.015638720770176792\n",
      "Total Loss: 0.49695901200175285\n",
      "------------------------------------ epoch 526 (3150 steps) ------------------------------------\n",
      "Max loss: 0.11352746188640594\n",
      "Min loss: 0.05047063156962395\n",
      "Mean loss: 0.08715930270651977\n",
      "Std loss: 0.024210814375892172\n",
      "Total Loss: 0.5229558162391186\n",
      "------------------------------------ epoch 527 (3156 steps) ------------------------------------\n",
      "Max loss: 0.09730087220668793\n",
      "Min loss: 0.060568057000637054\n",
      "Mean loss: 0.08184790735443433\n",
      "Std loss: 0.010878383536588212\n",
      "Total Loss: 0.491087444126606\n",
      "------------------------------------ epoch 528 (3162 steps) ------------------------------------\n",
      "Max loss: 0.10840296000242233\n",
      "Min loss: 0.06385403871536255\n",
      "Mean loss: 0.07493559395273526\n",
      "Std loss: 0.015152332809740236\n",
      "Total Loss: 0.4496135637164116\n",
      "------------------------------------ epoch 529 (3168 steps) ------------------------------------\n",
      "Max loss: 0.07566311955451965\n",
      "Min loss: 0.050707243382930756\n",
      "Mean loss: 0.06445758913954099\n",
      "Std loss: 0.008278055062110051\n",
      "Total Loss: 0.38674553483724594\n",
      "------------------------------------ epoch 530 (3174 steps) ------------------------------------\n",
      "Max loss: 0.09551498293876648\n",
      "Min loss: 0.051172807812690735\n",
      "Mean loss: 0.06490803696215153\n",
      "Std loss: 0.015241792883392865\n",
      "Total Loss: 0.38944822177290916\n",
      "------------------------------------ epoch 531 (3180 steps) ------------------------------------\n",
      "Max loss: 0.10666575282812119\n",
      "Min loss: 0.04665891081094742\n",
      "Mean loss: 0.08011505318184693\n",
      "Std loss: 0.024019245590836857\n",
      "Total Loss: 0.4806903190910816\n",
      "------------------------------------ epoch 532 (3186 steps) ------------------------------------\n",
      "Max loss: 0.1016770452260971\n",
      "Min loss: 0.053262799978256226\n",
      "Mean loss: 0.06896081132193406\n",
      "Std loss: 0.015622498712753168\n",
      "Total Loss: 0.4137648679316044\n",
      "------------------------------------ epoch 533 (3192 steps) ------------------------------------\n",
      "Max loss: 0.09544520080089569\n",
      "Min loss: 0.0569470189511776\n",
      "Mean loss: 0.07325285114347935\n",
      "Std loss: 0.013913716597922905\n",
      "Total Loss: 0.4395171068608761\n",
      "------------------------------------ epoch 534 (3198 steps) ------------------------------------\n",
      "Max loss: 0.11452212184667587\n",
      "Min loss: 0.06149783357977867\n",
      "Mean loss: 0.08834546245634556\n",
      "Std loss: 0.018889745524614737\n",
      "Total Loss: 0.5300727747380733\n",
      "------------------------------------ epoch 535 (3204 steps) ------------------------------------\n",
      "Max loss: 0.0824166014790535\n",
      "Min loss: 0.0482354611158371\n",
      "Mean loss: 0.06523967099686463\n",
      "Std loss: 0.011536074933495186\n",
      "Total Loss: 0.3914380259811878\n",
      "------------------------------------ epoch 536 (3210 steps) ------------------------------------\n",
      "Max loss: 0.11395793408155441\n",
      "Min loss: 0.04283877834677696\n",
      "Mean loss: 0.07910972461104393\n",
      "Std loss: 0.02394415715877894\n",
      "Total Loss: 0.4746583476662636\n",
      "------------------------------------ epoch 537 (3216 steps) ------------------------------------\n",
      "Max loss: 0.09977011382579803\n",
      "Min loss: 0.0510316900908947\n",
      "Mean loss: 0.06508610149224599\n",
      "Std loss: 0.01847738297167454\n",
      "Total Loss: 0.39051660895347595\n",
      "------------------------------------ epoch 538 (3222 steps) ------------------------------------\n",
      "Max loss: 0.12655559182167053\n",
      "Min loss: 0.041791655123233795\n",
      "Mean loss: 0.07264490177234013\n",
      "Std loss: 0.02670244231326829\n",
      "Total Loss: 0.43586941063404083\n",
      "------------------------------------ epoch 539 (3228 steps) ------------------------------------\n",
      "Max loss: 0.09698053449392319\n",
      "Min loss: 0.05666699260473251\n",
      "Mean loss: 0.06984187538425128\n",
      "Std loss: 0.013170466630297476\n",
      "Total Loss: 0.41905125230550766\n",
      "------------------------------------ epoch 540 (3234 steps) ------------------------------------\n",
      "Max loss: 0.12243398278951645\n",
      "Min loss: 0.046275194734334946\n",
      "Mean loss: 0.07282790293296178\n",
      "Std loss: 0.02392060552670767\n",
      "Total Loss: 0.4369674175977707\n",
      "------------------------------------ epoch 541 (3240 steps) ------------------------------------\n",
      "Max loss: 0.10063209384679794\n",
      "Min loss: 0.05054821819067001\n",
      "Mean loss: 0.07188616320490837\n",
      "Std loss: 0.015783607203614602\n",
      "Total Loss: 0.4313169792294502\n",
      "------------------------------------ epoch 542 (3246 steps) ------------------------------------\n",
      "Max loss: 0.10405321419239044\n",
      "Min loss: 0.051595866680145264\n",
      "Mean loss: 0.0668643048654\n",
      "Std loss: 0.017071469931863162\n",
      "Total Loss: 0.4011858291924\n",
      "------------------------------------ epoch 543 (3252 steps) ------------------------------------\n",
      "Max loss: 0.0636097714304924\n",
      "Min loss: 0.04488701373338699\n",
      "Mean loss: 0.053196278090278305\n",
      "Std loss: 0.008209504516950239\n",
      "Total Loss: 0.31917766854166985\n",
      "------------------------------------ epoch 544 (3258 steps) ------------------------------------\n",
      "Max loss: 0.0977415069937706\n",
      "Min loss: 0.043641433119773865\n",
      "Mean loss: 0.07174513364831607\n",
      "Std loss: 0.018485263782142075\n",
      "Total Loss: 0.4304708018898964\n",
      "------------------------------------ epoch 545 (3264 steps) ------------------------------------\n",
      "Max loss: 0.08752794563770294\n",
      "Min loss: 0.04951544106006622\n",
      "Mean loss: 0.06910740584135056\n",
      "Std loss: 0.014899976538219983\n",
      "Total Loss: 0.41464443504810333\n",
      "------------------------------------ epoch 546 (3270 steps) ------------------------------------\n",
      "Max loss: 0.09919297695159912\n",
      "Min loss: 0.06052607297897339\n",
      "Mean loss: 0.07203486561775208\n",
      "Std loss: 0.012981339841420641\n",
      "Total Loss: 0.43220919370651245\n",
      "------------------------------------ epoch 547 (3276 steps) ------------------------------------\n",
      "Max loss: 0.085560142993927\n",
      "Min loss: 0.052601732313632965\n",
      "Mean loss: 0.0621594674885273\n",
      "Std loss: 0.011437152049752075\n",
      "Total Loss: 0.3729568049311638\n",
      "------------------------------------ epoch 548 (3282 steps) ------------------------------------\n",
      "Max loss: 0.09448570013046265\n",
      "Min loss: 0.051532596349716187\n",
      "Mean loss: 0.0690576018144687\n",
      "Std loss: 0.01540233411862385\n",
      "Total Loss: 0.4143456108868122\n",
      "------------------------------------ epoch 549 (3288 steps) ------------------------------------\n",
      "Max loss: 0.09344299137592316\n",
      "Min loss: 0.04705287143588066\n",
      "Mean loss: 0.0666505762686332\n",
      "Std loss: 0.01585759113136561\n",
      "Total Loss: 0.39990345761179924\n",
      "------------------------------------ epoch 550 (3294 steps) ------------------------------------\n",
      "Max loss: 0.11747601628303528\n",
      "Min loss: 0.05258043855428696\n",
      "Mean loss: 0.07176129271586736\n",
      "Std loss: 0.021493233797177567\n",
      "Total Loss: 0.43056775629520416\n",
      "------------------------------------ epoch 551 (3300 steps) ------------------------------------\n",
      "Max loss: 0.08071830868721008\n",
      "Min loss: 0.04439504072070122\n",
      "Mean loss: 0.06846725506087144\n",
      "Std loss: 0.01354301409582817\n",
      "Total Loss: 0.41080353036522865\n",
      "------------------------------------ epoch 552 (3306 steps) ------------------------------------\n",
      "Max loss: 0.09368770569562912\n",
      "Min loss: 0.05349167808890343\n",
      "Mean loss: 0.06827759308119614\n",
      "Std loss: 0.013470988256473455\n",
      "Total Loss: 0.4096655584871769\n",
      "------------------------------------ epoch 553 (3312 steps) ------------------------------------\n",
      "Max loss: 0.0855071172118187\n",
      "Min loss: 0.050016455352306366\n",
      "Mean loss: 0.06814240105450153\n",
      "Std loss: 0.01506826486345708\n",
      "Total Loss: 0.4088544063270092\n",
      "------------------------------------ epoch 554 (3318 steps) ------------------------------------\n",
      "Max loss: 0.0945388600230217\n",
      "Min loss: 0.052840765565633774\n",
      "Mean loss: 0.06678758872052033\n",
      "Std loss: 0.013803251014556966\n",
      "Total Loss: 0.400725532323122\n",
      "------------------------------------ epoch 555 (3324 steps) ------------------------------------\n",
      "Max loss: 0.08119212836027145\n",
      "Min loss: 0.04050026461482048\n",
      "Mean loss: 0.05799740428725878\n",
      "Std loss: 0.01365142436922314\n",
      "Total Loss: 0.3479844257235527\n",
      "------------------------------------ epoch 556 (3330 steps) ------------------------------------\n",
      "Max loss: 0.07399969547986984\n",
      "Min loss: 0.04446304962038994\n",
      "Mean loss: 0.057434853787223496\n",
      "Std loss: 0.010926676360523508\n",
      "Total Loss: 0.344609122723341\n",
      "------------------------------------ epoch 557 (3336 steps) ------------------------------------\n",
      "Max loss: 0.08635672926902771\n",
      "Min loss: 0.05474236235022545\n",
      "Mean loss: 0.0675192264219125\n",
      "Std loss: 0.010351154302776333\n",
      "Total Loss: 0.40511535853147507\n",
      "------------------------------------ epoch 558 (3342 steps) ------------------------------------\n",
      "Max loss: 0.08562102168798447\n",
      "Min loss: 0.04393444582819939\n",
      "Mean loss: 0.062044987455010414\n",
      "Std loss: 0.01410749480073135\n",
      "Total Loss: 0.3722699247300625\n",
      "------------------------------------ epoch 559 (3348 steps) ------------------------------------\n",
      "Max loss: 0.10906821489334106\n",
      "Min loss: 0.053363554179668427\n",
      "Mean loss: 0.07107285037636757\n",
      "Std loss: 0.018706641819441747\n",
      "Total Loss: 0.4264371022582054\n",
      "------------------------------------ epoch 560 (3354 steps) ------------------------------------\n",
      "Max loss: 0.09626206755638123\n",
      "Min loss: 0.05058486387133598\n",
      "Mean loss: 0.07103081916769345\n",
      "Std loss: 0.014750032719210134\n",
      "Total Loss: 0.42618491500616074\n",
      "------------------------------------ epoch 561 (3360 steps) ------------------------------------\n",
      "Max loss: 0.07932619750499725\n",
      "Min loss: 0.03837209939956665\n",
      "Mean loss: 0.05964549506704012\n",
      "Std loss: 0.01656160768643194\n",
      "Total Loss: 0.35787297040224075\n",
      "------------------------------------ epoch 562 (3366 steps) ------------------------------------\n",
      "Max loss: 0.06468658894300461\n",
      "Min loss: 0.04407712072134018\n",
      "Mean loss: 0.05412566537658373\n",
      "Std loss: 0.008021636436648298\n",
      "Total Loss: 0.3247539922595024\n",
      "------------------------------------ epoch 563 (3372 steps) ------------------------------------\n",
      "Max loss: 0.08845978230237961\n",
      "Min loss: 0.04453858360648155\n",
      "Mean loss: 0.059859899803996086\n",
      "Std loss: 0.014352326894802417\n",
      "Total Loss: 0.3591593988239765\n",
      "------------------------------------ epoch 564 (3378 steps) ------------------------------------\n",
      "Max loss: 0.08095124363899231\n",
      "Min loss: 0.04058009013533592\n",
      "Mean loss: 0.06175869392851988\n",
      "Std loss: 0.014832287345602801\n",
      "Total Loss: 0.3705521635711193\n",
      "------------------------------------ epoch 565 (3384 steps) ------------------------------------\n",
      "Max loss: 0.07889670133590698\n",
      "Min loss: 0.034686967730522156\n",
      "Mean loss: 0.05452801597615083\n",
      "Std loss: 0.014660316828529302\n",
      "Total Loss: 0.327168095856905\n",
      "------------------------------------ epoch 566 (3390 steps) ------------------------------------\n",
      "Max loss: 0.0667659267783165\n",
      "Min loss: 0.04330698773264885\n",
      "Mean loss: 0.0536032517751058\n",
      "Std loss: 0.007287997333341339\n",
      "Total Loss: 0.32161951065063477\n",
      "------------------------------------ epoch 567 (3396 steps) ------------------------------------\n",
      "Max loss: 0.11430623382329941\n",
      "Min loss: 0.04518537223339081\n",
      "Mean loss: 0.07444554318984349\n",
      "Std loss: 0.022042016859712764\n",
      "Total Loss: 0.446673259139061\n",
      "------------------------------------ epoch 568 (3402 steps) ------------------------------------\n",
      "Max loss: 0.11079465597867966\n",
      "Min loss: 0.05303682014346123\n",
      "Mean loss: 0.07694097484151523\n",
      "Std loss: 0.02356979603202205\n",
      "Total Loss: 0.46164584904909134\n",
      "------------------------------------ epoch 569 (3408 steps) ------------------------------------\n",
      "Max loss: 0.08245016634464264\n",
      "Min loss: 0.037412356585264206\n",
      "Mean loss: 0.052766382694244385\n",
      "Std loss: 0.014350326578284642\n",
      "Total Loss: 0.3165982961654663\n",
      "------------------------------------ epoch 570 (3414 steps) ------------------------------------\n",
      "Max loss: 0.08452608436346054\n",
      "Min loss: 0.0387219563126564\n",
      "Mean loss: 0.06292937882244587\n",
      "Std loss: 0.016115474271907692\n",
      "Total Loss: 0.3775762729346752\n",
      "------------------------------------ epoch 571 (3420 steps) ------------------------------------\n",
      "Max loss: 0.10346786677837372\n",
      "Min loss: 0.03949647396802902\n",
      "Mean loss: 0.059395501390099525\n",
      "Std loss: 0.02050472050273784\n",
      "Total Loss: 0.35637300834059715\n",
      "------------------------------------ epoch 572 (3426 steps) ------------------------------------\n",
      "Max loss: 0.08112003654241562\n",
      "Min loss: 0.041276492178440094\n",
      "Mean loss: 0.05522662277022997\n",
      "Std loss: 0.014125586006973959\n",
      "Total Loss: 0.33135973662137985\n",
      "------------------------------------ epoch 573 (3432 steps) ------------------------------------\n",
      "Max loss: 0.0857013463973999\n",
      "Min loss: 0.037331853061914444\n",
      "Mean loss: 0.05386880785226822\n",
      "Std loss: 0.016124947496911242\n",
      "Total Loss: 0.3232128471136093\n",
      "------------------------------------ epoch 574 (3438 steps) ------------------------------------\n",
      "Max loss: 0.06974484771490097\n",
      "Min loss: 0.043157339096069336\n",
      "Mean loss: 0.050950576861699425\n",
      "Std loss: 0.008956024518032598\n",
      "Total Loss: 0.30570346117019653\n",
      "------------------------------------ epoch 575 (3444 steps) ------------------------------------\n",
      "Max loss: 0.10161098837852478\n",
      "Min loss: 0.05331826210021973\n",
      "Mean loss: 0.07007590495049953\n",
      "Std loss: 0.017237806682710858\n",
      "Total Loss: 0.4204554297029972\n",
      "------------------------------------ epoch 576 (3450 steps) ------------------------------------\n",
      "Max loss: 0.10206230729818344\n",
      "Min loss: 0.03625648841261864\n",
      "Mean loss: 0.0578406248241663\n",
      "Std loss: 0.021510819592773366\n",
      "Total Loss: 0.3470437489449978\n",
      "------------------------------------ epoch 577 (3456 steps) ------------------------------------\n",
      "Max loss: 0.10281138122081757\n",
      "Min loss: 0.03388205170631409\n",
      "Mean loss: 0.07022088455657165\n",
      "Std loss: 0.023584149517509225\n",
      "Total Loss: 0.42132530733942986\n",
      "------------------------------------ epoch 578 (3462 steps) ------------------------------------\n",
      "Max loss: 0.07226307690143585\n",
      "Min loss: 0.048251405358314514\n",
      "Mean loss: 0.05930283727745215\n",
      "Std loss: 0.00859796072000654\n",
      "Total Loss: 0.3558170236647129\n",
      "------------------------------------ epoch 579 (3468 steps) ------------------------------------\n",
      "Max loss: 0.07656031101942062\n",
      "Min loss: 0.04747520387172699\n",
      "Mean loss: 0.06312723395725091\n",
      "Std loss: 0.010156885546674141\n",
      "Total Loss: 0.3787634037435055\n",
      "------------------------------------ epoch 580 (3474 steps) ------------------------------------\n",
      "Max loss: 0.12182706594467163\n",
      "Min loss: 0.051575832068920135\n",
      "Mean loss: 0.08701447645823161\n",
      "Std loss: 0.025620450663453396\n",
      "Total Loss: 0.5220868587493896\n",
      "------------------------------------ epoch 581 (3480 steps) ------------------------------------\n",
      "Max loss: 0.10072095692157745\n",
      "Min loss: 0.03262343257665634\n",
      "Mean loss: 0.0566867912809054\n",
      "Std loss: 0.022082770647012584\n",
      "Total Loss: 0.34012074768543243\n",
      "------------------------------------ epoch 582 (3486 steps) ------------------------------------\n",
      "Max loss: 0.08391299843788147\n",
      "Min loss: 0.0416085422039032\n",
      "Mean loss: 0.05749797448515892\n",
      "Std loss: 0.014405494788649184\n",
      "Total Loss: 0.3449878469109535\n",
      "------------------------------------ epoch 583 (3492 steps) ------------------------------------\n",
      "Max loss: 0.09894095361232758\n",
      "Min loss: 0.05335887521505356\n",
      "Mean loss: 0.07165001146495342\n",
      "Std loss: 0.015311856584553972\n",
      "Total Loss: 0.42990006878972054\n",
      "------------------------------------ epoch 584 (3498 steps) ------------------------------------\n",
      "Max loss: 0.08242739737033844\n",
      "Min loss: 0.04037245362997055\n",
      "Mean loss: 0.06385665014386177\n",
      "Std loss: 0.014192550977339168\n",
      "Total Loss: 0.3831399008631706\n",
      "------------------------------------ epoch 585 (3504 steps) ------------------------------------\n",
      "Max loss: 0.06496713310480118\n",
      "Min loss: 0.04351301118731499\n",
      "Mean loss: 0.05572361499071121\n",
      "Std loss: 0.00885916833381392\n",
      "Total Loss: 0.3343416899442673\n",
      "------------------------------------ epoch 586 (3510 steps) ------------------------------------\n",
      "Max loss: 0.10834082961082458\n",
      "Min loss: 0.04234893247485161\n",
      "Mean loss: 0.07511602404216926\n",
      "Std loss: 0.02143726734037827\n",
      "Total Loss: 0.4506961442530155\n",
      "------------------------------------ epoch 587 (3516 steps) ------------------------------------\n",
      "Max loss: 0.09044457972049713\n",
      "Min loss: 0.05029372498393059\n",
      "Mean loss: 0.064599118505915\n",
      "Std loss: 0.016538242710057292\n",
      "Total Loss: 0.38759471103549004\n",
      "------------------------------------ epoch 588 (3522 steps) ------------------------------------\n",
      "Max loss: 0.11044954508543015\n",
      "Min loss: 0.03241512179374695\n",
      "Mean loss: 0.06071110938986143\n",
      "Std loss: 0.025487779689143328\n",
      "Total Loss: 0.36426665633916855\n",
      "------------------------------------ epoch 589 (3528 steps) ------------------------------------\n",
      "Max loss: 0.07051825523376465\n",
      "Min loss: 0.04383951053023338\n",
      "Mean loss: 0.05611286063989004\n",
      "Std loss: 0.008815225556875505\n",
      "Total Loss: 0.3366771638393402\n",
      "------------------------------------ epoch 590 (3534 steps) ------------------------------------\n",
      "Max loss: 0.10050749778747559\n",
      "Min loss: 0.04364310950040817\n",
      "Mean loss: 0.06427923838297527\n",
      "Std loss: 0.017945930387902886\n",
      "Total Loss: 0.38567543029785156\n",
      "------------------------------------ epoch 591 (3540 steps) ------------------------------------\n",
      "Max loss: 0.08085162192583084\n",
      "Min loss: 0.034432779997587204\n",
      "Mean loss: 0.05942859686911106\n",
      "Std loss: 0.015669105335666057\n",
      "Total Loss: 0.35657158121466637\n",
      "------------------------------------ epoch 592 (3546 steps) ------------------------------------\n",
      "Max loss: 0.079302579164505\n",
      "Min loss: 0.037976499646902084\n",
      "Mean loss: 0.05520041100680828\n",
      "Std loss: 0.016181330740420358\n",
      "Total Loss: 0.3312024660408497\n",
      "------------------------------------ epoch 593 (3552 steps) ------------------------------------\n",
      "Max loss: 0.1217796802520752\n",
      "Min loss: 0.03451436012983322\n",
      "Mean loss: 0.06527518853545189\n",
      "Std loss: 0.02777565285959967\n",
      "Total Loss: 0.39165113121271133\n",
      "------------------------------------ epoch 594 (3558 steps) ------------------------------------\n",
      "Max loss: 0.09180372953414917\n",
      "Min loss: 0.04390151426196098\n",
      "Mean loss: 0.06058960718413194\n",
      "Std loss: 0.016552930340452878\n",
      "Total Loss: 0.36353764310479164\n",
      "------------------------------------ epoch 595 (3564 steps) ------------------------------------\n",
      "Max loss: 0.09719386696815491\n",
      "Min loss: 0.043999411165714264\n",
      "Mean loss: 0.05793267178038756\n",
      "Std loss: 0.018112252598153217\n",
      "Total Loss: 0.34759603068232536\n",
      "------------------------------------ epoch 596 (3570 steps) ------------------------------------\n",
      "Max loss: 0.11161567270755768\n",
      "Min loss: 0.040436264127492905\n",
      "Mean loss: 0.06513233544925849\n",
      "Std loss: 0.022868260472776156\n",
      "Total Loss: 0.3907940126955509\n",
      "------------------------------------ epoch 597 (3576 steps) ------------------------------------\n",
      "Max loss: 0.05499964579939842\n",
      "Min loss: 0.030508097261190414\n",
      "Mean loss: 0.046756395449241005\n",
      "Std loss: 0.008276827449500049\n",
      "Total Loss: 0.280538372695446\n",
      "------------------------------------ epoch 598 (3582 steps) ------------------------------------\n",
      "Max loss: 0.07824940979480743\n",
      "Min loss: 0.03355873003602028\n",
      "Mean loss: 0.05584668181836605\n",
      "Std loss: 0.01489985414762697\n",
      "Total Loss: 0.3350800909101963\n",
      "------------------------------------ epoch 599 (3588 steps) ------------------------------------\n",
      "Max loss: 0.08979927003383636\n",
      "Min loss: 0.04002107307314873\n",
      "Mean loss: 0.05601698781053225\n",
      "Std loss: 0.015842114414482317\n",
      "Total Loss: 0.3361019268631935\n",
      "------------------------------------ epoch 600 (3594 steps) ------------------------------------\n",
      "Max loss: 0.08624225854873657\n",
      "Min loss: 0.0318591371178627\n",
      "Mean loss: 0.059907943631211914\n",
      "Std loss: 0.020565865601672353\n",
      "Total Loss: 0.3594476617872715\n",
      "------------------------------------ epoch 601 (3600 steps) ------------------------------------\n",
      "Max loss: 0.08054980635643005\n",
      "Min loss: 0.04296154901385307\n",
      "Mean loss: 0.05897231586277485\n",
      "Std loss: 0.014844987135945472\n",
      "Total Loss: 0.3538338951766491\n",
      "saved model at ./weights/model_601.pth\n",
      "------------------------------------ epoch 602 (3606 steps) ------------------------------------\n",
      "Max loss: 0.09558813273906708\n",
      "Min loss: 0.03684312105178833\n",
      "Mean loss: 0.060298433527350426\n",
      "Std loss: 0.022065402522785313\n",
      "Total Loss: 0.36179060116410255\n",
      "------------------------------------ epoch 603 (3612 steps) ------------------------------------\n",
      "Max loss: 0.08235324919223785\n",
      "Min loss: 0.040019333362579346\n",
      "Mean loss: 0.05874902072052161\n",
      "Std loss: 0.015537007308509002\n",
      "Total Loss: 0.35249412432312965\n",
      "------------------------------------ epoch 604 (3618 steps) ------------------------------------\n",
      "Max loss: 0.07534199953079224\n",
      "Min loss: 0.04107897728681564\n",
      "Mean loss: 0.05519775177041689\n",
      "Std loss: 0.012381395949320407\n",
      "Total Loss: 0.3311865106225014\n",
      "------------------------------------ epoch 605 (3624 steps) ------------------------------------\n",
      "Max loss: 0.08362101763486862\n",
      "Min loss: 0.03256041556596756\n",
      "Mean loss: 0.05101490827898184\n",
      "Std loss: 0.017433515872728567\n",
      "Total Loss: 0.30608944967389107\n",
      "------------------------------------ epoch 606 (3630 steps) ------------------------------------\n",
      "Max loss: 0.07316768169403076\n",
      "Min loss: 0.03675159811973572\n",
      "Mean loss: 0.05542772759993871\n",
      "Std loss: 0.01330282130425529\n",
      "Total Loss: 0.33256636559963226\n",
      "------------------------------------ epoch 607 (3636 steps) ------------------------------------\n",
      "Max loss: 0.15248942375183105\n",
      "Min loss: 0.042507849633693695\n",
      "Mean loss: 0.07087538825968902\n",
      "Std loss: 0.03769899040679707\n",
      "Total Loss: 0.4252523295581341\n",
      "------------------------------------ epoch 608 (3642 steps) ------------------------------------\n",
      "Max loss: 0.060588281601667404\n",
      "Min loss: 0.043260958045721054\n",
      "Mean loss: 0.047047884513934456\n",
      "Std loss: 0.006086872429044984\n",
      "Total Loss: 0.2822873070836067\n",
      "------------------------------------ epoch 609 (3648 steps) ------------------------------------\n",
      "Max loss: 0.05608745664358139\n",
      "Min loss: 0.03748202323913574\n",
      "Mean loss: 0.047013929734627403\n",
      "Std loss: 0.008254427791688161\n",
      "Total Loss: 0.28208357840776443\n",
      "------------------------------------ epoch 610 (3654 steps) ------------------------------------\n",
      "Max loss: 0.07536573708057404\n",
      "Min loss: 0.03250541538000107\n",
      "Mean loss: 0.05303467810153961\n",
      "Std loss: 0.013893500727600162\n",
      "Total Loss: 0.31820806860923767\n",
      "------------------------------------ epoch 611 (3660 steps) ------------------------------------\n",
      "Max loss: 0.06779668480157852\n",
      "Min loss: 0.040357254445552826\n",
      "Mean loss: 0.05383191257715225\n",
      "Std loss: 0.010330620331592717\n",
      "Total Loss: 0.3229914754629135\n",
      "------------------------------------ epoch 612 (3666 steps) ------------------------------------\n",
      "Max loss: 0.06550362706184387\n",
      "Min loss: 0.04200725257396698\n",
      "Mean loss: 0.0567725965132316\n",
      "Std loss: 0.008032641043921585\n",
      "Total Loss: 0.34063557907938957\n",
      "------------------------------------ epoch 613 (3672 steps) ------------------------------------\n",
      "Max loss: 0.0833219587802887\n",
      "Min loss: 0.030633971095085144\n",
      "Mean loss: 0.05013697097698847\n",
      "Std loss: 0.022672718570767737\n",
      "Total Loss: 0.30082182586193085\n",
      "------------------------------------ epoch 614 (3678 steps) ------------------------------------\n",
      "Max loss: 0.0744638592004776\n",
      "Min loss: 0.03538705036044121\n",
      "Mean loss: 0.050379591062664986\n",
      "Std loss: 0.013968170290314726\n",
      "Total Loss: 0.3022775463759899\n",
      "------------------------------------ epoch 615 (3684 steps) ------------------------------------\n",
      "Max loss: 0.10803022235631943\n",
      "Min loss: 0.02689993940293789\n",
      "Mean loss: 0.05723906525721153\n",
      "Std loss: 0.02452956542485607\n",
      "Total Loss: 0.34343439154326916\n",
      "------------------------------------ epoch 616 (3690 steps) ------------------------------------\n",
      "Max loss: 0.09505553543567657\n",
      "Min loss: 0.03796857222914696\n",
      "Mean loss: 0.05663030408322811\n",
      "Std loss: 0.01986140672006011\n",
      "Total Loss: 0.33978182449936867\n",
      "------------------------------------ epoch 617 (3696 steps) ------------------------------------\n",
      "Max loss: 0.0737803503870964\n",
      "Min loss: 0.032402247190475464\n",
      "Mean loss: 0.05295858780543009\n",
      "Std loss: 0.015592636510596065\n",
      "Total Loss: 0.31775152683258057\n",
      "------------------------------------ epoch 618 (3702 steps) ------------------------------------\n",
      "Max loss: 0.09439225494861603\n",
      "Min loss: 0.0400567464530468\n",
      "Mean loss: 0.06659103433291118\n",
      "Std loss: 0.018811467104310784\n",
      "Total Loss: 0.39954620599746704\n",
      "------------------------------------ epoch 619 (3708 steps) ------------------------------------\n",
      "Max loss: 0.10641350597143173\n",
      "Min loss: 0.0416397862136364\n",
      "Mean loss: 0.06086828435460726\n",
      "Std loss: 0.023414952437096466\n",
      "Total Loss: 0.3652097061276436\n",
      "------------------------------------ epoch 620 (3714 steps) ------------------------------------\n",
      "Max loss: 0.07129228860139847\n",
      "Min loss: 0.029924815520644188\n",
      "Mean loss: 0.04737795423716307\n",
      "Std loss: 0.014357465571825014\n",
      "Total Loss: 0.2842677254229784\n",
      "------------------------------------ epoch 621 (3720 steps) ------------------------------------\n",
      "Max loss: 0.09458484500646591\n",
      "Min loss: 0.05098189041018486\n",
      "Mean loss: 0.0637356135994196\n",
      "Std loss: 0.01515162268257771\n",
      "Total Loss: 0.38241368159651756\n",
      "------------------------------------ epoch 622 (3726 steps) ------------------------------------\n",
      "Max loss: 0.08115297555923462\n",
      "Min loss: 0.04560035467147827\n",
      "Mean loss: 0.06321662416060765\n",
      "Std loss: 0.01096117872796466\n",
      "Total Loss: 0.37929974496364594\n",
      "------------------------------------ epoch 623 (3732 steps) ------------------------------------\n",
      "Max loss: 0.0825180858373642\n",
      "Min loss: 0.03405863791704178\n",
      "Mean loss: 0.047843253860870995\n",
      "Std loss: 0.017768842828710763\n",
      "Total Loss: 0.287059523165226\n",
      "------------------------------------ epoch 624 (3738 steps) ------------------------------------\n",
      "Max loss: 0.10748433321714401\n",
      "Min loss: 0.04522807151079178\n",
      "Mean loss: 0.0669813019533952\n",
      "Std loss: 0.021164604204611356\n",
      "Total Loss: 0.40188781172037125\n",
      "------------------------------------ epoch 625 (3744 steps) ------------------------------------\n",
      "Max loss: 0.07017552852630615\n",
      "Min loss: 0.03566155210137367\n",
      "Mean loss: 0.05044586397707462\n",
      "Std loss: 0.01260537269385757\n",
      "Total Loss: 0.30267518386244774\n",
      "------------------------------------ epoch 626 (3750 steps) ------------------------------------\n",
      "Max loss: 0.06773348897695541\n",
      "Min loss: 0.04795891046524048\n",
      "Mean loss: 0.05929214134812355\n",
      "Std loss: 0.006372067045157421\n",
      "Total Loss: 0.3557528480887413\n",
      "------------------------------------ epoch 627 (3756 steps) ------------------------------------\n",
      "Max loss: 0.11029422283172607\n",
      "Min loss: 0.0318584106862545\n",
      "Mean loss: 0.05771335897346338\n",
      "Std loss: 0.02619359693675891\n",
      "Total Loss: 0.34628015384078026\n",
      "------------------------------------ epoch 628 (3762 steps) ------------------------------------\n",
      "Max loss: 0.06508687138557434\n",
      "Min loss: 0.040404319763183594\n",
      "Mean loss: 0.05314642935991287\n",
      "Std loss: 0.008912171861317408\n",
      "Total Loss: 0.31887857615947723\n",
      "------------------------------------ epoch 629 (3768 steps) ------------------------------------\n",
      "Max loss: 0.1221114844083786\n",
      "Min loss: 0.04935137555003166\n",
      "Mean loss: 0.06988248787820339\n",
      "Std loss: 0.023970333151666995\n",
      "Total Loss: 0.41929492726922035\n",
      "------------------------------------ epoch 630 (3774 steps) ------------------------------------\n",
      "Max loss: 0.09663282334804535\n",
      "Min loss: 0.04246196895837784\n",
      "Mean loss: 0.07164597883820534\n",
      "Std loss: 0.01649703001043038\n",
      "Total Loss: 0.429875873029232\n",
      "------------------------------------ epoch 631 (3780 steps) ------------------------------------\n",
      "Max loss: 0.05038844048976898\n",
      "Min loss: 0.037166859954595566\n",
      "Mean loss: 0.04325476412971815\n",
      "Std loss: 0.005260124864408598\n",
      "Total Loss: 0.25952858477830887\n",
      "------------------------------------ epoch 632 (3786 steps) ------------------------------------\n",
      "Max loss: 0.0706467479467392\n",
      "Min loss: 0.031441036611795425\n",
      "Mean loss: 0.04841180890798569\n",
      "Std loss: 0.012295072479967945\n",
      "Total Loss: 0.2904708534479141\n",
      "------------------------------------ epoch 633 (3792 steps) ------------------------------------\n",
      "Max loss: 0.052629660815000534\n",
      "Min loss: 0.03488712012767792\n",
      "Mean loss: 0.043027255684137344\n",
      "Std loss: 0.00578707000488328\n",
      "Total Loss: 0.25816353410482407\n",
      "------------------------------------ epoch 634 (3798 steps) ------------------------------------\n",
      "Max loss: 0.07902117818593979\n",
      "Min loss: 0.04395880550146103\n",
      "Mean loss: 0.0617139246314764\n",
      "Std loss: 0.01158772706419575\n",
      "Total Loss: 0.3702835477888584\n",
      "------------------------------------ epoch 635 (3804 steps) ------------------------------------\n",
      "Max loss: 0.06537844985723495\n",
      "Min loss: 0.034563176333904266\n",
      "Mean loss: 0.05005616260071596\n",
      "Std loss: 0.011805967358073901\n",
      "Total Loss: 0.30033697560429573\n",
      "------------------------------------ epoch 636 (3810 steps) ------------------------------------\n",
      "Max loss: 0.049802400171756744\n",
      "Min loss: 0.029602840542793274\n",
      "Mean loss: 0.03837933950126171\n",
      "Std loss: 0.006628863981502685\n",
      "Total Loss: 0.23027603700757027\n",
      "------------------------------------ epoch 637 (3816 steps) ------------------------------------\n",
      "Max loss: 0.07183044403791428\n",
      "Min loss: 0.03747999295592308\n",
      "Mean loss: 0.052565219501654305\n",
      "Std loss: 0.013991980526475878\n",
      "Total Loss: 0.31539131700992584\n",
      "------------------------------------ epoch 638 (3822 steps) ------------------------------------\n",
      "Max loss: 0.06319496035575867\n",
      "Min loss: 0.04523901641368866\n",
      "Mean loss: 0.05423063909014066\n",
      "Std loss: 0.0069257731231882344\n",
      "Total Loss: 0.32538383454084396\n",
      "------------------------------------ epoch 639 (3828 steps) ------------------------------------\n",
      "Max loss: 0.09668823331594467\n",
      "Min loss: 0.03781380131840706\n",
      "Mean loss: 0.076228737210234\n",
      "Std loss: 0.020845289711097597\n",
      "Total Loss: 0.45737242326140404\n",
      "------------------------------------ epoch 640 (3834 steps) ------------------------------------\n",
      "Max loss: 0.07365137338638306\n",
      "Min loss: 0.036670804023742676\n",
      "Mean loss: 0.05338983237743378\n",
      "Std loss: 0.012648713526804462\n",
      "Total Loss: 0.32033899426460266\n",
      "------------------------------------ epoch 641 (3840 steps) ------------------------------------\n",
      "Max loss: 0.12040464580059052\n",
      "Min loss: 0.04583638533949852\n",
      "Mean loss: 0.06978250294923782\n",
      "Std loss: 0.024874296708384803\n",
      "Total Loss: 0.41869501769542694\n",
      "------------------------------------ epoch 642 (3846 steps) ------------------------------------\n",
      "Max loss: 0.07336857169866562\n",
      "Min loss: 0.030096929520368576\n",
      "Mean loss: 0.0444403470804294\n",
      "Std loss: 0.014324832518639946\n",
      "Total Loss: 0.26664208248257637\n",
      "------------------------------------ epoch 643 (3852 steps) ------------------------------------\n",
      "Max loss: 0.06616547703742981\n",
      "Min loss: 0.032833196222782135\n",
      "Mean loss: 0.04477898279825846\n",
      "Std loss: 0.011901281302916797\n",
      "Total Loss: 0.2686738967895508\n",
      "------------------------------------ epoch 644 (3858 steps) ------------------------------------\n",
      "Max loss: 0.06225733086466789\n",
      "Min loss: 0.03906687721610069\n",
      "Mean loss: 0.04607559802631537\n",
      "Std loss: 0.0078037295029377935\n",
      "Total Loss: 0.2764535881578922\n",
      "------------------------------------ epoch 645 (3864 steps) ------------------------------------\n",
      "Max loss: 0.0673280656337738\n",
      "Min loss: 0.03926893323659897\n",
      "Mean loss: 0.052664601554473243\n",
      "Std loss: 0.00989665415054285\n",
      "Total Loss: 0.31598760932683945\n",
      "------------------------------------ epoch 646 (3870 steps) ------------------------------------\n",
      "Max loss: 0.0739559531211853\n",
      "Min loss: 0.030589330941438675\n",
      "Mean loss: 0.045266894002755485\n",
      "Std loss: 0.014427349012413063\n",
      "Total Loss: 0.2716013640165329\n",
      "------------------------------------ epoch 647 (3876 steps) ------------------------------------\n",
      "Max loss: 0.05851174518465996\n",
      "Min loss: 0.03097671829164028\n",
      "Mean loss: 0.04382414650171995\n",
      "Std loss: 0.00858626759568719\n",
      "Total Loss: 0.2629448790103197\n",
      "------------------------------------ epoch 648 (3882 steps) ------------------------------------\n",
      "Max loss: 0.07466678321361542\n",
      "Min loss: 0.0340348482131958\n",
      "Mean loss: 0.04505577124655247\n",
      "Std loss: 0.01348371770573436\n",
      "Total Loss: 0.2703346274793148\n",
      "------------------------------------ epoch 649 (3888 steps) ------------------------------------\n",
      "Max loss: 0.05933806300163269\n",
      "Min loss: 0.023385383188724518\n",
      "Mean loss: 0.04604062748452028\n",
      "Std loss: 0.011076805908841013\n",
      "Total Loss: 0.27624376490712166\n",
      "------------------------------------ epoch 650 (3894 steps) ------------------------------------\n",
      "Max loss: 0.05432306230068207\n",
      "Min loss: 0.03557199984788895\n",
      "Mean loss: 0.04271312057971954\n",
      "Std loss: 0.0072176834959244394\n",
      "Total Loss: 0.25627872347831726\n",
      "------------------------------------ epoch 651 (3900 steps) ------------------------------------\n",
      "Max loss: 0.10639356076717377\n",
      "Min loss: 0.03596026822924614\n",
      "Mean loss: 0.06444534286856651\n",
      "Std loss: 0.023082288305615924\n",
      "Total Loss: 0.3866720572113991\n",
      "------------------------------------ epoch 652 (3906 steps) ------------------------------------\n",
      "Max loss: 0.07718910276889801\n",
      "Min loss: 0.038525328040122986\n",
      "Mean loss: 0.05127937781314055\n",
      "Std loss: 0.012784465192857834\n",
      "Total Loss: 0.3076762668788433\n",
      "------------------------------------ epoch 653 (3912 steps) ------------------------------------\n",
      "Max loss: 0.06949356198310852\n",
      "Min loss: 0.03252603858709335\n",
      "Mean loss: 0.05492278685172399\n",
      "Std loss: 0.011801289948689668\n",
      "Total Loss: 0.32953672111034393\n",
      "------------------------------------ epoch 654 (3918 steps) ------------------------------------\n",
      "Max loss: 0.07390078902244568\n",
      "Min loss: 0.04236697405576706\n",
      "Mean loss: 0.05132587564488252\n",
      "Std loss: 0.010797300490580353\n",
      "Total Loss: 0.3079552538692951\n",
      "------------------------------------ epoch 655 (3924 steps) ------------------------------------\n",
      "Max loss: 0.0701080858707428\n",
      "Min loss: 0.0309927798807621\n",
      "Mean loss: 0.04633758092919985\n",
      "Std loss: 0.013109509825084846\n",
      "Total Loss: 0.2780254855751991\n",
      "------------------------------------ epoch 656 (3930 steps) ------------------------------------\n",
      "Max loss: 0.06838594377040863\n",
      "Min loss: 0.029646683484315872\n",
      "Mean loss: 0.048578827952345215\n",
      "Std loss: 0.011402617352217712\n",
      "Total Loss: 0.2914729677140713\n",
      "------------------------------------ epoch 657 (3936 steps) ------------------------------------\n",
      "Max loss: 0.06274530291557312\n",
      "Min loss: 0.034373823553323746\n",
      "Mean loss: 0.04809248757859071\n",
      "Std loss: 0.009193796321078682\n",
      "Total Loss: 0.28855492547154427\n",
      "------------------------------------ epoch 658 (3942 steps) ------------------------------------\n",
      "Max loss: 0.05666247010231018\n",
      "Min loss: 0.03449510037899017\n",
      "Mean loss: 0.04639613442122936\n",
      "Std loss: 0.007241996255986102\n",
      "Total Loss: 0.2783768065273762\n",
      "------------------------------------ epoch 659 (3948 steps) ------------------------------------\n",
      "Max loss: 0.077765554189682\n",
      "Min loss: 0.04276368021965027\n",
      "Mean loss: 0.05679526676734289\n",
      "Std loss: 0.013561307816020792\n",
      "Total Loss: 0.3407716006040573\n",
      "------------------------------------ epoch 660 (3954 steps) ------------------------------------\n",
      "Max loss: 0.0642758309841156\n",
      "Min loss: 0.029762020334601402\n",
      "Mean loss: 0.051420955918729305\n",
      "Std loss: 0.013354082386814764\n",
      "Total Loss: 0.30852573551237583\n",
      "------------------------------------ epoch 661 (3960 steps) ------------------------------------\n",
      "Max loss: 0.09460960328578949\n",
      "Min loss: 0.032806072384119034\n",
      "Mean loss: 0.05430790036916733\n",
      "Std loss: 0.0206876643518249\n",
      "Total Loss: 0.32584740221500397\n",
      "------------------------------------ epoch 662 (3966 steps) ------------------------------------\n",
      "Max loss: 0.059241991490125656\n",
      "Min loss: 0.03778949752449989\n",
      "Mean loss: 0.04852101393043995\n",
      "Std loss: 0.009112797849282612\n",
      "Total Loss: 0.2911260835826397\n",
      "------------------------------------ epoch 663 (3972 steps) ------------------------------------\n",
      "Max loss: 0.06457646191120148\n",
      "Min loss: 0.0356711819767952\n",
      "Mean loss: 0.04595065427323183\n",
      "Std loss: 0.009192119727031156\n",
      "Total Loss: 0.27570392563939095\n",
      "------------------------------------ epoch 664 (3978 steps) ------------------------------------\n",
      "Max loss: 0.0929114818572998\n",
      "Min loss: 0.03049425408244133\n",
      "Mean loss: 0.058729603265722595\n",
      "Std loss: 0.02298137706675526\n",
      "Total Loss: 0.35237761959433556\n",
      "------------------------------------ epoch 665 (3984 steps) ------------------------------------\n",
      "Max loss: 0.09451376646757126\n",
      "Min loss: 0.029899045825004578\n",
      "Mean loss: 0.05587483632067839\n",
      "Std loss: 0.02267958139482576\n",
      "Total Loss: 0.33524901792407036\n",
      "------------------------------------ epoch 666 (3990 steps) ------------------------------------\n",
      "Max loss: 0.08843173086643219\n",
      "Min loss: 0.0510457418859005\n",
      "Mean loss: 0.06263299224277337\n",
      "Std loss: 0.013235268993413223\n",
      "Total Loss: 0.37579795345664024\n",
      "------------------------------------ epoch 667 (3996 steps) ------------------------------------\n",
      "Max loss: 0.07017306983470917\n",
      "Min loss: 0.03128443658351898\n",
      "Mean loss: 0.050914875542124115\n",
      "Std loss: 0.012829766420310085\n",
      "Total Loss: 0.3054892532527447\n",
      "------------------------------------ epoch 668 (4002 steps) ------------------------------------\n",
      "Max loss: 0.08474357426166534\n",
      "Min loss: 0.029693234711885452\n",
      "Mean loss: 0.051767571518818535\n",
      "Std loss: 0.01668259960940604\n",
      "Total Loss: 0.3106054291129112\n",
      "------------------------------------ epoch 669 (4008 steps) ------------------------------------\n",
      "Max loss: 0.13273070752620697\n",
      "Min loss: 0.029436713084578514\n",
      "Mean loss: 0.05838519986718893\n",
      "Std loss: 0.03477082576031539\n",
      "Total Loss: 0.3503111992031336\n",
      "------------------------------------ epoch 670 (4014 steps) ------------------------------------\n",
      "Max loss: 0.12145715951919556\n",
      "Min loss: 0.0416703075170517\n",
      "Mean loss: 0.07171750751634438\n",
      "Std loss: 0.026155872926915125\n",
      "Total Loss: 0.43030504509806633\n",
      "------------------------------------ epoch 671 (4020 steps) ------------------------------------\n",
      "Max loss: 0.0823255330324173\n",
      "Min loss: 0.026945289224386215\n",
      "Mean loss: 0.045448132790625095\n",
      "Std loss: 0.019752465123938145\n",
      "Total Loss: 0.27268879674375057\n",
      "------------------------------------ epoch 672 (4026 steps) ------------------------------------\n",
      "Max loss: 0.07046689093112946\n",
      "Min loss: 0.03314485400915146\n",
      "Mean loss: 0.04861013280848662\n",
      "Std loss: 0.012553022995016561\n",
      "Total Loss: 0.2916607968509197\n",
      "------------------------------------ epoch 673 (4032 steps) ------------------------------------\n",
      "Max loss: 0.04930352419614792\n",
      "Min loss: 0.030838388949632645\n",
      "Mean loss: 0.041722132513920464\n",
      "Std loss: 0.006966242966284823\n",
      "Total Loss: 0.2503327950835228\n",
      "------------------------------------ epoch 674 (4038 steps) ------------------------------------\n",
      "Max loss: 0.062499038875103\n",
      "Min loss: 0.029659908264875412\n",
      "Mean loss: 0.04556279877821604\n",
      "Std loss: 0.0125234162681752\n",
      "Total Loss: 0.27337679266929626\n",
      "------------------------------------ epoch 675 (4044 steps) ------------------------------------\n",
      "Max loss: 0.061102673411369324\n",
      "Min loss: 0.027921363711357117\n",
      "Mean loss: 0.04174384909371535\n",
      "Std loss: 0.013756994692562745\n",
      "Total Loss: 0.2504630945622921\n",
      "------------------------------------ epoch 676 (4050 steps) ------------------------------------\n",
      "Max loss: 0.06857407093048096\n",
      "Min loss: 0.03034498542547226\n",
      "Mean loss: 0.04614862302939097\n",
      "Std loss: 0.013982505391904414\n",
      "Total Loss: 0.2768917381763458\n",
      "------------------------------------ epoch 677 (4056 steps) ------------------------------------\n",
      "Max loss: 0.061002038419246674\n",
      "Min loss: 0.0322742685675621\n",
      "Mean loss: 0.044591608146826424\n",
      "Std loss: 0.010597491171945597\n",
      "Total Loss: 0.26754964888095856\n",
      "------------------------------------ epoch 678 (4062 steps) ------------------------------------\n",
      "Max loss: 0.06815885752439499\n",
      "Min loss: 0.029060861095786095\n",
      "Mean loss: 0.05604494704554478\n",
      "Std loss: 0.014239723508091875\n",
      "Total Loss: 0.3362696822732687\n",
      "------------------------------------ epoch 679 (4068 steps) ------------------------------------\n",
      "Max loss: 0.08110591769218445\n",
      "Min loss: 0.032948192209005356\n",
      "Mean loss: 0.05224422179162502\n",
      "Std loss: 0.0157392464505453\n",
      "Total Loss: 0.31346533074975014\n",
      "------------------------------------ epoch 680 (4074 steps) ------------------------------------\n",
      "Max loss: 0.054387032985687256\n",
      "Min loss: 0.03356778249144554\n",
      "Mean loss: 0.04639997022847334\n",
      "Std loss: 0.006981666617024413\n",
      "Total Loss: 0.2783998213708401\n",
      "------------------------------------ epoch 681 (4080 steps) ------------------------------------\n",
      "Max loss: 0.07499778270721436\n",
      "Min loss: 0.04145883023738861\n",
      "Mean loss: 0.05483646007875601\n",
      "Std loss: 0.012819227947481072\n",
      "Total Loss: 0.3290187604725361\n",
      "------------------------------------ epoch 682 (4086 steps) ------------------------------------\n",
      "Max loss: 0.10825096070766449\n",
      "Min loss: 0.034896425902843475\n",
      "Mean loss: 0.05736530447999636\n",
      "Std loss: 0.02897438502208564\n",
      "Total Loss: 0.3441918268799782\n",
      "------------------------------------ epoch 683 (4092 steps) ------------------------------------\n",
      "Max loss: 0.06616118550300598\n",
      "Min loss: 0.03267999738454819\n",
      "Mean loss: 0.048400686432917915\n",
      "Std loss: 0.012206556049381726\n",
      "Total Loss: 0.2904041185975075\n",
      "------------------------------------ epoch 684 (4098 steps) ------------------------------------\n",
      "Max loss: 0.05765248462557793\n",
      "Min loss: 0.028190936893224716\n",
      "Mean loss: 0.04414397416015466\n",
      "Std loss: 0.01154919315592101\n",
      "Total Loss: 0.26486384496092796\n",
      "------------------------------------ epoch 685 (4104 steps) ------------------------------------\n",
      "Max loss: 0.059314578771591187\n",
      "Min loss: 0.04314231127500534\n",
      "Mean loss: 0.05146732491751512\n",
      "Std loss: 0.0065385769862224144\n",
      "Total Loss: 0.3088039495050907\n",
      "------------------------------------ epoch 686 (4110 steps) ------------------------------------\n",
      "Max loss: 0.08687441796064377\n",
      "Min loss: 0.03440342843532562\n",
      "Mean loss: 0.051734757920106254\n",
      "Std loss: 0.01752075789166745\n",
      "Total Loss: 0.3104085475206375\n",
      "------------------------------------ epoch 687 (4116 steps) ------------------------------------\n",
      "Max loss: 0.08600165694952011\n",
      "Min loss: 0.04224666953086853\n",
      "Mean loss: 0.05879884026944637\n",
      "Std loss: 0.016637195136715025\n",
      "Total Loss: 0.35279304161667824\n",
      "------------------------------------ epoch 688 (4122 steps) ------------------------------------\n",
      "Max loss: 0.06213448941707611\n",
      "Min loss: 0.04031413048505783\n",
      "Mean loss: 0.05344779479006926\n",
      "Std loss: 0.007309053698925695\n",
      "Total Loss: 0.3206867687404156\n",
      "------------------------------------ epoch 689 (4128 steps) ------------------------------------\n",
      "Max loss: 0.06235339492559433\n",
      "Min loss: 0.030159834772348404\n",
      "Mean loss: 0.04428859489659468\n",
      "Std loss: 0.012046949947900903\n",
      "Total Loss: 0.2657315693795681\n",
      "------------------------------------ epoch 690 (4134 steps) ------------------------------------\n",
      "Max loss: 0.07948323339223862\n",
      "Min loss: 0.028219163417816162\n",
      "Mean loss: 0.04885534755885601\n",
      "Std loss: 0.016665259115222295\n",
      "Total Loss: 0.29313208535313606\n",
      "------------------------------------ epoch 691 (4140 steps) ------------------------------------\n",
      "Max loss: 0.08804498612880707\n",
      "Min loss: 0.028973206877708435\n",
      "Mean loss: 0.04850040810803572\n",
      "Std loss: 0.01924485194589177\n",
      "Total Loss: 0.29100244864821434\n",
      "------------------------------------ epoch 692 (4146 steps) ------------------------------------\n",
      "Max loss: 0.076664499938488\n",
      "Min loss: 0.024524245411157608\n",
      "Mean loss: 0.043814986323316894\n",
      "Std loss: 0.0184216540833243\n",
      "Total Loss: 0.26288991793990135\n",
      "------------------------------------ epoch 693 (4152 steps) ------------------------------------\n",
      "Max loss: 0.08008883148431778\n",
      "Min loss: 0.02859547734260559\n",
      "Mean loss: 0.05109472696979841\n",
      "Std loss: 0.018573106940171287\n",
      "Total Loss: 0.30656836181879044\n",
      "------------------------------------ epoch 694 (4158 steps) ------------------------------------\n",
      "Max loss: 0.05798032507300377\n",
      "Min loss: 0.046397216618061066\n",
      "Mean loss: 0.05195708138247331\n",
      "Std loss: 0.004566365654325824\n",
      "Total Loss: 0.31174248829483986\n",
      "------------------------------------ epoch 695 (4164 steps) ------------------------------------\n",
      "Max loss: 0.07271122187376022\n",
      "Min loss: 0.028510697185993195\n",
      "Mean loss: 0.04854420324166616\n",
      "Std loss: 0.014406001612621464\n",
      "Total Loss: 0.29126521944999695\n",
      "------------------------------------ epoch 696 (4170 steps) ------------------------------------\n",
      "Max loss: 0.09877490252256393\n",
      "Min loss: 0.02907261811196804\n",
      "Mean loss: 0.05229418072849512\n",
      "Std loss: 0.02525685548007573\n",
      "Total Loss: 0.3137650843709707\n",
      "------------------------------------ epoch 697 (4176 steps) ------------------------------------\n",
      "Max loss: 0.07100073993206024\n",
      "Min loss: 0.03788009285926819\n",
      "Mean loss: 0.054145109529296555\n",
      "Std loss: 0.011823641487465713\n",
      "Total Loss: 0.32487065717577934\n",
      "------------------------------------ epoch 698 (4182 steps) ------------------------------------\n",
      "Max loss: 0.06887413561344147\n",
      "Min loss: 0.040493935346603394\n",
      "Mean loss: 0.04868010307351748\n",
      "Std loss: 0.010327533806537984\n",
      "Total Loss: 0.2920806184411049\n",
      "------------------------------------ epoch 699 (4188 steps) ------------------------------------\n",
      "Max loss: 0.05272013694047928\n",
      "Min loss: 0.03285341337323189\n",
      "Mean loss: 0.042076254884401955\n",
      "Std loss: 0.006996719821873657\n",
      "Total Loss: 0.25245752930641174\n",
      "------------------------------------ epoch 700 (4194 steps) ------------------------------------\n",
      "Max loss: 0.08976522088050842\n",
      "Min loss: 0.029586419463157654\n",
      "Mean loss: 0.046544311568140984\n",
      "Std loss: 0.020063469144034375\n",
      "Total Loss: 0.2792658694088459\n",
      "------------------------------------ epoch 701 (4200 steps) ------------------------------------\n",
      "Max loss: 0.10849672555923462\n",
      "Min loss: 0.034717682749032974\n",
      "Mean loss: 0.0625771035750707\n",
      "Std loss: 0.025665020341188612\n",
      "Total Loss: 0.3754626214504242\n",
      "saved model at ./weights/model_701.pth\n",
      "------------------------------------ epoch 702 (4206 steps) ------------------------------------\n",
      "Max loss: 0.12203367054462433\n",
      "Min loss: 0.024486321955919266\n",
      "Mean loss: 0.05083924097319444\n",
      "Std loss: 0.03301212895683989\n",
      "Total Loss: 0.30503544583916664\n",
      "------------------------------------ epoch 703 (4212 steps) ------------------------------------\n",
      "Max loss: 0.08039915561676025\n",
      "Min loss: 0.041577622294425964\n",
      "Mean loss: 0.05888639638821284\n",
      "Std loss: 0.014801999867738674\n",
      "Total Loss: 0.35331837832927704\n",
      "------------------------------------ epoch 704 (4218 steps) ------------------------------------\n",
      "Max loss: 0.06845279037952423\n",
      "Min loss: 0.02949586883187294\n",
      "Mean loss: 0.04619558403889338\n",
      "Std loss: 0.014339585065988725\n",
      "Total Loss: 0.2771735042333603\n",
      "------------------------------------ epoch 705 (4224 steps) ------------------------------------\n",
      "Max loss: 0.07629450410604477\n",
      "Min loss: 0.030350863933563232\n",
      "Mean loss: 0.04560572405656179\n",
      "Std loss: 0.016085972752011832\n",
      "Total Loss: 0.2736343443393707\n",
      "------------------------------------ epoch 706 (4230 steps) ------------------------------------\n",
      "Max loss: 0.07261842489242554\n",
      "Min loss: 0.034116603434085846\n",
      "Mean loss: 0.04598864167928696\n",
      "Std loss: 0.013648385725520705\n",
      "Total Loss: 0.27593185007572174\n",
      "------------------------------------ epoch 707 (4236 steps) ------------------------------------\n",
      "Max loss: 0.08069291710853577\n",
      "Min loss: 0.03629722446203232\n",
      "Mean loss: 0.05663590567807356\n",
      "Std loss: 0.014849065250812614\n",
      "Total Loss: 0.3398154340684414\n",
      "------------------------------------ epoch 708 (4242 steps) ------------------------------------\n",
      "Max loss: 0.098151296377182\n",
      "Min loss: 0.02279374562203884\n",
      "Mean loss: 0.05364617984741926\n",
      "Std loss: 0.023770949266937267\n",
      "Total Loss: 0.32187707908451557\n",
      "------------------------------------ epoch 709 (4248 steps) ------------------------------------\n",
      "Max loss: 0.08366498351097107\n",
      "Min loss: 0.029165789484977722\n",
      "Mean loss: 0.05429669593771299\n",
      "Std loss: 0.020532334481513145\n",
      "Total Loss: 0.3257801756262779\n",
      "------------------------------------ epoch 710 (4254 steps) ------------------------------------\n",
      "Max loss: 0.11527003347873688\n",
      "Min loss: 0.04597363993525505\n",
      "Mean loss: 0.0706595852971077\n",
      "Std loss: 0.02533839536538557\n",
      "Total Loss: 0.4239575117826462\n",
      "------------------------------------ epoch 711 (4260 steps) ------------------------------------\n",
      "Max loss: 0.10248210281133652\n",
      "Min loss: 0.04283636435866356\n",
      "Mean loss: 0.06313448275129001\n",
      "Std loss: 0.023511617142870946\n",
      "Total Loss: 0.37880689650774\n",
      "------------------------------------ epoch 712 (4266 steps) ------------------------------------\n",
      "Max loss: 0.06207755208015442\n",
      "Min loss: 0.02990599535405636\n",
      "Mean loss: 0.04505317627141873\n",
      "Std loss: 0.011459268595811003\n",
      "Total Loss: 0.2703190576285124\n",
      "------------------------------------ epoch 713 (4272 steps) ------------------------------------\n",
      "Max loss: 0.08146549761295319\n",
      "Min loss: 0.037517085671424866\n",
      "Mean loss: 0.05389655443529288\n",
      "Std loss: 0.019349693879422827\n",
      "Total Loss: 0.3233793266117573\n",
      "------------------------------------ epoch 714 (4278 steps) ------------------------------------\n",
      "Max loss: 0.1042492687702179\n",
      "Min loss: 0.03540297597646713\n",
      "Mean loss: 0.0563637043039004\n",
      "Std loss: 0.023084465671352472\n",
      "Total Loss: 0.3381822258234024\n",
      "------------------------------------ epoch 715 (4284 steps) ------------------------------------\n",
      "Max loss: 0.057083211839199066\n",
      "Min loss: 0.03585658594965935\n",
      "Mean loss: 0.04335893876850605\n",
      "Std loss: 0.006889094278797567\n",
      "Total Loss: 0.2601536326110363\n",
      "------------------------------------ epoch 716 (4290 steps) ------------------------------------\n",
      "Max loss: 0.0811225026845932\n",
      "Min loss: 0.03447951003909111\n",
      "Mean loss: 0.06177199197312196\n",
      "Std loss: 0.019233242651054555\n",
      "Total Loss: 0.37063195183873177\n",
      "------------------------------------ epoch 717 (4296 steps) ------------------------------------\n",
      "Max loss: 0.04878653213381767\n",
      "Min loss: 0.02648977003991604\n",
      "Mean loss: 0.03507020634909471\n",
      "Std loss: 0.00938944332270219\n",
      "Total Loss: 0.21042123809456825\n",
      "------------------------------------ epoch 718 (4302 steps) ------------------------------------\n",
      "Max loss: 0.068680539727211\n",
      "Min loss: 0.028878560289740562\n",
      "Mean loss: 0.0433948098992308\n",
      "Std loss: 0.015770420218102293\n",
      "Total Loss: 0.2603688593953848\n",
      "------------------------------------ epoch 719 (4308 steps) ------------------------------------\n",
      "Max loss: 0.07214222103357315\n",
      "Min loss: 0.028412653133273125\n",
      "Mean loss: 0.050490133153895535\n",
      "Std loss: 0.015728061047718157\n",
      "Total Loss: 0.3029407989233732\n",
      "------------------------------------ epoch 720 (4314 steps) ------------------------------------\n",
      "Max loss: 0.08068554848432541\n",
      "Min loss: 0.025901397690176964\n",
      "Mean loss: 0.05330443475395441\n",
      "Std loss: 0.0200372989224547\n",
      "Total Loss: 0.31982660852372646\n",
      "------------------------------------ epoch 721 (4320 steps) ------------------------------------\n",
      "Max loss: 0.08367155492305756\n",
      "Min loss: 0.042392827570438385\n",
      "Mean loss: 0.05593528971076012\n",
      "Std loss: 0.017217071448361196\n",
      "Total Loss: 0.3356117382645607\n",
      "------------------------------------ epoch 722 (4326 steps) ------------------------------------\n",
      "Max loss: 0.06253185123205185\n",
      "Min loss: 0.024728670716285706\n",
      "Mean loss: 0.04392503760755062\n",
      "Std loss: 0.01232443166667808\n",
      "Total Loss: 0.2635502256453037\n",
      "------------------------------------ epoch 723 (4332 steps) ------------------------------------\n",
      "Max loss: 0.09086544811725616\n",
      "Min loss: 0.03927917778491974\n",
      "Mean loss: 0.0540008166184028\n",
      "Std loss: 0.016946420023826122\n",
      "Total Loss: 0.3240048997104168\n",
      "------------------------------------ epoch 724 (4338 steps) ------------------------------------\n",
      "Max loss: 0.10647278279066086\n",
      "Min loss: 0.02919984608888626\n",
      "Mean loss: 0.05579731365044912\n",
      "Std loss: 0.024379037670956893\n",
      "Total Loss: 0.3347838819026947\n",
      "------------------------------------ epoch 725 (4344 steps) ------------------------------------\n",
      "Max loss: 0.13759803771972656\n",
      "Min loss: 0.035147566348314285\n",
      "Mean loss: 0.07248753185073535\n",
      "Std loss: 0.03577288499268763\n",
      "Total Loss: 0.4349251911044121\n",
      "------------------------------------ epoch 726 (4350 steps) ------------------------------------\n",
      "Max loss: 0.08670463413000107\n",
      "Min loss: 0.04155619442462921\n",
      "Mean loss: 0.062071471164623894\n",
      "Std loss: 0.015452752702240924\n",
      "Total Loss: 0.3724288269877434\n",
      "------------------------------------ epoch 727 (4356 steps) ------------------------------------\n",
      "Max loss: 0.08001753687858582\n",
      "Min loss: 0.03727779537439346\n",
      "Mean loss: 0.05961097590625286\n",
      "Std loss: 0.014610768743750113\n",
      "Total Loss: 0.35766585543751717\n",
      "------------------------------------ epoch 728 (4362 steps) ------------------------------------\n",
      "Max loss: 0.0566994845867157\n",
      "Min loss: 0.03459928557276726\n",
      "Mean loss: 0.04170003285010656\n",
      "Std loss: 0.008197077516627594\n",
      "Total Loss: 0.25020019710063934\n",
      "------------------------------------ epoch 729 (4368 steps) ------------------------------------\n",
      "Max loss: 0.05867929384112358\n",
      "Min loss: 0.02806822769343853\n",
      "Mean loss: 0.03792050170401732\n",
      "Std loss: 0.010795550369644049\n",
      "Total Loss: 0.22752301022410393\n",
      "------------------------------------ epoch 730 (4374 steps) ------------------------------------\n",
      "Max loss: 0.10141332447528839\n",
      "Min loss: 0.0475473515689373\n",
      "Mean loss: 0.06330912622312705\n",
      "Std loss: 0.017622928610504306\n",
      "Total Loss: 0.3798547573387623\n",
      "------------------------------------ epoch 731 (4380 steps) ------------------------------------\n",
      "Max loss: 0.05444273352622986\n",
      "Min loss: 0.02322372794151306\n",
      "Mean loss: 0.03808658756315708\n",
      "Std loss: 0.011623703693330678\n",
      "Total Loss: 0.2285195253789425\n",
      "------------------------------------ epoch 732 (4386 steps) ------------------------------------\n",
      "Max loss: 0.08341342210769653\n",
      "Min loss: 0.028245210647583008\n",
      "Mean loss: 0.049668086071809135\n",
      "Std loss: 0.019320288070254987\n",
      "Total Loss: 0.2980085164308548\n",
      "------------------------------------ epoch 733 (4392 steps) ------------------------------------\n",
      "Max loss: 0.06046842783689499\n",
      "Min loss: 0.028389697894454002\n",
      "Mean loss: 0.041051640175282955\n",
      "Std loss: 0.00980479532321956\n",
      "Total Loss: 0.24630984105169773\n",
      "------------------------------------ epoch 734 (4398 steps) ------------------------------------\n",
      "Max loss: 0.07146939635276794\n",
      "Min loss: 0.03220653533935547\n",
      "Mean loss: 0.05325231390694777\n",
      "Std loss: 0.01569441011199185\n",
      "Total Loss: 0.31951388344168663\n",
      "------------------------------------ epoch 735 (4404 steps) ------------------------------------\n",
      "Max loss: 0.08531290292739868\n",
      "Min loss: 0.036500100046396255\n",
      "Mean loss: 0.0517921776821216\n",
      "Std loss: 0.018608499885043213\n",
      "Total Loss: 0.31075306609272957\n",
      "------------------------------------ epoch 736 (4410 steps) ------------------------------------\n",
      "Max loss: 0.09035158157348633\n",
      "Min loss: 0.04033958166837692\n",
      "Mean loss: 0.056710511445999146\n",
      "Std loss: 0.018482094587846595\n",
      "Total Loss: 0.3402630686759949\n",
      "------------------------------------ epoch 737 (4416 steps) ------------------------------------\n",
      "Max loss: 0.06342653930187225\n",
      "Min loss: 0.0357232466340065\n",
      "Mean loss: 0.049904397378365196\n",
      "Std loss: 0.009754810207005858\n",
      "Total Loss: 0.2994263842701912\n",
      "------------------------------------ epoch 738 (4422 steps) ------------------------------------\n",
      "Max loss: 0.08798601478338242\n",
      "Min loss: 0.03615031763911247\n",
      "Mean loss: 0.05801683788498243\n",
      "Std loss: 0.021168061961492853\n",
      "Total Loss: 0.34810102730989456\n",
      "------------------------------------ epoch 739 (4428 steps) ------------------------------------\n",
      "Max loss: 0.048561181873083115\n",
      "Min loss: 0.029177572578191757\n",
      "Mean loss: 0.03858065170546373\n",
      "Std loss: 0.006471040847599726\n",
      "Total Loss: 0.23148391023278236\n",
      "------------------------------------ epoch 740 (4434 steps) ------------------------------------\n",
      "Max loss: 0.10520175099372864\n",
      "Min loss: 0.03314982354640961\n",
      "Mean loss: 0.05376411912341913\n",
      "Std loss: 0.02499511552086808\n",
      "Total Loss: 0.32258471474051476\n",
      "------------------------------------ epoch 741 (4440 steps) ------------------------------------\n",
      "Max loss: 0.06520041078329086\n",
      "Min loss: 0.028478270396590233\n",
      "Mean loss: 0.043537160071233906\n",
      "Std loss: 0.012008786982953188\n",
      "Total Loss: 0.26122296042740345\n",
      "------------------------------------ epoch 742 (4446 steps) ------------------------------------\n",
      "Max loss: 0.050694745033979416\n",
      "Min loss: 0.02848893217742443\n",
      "Mean loss: 0.03992532224704822\n",
      "Std loss: 0.007320021310528675\n",
      "Total Loss: 0.23955193348228931\n",
      "------------------------------------ epoch 743 (4452 steps) ------------------------------------\n",
      "Max loss: 0.07115195691585541\n",
      "Min loss: 0.02332555502653122\n",
      "Mean loss: 0.04353017173707485\n",
      "Std loss: 0.018046220910096784\n",
      "Total Loss: 0.2611810304224491\n",
      "------------------------------------ epoch 744 (4458 steps) ------------------------------------\n",
      "Max loss: 0.07882586121559143\n",
      "Min loss: 0.03728392720222473\n",
      "Mean loss: 0.052891879032055535\n",
      "Std loss: 0.01375574568476457\n",
      "Total Loss: 0.3173512741923332\n",
      "------------------------------------ epoch 745 (4464 steps) ------------------------------------\n",
      "Max loss: 0.08058872073888779\n",
      "Min loss: 0.030085772275924683\n",
      "Mean loss: 0.04630326324452957\n",
      "Std loss: 0.017946630361506663\n",
      "Total Loss: 0.2778195794671774\n",
      "------------------------------------ epoch 746 (4470 steps) ------------------------------------\n",
      "Max loss: 0.05966389179229736\n",
      "Min loss: 0.03680267184972763\n",
      "Mean loss: 0.046617066487669945\n",
      "Std loss: 0.008350991931602264\n",
      "Total Loss: 0.27970239892601967\n",
      "------------------------------------ epoch 747 (4476 steps) ------------------------------------\n",
      "Max loss: 0.0513056181371212\n",
      "Min loss: 0.02611258253455162\n",
      "Mean loss: 0.035594927767912544\n",
      "Std loss: 0.007631590653327196\n",
      "Total Loss: 0.21356956660747528\n",
      "------------------------------------ epoch 748 (4482 steps) ------------------------------------\n",
      "Max loss: 0.06179146468639374\n",
      "Min loss: 0.024974212050437927\n",
      "Mean loss: 0.03855355673780044\n",
      "Std loss: 0.011598654806362262\n",
      "Total Loss: 0.23132134042680264\n",
      "------------------------------------ epoch 749 (4488 steps) ------------------------------------\n",
      "Max loss: 0.0723404511809349\n",
      "Min loss: 0.04391402751207352\n",
      "Mean loss: 0.05855214595794678\n",
      "Std loss: 0.010175532646660473\n",
      "Total Loss: 0.35131287574768066\n",
      "------------------------------------ epoch 750 (4494 steps) ------------------------------------\n",
      "Max loss: 0.07694712281227112\n",
      "Min loss: 0.038678497076034546\n",
      "Mean loss: 0.05836493894457817\n",
      "Std loss: 0.014900898642553253\n",
      "Total Loss: 0.350189633667469\n",
      "------------------------------------ epoch 751 (4500 steps) ------------------------------------\n",
      "Max loss: 0.08285544812679291\n",
      "Min loss: 0.024347804486751556\n",
      "Mean loss: 0.049308511738975845\n",
      "Std loss: 0.018800936758984175\n",
      "Total Loss: 0.29585107043385506\n",
      "------------------------------------ epoch 752 (4506 steps) ------------------------------------\n",
      "Max loss: 0.0801396295428276\n",
      "Min loss: 0.03971637040376663\n",
      "Mean loss: 0.06197280436754227\n",
      "Std loss: 0.013732981572694716\n",
      "Total Loss: 0.3718368262052536\n",
      "------------------------------------ epoch 753 (4512 steps) ------------------------------------\n",
      "Max loss: 0.06124594807624817\n",
      "Min loss: 0.027283862233161926\n",
      "Mean loss: 0.042342943139374256\n",
      "Std loss: 0.01167372834385719\n",
      "Total Loss: 0.25405765883624554\n",
      "------------------------------------ epoch 754 (4518 steps) ------------------------------------\n",
      "Max loss: 0.08877688646316528\n",
      "Min loss: 0.032041773200035095\n",
      "Mean loss: 0.05999453862508138\n",
      "Std loss: 0.022002881942882017\n",
      "Total Loss: 0.3599672317504883\n",
      "------------------------------------ epoch 755 (4524 steps) ------------------------------------\n",
      "Max loss: 0.1016886755824089\n",
      "Min loss: 0.036425597965717316\n",
      "Mean loss: 0.05145539902150631\n",
      "Std loss: 0.02292834862046891\n",
      "Total Loss: 0.30873239412903786\n",
      "------------------------------------ epoch 756 (4530 steps) ------------------------------------\n",
      "Max loss: 0.08849664032459259\n",
      "Min loss: 0.037995584309101105\n",
      "Mean loss: 0.0525712122519811\n",
      "Std loss: 0.017748917386543103\n",
      "Total Loss: 0.3154272735118866\n",
      "------------------------------------ epoch 757 (4536 steps) ------------------------------------\n",
      "Max loss: 0.11113987863063812\n",
      "Min loss: 0.03482022136449814\n",
      "Mean loss: 0.06397031061351299\n",
      "Std loss: 0.025223668943894148\n",
      "Total Loss: 0.38382186368107796\n",
      "------------------------------------ epoch 758 (4542 steps) ------------------------------------\n",
      "Max loss: 0.08615760505199432\n",
      "Min loss: 0.03199753910303116\n",
      "Mean loss: 0.06391244692107041\n",
      "Std loss: 0.02014463295164002\n",
      "Total Loss: 0.3834746815264225\n",
      "------------------------------------ epoch 759 (4548 steps) ------------------------------------\n",
      "Max loss: 0.09731040894985199\n",
      "Min loss: 0.022075286135077477\n",
      "Mean loss: 0.052826522228618465\n",
      "Std loss: 0.023726541563656642\n",
      "Total Loss: 0.3169591333717108\n",
      "------------------------------------ epoch 760 (4554 steps) ------------------------------------\n",
      "Max loss: 0.05565769597887993\n",
      "Min loss: 0.03036939911544323\n",
      "Mean loss: 0.04756169362614552\n",
      "Std loss: 0.008072831731178148\n",
      "Total Loss: 0.28537016175687313\n",
      "------------------------------------ epoch 761 (4560 steps) ------------------------------------\n",
      "Max loss: 0.10486041009426117\n",
      "Min loss: 0.0341048538684845\n",
      "Mean loss: 0.0593748576939106\n",
      "Std loss: 0.0279557303596372\n",
      "Total Loss: 0.3562491461634636\n",
      "------------------------------------ epoch 762 (4566 steps) ------------------------------------\n",
      "Max loss: 0.05301425978541374\n",
      "Min loss: 0.04454633593559265\n",
      "Mean loss: 0.048535117879509926\n",
      "Std loss: 0.0031508411436440044\n",
      "Total Loss: 0.29121070727705956\n",
      "------------------------------------ epoch 763 (4572 steps) ------------------------------------\n",
      "Max loss: 0.08659704774618149\n",
      "Min loss: 0.02646688185632229\n",
      "Mean loss: 0.049997201189398766\n",
      "Std loss: 0.02261754808119093\n",
      "Total Loss: 0.2999832071363926\n",
      "------------------------------------ epoch 764 (4578 steps) ------------------------------------\n",
      "Max loss: 0.07012786716222763\n",
      "Min loss: 0.03567234426736832\n",
      "Mean loss: 0.055083188538750015\n",
      "Std loss: 0.013342320750750359\n",
      "Total Loss: 0.3304991312325001\n",
      "------------------------------------ epoch 765 (4584 steps) ------------------------------------\n",
      "Max loss: 0.06968113034963608\n",
      "Min loss: 0.03383468836545944\n",
      "Mean loss: 0.052168952922026314\n",
      "Std loss: 0.012823136585722988\n",
      "Total Loss: 0.3130137175321579\n",
      "------------------------------------ epoch 766 (4590 steps) ------------------------------------\n",
      "Max loss: 0.0683646947145462\n",
      "Min loss: 0.042016398161649704\n",
      "Mean loss: 0.056339163333177567\n",
      "Std loss: 0.009789131184560283\n",
      "Total Loss: 0.3380349799990654\n",
      "------------------------------------ epoch 767 (4596 steps) ------------------------------------\n",
      "Max loss: 0.049740683287382126\n",
      "Min loss: 0.03590615838766098\n",
      "Mean loss: 0.04419712473948797\n",
      "Std loss: 0.004253478441156267\n",
      "Total Loss: 0.2651827484369278\n",
      "------------------------------------ epoch 768 (4602 steps) ------------------------------------\n",
      "Max loss: 0.08106683194637299\n",
      "Min loss: 0.03618614003062248\n",
      "Mean loss: 0.0542603712528944\n",
      "Std loss: 0.014173135175366886\n",
      "Total Loss: 0.3255622275173664\n",
      "------------------------------------ epoch 769 (4608 steps) ------------------------------------\n",
      "Max loss: 0.05244320258498192\n",
      "Min loss: 0.03254995867609978\n",
      "Mean loss: 0.04406152479350567\n",
      "Std loss: 0.006558672628675072\n",
      "Total Loss: 0.264369148761034\n",
      "------------------------------------ epoch 770 (4614 steps) ------------------------------------\n",
      "Max loss: 0.05694970116019249\n",
      "Min loss: 0.01806703768670559\n",
      "Mean loss: 0.03839882370084524\n",
      "Std loss: 0.012152411622659676\n",
      "Total Loss: 0.23039294220507145\n",
      "------------------------------------ epoch 771 (4620 steps) ------------------------------------\n",
      "Max loss: 0.05094548314809799\n",
      "Min loss: 0.029853535816073418\n",
      "Mean loss: 0.03811505902558565\n",
      "Std loss: 0.00633327086891482\n",
      "Total Loss: 0.2286903541535139\n",
      "------------------------------------ epoch 772 (4626 steps) ------------------------------------\n",
      "Max loss: 0.104265496134758\n",
      "Min loss: 0.032308414578437805\n",
      "Mean loss: 0.04987087349096934\n",
      "Std loss: 0.025177326812389773\n",
      "Total Loss: 0.29922524094581604\n",
      "------------------------------------ epoch 773 (4632 steps) ------------------------------------\n",
      "Max loss: 0.06279998272657394\n",
      "Min loss: 0.030771806836128235\n",
      "Mean loss: 0.04210865683853626\n",
      "Std loss: 0.010305724447487832\n",
      "Total Loss: 0.2526519410312176\n",
      "------------------------------------ epoch 774 (4638 steps) ------------------------------------\n",
      "Max loss: 0.05994585156440735\n",
      "Min loss: 0.03404451906681061\n",
      "Mean loss: 0.04889040937026342\n",
      "Std loss: 0.010667465473751666\n",
      "Total Loss: 0.2933424562215805\n",
      "------------------------------------ epoch 775 (4644 steps) ------------------------------------\n",
      "Max loss: 0.07268016785383224\n",
      "Min loss: 0.025344563648104668\n",
      "Mean loss: 0.04656733603527149\n",
      "Std loss: 0.01641047664226651\n",
      "Total Loss: 0.2794040162116289\n",
      "------------------------------------ epoch 776 (4650 steps) ------------------------------------\n",
      "Max loss: 0.0626261979341507\n",
      "Min loss: 0.021231308579444885\n",
      "Mean loss: 0.03600654937326908\n",
      "Std loss: 0.01428591293340812\n",
      "Total Loss: 0.2160392962396145\n",
      "------------------------------------ epoch 777 (4656 steps) ------------------------------------\n",
      "Max loss: 0.04550943523645401\n",
      "Min loss: 0.028887927532196045\n",
      "Mean loss: 0.03887048860390981\n",
      "Std loss: 0.00551453762520358\n",
      "Total Loss: 0.23322293162345886\n",
      "------------------------------------ epoch 778 (4662 steps) ------------------------------------\n",
      "Max loss: 0.1012788638472557\n",
      "Min loss: 0.023866629227995872\n",
      "Mean loss: 0.05196736784030994\n",
      "Std loss: 0.02466790603830914\n",
      "Total Loss: 0.3118042070418596\n",
      "------------------------------------ epoch 779 (4668 steps) ------------------------------------\n",
      "Max loss: 0.05413893982768059\n",
      "Min loss: 0.030273526906967163\n",
      "Mean loss: 0.044965012619892754\n",
      "Std loss: 0.009125733084720663\n",
      "Total Loss: 0.26979007571935654\n",
      "------------------------------------ epoch 780 (4674 steps) ------------------------------------\n",
      "Max loss: 0.08364363014698029\n",
      "Min loss: 0.022349495440721512\n",
      "Mean loss: 0.05339577700942755\n",
      "Std loss: 0.021737214222099833\n",
      "Total Loss: 0.3203746620565653\n",
      "------------------------------------ epoch 781 (4680 steps) ------------------------------------\n",
      "Max loss: 0.09977298974990845\n",
      "Min loss: 0.02463526651263237\n",
      "Mean loss: 0.05573118974765142\n",
      "Std loss: 0.02531080191061455\n",
      "Total Loss: 0.3343871384859085\n",
      "------------------------------------ epoch 782 (4686 steps) ------------------------------------\n",
      "Max loss: 0.05922670289874077\n",
      "Min loss: 0.023876860737800598\n",
      "Mean loss: 0.04255232121795416\n",
      "Std loss: 0.011986367919911714\n",
      "Total Loss: 0.25531392730772495\n",
      "------------------------------------ epoch 783 (4692 steps) ------------------------------------\n",
      "Max loss: 0.04691598564386368\n",
      "Min loss: 0.026136478409171104\n",
      "Mean loss: 0.03899528613934914\n",
      "Std loss: 0.006380228866754359\n",
      "Total Loss: 0.23397171683609486\n",
      "------------------------------------ epoch 784 (4698 steps) ------------------------------------\n",
      "Max loss: 0.08538548648357391\n",
      "Min loss: 0.026273664087057114\n",
      "Mean loss: 0.05204550611476103\n",
      "Std loss: 0.018387000119896635\n",
      "Total Loss: 0.3122730366885662\n",
      "------------------------------------ epoch 785 (4704 steps) ------------------------------------\n",
      "Max loss: 0.0895024761557579\n",
      "Min loss: 0.01894301548600197\n",
      "Mean loss: 0.048283291359742485\n",
      "Std loss: 0.022762335453773683\n",
      "Total Loss: 0.2896997481584549\n",
      "------------------------------------ epoch 786 (4710 steps) ------------------------------------\n",
      "Max loss: 0.07229695469141006\n",
      "Min loss: 0.032841604202985764\n",
      "Mean loss: 0.047485593085487686\n",
      "Std loss: 0.012803589999727989\n",
      "Total Loss: 0.2849135585129261\n",
      "------------------------------------ epoch 787 (4716 steps) ------------------------------------\n",
      "Max loss: 0.06769251823425293\n",
      "Min loss: 0.028517432510852814\n",
      "Mean loss: 0.043409282341599464\n",
      "Std loss: 0.015370572442466972\n",
      "Total Loss: 0.2604556940495968\n",
      "------------------------------------ epoch 788 (4722 steps) ------------------------------------\n",
      "Max loss: 0.09228385984897614\n",
      "Min loss: 0.0308027695864439\n",
      "Mean loss: 0.05103135326256355\n",
      "Std loss: 0.02084389401030278\n",
      "Total Loss: 0.3061881195753813\n",
      "------------------------------------ epoch 789 (4728 steps) ------------------------------------\n",
      "Max loss: 0.0758608728647232\n",
      "Min loss: 0.03108958527445793\n",
      "Mean loss: 0.048315118377407394\n",
      "Std loss: 0.014529220144218551\n",
      "Total Loss: 0.28989071026444435\n",
      "------------------------------------ epoch 790 (4734 steps) ------------------------------------\n",
      "Max loss: 0.05280467867851257\n",
      "Min loss: 0.024481749162077904\n",
      "Mean loss: 0.040162975899875164\n",
      "Std loss: 0.009539277293817\n",
      "Total Loss: 0.24097785539925098\n",
      "------------------------------------ epoch 791 (4740 steps) ------------------------------------\n",
      "Max loss: 0.07546855509281158\n",
      "Min loss: 0.023325718939304352\n",
      "Mean loss: 0.04681961114207903\n",
      "Std loss: 0.016555793181068673\n",
      "Total Loss: 0.2809176668524742\n",
      "------------------------------------ epoch 792 (4746 steps) ------------------------------------\n",
      "Max loss: 0.08383157849311829\n",
      "Min loss: 0.031852319836616516\n",
      "Mean loss: 0.05335084224740664\n",
      "Std loss: 0.015861911082213777\n",
      "Total Loss: 0.32010505348443985\n",
      "------------------------------------ epoch 793 (4752 steps) ------------------------------------\n",
      "Max loss: 0.07593897730112076\n",
      "Min loss: 0.021987682208418846\n",
      "Mean loss: 0.039379947197933994\n",
      "Std loss: 0.01707130853029351\n",
      "Total Loss: 0.23627968318760395\n",
      "------------------------------------ epoch 794 (4758 steps) ------------------------------------\n",
      "Max loss: 0.06062864884734154\n",
      "Min loss: 0.030152156949043274\n",
      "Mean loss: 0.048057962829867996\n",
      "Std loss: 0.0109423409490479\n",
      "Total Loss: 0.288347776979208\n",
      "------------------------------------ epoch 795 (4764 steps) ------------------------------------\n",
      "Max loss: 0.08927386999130249\n",
      "Min loss: 0.03054805099964142\n",
      "Mean loss: 0.05074442798892657\n",
      "Std loss: 0.020099070594164364\n",
      "Total Loss: 0.3044665679335594\n",
      "------------------------------------ epoch 796 (4770 steps) ------------------------------------\n",
      "Max loss: 0.08790164440870285\n",
      "Min loss: 0.023182129487395287\n",
      "Mean loss: 0.0556247653439641\n",
      "Std loss: 0.020225867847403937\n",
      "Total Loss: 0.3337485920637846\n",
      "------------------------------------ epoch 797 (4776 steps) ------------------------------------\n",
      "Max loss: 0.09080376476049423\n",
      "Min loss: 0.030344566330313683\n",
      "Mean loss: 0.05429012297342221\n",
      "Std loss: 0.020523393484622003\n",
      "Total Loss: 0.32574073784053326\n",
      "------------------------------------ epoch 798 (4782 steps) ------------------------------------\n",
      "Max loss: 0.07001279294490814\n",
      "Min loss: 0.0346762090921402\n",
      "Mean loss: 0.04605454454819361\n",
      "Std loss: 0.012429116799287772\n",
      "Total Loss: 0.2763272672891617\n",
      "------------------------------------ epoch 799 (4788 steps) ------------------------------------\n",
      "Max loss: 0.074669249355793\n",
      "Min loss: 0.021597042679786682\n",
      "Mean loss: 0.05465260582665602\n",
      "Std loss: 0.017823302519608626\n",
      "Total Loss: 0.32791563495993614\n",
      "------------------------------------ epoch 800 (4794 steps) ------------------------------------\n",
      "Max loss: 0.052394360303878784\n",
      "Min loss: 0.026784747838974\n",
      "Mean loss: 0.0376727736244599\n",
      "Std loss: 0.011103638130318538\n",
      "Total Loss: 0.22603664174675941\n",
      "------------------------------------ epoch 801 (4800 steps) ------------------------------------\n",
      "Max loss: 0.08815215528011322\n",
      "Min loss: 0.026990167796611786\n",
      "Mean loss: 0.04972374439239502\n",
      "Std loss: 0.021928873997635774\n",
      "Total Loss: 0.2983424663543701\n",
      "saved model at ./weights/model_801.pth\n",
      "------------------------------------ epoch 802 (4806 steps) ------------------------------------\n",
      "Max loss: 0.06067635864019394\n",
      "Min loss: 0.03390315920114517\n",
      "Mean loss: 0.04899157335360845\n",
      "Std loss: 0.0082288862660866\n",
      "Total Loss: 0.2939494401216507\n",
      "------------------------------------ epoch 803 (4812 steps) ------------------------------------\n",
      "Max loss: 0.0528995618224144\n",
      "Min loss: 0.03406776860356331\n",
      "Mean loss: 0.043451523408293724\n",
      "Std loss: 0.006920604533305719\n",
      "Total Loss: 0.26070914044976234\n",
      "------------------------------------ epoch 804 (4818 steps) ------------------------------------\n",
      "Max loss: 0.07380886375904083\n",
      "Min loss: 0.0361710786819458\n",
      "Mean loss: 0.05101415577034155\n",
      "Std loss: 0.012220549367508906\n",
      "Total Loss: 0.30608493462204933\n",
      "------------------------------------ epoch 805 (4824 steps) ------------------------------------\n",
      "Max loss: 0.06545273959636688\n",
      "Min loss: 0.021288415417075157\n",
      "Mean loss: 0.04085122638692459\n",
      "Std loss: 0.013716298997282989\n",
      "Total Loss: 0.2451073583215475\n",
      "------------------------------------ epoch 806 (4830 steps) ------------------------------------\n",
      "Max loss: 0.06100590154528618\n",
      "Min loss: 0.02981686405837536\n",
      "Mean loss: 0.04310532007366419\n",
      "Std loss: 0.01185667481554538\n",
      "Total Loss: 0.25863192044198513\n",
      "------------------------------------ epoch 807 (4836 steps) ------------------------------------\n",
      "Max loss: 0.04996436461806297\n",
      "Min loss: 0.028429847210645676\n",
      "Mean loss: 0.0409456646690766\n",
      "Std loss: 0.00719254027226214\n",
      "Total Loss: 0.2456739880144596\n",
      "------------------------------------ epoch 808 (4842 steps) ------------------------------------\n",
      "Max loss: 0.08953358232975006\n",
      "Min loss: 0.028661441057920456\n",
      "Mean loss: 0.047849717239538826\n",
      "Std loss: 0.020171817997434078\n",
      "Total Loss: 0.28709830343723297\n",
      "------------------------------------ epoch 809 (4848 steps) ------------------------------------\n",
      "Max loss: 0.0977001041173935\n",
      "Min loss: 0.03533385321497917\n",
      "Mean loss: 0.05860271118581295\n",
      "Std loss: 0.020742590115806037\n",
      "Total Loss: 0.3516162671148777\n",
      "------------------------------------ epoch 810 (4854 steps) ------------------------------------\n",
      "Max loss: 0.06189129874110222\n",
      "Min loss: 0.03128283470869064\n",
      "Mean loss: 0.044910812129577\n",
      "Std loss: 0.010887014204124506\n",
      "Total Loss: 0.269464872777462\n",
      "------------------------------------ epoch 811 (4860 steps) ------------------------------------\n",
      "Max loss: 0.0744481012225151\n",
      "Min loss: 0.030058324337005615\n",
      "Mean loss: 0.04531722702085972\n",
      "Std loss: 0.014663813757102651\n",
      "Total Loss: 0.2719033621251583\n",
      "------------------------------------ epoch 812 (4866 steps) ------------------------------------\n",
      "Max loss: 0.07850795984268188\n",
      "Min loss: 0.025829926133155823\n",
      "Mean loss: 0.04133680214484533\n",
      "Std loss: 0.0195572811675491\n",
      "Total Loss: 0.24802081286907196\n",
      "------------------------------------ epoch 813 (4872 steps) ------------------------------------\n",
      "Max loss: 0.06508942693471909\n",
      "Min loss: 0.032096054404973984\n",
      "Mean loss: 0.04276405026515325\n",
      "Std loss: 0.012333956953941886\n",
      "Total Loss: 0.2565843015909195\n",
      "------------------------------------ epoch 814 (4878 steps) ------------------------------------\n",
      "Max loss: 0.06746344268321991\n",
      "Min loss: 0.024938037618994713\n",
      "Mean loss: 0.0491510151575009\n",
      "Std loss: 0.016284856852413595\n",
      "Total Loss: 0.2949060909450054\n",
      "------------------------------------ epoch 815 (4884 steps) ------------------------------------\n",
      "Max loss: 0.060063958168029785\n",
      "Min loss: 0.028528932482004166\n",
      "Mean loss: 0.04310956969857216\n",
      "Std loss: 0.012275386199573792\n",
      "Total Loss: 0.25865741819143295\n",
      "------------------------------------ epoch 816 (4890 steps) ------------------------------------\n",
      "Max loss: 0.09622126817703247\n",
      "Min loss: 0.03564184159040451\n",
      "Mean loss: 0.06388326237599055\n",
      "Std loss: 0.020840949328278423\n",
      "Total Loss: 0.3832995742559433\n",
      "------------------------------------ epoch 817 (4896 steps) ------------------------------------\n",
      "Max loss: 0.06332650035619736\n",
      "Min loss: 0.028564773499965668\n",
      "Mean loss: 0.04587597772479057\n",
      "Std loss: 0.010769041343111286\n",
      "Total Loss: 0.27525586634874344\n",
      "------------------------------------ epoch 818 (4902 steps) ------------------------------------\n",
      "Max loss: 0.07539674639701843\n",
      "Min loss: 0.03250083327293396\n",
      "Mean loss: 0.045987180123726525\n",
      "Std loss: 0.014376339761891548\n",
      "Total Loss: 0.27592308074235916\n",
      "------------------------------------ epoch 819 (4908 steps) ------------------------------------\n",
      "Max loss: 0.06592319905757904\n",
      "Min loss: 0.034113019704818726\n",
      "Mean loss: 0.04547454106311003\n",
      "Std loss: 0.00996313653899597\n",
      "Total Loss: 0.2728472463786602\n",
      "------------------------------------ epoch 820 (4914 steps) ------------------------------------\n",
      "Max loss: 0.05321114510297775\n",
      "Min loss: 0.02503993734717369\n",
      "Mean loss: 0.03829770193745693\n",
      "Std loss: 0.011127934999192261\n",
      "Total Loss: 0.22978621162474155\n",
      "------------------------------------ epoch 821 (4920 steps) ------------------------------------\n",
      "Max loss: 0.07448668777942657\n",
      "Min loss: 0.035550907254219055\n",
      "Mean loss: 0.06002113471428553\n",
      "Std loss: 0.012375018049882308\n",
      "Total Loss: 0.3601268082857132\n",
      "------------------------------------ epoch 822 (4926 steps) ------------------------------------\n",
      "Max loss: 0.04878373071551323\n",
      "Min loss: 0.027248045429587364\n",
      "Mean loss: 0.03732182116558155\n",
      "Std loss: 0.00695437097308749\n",
      "Total Loss: 0.22393092699348927\n",
      "------------------------------------ epoch 823 (4932 steps) ------------------------------------\n",
      "Max loss: 0.0708608627319336\n",
      "Min loss: 0.02358616143465042\n",
      "Mean loss: 0.040064131220181785\n",
      "Std loss: 0.015159330768177274\n",
      "Total Loss: 0.2403847873210907\n",
      "------------------------------------ epoch 824 (4938 steps) ------------------------------------\n",
      "Max loss: 0.06303101778030396\n",
      "Min loss: 0.02938574180006981\n",
      "Mean loss: 0.0434369221329689\n",
      "Std loss: 0.011308855486638847\n",
      "Total Loss: 0.2606215327978134\n",
      "------------------------------------ epoch 825 (4944 steps) ------------------------------------\n",
      "Max loss: 0.05089301988482475\n",
      "Min loss: 0.031316276639699936\n",
      "Mean loss: 0.03973365326722463\n",
      "Std loss: 0.006876585849388835\n",
      "Total Loss: 0.23840191960334778\n",
      "------------------------------------ epoch 826 (4950 steps) ------------------------------------\n",
      "Max loss: 0.05621091648936272\n",
      "Min loss: 0.036098986864089966\n",
      "Mean loss: 0.045642526199420295\n",
      "Std loss: 0.007222075537064371\n",
      "Total Loss: 0.27385515719652176\n",
      "------------------------------------ epoch 827 (4956 steps) ------------------------------------\n",
      "Max loss: 0.09155353903770447\n",
      "Min loss: 0.03132012486457825\n",
      "Mean loss: 0.055365391075611115\n",
      "Std loss: 0.02029020526861308\n",
      "Total Loss: 0.3321923464536667\n",
      "------------------------------------ epoch 828 (4962 steps) ------------------------------------\n",
      "Max loss: 0.04429718852043152\n",
      "Min loss: 0.02912072464823723\n",
      "Mean loss: 0.03510367497801781\n",
      "Std loss: 0.0048086633170723305\n",
      "Total Loss: 0.21062204986810684\n",
      "------------------------------------ epoch 829 (4968 steps) ------------------------------------\n",
      "Max loss: 0.11032774299383163\n",
      "Min loss: 0.026370670646429062\n",
      "Mean loss: 0.04863585935284694\n",
      "Std loss: 0.02902463939294342\n",
      "Total Loss: 0.29181515611708164\n",
      "------------------------------------ epoch 830 (4974 steps) ------------------------------------\n",
      "Max loss: 0.0595366470515728\n",
      "Min loss: 0.037761837244033813\n",
      "Mean loss: 0.04994641803205013\n",
      "Std loss: 0.007885387152247493\n",
      "Total Loss: 0.2996785081923008\n",
      "------------------------------------ epoch 831 (4980 steps) ------------------------------------\n",
      "Max loss: 0.08755579590797424\n",
      "Min loss: 0.028569310903549194\n",
      "Mean loss: 0.046665395299593605\n",
      "Std loss: 0.02026363381370718\n",
      "Total Loss: 0.27999237179756165\n",
      "------------------------------------ epoch 832 (4986 steps) ------------------------------------\n",
      "Max loss: 0.059124451130628586\n",
      "Min loss: 0.02998010814189911\n",
      "Mean loss: 0.04180435215433439\n",
      "Std loss: 0.012008050372665181\n",
      "Total Loss: 0.2508261129260063\n",
      "------------------------------------ epoch 833 (4992 steps) ------------------------------------\n",
      "Max loss: 0.06451871991157532\n",
      "Min loss: 0.029384447261691093\n",
      "Mean loss: 0.05024601115534703\n",
      "Std loss: 0.01356496019210036\n",
      "Total Loss: 0.3014760669320822\n",
      "------------------------------------ epoch 834 (4998 steps) ------------------------------------\n",
      "Max loss: 0.12206437438726425\n",
      "Min loss: 0.02523854188621044\n",
      "Mean loss: 0.0551310737306873\n",
      "Std loss: 0.03127182580244516\n",
      "Total Loss: 0.3307864423841238\n",
      "------------------------------------ epoch 835 (5004 steps) ------------------------------------\n",
      "Max loss: 0.0732429102063179\n",
      "Min loss: 0.029605820775032043\n",
      "Mean loss: 0.04982383983830611\n",
      "Std loss: 0.013041691705754652\n",
      "Total Loss: 0.29894303902983665\n",
      "------------------------------------ epoch 836 (5010 steps) ------------------------------------\n",
      "Max loss: 0.05948299914598465\n",
      "Min loss: 0.020508240908384323\n",
      "Mean loss: 0.042523933574557304\n",
      "Std loss: 0.012572274344059707\n",
      "Total Loss: 0.2551436014473438\n",
      "------------------------------------ epoch 837 (5016 steps) ------------------------------------\n",
      "Max loss: 0.08702098578214645\n",
      "Min loss: 0.017444174736738205\n",
      "Mean loss: 0.03844323816398779\n",
      "Std loss: 0.022889977067522525\n",
      "Total Loss: 0.23065942898392677\n",
      "------------------------------------ epoch 838 (5022 steps) ------------------------------------\n",
      "Max loss: 0.10741766542196274\n",
      "Min loss: 0.02836875431239605\n",
      "Mean loss: 0.053165567107498646\n",
      "Std loss: 0.027785435650879944\n",
      "Total Loss: 0.3189934026449919\n",
      "------------------------------------ epoch 839 (5028 steps) ------------------------------------\n",
      "Max loss: 0.07275065779685974\n",
      "Min loss: 0.027588173747062683\n",
      "Mean loss: 0.04616519622504711\n",
      "Std loss: 0.016491472998486887\n",
      "Total Loss: 0.27699117735028267\n",
      "------------------------------------ epoch 840 (5034 steps) ------------------------------------\n",
      "Max loss: 0.07018731534481049\n",
      "Min loss: 0.029202556237578392\n",
      "Mean loss: 0.04602864167342583\n",
      "Std loss: 0.014570466465798107\n",
      "Total Loss: 0.276171850040555\n",
      "------------------------------------ epoch 841 (5040 steps) ------------------------------------\n",
      "Max loss: 0.06588828563690186\n",
      "Min loss: 0.025231346487998962\n",
      "Mean loss: 0.04312961641699076\n",
      "Std loss: 0.013031116920482266\n",
      "Total Loss: 0.25877769850194454\n",
      "------------------------------------ epoch 842 (5046 steps) ------------------------------------\n",
      "Max loss: 0.04984762519598007\n",
      "Min loss: 0.023801011964678764\n",
      "Mean loss: 0.038423796805242695\n",
      "Std loss: 0.009125676094128244\n",
      "Total Loss: 0.23054278083145618\n",
      "------------------------------------ epoch 843 (5052 steps) ------------------------------------\n",
      "Max loss: 0.07144703716039658\n",
      "Min loss: 0.023990660905838013\n",
      "Mean loss: 0.04405946098268032\n",
      "Std loss: 0.014552277843291803\n",
      "Total Loss: 0.2643567658960819\n",
      "------------------------------------ epoch 844 (5058 steps) ------------------------------------\n",
      "Max loss: 0.05474003404378891\n",
      "Min loss: 0.030666694045066833\n",
      "Mean loss: 0.043099419524272285\n",
      "Std loss: 0.0073997299422551045\n",
      "Total Loss: 0.2585965171456337\n",
      "------------------------------------ epoch 845 (5064 steps) ------------------------------------\n",
      "Max loss: 0.06681177020072937\n",
      "Min loss: 0.02207791432738304\n",
      "Mean loss: 0.04106425618131956\n",
      "Std loss: 0.014836839498089444\n",
      "Total Loss: 0.24638553708791733\n",
      "------------------------------------ epoch 846 (5070 steps) ------------------------------------\n",
      "Max loss: 0.08516554534435272\n",
      "Min loss: 0.03120165318250656\n",
      "Mean loss: 0.046455848341186844\n",
      "Std loss: 0.0180858412414075\n",
      "Total Loss: 0.27873509004712105\n",
      "------------------------------------ epoch 847 (5076 steps) ------------------------------------\n",
      "Max loss: 0.07258656620979309\n",
      "Min loss: 0.03334563225507736\n",
      "Mean loss: 0.048506823678811394\n",
      "Std loss: 0.013343006058271777\n",
      "Total Loss: 0.29104094207286835\n",
      "------------------------------------ epoch 848 (5082 steps) ------------------------------------\n",
      "Max loss: 0.05960143730044365\n",
      "Min loss: 0.026364890858530998\n",
      "Mean loss: 0.03649197115252415\n",
      "Std loss: 0.010727874967513911\n",
      "Total Loss: 0.21895182691514492\n",
      "------------------------------------ epoch 849 (5088 steps) ------------------------------------\n",
      "Max loss: 0.07347293198108673\n",
      "Min loss: 0.038655977696180344\n",
      "Mean loss: 0.052793788413206734\n",
      "Std loss: 0.011658003552384501\n",
      "Total Loss: 0.3167627304792404\n",
      "------------------------------------ epoch 850 (5094 steps) ------------------------------------\n",
      "Max loss: 0.048123352229595184\n",
      "Min loss: 0.024747103452682495\n",
      "Mean loss: 0.03479980708410343\n",
      "Std loss: 0.009308420108335937\n",
      "Total Loss: 0.20879884250462055\n",
      "------------------------------------ epoch 851 (5100 steps) ------------------------------------\n",
      "Max loss: 0.06575226038694382\n",
      "Min loss: 0.02767038717865944\n",
      "Mean loss: 0.04168441519141197\n",
      "Std loss: 0.012421790590565833\n",
      "Total Loss: 0.25010649114847183\n",
      "------------------------------------ epoch 852 (5106 steps) ------------------------------------\n",
      "Max loss: 0.07833857089281082\n",
      "Min loss: 0.029734104871749878\n",
      "Mean loss: 0.059715256094932556\n",
      "Std loss: 0.015330374046980818\n",
      "Total Loss: 0.35829153656959534\n",
      "------------------------------------ epoch 853 (5112 steps) ------------------------------------\n",
      "Max loss: 0.0479048490524292\n",
      "Min loss: 0.023591678589582443\n",
      "Mean loss: 0.03699142920474211\n",
      "Std loss: 0.007511086591070389\n",
      "Total Loss: 0.22194857522845268\n",
      "------------------------------------ epoch 854 (5118 steps) ------------------------------------\n",
      "Max loss: 0.06460215896368027\n",
      "Min loss: 0.02114861086010933\n",
      "Mean loss: 0.03666720073670149\n",
      "Std loss: 0.014691875332516917\n",
      "Total Loss: 0.22000320442020893\n",
      "------------------------------------ epoch 855 (5124 steps) ------------------------------------\n",
      "Max loss: 0.06342293322086334\n",
      "Min loss: 0.03768382966518402\n",
      "Mean loss: 0.045430378367503486\n",
      "Std loss: 0.008557630980598484\n",
      "Total Loss: 0.2725822702050209\n",
      "------------------------------------ epoch 856 (5130 steps) ------------------------------------\n",
      "Max loss: 0.042153697460889816\n",
      "Min loss: 0.027235805988311768\n",
      "Mean loss: 0.03453174668053786\n",
      "Std loss: 0.0045822414435325014\n",
      "Total Loss: 0.20719048008322716\n",
      "------------------------------------ epoch 857 (5136 steps) ------------------------------------\n",
      "Max loss: 0.05400782823562622\n",
      "Min loss: 0.029035016894340515\n",
      "Mean loss: 0.04325056510667006\n",
      "Std loss: 0.00946203606724092\n",
      "Total Loss: 0.25950339064002037\n",
      "------------------------------------ epoch 858 (5142 steps) ------------------------------------\n",
      "Max loss: 0.09878548234701157\n",
      "Min loss: 0.02191031537950039\n",
      "Mean loss: 0.05531668942421675\n",
      "Std loss: 0.024502626451086505\n",
      "Total Loss: 0.3319001365453005\n",
      "------------------------------------ epoch 859 (5148 steps) ------------------------------------\n",
      "Max loss: 0.10075883567333221\n",
      "Min loss: 0.02993471547961235\n",
      "Mean loss: 0.05210300659139951\n",
      "Std loss: 0.023256647704713224\n",
      "Total Loss: 0.31261803954839706\n",
      "------------------------------------ epoch 860 (5154 steps) ------------------------------------\n",
      "Max loss: 0.08231811970472336\n",
      "Min loss: 0.029060637578368187\n",
      "Mean loss: 0.052027687119940914\n",
      "Std loss: 0.018094646002304076\n",
      "Total Loss: 0.3121661227196455\n",
      "------------------------------------ epoch 861 (5160 steps) ------------------------------------\n",
      "Max loss: 0.050798334181308746\n",
      "Min loss: 0.01993916742503643\n",
      "Mean loss: 0.03870052409668764\n",
      "Std loss: 0.012767836172935143\n",
      "Total Loss: 0.2322031445801258\n",
      "------------------------------------ epoch 862 (5166 steps) ------------------------------------\n",
      "Max loss: 0.049626391381025314\n",
      "Min loss: 0.02216339111328125\n",
      "Mean loss: 0.03707488222668568\n",
      "Std loss: 0.010315248212679526\n",
      "Total Loss: 0.2224492933601141\n",
      "------------------------------------ epoch 863 (5172 steps) ------------------------------------\n",
      "Max loss: 0.07552234828472137\n",
      "Min loss: 0.020264392718672752\n",
      "Mean loss: 0.04647492524236441\n",
      "Std loss: 0.017667529573788095\n",
      "Total Loss: 0.27884955145418644\n",
      "------------------------------------ epoch 864 (5178 steps) ------------------------------------\n",
      "Max loss: 0.07152707874774933\n",
      "Min loss: 0.028854649513959885\n",
      "Mean loss: 0.04871092736721039\n",
      "Std loss: 0.01575346613329358\n",
      "Total Loss: 0.29226556420326233\n",
      "------------------------------------ epoch 865 (5184 steps) ------------------------------------\n",
      "Max loss: 0.047306887805461884\n",
      "Min loss: 0.02761615812778473\n",
      "Mean loss: 0.03833572752773762\n",
      "Std loss: 0.0065009754706023375\n",
      "Total Loss: 0.2300143651664257\n",
      "------------------------------------ epoch 866 (5190 steps) ------------------------------------\n",
      "Max loss: 0.044786982238292694\n",
      "Min loss: 0.02740391157567501\n",
      "Mean loss: 0.03437691026677688\n",
      "Std loss: 0.006752141410559489\n",
      "Total Loss: 0.20626146160066128\n",
      "------------------------------------ epoch 867 (5196 steps) ------------------------------------\n",
      "Max loss: 0.06347599625587463\n",
      "Min loss: 0.03654854744672775\n",
      "Mean loss: 0.051346806809306145\n",
      "Std loss: 0.010105926540542518\n",
      "Total Loss: 0.30808084085583687\n",
      "------------------------------------ epoch 868 (5202 steps) ------------------------------------\n",
      "Max loss: 0.060817621648311615\n",
      "Min loss: 0.020892959088087082\n",
      "Mean loss: 0.04432207097609838\n",
      "Std loss: 0.014022059765444265\n",
      "Total Loss: 0.26593242585659027\n",
      "------------------------------------ epoch 869 (5208 steps) ------------------------------------\n",
      "Max loss: 0.05282675474882126\n",
      "Min loss: 0.027099117636680603\n",
      "Mean loss: 0.03521908881763617\n",
      "Std loss: 0.010850603452817891\n",
      "Total Loss: 0.21131453290581703\n",
      "------------------------------------ epoch 870 (5214 steps) ------------------------------------\n",
      "Max loss: 0.047711651772260666\n",
      "Min loss: 0.026802590116858482\n",
      "Mean loss: 0.0398499546572566\n",
      "Std loss: 0.007482263853567636\n",
      "Total Loss: 0.23909972794353962\n",
      "------------------------------------ epoch 871 (5220 steps) ------------------------------------\n",
      "Max loss: 0.04878108948469162\n",
      "Min loss: 0.024905553087592125\n",
      "Mean loss: 0.03855337233593067\n",
      "Std loss: 0.00890986151906525\n",
      "Total Loss: 0.231320234015584\n",
      "------------------------------------ epoch 872 (5226 steps) ------------------------------------\n",
      "Max loss: 0.06425342708826065\n",
      "Min loss: 0.03512081503868103\n",
      "Mean loss: 0.04960508085787296\n",
      "Std loss: 0.011618168071615474\n",
      "Total Loss: 0.2976304851472378\n",
      "------------------------------------ epoch 873 (5232 steps) ------------------------------------\n",
      "Max loss: 0.06593340635299683\n",
      "Min loss: 0.02731778845191002\n",
      "Mean loss: 0.0466554785768191\n",
      "Std loss: 0.012024602200689682\n",
      "Total Loss: 0.2799328714609146\n",
      "------------------------------------ epoch 874 (5238 steps) ------------------------------------\n",
      "Max loss: 0.09739961475133896\n",
      "Min loss: 0.027960479259490967\n",
      "Mean loss: 0.0554621797055006\n",
      "Std loss: 0.02451983319349902\n",
      "Total Loss: 0.3327730782330036\n",
      "------------------------------------ epoch 875 (5244 steps) ------------------------------------\n",
      "Max loss: 0.0722409188747406\n",
      "Min loss: 0.02714332565665245\n",
      "Mean loss: 0.04290331341326237\n",
      "Std loss: 0.014085074232068964\n",
      "Total Loss: 0.2574198804795742\n",
      "------------------------------------ epoch 876 (5250 steps) ------------------------------------\n",
      "Max loss: 0.060144226998090744\n",
      "Min loss: 0.027396753430366516\n",
      "Mean loss: 0.03925988761087259\n",
      "Std loss: 0.010892463754751955\n",
      "Total Loss: 0.23555932566523552\n",
      "------------------------------------ epoch 877 (5256 steps) ------------------------------------\n",
      "Max loss: 0.08315782248973846\n",
      "Min loss: 0.02638990245759487\n",
      "Mean loss: 0.04690023139119148\n",
      "Std loss: 0.021330324016652677\n",
      "Total Loss: 0.2814013883471489\n",
      "------------------------------------ epoch 878 (5262 steps) ------------------------------------\n",
      "Max loss: 0.04807001352310181\n",
      "Min loss: 0.022984500974416733\n",
      "Mean loss: 0.036417556616167225\n",
      "Std loss: 0.009601647229446463\n",
      "Total Loss: 0.21850533969700336\n",
      "------------------------------------ epoch 879 (5268 steps) ------------------------------------\n",
      "Max loss: 0.09283187240362167\n",
      "Min loss: 0.035519108176231384\n",
      "Mean loss: 0.05185826743642489\n",
      "Std loss: 0.019962042091829783\n",
      "Total Loss: 0.31114960461854935\n",
      "------------------------------------ epoch 880 (5274 steps) ------------------------------------\n",
      "Max loss: 0.09925337135791779\n",
      "Min loss: 0.029065284878015518\n",
      "Mean loss: 0.0562778003513813\n",
      "Std loss: 0.02734089200734251\n",
      "Total Loss: 0.3376668021082878\n",
      "------------------------------------ epoch 881 (5280 steps) ------------------------------------\n",
      "Max loss: 0.06971697509288788\n",
      "Min loss: 0.032704439014196396\n",
      "Mean loss: 0.05121878224114577\n",
      "Std loss: 0.011987503508659167\n",
      "Total Loss: 0.3073126934468746\n",
      "------------------------------------ epoch 882 (5286 steps) ------------------------------------\n",
      "Max loss: 0.0547248050570488\n",
      "Min loss: 0.027727652341127396\n",
      "Mean loss: 0.04411561662952105\n",
      "Std loss: 0.011124602708745924\n",
      "Total Loss: 0.2646936997771263\n",
      "------------------------------------ epoch 883 (5292 steps) ------------------------------------\n",
      "Max loss: 0.07585588097572327\n",
      "Min loss: 0.025985952466726303\n",
      "Mean loss: 0.04322248324751854\n",
      "Std loss: 0.016410212407350962\n",
      "Total Loss: 0.25933489948511124\n",
      "------------------------------------ epoch 884 (5298 steps) ------------------------------------\n",
      "Max loss: 0.10556550323963165\n",
      "Min loss: 0.025357775390148163\n",
      "Mean loss: 0.04951595701277256\n",
      "Std loss: 0.02747070756129043\n",
      "Total Loss: 0.29709574207663536\n",
      "------------------------------------ epoch 885 (5304 steps) ------------------------------------\n",
      "Max loss: 0.045510128140449524\n",
      "Min loss: 0.026319628581404686\n",
      "Mean loss: 0.03652792486051718\n",
      "Std loss: 0.0076374798508723005\n",
      "Total Loss: 0.2191675491631031\n",
      "------------------------------------ epoch 886 (5310 steps) ------------------------------------\n",
      "Max loss: 0.059365492314100266\n",
      "Min loss: 0.028505727648735046\n",
      "Mean loss: 0.04126113715271155\n",
      "Std loss: 0.010332562486380282\n",
      "Total Loss: 0.2475668229162693\n",
      "------------------------------------ epoch 887 (5316 steps) ------------------------------------\n",
      "Max loss: 0.07634727656841278\n",
      "Min loss: 0.026610340923070908\n",
      "Mean loss: 0.044871390176316105\n",
      "Std loss: 0.019288954044561074\n",
      "Total Loss: 0.2692283410578966\n",
      "------------------------------------ epoch 888 (5322 steps) ------------------------------------\n",
      "Max loss: 0.044776152819395065\n",
      "Min loss: 0.020316502079367638\n",
      "Mean loss: 0.034997932923336826\n",
      "Std loss: 0.007761013539044676\n",
      "Total Loss: 0.20998759754002094\n",
      "------------------------------------ epoch 889 (5328 steps) ------------------------------------\n",
      "Max loss: 0.09748846292495728\n",
      "Min loss: 0.021441057324409485\n",
      "Mean loss: 0.045754010478655495\n",
      "Std loss: 0.024935861803371234\n",
      "Total Loss: 0.274524062871933\n",
      "------------------------------------ epoch 890 (5334 steps) ------------------------------------\n",
      "Max loss: 0.10419439524412155\n",
      "Min loss: 0.03265151008963585\n",
      "Mean loss: 0.04625221652289232\n",
      "Std loss: 0.025946117049327196\n",
      "Total Loss: 0.2775132991373539\n",
      "------------------------------------ epoch 891 (5340 steps) ------------------------------------\n",
      "Max loss: 0.0503176748752594\n",
      "Min loss: 0.023303158581256866\n",
      "Mean loss: 0.03266816399991512\n",
      "Std loss: 0.008987076916225668\n",
      "Total Loss: 0.19600898399949074\n",
      "------------------------------------ epoch 892 (5346 steps) ------------------------------------\n",
      "Max loss: 0.07798859477043152\n",
      "Min loss: 0.026263844221830368\n",
      "Mean loss: 0.048581657310326896\n",
      "Std loss: 0.01847358655330868\n",
      "Total Loss: 0.29148994386196136\n",
      "------------------------------------ epoch 893 (5352 steps) ------------------------------------\n",
      "Max loss: 0.08319513499736786\n",
      "Min loss: 0.020792555063962936\n",
      "Mean loss: 0.04435069486498833\n",
      "Std loss: 0.019000975097761986\n",
      "Total Loss: 0.26610416918992996\n",
      "------------------------------------ epoch 894 (5358 steps) ------------------------------------\n",
      "Max loss: 0.058383069932460785\n",
      "Min loss: 0.0446256548166275\n",
      "Mean loss: 0.04998702183365822\n",
      "Std loss: 0.004932938047142403\n",
      "Total Loss: 0.2999221310019493\n",
      "------------------------------------ epoch 895 (5364 steps) ------------------------------------\n",
      "Max loss: 0.09654825925827026\n",
      "Min loss: 0.02400420978665352\n",
      "Mean loss: 0.04192544426769018\n",
      "Std loss: 0.024924386629485158\n",
      "Total Loss: 0.2515526656061411\n",
      "------------------------------------ epoch 896 (5370 steps) ------------------------------------\n",
      "Max loss: 0.08593665808439255\n",
      "Min loss: 0.019425131380558014\n",
      "Mean loss: 0.05142550729215145\n",
      "Std loss: 0.020874849735653112\n",
      "Total Loss: 0.3085530437529087\n",
      "------------------------------------ epoch 897 (5376 steps) ------------------------------------\n",
      "Max loss: 0.11346914619207382\n",
      "Min loss: 0.042468734085559845\n",
      "Mean loss: 0.06195339001715183\n",
      "Std loss: 0.024088116355998744\n",
      "Total Loss: 0.371720340102911\n",
      "------------------------------------ epoch 898 (5382 steps) ------------------------------------\n",
      "Max loss: 0.06744467467069626\n",
      "Min loss: 0.02580009028315544\n",
      "Mean loss: 0.0444123987108469\n",
      "Std loss: 0.014935643155118106\n",
      "Total Loss: 0.2664743922650814\n",
      "------------------------------------ epoch 899 (5388 steps) ------------------------------------\n",
      "Max loss: 0.07717178016901016\n",
      "Min loss: 0.01869683340191841\n",
      "Mean loss: 0.04299863365789255\n",
      "Std loss: 0.019296659660539124\n",
      "Total Loss: 0.25799180194735527\n",
      "------------------------------------ epoch 900 (5394 steps) ------------------------------------\n",
      "Max loss: 0.06526908278465271\n",
      "Min loss: 0.025312554091215134\n",
      "Mean loss: 0.044329252714912094\n",
      "Std loss: 0.01572279154834141\n",
      "Total Loss: 0.2659755162894726\n",
      "------------------------------------ epoch 901 (5400 steps) ------------------------------------\n",
      "Max loss: 0.06275027245283127\n",
      "Min loss: 0.020697422325611115\n",
      "Mean loss: 0.041165148839354515\n",
      "Std loss: 0.01347725461167021\n",
      "Total Loss: 0.2469908930361271\n",
      "saved model at ./weights/model_901.pth\n",
      "------------------------------------ epoch 902 (5406 steps) ------------------------------------\n",
      "Max loss: 0.06970670074224472\n",
      "Min loss: 0.02203921601176262\n",
      "Mean loss: 0.04379338398575783\n",
      "Std loss: 0.014272679722937893\n",
      "Total Loss: 0.26276030391454697\n",
      "------------------------------------ epoch 903 (5412 steps) ------------------------------------\n",
      "Max loss: 0.05565499886870384\n",
      "Min loss: 0.02494398131966591\n",
      "Mean loss: 0.03789210350563129\n",
      "Std loss: 0.012186623120610291\n",
      "Total Loss: 0.22735262103378773\n",
      "------------------------------------ epoch 904 (5418 steps) ------------------------------------\n",
      "Max loss: 0.06037501245737076\n",
      "Min loss: 0.03143197298049927\n",
      "Mean loss: 0.04835758047799269\n",
      "Std loss: 0.008765362642408342\n",
      "Total Loss: 0.29014548286795616\n",
      "------------------------------------ epoch 905 (5424 steps) ------------------------------------\n",
      "Max loss: 0.08230359852313995\n",
      "Min loss: 0.03131481632590294\n",
      "Mean loss: 0.05150762262443701\n",
      "Std loss: 0.015348568563753058\n",
      "Total Loss: 0.3090457357466221\n",
      "------------------------------------ epoch 906 (5430 steps) ------------------------------------\n",
      "Max loss: 0.05283556506037712\n",
      "Min loss: 0.021268723532557487\n",
      "Mean loss: 0.038449521797398724\n",
      "Std loss: 0.010069992222983376\n",
      "Total Loss: 0.23069713078439236\n",
      "------------------------------------ epoch 907 (5436 steps) ------------------------------------\n",
      "Max loss: 0.05995572730898857\n",
      "Min loss: 0.022699713706970215\n",
      "Mean loss: 0.04418243716160456\n",
      "Std loss: 0.01225683316465964\n",
      "Total Loss: 0.2650946229696274\n",
      "------------------------------------ epoch 908 (5442 steps) ------------------------------------\n",
      "Max loss: 0.07183551788330078\n",
      "Min loss: 0.020865842700004578\n",
      "Mean loss: 0.04530309389034907\n",
      "Std loss: 0.01651625462853707\n",
      "Total Loss: 0.2718185633420944\n",
      "------------------------------------ epoch 909 (5448 steps) ------------------------------------\n",
      "Max loss: 0.10070881247520447\n",
      "Min loss: 0.030507227405905724\n",
      "Mean loss: 0.06047552296270927\n",
      "Std loss: 0.022802902845547406\n",
      "Total Loss: 0.3628531377762556\n",
      "------------------------------------ epoch 910 (5454 steps) ------------------------------------\n",
      "Max loss: 0.08733955025672913\n",
      "Min loss: 0.01943589746952057\n",
      "Mean loss: 0.05231304777165254\n",
      "Std loss: 0.025477272097448357\n",
      "Total Loss: 0.31387828662991524\n",
      "------------------------------------ epoch 911 (5460 steps) ------------------------------------\n",
      "Max loss: 0.08744841068983078\n",
      "Min loss: 0.04051077738404274\n",
      "Mean loss: 0.05617846424380938\n",
      "Std loss: 0.015620636236132782\n",
      "Total Loss: 0.3370707854628563\n",
      "------------------------------------ epoch 912 (5466 steps) ------------------------------------\n",
      "Max loss: 0.10424104332923889\n",
      "Min loss: 0.023070186376571655\n",
      "Mean loss: 0.05044921673834324\n",
      "Std loss: 0.026601390776869058\n",
      "Total Loss: 0.30269530043005943\n",
      "------------------------------------ epoch 913 (5472 steps) ------------------------------------\n",
      "Max loss: 0.08741893619298935\n",
      "Min loss: 0.028122462332248688\n",
      "Mean loss: 0.054282063618302345\n",
      "Std loss: 0.022031461213686484\n",
      "Total Loss: 0.32569238170981407\n",
      "------------------------------------ epoch 914 (5478 steps) ------------------------------------\n",
      "Max loss: 0.09169825166463852\n",
      "Min loss: 0.029553627595305443\n",
      "Mean loss: 0.05490323373426994\n",
      "Std loss: 0.022722234288386205\n",
      "Total Loss: 0.3294194024056196\n",
      "------------------------------------ epoch 915 (5484 steps) ------------------------------------\n",
      "Max loss: 0.0696498453617096\n",
      "Min loss: 0.03176546096801758\n",
      "Mean loss: 0.040666953970988594\n",
      "Std loss: 0.013253082053716423\n",
      "Total Loss: 0.24400172382593155\n",
      "------------------------------------ epoch 916 (5490 steps) ------------------------------------\n",
      "Max loss: 0.07149209082126617\n",
      "Min loss: 0.022889330983161926\n",
      "Mean loss: 0.043773104747136436\n",
      "Std loss: 0.016169231211638553\n",
      "Total Loss: 0.2626386284828186\n",
      "------------------------------------ epoch 917 (5496 steps) ------------------------------------\n",
      "Max loss: 0.05879939720034599\n",
      "Min loss: 0.024895450100302696\n",
      "Mean loss: 0.04174869476507107\n",
      "Std loss: 0.010439423439477246\n",
      "Total Loss: 0.25049216859042645\n",
      "------------------------------------ epoch 918 (5502 steps) ------------------------------------\n",
      "Max loss: 0.07125886529684067\n",
      "Min loss: 0.025223692879080772\n",
      "Mean loss: 0.05188592802733183\n",
      "Std loss: 0.014182458991200478\n",
      "Total Loss: 0.311315568163991\n",
      "------------------------------------ epoch 919 (5508 steps) ------------------------------------\n",
      "Max loss: 0.05740758404135704\n",
      "Min loss: 0.03176111727952957\n",
      "Mean loss: 0.04005267719427744\n",
      "Std loss: 0.008511543169028529\n",
      "Total Loss: 0.24031606316566467\n",
      "------------------------------------ epoch 920 (5514 steps) ------------------------------------\n",
      "Max loss: 0.060148537158966064\n",
      "Min loss: 0.02538674883544445\n",
      "Mean loss: 0.038516007053355374\n",
      "Std loss: 0.01175548079130775\n",
      "Total Loss: 0.23109604232013226\n",
      "------------------------------------ epoch 921 (5520 steps) ------------------------------------\n",
      "Max loss: 0.060863982886075974\n",
      "Min loss: 0.024885205551981926\n",
      "Mean loss: 0.032681055987874665\n",
      "Std loss: 0.012859418345777897\n",
      "Total Loss: 0.196086335927248\n",
      "------------------------------------ epoch 922 (5526 steps) ------------------------------------\n",
      "Max loss: 0.054910700768232346\n",
      "Min loss: 0.0258683692663908\n",
      "Mean loss: 0.04064858301232258\n",
      "Std loss: 0.011775102404668199\n",
      "Total Loss: 0.2438914980739355\n",
      "------------------------------------ epoch 923 (5532 steps) ------------------------------------\n",
      "Max loss: 0.0685090646147728\n",
      "Min loss: 0.024120263755321503\n",
      "Mean loss: 0.04878271743655205\n",
      "Std loss: 0.013475162312412659\n",
      "Total Loss: 0.2926963046193123\n",
      "------------------------------------ epoch 924 (5538 steps) ------------------------------------\n",
      "Max loss: 0.08398013561964035\n",
      "Min loss: 0.0348978117108345\n",
      "Mean loss: 0.04520636424422264\n",
      "Std loss: 0.01742912036367323\n",
      "Total Loss: 0.27123818546533585\n",
      "------------------------------------ epoch 925 (5544 steps) ------------------------------------\n",
      "Max loss: 0.08852648735046387\n",
      "Min loss: 0.028347529470920563\n",
      "Mean loss: 0.05183962484200796\n",
      "Std loss: 0.024404434143775616\n",
      "Total Loss: 0.31103774905204773\n",
      "------------------------------------ epoch 926 (5550 steps) ------------------------------------\n",
      "Max loss: 0.06475794315338135\n",
      "Min loss: 0.023911355063319206\n",
      "Mean loss: 0.043443724823494755\n",
      "Std loss: 0.014540552554759936\n",
      "Total Loss: 0.2606623489409685\n",
      "------------------------------------ epoch 927 (5556 steps) ------------------------------------\n",
      "Max loss: 0.06308364868164062\n",
      "Min loss: 0.02858736924827099\n",
      "Mean loss: 0.04511260582755009\n",
      "Std loss: 0.013773342017393778\n",
      "Total Loss: 0.27067563496530056\n",
      "------------------------------------ epoch 928 (5562 steps) ------------------------------------\n",
      "Max loss: 0.09601881355047226\n",
      "Min loss: 0.031985413283109665\n",
      "Mean loss: 0.054845405742526054\n",
      "Std loss: 0.021723160997970763\n",
      "Total Loss: 0.3290724344551563\n",
      "------------------------------------ epoch 929 (5568 steps) ------------------------------------\n",
      "Max loss: 0.06494694948196411\n",
      "Min loss: 0.029786093160510063\n",
      "Mean loss: 0.04859529218326012\n",
      "Std loss: 0.012229232718034553\n",
      "Total Loss: 0.29157175309956074\n",
      "------------------------------------ epoch 930 (5574 steps) ------------------------------------\n",
      "Max loss: 0.06408505886793137\n",
      "Min loss: 0.02989630214869976\n",
      "Mean loss: 0.04793093136201302\n",
      "Std loss: 0.012202801968201504\n",
      "Total Loss: 0.28758558817207813\n",
      "------------------------------------ epoch 931 (5580 steps) ------------------------------------\n",
      "Max loss: 0.07268889993429184\n",
      "Min loss: 0.033333420753479004\n",
      "Mean loss: 0.048984142020344734\n",
      "Std loss: 0.013519956279250173\n",
      "Total Loss: 0.2939048521220684\n",
      "------------------------------------ epoch 932 (5586 steps) ------------------------------------\n",
      "Max loss: 0.056125130504369736\n",
      "Min loss: 0.01804312691092491\n",
      "Mean loss: 0.03423104931910833\n",
      "Std loss: 0.012967716922083433\n",
      "Total Loss: 0.20538629591464996\n",
      "------------------------------------ epoch 933 (5592 steps) ------------------------------------\n",
      "Max loss: 0.07380525767803192\n",
      "Min loss: 0.024910753592848778\n",
      "Mean loss: 0.040356215400000416\n",
      "Std loss: 0.016678386887417303\n",
      "Total Loss: 0.24213729240000248\n",
      "------------------------------------ epoch 934 (5598 steps) ------------------------------------\n",
      "Max loss: 0.11355933547019958\n",
      "Min loss: 0.03805350139737129\n",
      "Mean loss: 0.05605880978206793\n",
      "Std loss: 0.026160786088032042\n",
      "Total Loss: 0.3363528586924076\n",
      "------------------------------------ epoch 935 (5604 steps) ------------------------------------\n",
      "Max loss: 0.05438343808054924\n",
      "Min loss: 0.01883825473487377\n",
      "Mean loss: 0.03544871415942907\n",
      "Std loss: 0.011979691686977949\n",
      "Total Loss: 0.21269228495657444\n",
      "------------------------------------ epoch 936 (5610 steps) ------------------------------------\n",
      "Max loss: 0.06300458312034607\n",
      "Min loss: 0.02814219705760479\n",
      "Mean loss: 0.05155053331206242\n",
      "Std loss: 0.011458313333591737\n",
      "Total Loss: 0.30930319987237453\n",
      "------------------------------------ epoch 937 (5616 steps) ------------------------------------\n",
      "Max loss: 0.05670612305402756\n",
      "Min loss: 0.023027582094073296\n",
      "Mean loss: 0.03985163321097692\n",
      "Std loss: 0.012449462804746445\n",
      "Total Loss: 0.2391097992658615\n",
      "------------------------------------ epoch 938 (5622 steps) ------------------------------------\n",
      "Max loss: 0.0571330301463604\n",
      "Min loss: 0.029518552124500275\n",
      "Mean loss: 0.0405162014067173\n",
      "Std loss: 0.009083361944089324\n",
      "Total Loss: 0.2430972084403038\n",
      "------------------------------------ epoch 939 (5628 steps) ------------------------------------\n",
      "Max loss: 0.08983874320983887\n",
      "Min loss: 0.033537935465574265\n",
      "Mean loss: 0.05436272174119949\n",
      "Std loss: 0.01781004731242966\n",
      "Total Loss: 0.32617633044719696\n",
      "------------------------------------ epoch 940 (5634 steps) ------------------------------------\n",
      "Max loss: 0.05329525098204613\n",
      "Min loss: 0.029485097154974937\n",
      "Mean loss: 0.04255550013234218\n",
      "Std loss: 0.00880135885031111\n",
      "Total Loss: 0.2553330007940531\n",
      "------------------------------------ epoch 941 (5640 steps) ------------------------------------\n",
      "Max loss: 0.0649511069059372\n",
      "Min loss: 0.03255976364016533\n",
      "Mean loss: 0.04402937553822994\n",
      "Std loss: 0.012001225458399451\n",
      "Total Loss: 0.26417625322937965\n",
      "------------------------------------ epoch 942 (5646 steps) ------------------------------------\n",
      "Max loss: 0.0673348605632782\n",
      "Min loss: 0.0305798202753067\n",
      "Mean loss: 0.04905939971407255\n",
      "Std loss: 0.014936168697920018\n",
      "Total Loss: 0.29435639828443527\n",
      "------------------------------------ epoch 943 (5652 steps) ------------------------------------\n",
      "Max loss: 0.04894553869962692\n",
      "Min loss: 0.03418245166540146\n",
      "Mean loss: 0.04333211605747541\n",
      "Std loss: 0.0053174306174986405\n",
      "Total Loss: 0.25999269634485245\n",
      "------------------------------------ epoch 944 (5658 steps) ------------------------------------\n",
      "Max loss: 0.04913603514432907\n",
      "Min loss: 0.030037349089980125\n",
      "Mean loss: 0.04176567339648803\n",
      "Std loss: 0.006381718546187451\n",
      "Total Loss: 0.2505940403789282\n",
      "------------------------------------ epoch 945 (5664 steps) ------------------------------------\n",
      "Max loss: 0.07254135608673096\n",
      "Min loss: 0.03388816863298416\n",
      "Mean loss: 0.05366385790208975\n",
      "Std loss: 0.014172400889698294\n",
      "Total Loss: 0.32198314741253853\n",
      "------------------------------------ epoch 946 (5670 steps) ------------------------------------\n",
      "Max loss: 0.1009480357170105\n",
      "Min loss: 0.03736649453639984\n",
      "Mean loss: 0.05869557149708271\n",
      "Std loss: 0.021583021483524464\n",
      "Total Loss: 0.35217342898249626\n",
      "------------------------------------ epoch 947 (5676 steps) ------------------------------------\n",
      "Max loss: 0.04377424716949463\n",
      "Min loss: 0.02756207063794136\n",
      "Mean loss: 0.036730934555331864\n",
      "Std loss: 0.005857676362952457\n",
      "Total Loss: 0.2203856073319912\n",
      "------------------------------------ epoch 948 (5682 steps) ------------------------------------\n",
      "Max loss: 0.047270409762859344\n",
      "Min loss: 0.018285619094967842\n",
      "Mean loss: 0.040166521755357586\n",
      "Std loss: 0.009934817103018653\n",
      "Total Loss: 0.2409991305321455\n",
      "------------------------------------ epoch 949 (5688 steps) ------------------------------------\n",
      "Max loss: 0.0825924277305603\n",
      "Min loss: 0.04438462108373642\n",
      "Mean loss: 0.05588540807366371\n",
      "Std loss: 0.012640236155502197\n",
      "Total Loss: 0.33531244844198227\n",
      "------------------------------------ epoch 950 (5694 steps) ------------------------------------\n",
      "Max loss: 0.07926289737224579\n",
      "Min loss: 0.023432612419128418\n",
      "Mean loss: 0.048811050752798714\n",
      "Std loss: 0.02097751836290411\n",
      "Total Loss: 0.2928663045167923\n",
      "------------------------------------ epoch 951 (5700 steps) ------------------------------------\n",
      "Max loss: 0.05544815585017204\n",
      "Min loss: 0.0264181699603796\n",
      "Mean loss: 0.04221927157292763\n",
      "Std loss: 0.008637884964504293\n",
      "Total Loss: 0.2533156294375658\n",
      "------------------------------------ epoch 952 (5706 steps) ------------------------------------\n",
      "Max loss: 0.05832202360033989\n",
      "Min loss: 0.031748197972774506\n",
      "Mean loss: 0.047705154245098434\n",
      "Std loss: 0.010402422854520489\n",
      "Total Loss: 0.2862309254705906\n",
      "------------------------------------ epoch 953 (5712 steps) ------------------------------------\n",
      "Max loss: 0.07112617790699005\n",
      "Min loss: 0.031320974230766296\n",
      "Mean loss: 0.053625332191586494\n",
      "Std loss: 0.015598657991194875\n",
      "Total Loss: 0.32175199314951897\n",
      "------------------------------------ epoch 954 (5718 steps) ------------------------------------\n",
      "Max loss: 0.08230274170637131\n",
      "Min loss: 0.020237434655427933\n",
      "Mean loss: 0.05213885412861904\n",
      "Std loss: 0.02581529327659762\n",
      "Total Loss: 0.3128331247717142\n",
      "------------------------------------ epoch 955 (5724 steps) ------------------------------------\n",
      "Max loss: 0.05538829416036606\n",
      "Min loss: 0.027337856590747833\n",
      "Mean loss: 0.0450666726877292\n",
      "Std loss: 0.010594493306215026\n",
      "Total Loss: 0.2704000361263752\n",
      "------------------------------------ epoch 956 (5730 steps) ------------------------------------\n",
      "Max loss: 0.05738989636301994\n",
      "Min loss: 0.03079664520919323\n",
      "Mean loss: 0.041366283781826496\n",
      "Std loss: 0.009141868929545218\n",
      "Total Loss: 0.24819770269095898\n",
      "------------------------------------ epoch 957 (5736 steps) ------------------------------------\n",
      "Max loss: 0.08833640813827515\n",
      "Min loss: 0.0319141261279583\n",
      "Mean loss: 0.05243397379914919\n",
      "Std loss: 0.02060521548994111\n",
      "Total Loss: 0.31460384279489517\n",
      "------------------------------------ epoch 958 (5742 steps) ------------------------------------\n",
      "Max loss: 0.08449089527130127\n",
      "Min loss: 0.02731790766119957\n",
      "Mean loss: 0.04404864522318045\n",
      "Std loss: 0.018906880856238463\n",
      "Total Loss: 0.2642918713390827\n",
      "------------------------------------ epoch 959 (5748 steps) ------------------------------------\n",
      "Max loss: 0.050694555044174194\n",
      "Min loss: 0.02612968347966671\n",
      "Mean loss: 0.03888109978288412\n",
      "Std loss: 0.00991163989632011\n",
      "Total Loss: 0.23328659869730473\n",
      "------------------------------------ epoch 960 (5754 steps) ------------------------------------\n",
      "Max loss: 0.07169168442487717\n",
      "Min loss: 0.024846671149134636\n",
      "Mean loss: 0.04028888139873743\n",
      "Std loss: 0.01700364639853399\n",
      "Total Loss: 0.24173328839242458\n",
      "------------------------------------ epoch 961 (5760 steps) ------------------------------------\n",
      "Max loss: 0.07959061861038208\n",
      "Min loss: 0.023845814168453217\n",
      "Mean loss: 0.04092336321870486\n",
      "Std loss: 0.01987053886427953\n",
      "Total Loss: 0.24554017931222916\n",
      "------------------------------------ epoch 962 (5766 steps) ------------------------------------\n",
      "Max loss: 0.060457758605480194\n",
      "Min loss: 0.03721771761775017\n",
      "Mean loss: 0.05039235514899095\n",
      "Std loss: 0.007499061831940277\n",
      "Total Loss: 0.3023541308939457\n",
      "------------------------------------ epoch 963 (5772 steps) ------------------------------------\n",
      "Max loss: 0.08791051805019379\n",
      "Min loss: 0.035797201097011566\n",
      "Mean loss: 0.052086355785528816\n",
      "Std loss: 0.018340920190740004\n",
      "Total Loss: 0.3125181347131729\n",
      "------------------------------------ epoch 964 (5778 steps) ------------------------------------\n",
      "Max loss: 0.04517246410250664\n",
      "Min loss: 0.021961554884910583\n",
      "Mean loss: 0.03431191109120846\n",
      "Std loss: 0.008269963484124376\n",
      "Total Loss: 0.20587146654725075\n",
      "------------------------------------ epoch 965 (5784 steps) ------------------------------------\n",
      "Max loss: 0.07197591662406921\n",
      "Min loss: 0.01586197316646576\n",
      "Mean loss: 0.04845603431264559\n",
      "Std loss: 0.018219236580333538\n",
      "Total Loss: 0.29073620587587357\n",
      "------------------------------------ epoch 966 (5790 steps) ------------------------------------\n",
      "Max loss: 0.06375651061534882\n",
      "Min loss: 0.028752710670232773\n",
      "Mean loss: 0.045087797567248344\n",
      "Std loss: 0.013000722645162797\n",
      "Total Loss: 0.27052678540349007\n",
      "------------------------------------ epoch 967 (5796 steps) ------------------------------------\n",
      "Max loss: 0.04752141237258911\n",
      "Min loss: 0.020372387021780014\n",
      "Mean loss: 0.034074391859273113\n",
      "Std loss: 0.009436885237168654\n",
      "Total Loss: 0.2044463511556387\n",
      "------------------------------------ epoch 968 (5802 steps) ------------------------------------\n",
      "Max loss: 0.08169279992580414\n",
      "Min loss: 0.02329016476869583\n",
      "Mean loss: 0.045247734834750496\n",
      "Std loss: 0.01833302630349234\n",
      "Total Loss: 0.27148640900850296\n",
      "------------------------------------ epoch 969 (5808 steps) ------------------------------------\n",
      "Max loss: 0.11225944757461548\n",
      "Min loss: 0.03374510258436203\n",
      "Mean loss: 0.05990201234817505\n",
      "Std loss: 0.028854300858418794\n",
      "Total Loss: 0.3594120740890503\n",
      "------------------------------------ epoch 970 (5814 steps) ------------------------------------\n",
      "Max loss: 0.0620821937918663\n",
      "Min loss: 0.031677599996328354\n",
      "Mean loss: 0.04537452012300491\n",
      "Std loss: 0.01232872338661446\n",
      "Total Loss: 0.2722471207380295\n",
      "------------------------------------ epoch 971 (5820 steps) ------------------------------------\n",
      "Max loss: 0.06662310659885406\n",
      "Min loss: 0.03235981985926628\n",
      "Mean loss: 0.05076329099635283\n",
      "Std loss: 0.012928299289660732\n",
      "Total Loss: 0.304579745978117\n",
      "------------------------------------ epoch 972 (5826 steps) ------------------------------------\n",
      "Max loss: 0.0636693611741066\n",
      "Min loss: 0.02197839878499508\n",
      "Mean loss: 0.04431324607382218\n",
      "Std loss: 0.01231361402306385\n",
      "Total Loss: 0.2658794764429331\n",
      "------------------------------------ epoch 973 (5832 steps) ------------------------------------\n",
      "Max loss: 0.06568323820829391\n",
      "Min loss: 0.021328886970877647\n",
      "Mean loss: 0.03793974810590347\n",
      "Std loss: 0.015823634654676885\n",
      "Total Loss: 0.2276384886354208\n",
      "------------------------------------ epoch 974 (5838 steps) ------------------------------------\n",
      "Max loss: 0.0533614382147789\n",
      "Min loss: 0.031238730996847153\n",
      "Mean loss: 0.04327605230112871\n",
      "Std loss: 0.0069618316681361635\n",
      "Total Loss: 0.25965631380677223\n",
      "------------------------------------ epoch 975 (5844 steps) ------------------------------------\n",
      "Max loss: 0.05552917718887329\n",
      "Min loss: 0.02361162006855011\n",
      "Mean loss: 0.040215820694963135\n",
      "Std loss: 0.012106993234185664\n",
      "Total Loss: 0.24129492416977882\n",
      "------------------------------------ epoch 976 (5850 steps) ------------------------------------\n",
      "Max loss: 0.08128136396408081\n",
      "Min loss: 0.019444553181529045\n",
      "Mean loss: 0.03986354762067398\n",
      "Std loss: 0.02068103649945549\n",
      "Total Loss: 0.23918128572404385\n",
      "------------------------------------ epoch 977 (5856 steps) ------------------------------------\n",
      "Max loss: 0.04716169089078903\n",
      "Min loss: 0.0215701125562191\n",
      "Mean loss: 0.03252980547646681\n",
      "Std loss: 0.009291410006269409\n",
      "Total Loss: 0.1951788328588009\n",
      "------------------------------------ epoch 978 (5862 steps) ------------------------------------\n",
      "Max loss: 0.07769154012203217\n",
      "Min loss: 0.02065068483352661\n",
      "Mean loss: 0.050546263034145035\n",
      "Std loss: 0.017928660616051622\n",
      "Total Loss: 0.3032775782048702\n",
      "------------------------------------ epoch 979 (5868 steps) ------------------------------------\n",
      "Max loss: 0.0438932329416275\n",
      "Min loss: 0.01805303990840912\n",
      "Mean loss: 0.03186074333886305\n",
      "Std loss: 0.007631917435217187\n",
      "Total Loss: 0.19116446003317833\n",
      "------------------------------------ epoch 980 (5874 steps) ------------------------------------\n",
      "Max loss: 0.07532843947410583\n",
      "Min loss: 0.022162120789289474\n",
      "Mean loss: 0.04970926729341348\n",
      "Std loss: 0.016487453757233306\n",
      "Total Loss: 0.2982556037604809\n",
      "------------------------------------ epoch 981 (5880 steps) ------------------------------------\n",
      "Max loss: 0.07547968626022339\n",
      "Min loss: 0.02409125491976738\n",
      "Mean loss: 0.046485173826416336\n",
      "Std loss: 0.01746674920321015\n",
      "Total Loss: 0.278911042958498\n",
      "------------------------------------ epoch 982 (5886 steps) ------------------------------------\n",
      "Max loss: 0.061621230095624924\n",
      "Min loss: 0.024424850940704346\n",
      "Mean loss: 0.03729834749052922\n",
      "Std loss: 0.013332556364458339\n",
      "Total Loss: 0.22379008494317532\n",
      "------------------------------------ epoch 983 (5892 steps) ------------------------------------\n",
      "Max loss: 0.052945997565984726\n",
      "Min loss: 0.023633135482668877\n",
      "Mean loss: 0.03953971869001786\n",
      "Std loss: 0.009311486624429358\n",
      "Total Loss: 0.23723831214010715\n",
      "------------------------------------ epoch 984 (5898 steps) ------------------------------------\n",
      "Max loss: 0.07205705344676971\n",
      "Min loss: 0.016402075067162514\n",
      "Mean loss: 0.03416310716420412\n",
      "Std loss: 0.017803622595639532\n",
      "Total Loss: 0.20497864298522472\n",
      "------------------------------------ epoch 985 (5904 steps) ------------------------------------\n",
      "Max loss: 0.09024281799793243\n",
      "Min loss: 0.02519969269633293\n",
      "Mean loss: 0.04516276282568773\n",
      "Std loss: 0.021150289566633795\n",
      "Total Loss: 0.27097657695412636\n",
      "------------------------------------ epoch 986 (5910 steps) ------------------------------------\n",
      "Max loss: 0.06578148156404495\n",
      "Min loss: 0.039966657757759094\n",
      "Mean loss: 0.05520714881519476\n",
      "Std loss: 0.010414740440574938\n",
      "Total Loss: 0.3312428928911686\n",
      "------------------------------------ epoch 987 (5916 steps) ------------------------------------\n",
      "Max loss: 0.05635537952184677\n",
      "Min loss: 0.025709589943289757\n",
      "Mean loss: 0.04109344786653916\n",
      "Std loss: 0.010658359397864947\n",
      "Total Loss: 0.24656068719923496\n",
      "------------------------------------ epoch 988 (5922 steps) ------------------------------------\n",
      "Max loss: 0.08703004568815231\n",
      "Min loss: 0.05273346230387688\n",
      "Mean loss: 0.061061072473724685\n",
      "Std loss: 0.011757322514499497\n",
      "Total Loss: 0.3663664348423481\n",
      "------------------------------------ epoch 989 (5928 steps) ------------------------------------\n",
      "Max loss: 0.06631152331829071\n",
      "Min loss: 0.025562476366758347\n",
      "Mean loss: 0.04525230141977469\n",
      "Std loss: 0.014715826320219514\n",
      "Total Loss: 0.27151380851864815\n",
      "------------------------------------ epoch 990 (5934 steps) ------------------------------------\n",
      "Max loss: 0.06712260842323303\n",
      "Min loss: 0.025068342685699463\n",
      "Mean loss: 0.03767532700051864\n",
      "Std loss: 0.014409287439320453\n",
      "Total Loss: 0.22605196200311184\n",
      "------------------------------------ epoch 991 (5940 steps) ------------------------------------\n",
      "Max loss: 0.06972581148147583\n",
      "Min loss: 0.04455939680337906\n",
      "Mean loss: 0.054632093757390976\n",
      "Std loss: 0.007856074161090057\n",
      "Total Loss: 0.32779256254434586\n",
      "------------------------------------ epoch 992 (5946 steps) ------------------------------------\n",
      "Max loss: 0.06950446963310242\n",
      "Min loss: 0.025098085403442383\n",
      "Mean loss: 0.046507732942700386\n",
      "Std loss: 0.014315920767171935\n",
      "Total Loss: 0.2790463976562023\n",
      "------------------------------------ epoch 993 (5952 steps) ------------------------------------\n",
      "Max loss: 0.07034961879253387\n",
      "Min loss: 0.026112711057066917\n",
      "Mean loss: 0.04303163383156061\n",
      "Std loss: 0.013649455059441908\n",
      "Total Loss: 0.25818980298936367\n",
      "------------------------------------ epoch 994 (5958 steps) ------------------------------------\n",
      "Max loss: 0.044366203248500824\n",
      "Min loss: 0.025578361004590988\n",
      "Mean loss: 0.037439064433177315\n",
      "Std loss: 0.0060262189011810975\n",
      "Total Loss: 0.22463438659906387\n",
      "------------------------------------ epoch 995 (5964 steps) ------------------------------------\n",
      "Max loss: 0.10520076006650925\n",
      "Min loss: 0.02565060369670391\n",
      "Mean loss: 0.04794097226113081\n",
      "Std loss: 0.02681855594246854\n",
      "Total Loss: 0.28764583356678486\n",
      "------------------------------------ epoch 996 (5970 steps) ------------------------------------\n",
      "Max loss: 0.06405380368232727\n",
      "Min loss: 0.020531954243779182\n",
      "Mean loss: 0.04240777933349212\n",
      "Std loss: 0.01599381654224184\n",
      "Total Loss: 0.2544466760009527\n",
      "------------------------------------ epoch 997 (5976 steps) ------------------------------------\n",
      "Max loss: 0.06890250742435455\n",
      "Min loss: 0.026669159531593323\n",
      "Mean loss: 0.04501917213201523\n",
      "Std loss: 0.013100574119027918\n",
      "Total Loss: 0.27011503279209137\n",
      "------------------------------------ epoch 998 (5982 steps) ------------------------------------\n",
      "Max loss: 0.06401655822992325\n",
      "Min loss: 0.017181988805532455\n",
      "Mean loss: 0.04390189299980799\n",
      "Std loss: 0.015747698337210344\n",
      "Total Loss: 0.26341135799884796\n",
      "------------------------------------ epoch 999 (5988 steps) ------------------------------------\n",
      "Max loss: 0.11027421802282333\n",
      "Min loss: 0.02965318039059639\n",
      "Mean loss: 0.06518240583439668\n",
      "Std loss: 0.026524331910535638\n",
      "Total Loss: 0.3910944350063801\n",
      "------------------------------------ epoch 1000 (5994 steps) ------------------------------------\n",
      "Max loss: 0.12201818078756332\n",
      "Min loss: 0.025360312312841415\n",
      "Mean loss: 0.05788181225458781\n",
      "Std loss: 0.03182193956786724\n",
      "Total Loss: 0.34729087352752686\n",
      "------------------------------------ epoch 1001 (6000 steps) ------------------------------------\n",
      "Max loss: 0.05289033055305481\n",
      "Min loss: 0.03705589100718498\n",
      "Mean loss: 0.04416754158834616\n",
      "Std loss: 0.00596318645001516\n",
      "Total Loss: 0.265005249530077\n",
      "saved model at ./weights/model_1001.pth\n",
      "------------------------------------ epoch 1002 (6006 steps) ------------------------------------\n",
      "Max loss: 0.05517546087503433\n",
      "Min loss: 0.03174255043268204\n",
      "Mean loss: 0.04177792246143023\n",
      "Std loss: 0.008574474803385064\n",
      "Total Loss: 0.2506675347685814\n",
      "------------------------------------ epoch 1003 (6012 steps) ------------------------------------\n",
      "Max loss: 0.07205288857221603\n",
      "Min loss: 0.025268618017435074\n",
      "Mean loss: 0.04102075317253669\n",
      "Std loss: 0.01951538953472791\n",
      "Total Loss: 0.24612451903522015\n",
      "------------------------------------ epoch 1004 (6018 steps) ------------------------------------\n",
      "Max loss: 0.05948805809020996\n",
      "Min loss: 0.02389897219836712\n",
      "Mean loss: 0.04012499594440063\n",
      "Std loss: 0.011953722365044356\n",
      "Total Loss: 0.24074997566640377\n",
      "------------------------------------ epoch 1005 (6024 steps) ------------------------------------\n",
      "Max loss: 0.05297602340579033\n",
      "Min loss: 0.030026737600564957\n",
      "Mean loss: 0.04135052983959516\n",
      "Std loss: 0.007391996059101493\n",
      "Total Loss: 0.24810317903757095\n",
      "------------------------------------ epoch 1006 (6030 steps) ------------------------------------\n",
      "Max loss: 0.06605441868305206\n",
      "Min loss: 0.025101378560066223\n",
      "Mean loss: 0.0419986347357432\n",
      "Std loss: 0.013002280701536454\n",
      "Total Loss: 0.25199180841445923\n",
      "------------------------------------ epoch 1007 (6036 steps) ------------------------------------\n",
      "Max loss: 0.06618702411651611\n",
      "Min loss: 0.024602964520454407\n",
      "Mean loss: 0.03761753750344118\n",
      "Std loss: 0.0135780429236326\n",
      "Total Loss: 0.22570522502064705\n",
      "------------------------------------ epoch 1008 (6042 steps) ------------------------------------\n",
      "Max loss: 0.05219513177871704\n",
      "Min loss: 0.025868019089102745\n",
      "Mean loss: 0.034404520566264786\n",
      "Std loss: 0.008604928664882074\n",
      "Total Loss: 0.20642712339758873\n",
      "------------------------------------ epoch 1009 (6048 steps) ------------------------------------\n",
      "Max loss: 0.07286026328802109\n",
      "Min loss: 0.02116786502301693\n",
      "Mean loss: 0.03754643506060044\n",
      "Std loss: 0.01806948228768072\n",
      "Total Loss: 0.22527861036360264\n",
      "------------------------------------ epoch 1010 (6054 steps) ------------------------------------\n",
      "Max loss: 0.07747448235750198\n",
      "Min loss: 0.02738729491829872\n",
      "Mean loss: 0.04756108236809572\n",
      "Std loss: 0.017412593975339653\n",
      "Total Loss: 0.2853664942085743\n",
      "------------------------------------ epoch 1011 (6060 steps) ------------------------------------\n",
      "Max loss: 0.05549980327486992\n",
      "Min loss: 0.029061242938041687\n",
      "Mean loss: 0.042639076709747314\n",
      "Std loss: 0.00903937559497415\n",
      "Total Loss: 0.2558344602584839\n",
      "------------------------------------ epoch 1012 (6066 steps) ------------------------------------\n",
      "Max loss: 0.0500892698764801\n",
      "Min loss: 0.02462609112262726\n",
      "Mean loss: 0.035404788330197334\n",
      "Std loss: 0.009017524262193268\n",
      "Total Loss: 0.212428729981184\n",
      "------------------------------------ epoch 1013 (6072 steps) ------------------------------------\n",
      "Max loss: 0.06076579913496971\n",
      "Min loss: 0.0193130224943161\n",
      "Mean loss: 0.03404548671096563\n",
      "Std loss: 0.01468914937326336\n",
      "Total Loss: 0.2042729202657938\n",
      "------------------------------------ epoch 1014 (6078 steps) ------------------------------------\n",
      "Max loss: 0.08382193744182587\n",
      "Min loss: 0.03819568082690239\n",
      "Mean loss: 0.05182236867646376\n",
      "Std loss: 0.015305403078619929\n",
      "Total Loss: 0.3109342120587826\n",
      "------------------------------------ epoch 1015 (6084 steps) ------------------------------------\n",
      "Max loss: 0.07612515240907669\n",
      "Min loss: 0.024366026744246483\n",
      "Mean loss: 0.04670647997409105\n",
      "Std loss: 0.017358032795470338\n",
      "Total Loss: 0.2802388798445463\n",
      "------------------------------------ epoch 1016 (6090 steps) ------------------------------------\n",
      "Max loss: 0.03983742743730545\n",
      "Min loss: 0.024164289236068726\n",
      "Mean loss: 0.031515962754686676\n",
      "Std loss: 0.005081258185322023\n",
      "Total Loss: 0.18909577652812004\n",
      "------------------------------------ epoch 1017 (6096 steps) ------------------------------------\n",
      "Max loss: 0.10901131480932236\n",
      "Min loss: 0.027608273550868034\n",
      "Mean loss: 0.04932235895345608\n",
      "Std loss: 0.027668443100462476\n",
      "Total Loss: 0.2959341537207365\n",
      "------------------------------------ epoch 1018 (6102 steps) ------------------------------------\n",
      "Max loss: 0.05416169390082359\n",
      "Min loss: 0.024228086695075035\n",
      "Mean loss: 0.04131634347140789\n",
      "Std loss: 0.01036508170864852\n",
      "Total Loss: 0.24789806082844734\n",
      "------------------------------------ epoch 1019 (6108 steps) ------------------------------------\n",
      "Max loss: 0.04874170944094658\n",
      "Min loss: 0.030467525124549866\n",
      "Mean loss: 0.03801830547551314\n",
      "Std loss: 0.0072173634306863\n",
      "Total Loss: 0.22810983285307884\n",
      "------------------------------------ epoch 1020 (6114 steps) ------------------------------------\n",
      "Max loss: 0.05189047381281853\n",
      "Min loss: 0.028833474963903427\n",
      "Mean loss: 0.041776912907759346\n",
      "Std loss: 0.007326211668313668\n",
      "Total Loss: 0.2506614774465561\n",
      "------------------------------------ epoch 1021 (6120 steps) ------------------------------------\n",
      "Max loss: 0.05273132771253586\n",
      "Min loss: 0.026861581951379776\n",
      "Mean loss: 0.0364320936302344\n",
      "Std loss: 0.009705390545762622\n",
      "Total Loss: 0.2185925617814064\n",
      "------------------------------------ epoch 1022 (6126 steps) ------------------------------------\n",
      "Max loss: 0.09598370641469955\n",
      "Min loss: 0.030161291360855103\n",
      "Mean loss: 0.054685346161325775\n",
      "Std loss: 0.024538708782422453\n",
      "Total Loss: 0.32811207696795464\n",
      "------------------------------------ epoch 1023 (6132 steps) ------------------------------------\n",
      "Max loss: 0.05113985389471054\n",
      "Min loss: 0.022558582946658134\n",
      "Mean loss: 0.036397772220273815\n",
      "Std loss: 0.01156877267026397\n",
      "Total Loss: 0.21838663332164288\n",
      "------------------------------------ epoch 1024 (6138 steps) ------------------------------------\n",
      "Max loss: 0.05079541355371475\n",
      "Min loss: 0.025563839823007584\n",
      "Mean loss: 0.03390483744442463\n",
      "Std loss: 0.007896241218591461\n",
      "Total Loss: 0.20342902466654778\n",
      "------------------------------------ epoch 1025 (6144 steps) ------------------------------------\n",
      "Max loss: 0.07373921573162079\n",
      "Min loss: 0.035258352756500244\n",
      "Mean loss: 0.048492311189572014\n",
      "Std loss: 0.01264863278206294\n",
      "Total Loss: 0.2909538671374321\n",
      "------------------------------------ epoch 1026 (6150 steps) ------------------------------------\n",
      "Max loss: 0.06083765625953674\n",
      "Min loss: 0.02377874031662941\n",
      "Mean loss: 0.034459444073339306\n",
      "Std loss: 0.0129330430141704\n",
      "Total Loss: 0.20675666444003582\n",
      "------------------------------------ epoch 1027 (6156 steps) ------------------------------------\n",
      "Max loss: 0.06275343894958496\n",
      "Min loss: 0.021528249606490135\n",
      "Mean loss: 0.04044848773628473\n",
      "Std loss: 0.014412967747726062\n",
      "Total Loss: 0.2426909264177084\n",
      "------------------------------------ epoch 1028 (6162 steps) ------------------------------------\n",
      "Max loss: 0.04460323601961136\n",
      "Min loss: 0.021983353421092033\n",
      "Mean loss: 0.03635175681362549\n",
      "Std loss: 0.007125008816394738\n",
      "Total Loss: 0.21811054088175297\n",
      "------------------------------------ epoch 1029 (6168 steps) ------------------------------------\n",
      "Max loss: 0.05472376197576523\n",
      "Min loss: 0.01895768754184246\n",
      "Mean loss: 0.037149715858201184\n",
      "Std loss: 0.013619855200652354\n",
      "Total Loss: 0.22289829514920712\n",
      "------------------------------------ epoch 1030 (6174 steps) ------------------------------------\n",
      "Max loss: 0.06320703029632568\n",
      "Min loss: 0.039006393402814865\n",
      "Mean loss: 0.05076588566104571\n",
      "Std loss: 0.007745563308147317\n",
      "Total Loss: 0.30459531396627426\n",
      "------------------------------------ epoch 1031 (6180 steps) ------------------------------------\n",
      "Max loss: 0.05437948927283287\n",
      "Min loss: 0.02536870911717415\n",
      "Mean loss: 0.03764257921526829\n",
      "Std loss: 0.010503936145248204\n",
      "Total Loss: 0.22585547529160976\n",
      "------------------------------------ epoch 1032 (6186 steps) ------------------------------------\n",
      "Max loss: 0.0411866120994091\n",
      "Min loss: 0.02036753110587597\n",
      "Mean loss: 0.03400353870044152\n",
      "Std loss: 0.006808089845408519\n",
      "Total Loss: 0.20402123220264912\n",
      "------------------------------------ epoch 1033 (6192 steps) ------------------------------------\n",
      "Max loss: 0.07822322845458984\n",
      "Min loss: 0.026700064539909363\n",
      "Mean loss: 0.048127767319480576\n",
      "Std loss: 0.017957005352499285\n",
      "Total Loss: 0.28876660391688347\n",
      "------------------------------------ epoch 1034 (6198 steps) ------------------------------------\n",
      "Max loss: 0.08106088638305664\n",
      "Min loss: 0.024692406877875328\n",
      "Mean loss: 0.045053024776279926\n",
      "Std loss: 0.019863033301636548\n",
      "Total Loss: 0.27031814865767956\n",
      "------------------------------------ epoch 1035 (6204 steps) ------------------------------------\n",
      "Max loss: 0.07573845982551575\n",
      "Min loss: 0.03201661631464958\n",
      "Mean loss: 0.04994045135875543\n",
      "Std loss: 0.014756413237472631\n",
      "Total Loss: 0.2996427081525326\n",
      "------------------------------------ epoch 1036 (6210 steps) ------------------------------------\n",
      "Max loss: 0.08021002262830734\n",
      "Min loss: 0.029938822612166405\n",
      "Mean loss: 0.04521630300829808\n",
      "Std loss: 0.016674254693035212\n",
      "Total Loss: 0.2712978180497885\n",
      "------------------------------------ epoch 1037 (6216 steps) ------------------------------------\n",
      "Max loss: 0.04096035659313202\n",
      "Min loss: 0.02766832336783409\n",
      "Mean loss: 0.03584248137970766\n",
      "Std loss: 0.004386916071464472\n",
      "Total Loss: 0.21505488827824593\n",
      "------------------------------------ epoch 1038 (6222 steps) ------------------------------------\n",
      "Max loss: 0.04992155358195305\n",
      "Min loss: 0.018277525901794434\n",
      "Mean loss: 0.03481824820240339\n",
      "Std loss: 0.00933737581815186\n",
      "Total Loss: 0.20890948921442032\n",
      "------------------------------------ epoch 1039 (6228 steps) ------------------------------------\n",
      "Max loss: 0.12016036361455917\n",
      "Min loss: 0.022663038223981857\n",
      "Mean loss: 0.05018456528584162\n",
      "Std loss: 0.032382486705821424\n",
      "Total Loss: 0.30110739171504974\n",
      "------------------------------------ epoch 1040 (6234 steps) ------------------------------------\n",
      "Max loss: 0.05360260233283043\n",
      "Min loss: 0.032653965055942535\n",
      "Mean loss: 0.03816613554954529\n",
      "Std loss: 0.00727797670768359\n",
      "Total Loss: 0.22899681329727173\n",
      "------------------------------------ epoch 1041 (6240 steps) ------------------------------------\n",
      "Max loss: 0.05411528795957565\n",
      "Min loss: 0.031156383454799652\n",
      "Mean loss: 0.041767584780852\n",
      "Std loss: 0.007829515733535552\n",
      "Total Loss: 0.250605508685112\n",
      "------------------------------------ epoch 1042 (6246 steps) ------------------------------------\n",
      "Max loss: 0.04926937445998192\n",
      "Min loss: 0.020366664975881577\n",
      "Mean loss: 0.03608225534359614\n",
      "Std loss: 0.010473947069248068\n",
      "Total Loss: 0.21649353206157684\n",
      "------------------------------------ epoch 1043 (6252 steps) ------------------------------------\n",
      "Max loss: 0.05833832547068596\n",
      "Min loss: 0.02776065282523632\n",
      "Mean loss: 0.04123482387512922\n",
      "Std loss: 0.011503339840059886\n",
      "Total Loss: 0.24740894325077534\n",
      "------------------------------------ epoch 1044 (6258 steps) ------------------------------------\n",
      "Max loss: 0.05038432776927948\n",
      "Min loss: 0.025481557473540306\n",
      "Mean loss: 0.03401225029180447\n",
      "Std loss: 0.008087246232557172\n",
      "Total Loss: 0.20407350175082684\n",
      "------------------------------------ epoch 1045 (6264 steps) ------------------------------------\n",
      "Max loss: 0.05338113754987717\n",
      "Min loss: 0.02166585996747017\n",
      "Mean loss: 0.03803849779069424\n",
      "Std loss: 0.009558725564321984\n",
      "Total Loss: 0.22823098674416542\n",
      "------------------------------------ epoch 1046 (6270 steps) ------------------------------------\n",
      "Max loss: 0.058342061936855316\n",
      "Min loss: 0.027825145050883293\n",
      "Mean loss: 0.040154172418018184\n",
      "Std loss: 0.01080694078308324\n",
      "Total Loss: 0.2409250345081091\n",
      "------------------------------------ epoch 1047 (6276 steps) ------------------------------------\n",
      "Max loss: 0.059222638607025146\n",
      "Min loss: 0.02405792474746704\n",
      "Mean loss: 0.03897219058126211\n",
      "Std loss: 0.01214813779699511\n",
      "Total Loss: 0.23383314348757267\n",
      "------------------------------------ epoch 1048 (6282 steps) ------------------------------------\n",
      "Max loss: 0.057844288647174835\n",
      "Min loss: 0.028350910171866417\n",
      "Mean loss: 0.04410586475084225\n",
      "Std loss: 0.010054299527940803\n",
      "Total Loss: 0.2646351885050535\n",
      "------------------------------------ epoch 1049 (6288 steps) ------------------------------------\n",
      "Max loss: 0.0804644376039505\n",
      "Min loss: 0.018936920911073685\n",
      "Mean loss: 0.03665219650914272\n",
      "Std loss: 0.02012094553646124\n",
      "Total Loss: 0.2199131790548563\n",
      "------------------------------------ epoch 1050 (6294 steps) ------------------------------------\n",
      "Max loss: 0.06883630156517029\n",
      "Min loss: 0.029889415949583054\n",
      "Mean loss: 0.04173068640132745\n",
      "Std loss: 0.01288901038354187\n",
      "Total Loss: 0.2503841184079647\n",
      "------------------------------------ epoch 1051 (6300 steps) ------------------------------------\n",
      "Max loss: 0.0546158067882061\n",
      "Min loss: 0.025103621184825897\n",
      "Mean loss: 0.04173078077534834\n",
      "Std loss: 0.01079617992617891\n",
      "Total Loss: 0.2503846846520901\n",
      "------------------------------------ epoch 1052 (6306 steps) ------------------------------------\n",
      "Max loss: 0.06484314799308777\n",
      "Min loss: 0.020872902125120163\n",
      "Mean loss: 0.035572996052602925\n",
      "Std loss: 0.014611318240739728\n",
      "Total Loss: 0.21343797631561756\n",
      "------------------------------------ epoch 1053 (6312 steps) ------------------------------------\n",
      "Max loss: 0.051969967782497406\n",
      "Min loss: 0.028759703040122986\n",
      "Mean loss: 0.036397247575223446\n",
      "Std loss: 0.009594257114783346\n",
      "Total Loss: 0.21838348545134068\n",
      "------------------------------------ epoch 1054 (6318 steps) ------------------------------------\n",
      "Max loss: 0.05182565003633499\n",
      "Min loss: 0.02058510296046734\n",
      "Mean loss: 0.03629855780551831\n",
      "Std loss: 0.009482048258113015\n",
      "Total Loss: 0.21779134683310986\n",
      "------------------------------------ epoch 1055 (6324 steps) ------------------------------------\n",
      "Max loss: 0.09242129325866699\n",
      "Min loss: 0.034548427909612656\n",
      "Mean loss: 0.056422810380657516\n",
      "Std loss: 0.022379639150857965\n",
      "Total Loss: 0.3385368622839451\n",
      "------------------------------------ epoch 1056 (6330 steps) ------------------------------------\n",
      "Max loss: 0.07263064384460449\n",
      "Min loss: 0.02784276381134987\n",
      "Mean loss: 0.04912435511747996\n",
      "Std loss: 0.017466078298088866\n",
      "Total Loss: 0.29474613070487976\n",
      "------------------------------------ epoch 1057 (6336 steps) ------------------------------------\n",
      "Max loss: 0.05002307891845703\n",
      "Min loss: 0.02161286026239395\n",
      "Mean loss: 0.03565210849046707\n",
      "Std loss: 0.0085215929232418\n",
      "Total Loss: 0.21391265094280243\n",
      "------------------------------------ epoch 1058 (6342 steps) ------------------------------------\n",
      "Max loss: 0.1199459582567215\n",
      "Min loss: 0.029456328600645065\n",
      "Mean loss: 0.05943403082589308\n",
      "Std loss: 0.02919320389890822\n",
      "Total Loss: 0.3566041849553585\n",
      "------------------------------------ epoch 1059 (6348 steps) ------------------------------------\n",
      "Max loss: 0.08148705959320068\n",
      "Min loss: 0.0280311219394207\n",
      "Mean loss: 0.04553299397230148\n",
      "Std loss: 0.01741499431374073\n",
      "Total Loss: 0.2731979638338089\n",
      "------------------------------------ epoch 1060 (6354 steps) ------------------------------------\n",
      "Max loss: 0.07432358711957932\n",
      "Min loss: 0.02646535262465477\n",
      "Mean loss: 0.03937722556293011\n",
      "Std loss: 0.016353043411288123\n",
      "Total Loss: 0.23626335337758064\n",
      "------------------------------------ epoch 1061 (6360 steps) ------------------------------------\n",
      "Max loss: 0.09991566836833954\n",
      "Min loss: 0.030873477458953857\n",
      "Mean loss: 0.05631980672478676\n",
      "Std loss: 0.021262370931557934\n",
      "Total Loss: 0.33791884034872055\n",
      "------------------------------------ epoch 1062 (6366 steps) ------------------------------------\n",
      "Max loss: 0.04928657412528992\n",
      "Min loss: 0.022857148200273514\n",
      "Mean loss: 0.03820912167429924\n",
      "Std loss: 0.010137874487852373\n",
      "Total Loss: 0.22925473004579544\n",
      "------------------------------------ epoch 1063 (6372 steps) ------------------------------------\n",
      "Max loss: 0.06174907833337784\n",
      "Min loss: 0.01613897830247879\n",
      "Mean loss: 0.034065089498957\n",
      "Std loss: 0.014603158226658919\n",
      "Total Loss: 0.204390536993742\n",
      "------------------------------------ epoch 1064 (6378 steps) ------------------------------------\n",
      "Max loss: 0.04708833619952202\n",
      "Min loss: 0.02527972124516964\n",
      "Mean loss: 0.0336111510793368\n",
      "Std loss: 0.00855967634465654\n",
      "Total Loss: 0.2016669064760208\n",
      "------------------------------------ epoch 1065 (6384 steps) ------------------------------------\n",
      "Max loss: 0.05896517634391785\n",
      "Min loss: 0.021083083003759384\n",
      "Mean loss: 0.034466369077563286\n",
      "Std loss: 0.013209419222546436\n",
      "Total Loss: 0.20679821446537971\n",
      "------------------------------------ epoch 1066 (6390 steps) ------------------------------------\n",
      "Max loss: 0.05010420083999634\n",
      "Min loss: 0.01975850947201252\n",
      "Mean loss: 0.03660236609478792\n",
      "Std loss: 0.010783423088390011\n",
      "Total Loss: 0.2196141965687275\n",
      "------------------------------------ epoch 1067 (6396 steps) ------------------------------------\n",
      "Max loss: 0.0631253570318222\n",
      "Min loss: 0.03326178342103958\n",
      "Mean loss: 0.04892177569369475\n",
      "Std loss: 0.009573080464417398\n",
      "Total Loss: 0.2935306541621685\n",
      "------------------------------------ epoch 1068 (6402 steps) ------------------------------------\n",
      "Max loss: 0.1110544204711914\n",
      "Min loss: 0.02308308705687523\n",
      "Mean loss: 0.05324386681119601\n",
      "Std loss: 0.028642794226652744\n",
      "Total Loss: 0.31946320086717606\n",
      "------------------------------------ epoch 1069 (6408 steps) ------------------------------------\n",
      "Max loss: 0.054402805864810944\n",
      "Min loss: 0.02137848548591137\n",
      "Mean loss: 0.03526436910033226\n",
      "Std loss: 0.012699670270433571\n",
      "Total Loss: 0.21158621460199356\n",
      "------------------------------------ epoch 1070 (6414 steps) ------------------------------------\n",
      "Max loss: 0.07496684789657593\n",
      "Min loss: 0.02189411222934723\n",
      "Mean loss: 0.04422001509616772\n",
      "Std loss: 0.018778278075506156\n",
      "Total Loss: 0.26532009057700634\n",
      "------------------------------------ epoch 1071 (6420 steps) ------------------------------------\n",
      "Max loss: 0.05620716139674187\n",
      "Min loss: 0.016115615144371986\n",
      "Mean loss: 0.034787182696163654\n",
      "Std loss: 0.014118928165467106\n",
      "Total Loss: 0.20872309617698193\n",
      "------------------------------------ epoch 1072 (6426 steps) ------------------------------------\n",
      "Max loss: 0.07020168751478195\n",
      "Min loss: 0.029565980657935143\n",
      "Mean loss: 0.04564105564107498\n",
      "Std loss: 0.012406532674861586\n",
      "Total Loss: 0.27384633384644985\n",
      "------------------------------------ epoch 1073 (6432 steps) ------------------------------------\n",
      "Max loss: 0.06323041021823883\n",
      "Min loss: 0.02108779363334179\n",
      "Mean loss: 0.03654707813014587\n",
      "Std loss: 0.014226429371048501\n",
      "Total Loss: 0.2192824687808752\n",
      "------------------------------------ epoch 1074 (6438 steps) ------------------------------------\n",
      "Max loss: 0.05292963236570358\n",
      "Min loss: 0.01865389570593834\n",
      "Mean loss: 0.036935620630780854\n",
      "Std loss: 0.011292641918472717\n",
      "Total Loss: 0.22161372378468513\n",
      "------------------------------------ epoch 1075 (6444 steps) ------------------------------------\n",
      "Max loss: 0.09681498259305954\n",
      "Min loss: 0.023488406091928482\n",
      "Mean loss: 0.051298597206672035\n",
      "Std loss: 0.02299427847178761\n",
      "Total Loss: 0.3077915832400322\n",
      "------------------------------------ epoch 1076 (6450 steps) ------------------------------------\n",
      "Max loss: 0.08558353781700134\n",
      "Min loss: 0.03246718645095825\n",
      "Mean loss: 0.05118676833808422\n",
      "Std loss: 0.01877730225966343\n",
      "Total Loss: 0.3071206100285053\n",
      "------------------------------------ epoch 1077 (6456 steps) ------------------------------------\n",
      "Max loss: 0.04551731050014496\n",
      "Min loss: 0.02679135650396347\n",
      "Mean loss: 0.039183346554636955\n",
      "Std loss: 0.006512171676395485\n",
      "Total Loss: 0.23510007932782173\n",
      "------------------------------------ epoch 1078 (6462 steps) ------------------------------------\n",
      "Max loss: 0.05938684940338135\n",
      "Min loss: 0.03104468062520027\n",
      "Mean loss: 0.04036674710611502\n",
      "Std loss: 0.009513688791814346\n",
      "Total Loss: 0.24220048263669014\n",
      "------------------------------------ epoch 1079 (6468 steps) ------------------------------------\n",
      "Max loss: 0.06370508670806885\n",
      "Min loss: 0.029461488127708435\n",
      "Mean loss: 0.042756822581092514\n",
      "Std loss: 0.011447778492131182\n",
      "Total Loss: 0.2565409354865551\n",
      "------------------------------------ epoch 1080 (6474 steps) ------------------------------------\n",
      "Max loss: 0.05516591668128967\n",
      "Min loss: 0.019870273768901825\n",
      "Mean loss: 0.03763565017531315\n",
      "Std loss: 0.013158385716943756\n",
      "Total Loss: 0.22581390105187893\n",
      "------------------------------------ epoch 1081 (6480 steps) ------------------------------------\n",
      "Max loss: 0.05546468496322632\n",
      "Min loss: 0.02212200127542019\n",
      "Mean loss: 0.038258498845001064\n",
      "Std loss: 0.01206929113408261\n",
      "Total Loss: 0.22955099307000637\n",
      "------------------------------------ epoch 1082 (6486 steps) ------------------------------------\n",
      "Max loss: 0.05486277863383293\n",
      "Min loss: 0.018202368170022964\n",
      "Mean loss: 0.03573152547081312\n",
      "Std loss: 0.014383776619994322\n",
      "Total Loss: 0.2143891528248787\n",
      "------------------------------------ epoch 1083 (6492 steps) ------------------------------------\n",
      "Max loss: 0.05262972414493561\n",
      "Min loss: 0.0327439159154892\n",
      "Mean loss: 0.04374261200428009\n",
      "Std loss: 0.007707689554586612\n",
      "Total Loss: 0.26245567202568054\n",
      "------------------------------------ epoch 1084 (6498 steps) ------------------------------------\n",
      "Max loss: 0.06173348426818848\n",
      "Min loss: 0.02253877744078636\n",
      "Mean loss: 0.03690974693745375\n",
      "Std loss: 0.013121987382981534\n",
      "Total Loss: 0.22145848162472248\n",
      "------------------------------------ epoch 1085 (6504 steps) ------------------------------------\n",
      "Max loss: 0.09924587607383728\n",
      "Min loss: 0.03197108209133148\n",
      "Mean loss: 0.05885602533817291\n",
      "Std loss: 0.022644343331949437\n",
      "Total Loss: 0.3531361520290375\n",
      "------------------------------------ epoch 1086 (6510 steps) ------------------------------------\n",
      "Max loss: 0.06950177997350693\n",
      "Min loss: 0.016951661556959152\n",
      "Mean loss: 0.039700151731570564\n",
      "Std loss: 0.01631168793250203\n",
      "Total Loss: 0.23820091038942337\n",
      "------------------------------------ epoch 1087 (6516 steps) ------------------------------------\n",
      "Max loss: 0.05424303561449051\n",
      "Min loss: 0.02240162156522274\n",
      "Mean loss: 0.04054237063974142\n",
      "Std loss: 0.012150870062586729\n",
      "Total Loss: 0.24325422383844852\n",
      "------------------------------------ epoch 1088 (6522 steps) ------------------------------------\n",
      "Max loss: 0.0543149933218956\n",
      "Min loss: 0.025268152356147766\n",
      "Mean loss: 0.03575873685379823\n",
      "Std loss: 0.009277246218890166\n",
      "Total Loss: 0.21455242112278938\n",
      "------------------------------------ epoch 1089 (6528 steps) ------------------------------------\n",
      "Max loss: 0.06093687564134598\n",
      "Min loss: 0.033556580543518066\n",
      "Mean loss: 0.04495414781073729\n",
      "Std loss: 0.009239952122875936\n",
      "Total Loss: 0.26972488686442375\n",
      "------------------------------------ epoch 1090 (6534 steps) ------------------------------------\n",
      "Max loss: 0.052933722734451294\n",
      "Min loss: 0.019043942913413048\n",
      "Mean loss: 0.03565689424673716\n",
      "Std loss: 0.012683998208667125\n",
      "Total Loss: 0.21394136548042297\n",
      "------------------------------------ epoch 1091 (6540 steps) ------------------------------------\n",
      "Max loss: 0.04659517854452133\n",
      "Min loss: 0.036903999745845795\n",
      "Mean loss: 0.04211400511364142\n",
      "Std loss: 0.0033409002794408346\n",
      "Total Loss: 0.2526840306818485\n",
      "------------------------------------ epoch 1092 (6546 steps) ------------------------------------\n",
      "Max loss: 0.044743236154317856\n",
      "Min loss: 0.019973762333393097\n",
      "Mean loss: 0.03476002377768358\n",
      "Std loss: 0.007451534403850863\n",
      "Total Loss: 0.20856014266610146\n",
      "------------------------------------ epoch 1093 (6552 steps) ------------------------------------\n",
      "Max loss: 0.041686441749334335\n",
      "Min loss: 0.02612493745982647\n",
      "Mean loss: 0.032557943214972816\n",
      "Std loss: 0.005426381343299533\n",
      "Total Loss: 0.19534765928983688\n",
      "------------------------------------ epoch 1094 (6558 steps) ------------------------------------\n",
      "Max loss: 0.11720141768455505\n",
      "Min loss: 0.02352868765592575\n",
      "Mean loss: 0.06206713803112507\n",
      "Std loss: 0.031638404722492464\n",
      "Total Loss: 0.3724028281867504\n",
      "------------------------------------ epoch 1095 (6564 steps) ------------------------------------\n",
      "Max loss: 0.08150329440832138\n",
      "Min loss: 0.026385044679045677\n",
      "Mean loss: 0.04742736089974642\n",
      "Std loss: 0.019517874897963985\n",
      "Total Loss: 0.2845641653984785\n",
      "------------------------------------ epoch 1096 (6570 steps) ------------------------------------\n",
      "Max loss: 0.07088182866573334\n",
      "Min loss: 0.018322987481951714\n",
      "Mean loss: 0.037886325580378376\n",
      "Std loss: 0.01778627741162745\n",
      "Total Loss: 0.22731795348227024\n",
      "------------------------------------ epoch 1097 (6576 steps) ------------------------------------\n",
      "Max loss: 0.07893829047679901\n",
      "Min loss: 0.02519259974360466\n",
      "Mean loss: 0.04086329663793246\n",
      "Std loss: 0.019636148166667297\n",
      "Total Loss: 0.24517977982759476\n",
      "------------------------------------ epoch 1098 (6582 steps) ------------------------------------\n",
      "Max loss: 0.05982062965631485\n",
      "Min loss: 0.018971804529428482\n",
      "Mean loss: 0.04144332309563955\n",
      "Std loss: 0.013790977568529119\n",
      "Total Loss: 0.24865993857383728\n",
      "------------------------------------ epoch 1099 (6588 steps) ------------------------------------\n",
      "Max loss: 0.07534673810005188\n",
      "Min loss: 0.03015155903995037\n",
      "Mean loss: 0.04839082031200329\n",
      "Std loss: 0.014141199449826317\n",
      "Total Loss: 0.29034492187201977\n",
      "------------------------------------ epoch 1100 (6594 steps) ------------------------------------\n",
      "Max loss: 0.08365137875080109\n",
      "Min loss: 0.02950224094092846\n",
      "Mean loss: 0.04084304999560118\n",
      "Std loss: 0.01930335348800531\n",
      "Total Loss: 0.24505829997360706\n",
      "------------------------------------ epoch 1101 (6600 steps) ------------------------------------\n",
      "Max loss: 0.056121036410331726\n",
      "Min loss: 0.031015470623970032\n",
      "Mean loss: 0.04250766523182392\n",
      "Std loss: 0.009169004307240422\n",
      "Total Loss: 0.2550459913909435\n",
      "saved model at ./weights/model_1101.pth\n",
      "------------------------------------ epoch 1102 (6606 steps) ------------------------------------\n",
      "Max loss: 0.12133502960205078\n",
      "Min loss: 0.015423476696014404\n",
      "Mean loss: 0.06445721909403801\n",
      "Std loss: 0.035729026351620646\n",
      "Total Loss: 0.38674331456422806\n",
      "------------------------------------ epoch 1103 (6612 steps) ------------------------------------\n",
      "Max loss: 0.0655878335237503\n",
      "Min loss: 0.02732158824801445\n",
      "Mean loss: 0.03917670901864767\n",
      "Std loss: 0.01494661646926488\n",
      "Total Loss: 0.23506025411188602\n",
      "------------------------------------ epoch 1104 (6618 steps) ------------------------------------\n",
      "Max loss: 0.10841763019561768\n",
      "Min loss: 0.0223773792386055\n",
      "Mean loss: 0.052648164331912994\n",
      "Std loss: 0.028640718931485468\n",
      "Total Loss: 0.31588898599147797\n",
      "------------------------------------ epoch 1105 (6624 steps) ------------------------------------\n",
      "Max loss: 0.2381787747144699\n",
      "Min loss: 0.058713994920253754\n",
      "Mean loss: 0.1397882973154386\n",
      "Std loss: 0.07202151336787252\n",
      "Total Loss: 0.8387297838926315\n",
      "------------------------------------ epoch 1106 (6630 steps) ------------------------------------\n",
      "Max loss: 1.0868210792541504\n",
      "Min loss: 0.19396865367889404\n",
      "Mean loss: 0.5696194271246592\n",
      "Std loss: 0.27675752772453\n",
      "Total Loss: 3.4177165627479553\n",
      "------------------------------------ epoch 1107 (6636 steps) ------------------------------------\n",
      "Max loss: 0.6872867941856384\n",
      "Min loss: 0.4083802103996277\n",
      "Mean loss: 0.516181007027626\n",
      "Std loss: 0.11326257186576631\n",
      "Total Loss: 3.0970860421657562\n",
      "------------------------------------ epoch 1108 (6642 steps) ------------------------------------\n",
      "Max loss: 0.4114462733268738\n",
      "Min loss: 0.28527456521987915\n",
      "Mean loss: 0.3425566206375758\n",
      "Std loss: 0.04989490394623398\n",
      "Total Loss: 2.0553397238254547\n",
      "------------------------------------ epoch 1109 (6648 steps) ------------------------------------\n",
      "Max loss: 0.6537636518478394\n",
      "Min loss: 0.28242409229278564\n",
      "Mean loss: 0.3998870551586151\n",
      "Std loss: 0.1256235032182697\n",
      "Total Loss: 2.3993223309516907\n",
      "------------------------------------ epoch 1110 (6654 steps) ------------------------------------\n",
      "Max loss: 0.3032434582710266\n",
      "Min loss: 0.22757163643836975\n",
      "Mean loss: 0.2693224946657817\n",
      "Std loss: 0.02507339068479728\n",
      "Total Loss: 1.61593496799469\n",
      "------------------------------------ epoch 1111 (6660 steps) ------------------------------------\n",
      "Max loss: 0.23901180922985077\n",
      "Min loss: 0.19550195336341858\n",
      "Mean loss: 0.22397030889987946\n",
      "Std loss: 0.014408397916863372\n",
      "Total Loss: 1.3438218533992767\n",
      "------------------------------------ epoch 1112 (6666 steps) ------------------------------------\n",
      "Max loss: 0.21651029586791992\n",
      "Min loss: 0.1630164533853531\n",
      "Mean loss: 0.1913611739873886\n",
      "Std loss: 0.0168623159936623\n",
      "Total Loss: 1.1481670439243317\n",
      "------------------------------------ epoch 1113 (6672 steps) ------------------------------------\n",
      "Max loss: 0.2454877495765686\n",
      "Min loss: 0.13150319457054138\n",
      "Mean loss: 0.17417373756567636\n",
      "Std loss: 0.03610806405911447\n",
      "Total Loss: 1.0450424253940582\n",
      "------------------------------------ epoch 1114 (6678 steps) ------------------------------------\n",
      "Max loss: 0.16053304076194763\n",
      "Min loss: 0.10723444074392319\n",
      "Mean loss: 0.1331728659570217\n",
      "Std loss: 0.020397370316391442\n",
      "Total Loss: 0.7990371957421303\n",
      "------------------------------------ epoch 1115 (6684 steps) ------------------------------------\n",
      "Max loss: 0.13774050772190094\n",
      "Min loss: 0.08290785551071167\n",
      "Mean loss: 0.11867155383030574\n",
      "Std loss: 0.017975534952672897\n",
      "Total Loss: 0.7120293229818344\n",
      "------------------------------------ epoch 1116 (6690 steps) ------------------------------------\n",
      "Max loss: 0.11492624133825302\n",
      "Min loss: 0.09226726740598679\n",
      "Mean loss: 0.1045744518438975\n",
      "Std loss: 0.008124822901913497\n",
      "Total Loss: 0.627446711063385\n",
      "------------------------------------ epoch 1117 (6696 steps) ------------------------------------\n",
      "Max loss: 0.10159620642662048\n",
      "Min loss: 0.0701892152428627\n",
      "Mean loss: 0.092139333486557\n",
      "Std loss: 0.012337725545070632\n",
      "Total Loss: 0.552836000919342\n",
      "------------------------------------ epoch 1118 (6702 steps) ------------------------------------\n",
      "Max loss: 0.0949631929397583\n",
      "Min loss: 0.07088372856378555\n",
      "Mean loss: 0.08380475143591563\n",
      "Std loss: 0.008864110810770666\n",
      "Total Loss: 0.5028285086154938\n",
      "------------------------------------ epoch 1119 (6708 steps) ------------------------------------\n",
      "Max loss: 0.09867838025093079\n",
      "Min loss: 0.050684597343206406\n",
      "Mean loss: 0.07631958710650603\n",
      "Std loss: 0.017290766294826322\n",
      "Total Loss: 0.4579175226390362\n",
      "------------------------------------ epoch 1120 (6714 steps) ------------------------------------\n",
      "Max loss: 0.09473811835050583\n",
      "Min loss: 0.05535194277763367\n",
      "Mean loss: 0.06717988600333531\n",
      "Std loss: 0.012883811085268844\n",
      "Total Loss: 0.4030793160200119\n",
      "------------------------------------ epoch 1121 (6720 steps) ------------------------------------\n",
      "Max loss: 0.10066302865743637\n",
      "Min loss: 0.051346875727176666\n",
      "Mean loss: 0.0701120135684808\n",
      "Std loss: 0.02051975297857855\n",
      "Total Loss: 0.42067208141088486\n",
      "------------------------------------ epoch 1122 (6726 steps) ------------------------------------\n",
      "Max loss: 0.07975152134895325\n",
      "Min loss: 0.03623880073428154\n",
      "Mean loss: 0.05677494530876478\n",
      "Std loss: 0.015702650194663527\n",
      "Total Loss: 0.34064967185258865\n",
      "------------------------------------ epoch 1123 (6732 steps) ------------------------------------\n",
      "Max loss: 0.06639021635055542\n",
      "Min loss: 0.03817061707377434\n",
      "Mean loss: 0.05271087276438872\n",
      "Std loss: 0.011179664090473891\n",
      "Total Loss: 0.3162652365863323\n",
      "------------------------------------ epoch 1124 (6738 steps) ------------------------------------\n",
      "Max loss: 0.08709923923015594\n",
      "Min loss: 0.03535446897149086\n",
      "Mean loss: 0.060937161867817245\n",
      "Std loss: 0.016468636665104577\n",
      "Total Loss: 0.36562297120690346\n",
      "------------------------------------ epoch 1125 (6744 steps) ------------------------------------\n",
      "Max loss: 0.09532001614570618\n",
      "Min loss: 0.04487830027937889\n",
      "Mean loss: 0.06678715037802856\n",
      "Std loss: 0.018284866806717648\n",
      "Total Loss: 0.4007229022681713\n",
      "------------------------------------ epoch 1126 (6750 steps) ------------------------------------\n",
      "Max loss: 0.09187202900648117\n",
      "Min loss: 0.04797780513763428\n",
      "Mean loss: 0.06571176337699096\n",
      "Std loss: 0.014293897380374244\n",
      "Total Loss: 0.3942705802619457\n",
      "------------------------------------ epoch 1127 (6756 steps) ------------------------------------\n",
      "Max loss: 0.11086851358413696\n",
      "Min loss: 0.033795349299907684\n",
      "Mean loss: 0.07300569241245587\n",
      "Std loss: 0.027295398996164427\n",
      "Total Loss: 0.43803415447473526\n",
      "------------------------------------ epoch 1128 (6762 steps) ------------------------------------\n",
      "Max loss: 0.09027888625860214\n",
      "Min loss: 0.03664442151784897\n",
      "Mean loss: 0.060152050107717514\n",
      "Std loss: 0.02041122664420907\n",
      "Total Loss: 0.3609123006463051\n",
      "------------------------------------ epoch 1129 (6768 steps) ------------------------------------\n",
      "Max loss: 0.10867297649383545\n",
      "Min loss: 0.03264148533344269\n",
      "Mean loss: 0.06383587109545867\n",
      "Std loss: 0.02659962867433755\n",
      "Total Loss: 0.383015226572752\n",
      "------------------------------------ epoch 1130 (6774 steps) ------------------------------------\n",
      "Max loss: 0.06476899236440659\n",
      "Min loss: 0.033575236797332764\n",
      "Mean loss: 0.05628815107047558\n",
      "Std loss: 0.010875526418625426\n",
      "Total Loss: 0.33772890642285347\n",
      "------------------------------------ epoch 1131 (6780 steps) ------------------------------------\n",
      "Max loss: 0.10122563689947128\n",
      "Min loss: 0.036993175745010376\n",
      "Mean loss: 0.05196620710194111\n",
      "Std loss: 0.02257915629620648\n",
      "Total Loss: 0.31179724261164665\n",
      "------------------------------------ epoch 1132 (6786 steps) ------------------------------------\n",
      "Max loss: 0.07031017541885376\n",
      "Min loss: 0.030611515045166016\n",
      "Mean loss: 0.054107593993345894\n",
      "Std loss: 0.01318152390165261\n",
      "Total Loss: 0.3246455639600754\n",
      "------------------------------------ epoch 1133 (6792 steps) ------------------------------------\n",
      "Max loss: 0.08071659505367279\n",
      "Min loss: 0.03318719565868378\n",
      "Mean loss: 0.054439581309755646\n",
      "Std loss: 0.0168015916499031\n",
      "Total Loss: 0.32663748785853386\n",
      "------------------------------------ epoch 1134 (6798 steps) ------------------------------------\n",
      "Max loss: 0.06753240525722504\n",
      "Min loss: 0.027957741171121597\n",
      "Mean loss: 0.04606531312068304\n",
      "Std loss: 0.012769862393847616\n",
      "Total Loss: 0.2763918787240982\n",
      "------------------------------------ epoch 1135 (6804 steps) ------------------------------------\n",
      "Max loss: 0.0721074566245079\n",
      "Min loss: 0.03219614923000336\n",
      "Mean loss: 0.05159167945384979\n",
      "Std loss: 0.015249926498501744\n",
      "Total Loss: 0.30955007672309875\n",
      "------------------------------------ epoch 1136 (6810 steps) ------------------------------------\n",
      "Max loss: 0.07687895745038986\n",
      "Min loss: 0.02674589678645134\n",
      "Mean loss: 0.04743430887659391\n",
      "Std loss: 0.016683848292073675\n",
      "Total Loss: 0.28460585325956345\n",
      "------------------------------------ epoch 1137 (6816 steps) ------------------------------------\n",
      "Max loss: 0.05615391582250595\n",
      "Min loss: 0.030150143429636955\n",
      "Mean loss: 0.04031283253182968\n",
      "Std loss: 0.00886928271622754\n",
      "Total Loss: 0.24187699519097805\n",
      "------------------------------------ epoch 1138 (6822 steps) ------------------------------------\n",
      "Max loss: 0.08859767019748688\n",
      "Min loss: 0.026986408978700638\n",
      "Mean loss: 0.04547640588134527\n",
      "Std loss: 0.020912014929091227\n",
      "Total Loss: 0.27285843528807163\n",
      "------------------------------------ epoch 1139 (6828 steps) ------------------------------------\n",
      "Max loss: 0.0998988002538681\n",
      "Min loss: 0.03793663531541824\n",
      "Mean loss: 0.054373313362399735\n",
      "Std loss: 0.021010932601506955\n",
      "Total Loss: 0.3262398801743984\n",
      "------------------------------------ epoch 1140 (6834 steps) ------------------------------------\n",
      "Max loss: 0.1115727648139\n",
      "Min loss: 0.03089803084731102\n",
      "Mean loss: 0.0589219064762195\n",
      "Std loss: 0.02606745411702402\n",
      "Total Loss: 0.35353143885731697\n",
      "------------------------------------ epoch 1141 (6840 steps) ------------------------------------\n",
      "Max loss: 0.07010228931903839\n",
      "Min loss: 0.024823009967803955\n",
      "Mean loss: 0.04808592858413855\n",
      "Std loss: 0.016273325942525278\n",
      "Total Loss: 0.2885155715048313\n",
      "------------------------------------ epoch 1142 (6846 steps) ------------------------------------\n",
      "Max loss: 0.09273924678564072\n",
      "Min loss: 0.03181193768978119\n",
      "Mean loss: 0.05356624784568945\n",
      "Std loss: 0.019110337221922676\n",
      "Total Loss: 0.32139748707413673\n",
      "------------------------------------ epoch 1143 (6852 steps) ------------------------------------\n",
      "Max loss: 0.05351518094539642\n",
      "Min loss: 0.032211463898420334\n",
      "Mean loss: 0.04136521679659685\n",
      "Std loss: 0.008358615177279054\n",
      "Total Loss: 0.24819130077958107\n",
      "------------------------------------ epoch 1144 (6858 steps) ------------------------------------\n",
      "Max loss: 0.06276800483465195\n",
      "Min loss: 0.029815178364515305\n",
      "Mean loss: 0.04313525930047035\n",
      "Std loss: 0.010641884413172752\n",
      "Total Loss: 0.2588115558028221\n",
      "------------------------------------ epoch 1145 (6864 steps) ------------------------------------\n",
      "Max loss: 0.06013001129031181\n",
      "Min loss: 0.031119227409362793\n",
      "Mean loss: 0.04651146630446116\n",
      "Std loss: 0.010140820627177425\n",
      "Total Loss: 0.27906879782676697\n",
      "------------------------------------ epoch 1146 (6870 steps) ------------------------------------\n",
      "Max loss: 0.0584120899438858\n",
      "Min loss: 0.027751874178647995\n",
      "Mean loss: 0.040842728689312935\n",
      "Std loss: 0.009052255988402464\n",
      "Total Loss: 0.2450563721358776\n",
      "------------------------------------ epoch 1147 (6876 steps) ------------------------------------\n",
      "Max loss: 0.05980193614959717\n",
      "Min loss: 0.0323055237531662\n",
      "Mean loss: 0.04245158160726229\n",
      "Std loss: 0.009096914973783843\n",
      "Total Loss: 0.25470948964357376\n",
      "------------------------------------ epoch 1148 (6882 steps) ------------------------------------\n",
      "Max loss: 0.1101493164896965\n",
      "Min loss: 0.032439857721328735\n",
      "Mean loss: 0.06402947381138802\n",
      "Std loss: 0.026753459357324635\n",
      "Total Loss: 0.3841768428683281\n",
      "------------------------------------ epoch 1149 (6888 steps) ------------------------------------\n",
      "Max loss: 0.05333803594112396\n",
      "Min loss: 0.02717227302491665\n",
      "Mean loss: 0.04023041048397621\n",
      "Std loss: 0.008958591462949004\n",
      "Total Loss: 0.24138246290385723\n",
      "------------------------------------ epoch 1150 (6894 steps) ------------------------------------\n",
      "Max loss: 0.056755922734737396\n",
      "Min loss: 0.03139343112707138\n",
      "Mean loss: 0.0452905942996343\n",
      "Std loss: 0.009773821798566333\n",
      "Total Loss: 0.2717435657978058\n",
      "------------------------------------ epoch 1151 (6900 steps) ------------------------------------\n",
      "Max loss: 0.08387814462184906\n",
      "Min loss: 0.03320644050836563\n",
      "Mean loss: 0.049181185041864715\n",
      "Std loss: 0.017595338411180328\n",
      "Total Loss: 0.2950871102511883\n",
      "------------------------------------ epoch 1152 (6906 steps) ------------------------------------\n",
      "Max loss: 0.08151635527610779\n",
      "Min loss: 0.028951002284884453\n",
      "Mean loss: 0.046795038195947804\n",
      "Std loss: 0.017559479322760974\n",
      "Total Loss: 0.28077022917568684\n",
      "------------------------------------ epoch 1153 (6912 steps) ------------------------------------\n",
      "Max loss: 0.09389995038509369\n",
      "Min loss: 0.03334809094667435\n",
      "Mean loss: 0.053427752728263535\n",
      "Std loss: 0.019200800143740312\n",
      "Total Loss: 0.3205665163695812\n",
      "------------------------------------ epoch 1154 (6918 steps) ------------------------------------\n",
      "Max loss: 0.05850347504019737\n",
      "Min loss: 0.034458547830581665\n",
      "Mean loss: 0.0466814481963714\n",
      "Std loss: 0.008009830058555994\n",
      "Total Loss: 0.2800886891782284\n",
      "------------------------------------ epoch 1155 (6924 steps) ------------------------------------\n",
      "Max loss: 0.06728238612413406\n",
      "Min loss: 0.021830271929502487\n",
      "Mean loss: 0.038410306287308536\n",
      "Std loss: 0.015922419059365773\n",
      "Total Loss: 0.2304618377238512\n",
      "------------------------------------ epoch 1156 (6930 steps) ------------------------------------\n",
      "Max loss: 0.05859159678220749\n",
      "Min loss: 0.024110212922096252\n",
      "Mean loss: 0.044067803459862866\n",
      "Std loss: 0.012781671564340747\n",
      "Total Loss: 0.2644068207591772\n",
      "------------------------------------ epoch 1157 (6936 steps) ------------------------------------\n",
      "Max loss: 0.08399297297000885\n",
      "Min loss: 0.04718165099620819\n",
      "Mean loss: 0.06020683919390043\n",
      "Std loss: 0.013738681540179378\n",
      "Total Loss: 0.36124103516340256\n",
      "------------------------------------ epoch 1158 (6942 steps) ------------------------------------\n",
      "Max loss: 0.0624934546649456\n",
      "Min loss: 0.028200779110193253\n",
      "Mean loss: 0.04294357790301243\n",
      "Std loss: 0.013090825095331899\n",
      "Total Loss: 0.2576614674180746\n",
      "------------------------------------ epoch 1159 (6948 steps) ------------------------------------\n",
      "Max loss: 0.06743960082530975\n",
      "Min loss: 0.036129534244537354\n",
      "Mean loss: 0.05078932146231333\n",
      "Std loss: 0.01223451028127043\n",
      "Total Loss: 0.30473592877388\n",
      "------------------------------------ epoch 1160 (6954 steps) ------------------------------------\n",
      "Max loss: 0.05602429062128067\n",
      "Min loss: 0.029641618952155113\n",
      "Mean loss: 0.04042331843326489\n",
      "Std loss: 0.00919343792937625\n",
      "Total Loss: 0.24253991059958935\n",
      "------------------------------------ epoch 1161 (6960 steps) ------------------------------------\n",
      "Max loss: 0.05571123957633972\n",
      "Min loss: 0.026229670271277428\n",
      "Mean loss: 0.038500319235026836\n",
      "Std loss: 0.010640813247989041\n",
      "Total Loss: 0.23100191541016102\n",
      "------------------------------------ epoch 1162 (6966 steps) ------------------------------------\n",
      "Max loss: 0.05701073631644249\n",
      "Min loss: 0.03064851090312004\n",
      "Mean loss: 0.0460855724910895\n",
      "Std loss: 0.009193951470726004\n",
      "Total Loss: 0.276513434946537\n",
      "------------------------------------ epoch 1163 (6972 steps) ------------------------------------\n",
      "Max loss: 0.06059850752353668\n",
      "Min loss: 0.04301115497946739\n",
      "Mean loss: 0.04865834986170133\n",
      "Std loss: 0.0061272150779280755\n",
      "Total Loss: 0.291950099170208\n",
      "------------------------------------ epoch 1164 (6978 steps) ------------------------------------\n",
      "Max loss: 0.08314406871795654\n",
      "Min loss: 0.03002489171922207\n",
      "Mean loss: 0.04753151753296455\n",
      "Std loss: 0.01872895825020771\n",
      "Total Loss: 0.2851891051977873\n",
      "------------------------------------ epoch 1165 (6984 steps) ------------------------------------\n",
      "Max loss: 0.062245409935712814\n",
      "Min loss: 0.02408638969063759\n",
      "Mean loss: 0.04481090729435285\n",
      "Std loss: 0.011870115160690003\n",
      "Total Loss: 0.2688654437661171\n",
      "------------------------------------ epoch 1166 (6990 steps) ------------------------------------\n",
      "Max loss: 0.06906619668006897\n",
      "Min loss: 0.032315999269485474\n",
      "Mean loss: 0.04962743632495403\n",
      "Std loss: 0.013297666416528296\n",
      "Total Loss: 0.2977646179497242\n",
      "------------------------------------ epoch 1167 (6996 steps) ------------------------------------\n",
      "Max loss: 0.07102341949939728\n",
      "Min loss: 0.024014566093683243\n",
      "Mean loss: 0.04004353409012159\n",
      "Std loss: 0.01518726312447648\n",
      "Total Loss: 0.24026120454072952\n",
      "------------------------------------ epoch 1168 (7002 steps) ------------------------------------\n",
      "Max loss: 0.05657484754920006\n",
      "Min loss: 0.03049151599407196\n",
      "Mean loss: 0.0404875340561072\n",
      "Std loss: 0.008030223779916176\n",
      "Total Loss: 0.24292520433664322\n",
      "------------------------------------ epoch 1169 (7008 steps) ------------------------------------\n",
      "Max loss: 0.053380392491817474\n",
      "Min loss: 0.02664783224463463\n",
      "Mean loss: 0.03752675962944826\n",
      "Std loss: 0.009768731856710379\n",
      "Total Loss: 0.22516055777668953\n",
      "------------------------------------ epoch 1170 (7014 steps) ------------------------------------\n",
      "Max loss: 0.07539740204811096\n",
      "Min loss: 0.028005298227071762\n",
      "Mean loss: 0.049649051701029144\n",
      "Std loss: 0.017068723743556165\n",
      "Total Loss: 0.29789431020617485\n",
      "------------------------------------ epoch 1171 (7020 steps) ------------------------------------\n",
      "Max loss: 0.09738393872976303\n",
      "Min loss: 0.033687740564346313\n",
      "Mean loss: 0.055730460211634636\n",
      "Std loss: 0.02170441330349796\n",
      "Total Loss: 0.3343827612698078\n",
      "------------------------------------ epoch 1172 (7026 steps) ------------------------------------\n",
      "Max loss: 0.057735003530979156\n",
      "Min loss: 0.02607998251914978\n",
      "Mean loss: 0.04287046131988367\n",
      "Std loss: 0.012339914427134155\n",
      "Total Loss: 0.257222767919302\n",
      "------------------------------------ epoch 1173 (7032 steps) ------------------------------------\n",
      "Max loss: 0.05000128597021103\n",
      "Min loss: 0.025192197412252426\n",
      "Mean loss: 0.035496397564808525\n",
      "Std loss: 0.007754553527979013\n",
      "Total Loss: 0.21297838538885117\n",
      "------------------------------------ epoch 1174 (7038 steps) ------------------------------------\n",
      "Max loss: 0.1026991605758667\n",
      "Min loss: 0.02518766187131405\n",
      "Mean loss: 0.05734140332788229\n",
      "Std loss: 0.027985625370039686\n",
      "Total Loss: 0.34404841996729374\n",
      "------------------------------------ epoch 1175 (7044 steps) ------------------------------------\n",
      "Max loss: 0.07111579924821854\n",
      "Min loss: 0.03040323033928871\n",
      "Mean loss: 0.04770195111632347\n",
      "Std loss: 0.014775102882072268\n",
      "Total Loss: 0.2862117066979408\n",
      "------------------------------------ epoch 1176 (7050 steps) ------------------------------------\n",
      "Max loss: 0.06144684553146362\n",
      "Min loss: 0.027928978204727173\n",
      "Mean loss: 0.03722713701426983\n",
      "Std loss: 0.011268606405070016\n",
      "Total Loss: 0.22336282208561897\n",
      "------------------------------------ epoch 1177 (7056 steps) ------------------------------------\n",
      "Max loss: 0.06784256547689438\n",
      "Min loss: 0.025231145322322845\n",
      "Mean loss: 0.03641885953644911\n",
      "Std loss: 0.014894325173237005\n",
      "Total Loss: 0.2185131572186947\n",
      "------------------------------------ epoch 1178 (7062 steps) ------------------------------------\n",
      "Max loss: 0.05104304477572441\n",
      "Min loss: 0.026127323508262634\n",
      "Mean loss: 0.036081950490673385\n",
      "Std loss: 0.008417586868141133\n",
      "Total Loss: 0.2164917029440403\n",
      "------------------------------------ epoch 1179 (7068 steps) ------------------------------------\n",
      "Max loss: 0.0859038382768631\n",
      "Min loss: 0.019520364701747894\n",
      "Mean loss: 0.048478189234932266\n",
      "Std loss: 0.024158859818475695\n",
      "Total Loss: 0.2908691354095936\n",
      "------------------------------------ epoch 1180 (7074 steps) ------------------------------------\n",
      "Max loss: 0.06448124349117279\n",
      "Min loss: 0.033201005309820175\n",
      "Mean loss: 0.04622977785766125\n",
      "Std loss: 0.010459757546172115\n",
      "Total Loss: 0.2773786671459675\n",
      "------------------------------------ epoch 1181 (7080 steps) ------------------------------------\n",
      "Max loss: 0.05980544909834862\n",
      "Min loss: 0.0319691002368927\n",
      "Mean loss: 0.04167322628200054\n",
      "Std loss: 0.01057209448372336\n",
      "Total Loss: 0.25003935769200325\n",
      "------------------------------------ epoch 1182 (7086 steps) ------------------------------------\n",
      "Max loss: 0.042714428156614304\n",
      "Min loss: 0.023584526032209396\n",
      "Mean loss: 0.03082865010946989\n",
      "Std loss: 0.006856994256391625\n",
      "Total Loss: 0.18497190065681934\n",
      "------------------------------------ epoch 1183 (7092 steps) ------------------------------------\n",
      "Max loss: 0.07794377207756042\n",
      "Min loss: 0.03340788185596466\n",
      "Mean loss: 0.04518093168735504\n",
      "Std loss: 0.016161939196470767\n",
      "Total Loss: 0.27108559012413025\n",
      "------------------------------------ epoch 1184 (7098 steps) ------------------------------------\n",
      "Max loss: 0.04160180687904358\n",
      "Min loss: 0.025275085121393204\n",
      "Mean loss: 0.030381070139507454\n",
      "Std loss: 0.0056795757449965316\n",
      "Total Loss: 0.18228642083704472\n",
      "------------------------------------ epoch 1185 (7104 steps) ------------------------------------\n",
      "Max loss: 0.10752271860837936\n",
      "Min loss: 0.019733738154172897\n",
      "Mean loss: 0.04930858748654524\n",
      "Std loss: 0.028176303526642073\n",
      "Total Loss: 0.29585152491927147\n",
      "------------------------------------ epoch 1186 (7110 steps) ------------------------------------\n",
      "Max loss: 0.06280719488859177\n",
      "Min loss: 0.030848577618598938\n",
      "Mean loss: 0.04614014675219854\n",
      "Std loss: 0.010803240787247958\n",
      "Total Loss: 0.2768408805131912\n",
      "------------------------------------ epoch 1187 (7116 steps) ------------------------------------\n",
      "Max loss: 0.08789929002523422\n",
      "Min loss: 0.033926673233509064\n",
      "Mean loss: 0.05219480892022451\n",
      "Std loss: 0.018563105168836615\n",
      "Total Loss: 0.31316885352134705\n",
      "------------------------------------ epoch 1188 (7122 steps) ------------------------------------\n",
      "Max loss: 0.08387535810470581\n",
      "Min loss: 0.027029424905776978\n",
      "Mean loss: 0.040910713374614716\n",
      "Std loss: 0.01951736633803878\n",
      "Total Loss: 0.2454642802476883\n",
      "------------------------------------ epoch 1189 (7128 steps) ------------------------------------\n",
      "Max loss: 0.07101947069168091\n",
      "Min loss: 0.01829145848751068\n",
      "Mean loss: 0.04415712753931681\n",
      "Std loss: 0.01552138563542413\n",
      "Total Loss: 0.2649427652359009\n",
      "------------------------------------ epoch 1190 (7134 steps) ------------------------------------\n",
      "Max loss: 0.09630481898784637\n",
      "Min loss: 0.025707123801112175\n",
      "Mean loss: 0.052446198339263596\n",
      "Std loss: 0.02412412153574597\n",
      "Total Loss: 0.3146771900355816\n",
      "------------------------------------ epoch 1191 (7140 steps) ------------------------------------\n",
      "Max loss: 0.050651293247938156\n",
      "Min loss: 0.029416104778647423\n",
      "Mean loss: 0.04004241184641918\n",
      "Std loss: 0.007575743290714393\n",
      "Total Loss: 0.24025447107851505\n",
      "------------------------------------ epoch 1192 (7146 steps) ------------------------------------\n",
      "Max loss: 0.06234325096011162\n",
      "Min loss: 0.029271923005580902\n",
      "Mean loss: 0.03830203196654717\n",
      "Std loss: 0.011602430900591546\n",
      "Total Loss: 0.22981219179928303\n",
      "------------------------------------ epoch 1193 (7152 steps) ------------------------------------\n",
      "Max loss: 0.0827559381723404\n",
      "Min loss: 0.028977518901228905\n",
      "Mean loss: 0.04447412211447954\n",
      "Std loss: 0.018474765851848148\n",
      "Total Loss: 0.26684473268687725\n",
      "------------------------------------ epoch 1194 (7158 steps) ------------------------------------\n",
      "Max loss: 0.06219087541103363\n",
      "Min loss: 0.03278317674994469\n",
      "Mean loss: 0.046473510563373566\n",
      "Std loss: 0.009913649321035955\n",
      "Total Loss: 0.2788410633802414\n",
      "------------------------------------ epoch 1195 (7164 steps) ------------------------------------\n",
      "Max loss: 0.08289393782615662\n",
      "Min loss: 0.026463424786925316\n",
      "Mean loss: 0.047739118337631226\n",
      "Std loss: 0.019750196444375857\n",
      "Total Loss: 0.28643471002578735\n",
      "------------------------------------ epoch 1196 (7170 steps) ------------------------------------\n",
      "Max loss: 0.06324232369661331\n",
      "Min loss: 0.02978166565299034\n",
      "Mean loss: 0.046798725302020706\n",
      "Std loss: 0.012492734791327955\n",
      "Total Loss: 0.28079235181212425\n",
      "------------------------------------ epoch 1197 (7176 steps) ------------------------------------\n",
      "Max loss: 0.053724825382232666\n",
      "Min loss: 0.025392014533281326\n",
      "Mean loss: 0.040182920172810555\n",
      "Std loss: 0.010054326774587992\n",
      "Total Loss: 0.24109752103686333\n",
      "------------------------------------ epoch 1198 (7182 steps) ------------------------------------\n",
      "Max loss: 0.0563042089343071\n",
      "Min loss: 0.025180719792842865\n",
      "Mean loss: 0.03926152673860391\n",
      "Std loss: 0.01198582674380508\n",
      "Total Loss: 0.23556916043162346\n",
      "------------------------------------ epoch 1199 (7188 steps) ------------------------------------\n",
      "Max loss: 0.05311859771609306\n",
      "Min loss: 0.019733840599656105\n",
      "Mean loss: 0.034194969261686005\n",
      "Std loss: 0.010733266168033113\n",
      "Total Loss: 0.20516981557011604\n",
      "------------------------------------ epoch 1200 (7194 steps) ------------------------------------\n",
      "Max loss: 0.06297394633293152\n",
      "Min loss: 0.020024336874485016\n",
      "Mean loss: 0.042075752591093384\n",
      "Std loss: 0.016012791914702295\n",
      "Total Loss: 0.2524545155465603\n",
      "------------------------------------ epoch 1201 (7200 steps) ------------------------------------\n",
      "Max loss: 0.04944339022040367\n",
      "Min loss: 0.022542452439665794\n",
      "Mean loss: 0.0404053283855319\n",
      "Std loss: 0.008619503852666488\n",
      "Total Loss: 0.24243197031319141\n",
      "saved model at ./weights/model_1201.pth\n",
      "------------------------------------ epoch 1202 (7206 steps) ------------------------------------\n",
      "Max loss: 0.07044684886932373\n",
      "Min loss: 0.027917003259062767\n",
      "Mean loss: 0.04492436380436023\n",
      "Std loss: 0.013710618538486668\n",
      "Total Loss: 0.2695461828261614\n",
      "------------------------------------ epoch 1203 (7212 steps) ------------------------------------\n",
      "Max loss: 0.08204174041748047\n",
      "Min loss: 0.035744763910770416\n",
      "Mean loss: 0.0536931666235129\n",
      "Std loss: 0.01706696843687297\n",
      "Total Loss: 0.3221589997410774\n",
      "------------------------------------ epoch 1204 (7218 steps) ------------------------------------\n",
      "Max loss: 0.06681350618600845\n",
      "Min loss: 0.03292760252952576\n",
      "Mean loss: 0.0494440079977115\n",
      "Std loss: 0.011870859889916839\n",
      "Total Loss: 0.296664047986269\n",
      "------------------------------------ epoch 1205 (7224 steps) ------------------------------------\n",
      "Max loss: 0.10775281488895416\n",
      "Min loss: 0.026001911610364914\n",
      "Mean loss: 0.04861986512939135\n",
      "Std loss: 0.027992739892612417\n",
      "Total Loss: 0.2917191907763481\n",
      "------------------------------------ epoch 1206 (7230 steps) ------------------------------------\n",
      "Max loss: 0.04771202430129051\n",
      "Min loss: 0.031299613416194916\n",
      "Mean loss: 0.03888653963804245\n",
      "Std loss: 0.005084213046541209\n",
      "Total Loss: 0.2333192378282547\n",
      "------------------------------------ epoch 1207 (7236 steps) ------------------------------------\n",
      "Max loss: 0.04837031289935112\n",
      "Min loss: 0.028179079294204712\n",
      "Mean loss: 0.03601145992676417\n",
      "Std loss: 0.00743823757308721\n",
      "Total Loss: 0.21606875956058502\n",
      "------------------------------------ epoch 1208 (7242 steps) ------------------------------------\n",
      "Max loss: 0.053852565586566925\n",
      "Min loss: 0.032881058752536774\n",
      "Mean loss: 0.04126291535794735\n",
      "Std loss: 0.008080701127002933\n",
      "Total Loss: 0.2475774921476841\n",
      "------------------------------------ epoch 1209 (7248 steps) ------------------------------------\n",
      "Max loss: 0.07422613352537155\n",
      "Min loss: 0.023765048012137413\n",
      "Mean loss: 0.05144378884385029\n",
      "Std loss: 0.01739305361130813\n",
      "Total Loss: 0.30866273306310177\n",
      "------------------------------------ epoch 1210 (7254 steps) ------------------------------------\n",
      "Max loss: 0.09202013909816742\n",
      "Min loss: 0.024978291243314743\n",
      "Mean loss: 0.04755520013471445\n",
      "Std loss: 0.022683722550885486\n",
      "Total Loss: 0.28533120080828667\n",
      "------------------------------------ epoch 1211 (7260 steps) ------------------------------------\n",
      "Max loss: 0.04271430894732475\n",
      "Min loss: 0.0225447379052639\n",
      "Mean loss: 0.03452541617055734\n",
      "Std loss: 0.0062858766828207254\n",
      "Total Loss: 0.20715249702334404\n",
      "------------------------------------ epoch 1212 (7266 steps) ------------------------------------\n",
      "Max loss: 0.049354732036590576\n",
      "Min loss: 0.020941084250807762\n",
      "Mean loss: 0.035037247774501644\n",
      "Std loss: 0.010142189061967457\n",
      "Total Loss: 0.21022348664700985\n",
      "------------------------------------ epoch 1213 (7272 steps) ------------------------------------\n",
      "Max loss: 0.06963995099067688\n",
      "Min loss: 0.019923117011785507\n",
      "Mean loss: 0.043711328878998756\n",
      "Std loss: 0.01729089966416052\n",
      "Total Loss: 0.26226797327399254\n",
      "------------------------------------ epoch 1214 (7278 steps) ------------------------------------\n",
      "Max loss: 0.07696744054555893\n",
      "Min loss: 0.021547242999076843\n",
      "Mean loss: 0.04368049961825212\n",
      "Std loss: 0.01747474659474099\n",
      "Total Loss: 0.2620829977095127\n",
      "------------------------------------ epoch 1215 (7284 steps) ------------------------------------\n",
      "Max loss: 0.060323845595121384\n",
      "Min loss: 0.029616309329867363\n",
      "Mean loss: 0.037908026638130345\n",
      "Std loss: 0.010304902488639286\n",
      "Total Loss: 0.22744815982878208\n",
      "------------------------------------ epoch 1216 (7290 steps) ------------------------------------\n",
      "Max loss: 0.05898783355951309\n",
      "Min loss: 0.03008100390434265\n",
      "Mean loss: 0.04459483238557974\n",
      "Std loss: 0.010379433080736973\n",
      "Total Loss: 0.26756899431347847\n",
      "------------------------------------ epoch 1217 (7296 steps) ------------------------------------\n",
      "Max loss: 0.06041192635893822\n",
      "Min loss: 0.025740955024957657\n",
      "Mean loss: 0.045911494021614395\n",
      "Std loss: 0.012919697683098459\n",
      "Total Loss: 0.27546896412968636\n",
      "------------------------------------ epoch 1218 (7302 steps) ------------------------------------\n",
      "Max loss: 0.05503338575363159\n",
      "Min loss: 0.021654929965734482\n",
      "Mean loss: 0.03620361536741257\n",
      "Std loss: 0.011355771016632616\n",
      "Total Loss: 0.2172216922044754\n",
      "------------------------------------ epoch 1219 (7308 steps) ------------------------------------\n",
      "Max loss: 0.0592634454369545\n",
      "Min loss: 0.022189032286405563\n",
      "Mean loss: 0.039381635996202626\n",
      "Std loss: 0.012741820491662418\n",
      "Total Loss: 0.23628981597721577\n",
      "------------------------------------ epoch 1220 (7314 steps) ------------------------------------\n",
      "Max loss: 0.06605013459920883\n",
      "Min loss: 0.026897570118308067\n",
      "Mean loss: 0.04642104512701432\n",
      "Std loss: 0.016789029658257636\n",
      "Total Loss: 0.2785262707620859\n",
      "------------------------------------ epoch 1221 (7320 steps) ------------------------------------\n",
      "Max loss: 0.04726405441761017\n",
      "Min loss: 0.017453964799642563\n",
      "Mean loss: 0.031152230376998585\n",
      "Std loss: 0.009361549978391564\n",
      "Total Loss: 0.1869133822619915\n",
      "------------------------------------ epoch 1222 (7326 steps) ------------------------------------\n",
      "Max loss: 0.06801566481590271\n",
      "Min loss: 0.024493521079421043\n",
      "Mean loss: 0.03779722346613804\n",
      "Std loss: 0.01432239262125647\n",
      "Total Loss: 0.22678334079682827\n",
      "------------------------------------ epoch 1223 (7332 steps) ------------------------------------\n",
      "Max loss: 0.05632547661662102\n",
      "Min loss: 0.02767760306596756\n",
      "Mean loss: 0.04176246499021848\n",
      "Std loss: 0.010132788983583309\n",
      "Total Loss: 0.2505747899413109\n",
      "------------------------------------ epoch 1224 (7338 steps) ------------------------------------\n",
      "Max loss: 0.0539630725979805\n",
      "Min loss: 0.02230660244822502\n",
      "Mean loss: 0.03192871902137995\n",
      "Std loss: 0.011081502083378688\n",
      "Total Loss: 0.19157231412827969\n",
      "------------------------------------ epoch 1225 (7344 steps) ------------------------------------\n",
      "Max loss: 0.08977480232715607\n",
      "Min loss: 0.03128120303153992\n",
      "Mean loss: 0.0491827999552091\n",
      "Std loss: 0.02103290689531849\n",
      "Total Loss: 0.2950967997312546\n",
      "------------------------------------ epoch 1226 (7350 steps) ------------------------------------\n",
      "Max loss: 0.06672778725624084\n",
      "Min loss: 0.029970254749059677\n",
      "Mean loss: 0.0450960248708725\n",
      "Std loss: 0.013323707901903957\n",
      "Total Loss: 0.270576149225235\n",
      "------------------------------------ epoch 1227 (7356 steps) ------------------------------------\n",
      "Max loss: 0.07185754179954529\n",
      "Min loss: 0.031012674793601036\n",
      "Mean loss: 0.0478877384836475\n",
      "Std loss: 0.01610243978100755\n",
      "Total Loss: 0.28732643090188503\n",
      "------------------------------------ epoch 1228 (7362 steps) ------------------------------------\n",
      "Max loss: 0.06445538252592087\n",
      "Min loss: 0.024549653753638268\n",
      "Mean loss: 0.045135581555465855\n",
      "Std loss: 0.014798423063661817\n",
      "Total Loss: 0.27081348933279514\n",
      "------------------------------------ epoch 1229 (7368 steps) ------------------------------------\n",
      "Max loss: 0.052251897752285004\n",
      "Min loss: 0.019893702119588852\n",
      "Mean loss: 0.03436197185268005\n",
      "Std loss: 0.010431360008743819\n",
      "Total Loss: 0.20617183111608028\n",
      "------------------------------------ epoch 1230 (7374 steps) ------------------------------------\n",
      "Max loss: 0.0647818073630333\n",
      "Min loss: 0.025435686111450195\n",
      "Mean loss: 0.04597500463326772\n",
      "Std loss: 0.013836715146083485\n",
      "Total Loss: 0.2758500277996063\n",
      "------------------------------------ epoch 1231 (7380 steps) ------------------------------------\n",
      "Max loss: 0.08181652426719666\n",
      "Min loss: 0.03220769762992859\n",
      "Mean loss: 0.05066885116199652\n",
      "Std loss: 0.016169937562778295\n",
      "Total Loss: 0.30401310697197914\n",
      "------------------------------------ epoch 1232 (7386 steps) ------------------------------------\n",
      "Max loss: 0.06191563606262207\n",
      "Min loss: 0.020219799131155014\n",
      "Mean loss: 0.043108793596426644\n",
      "Std loss: 0.014253384457854719\n",
      "Total Loss: 0.2586527615785599\n",
      "------------------------------------ epoch 1233 (7392 steps) ------------------------------------\n",
      "Max loss: 0.05047190934419632\n",
      "Min loss: 0.023598849773406982\n",
      "Mean loss: 0.037255020812153816\n",
      "Std loss: 0.008359425201420051\n",
      "Total Loss: 0.2235301248729229\n",
      "------------------------------------ epoch 1234 (7398 steps) ------------------------------------\n",
      "Max loss: 0.05195930227637291\n",
      "Min loss: 0.02214209735393524\n",
      "Mean loss: 0.03755920069913069\n",
      "Std loss: 0.010631047007480337\n",
      "Total Loss: 0.22535520419478416\n",
      "------------------------------------ epoch 1235 (7404 steps) ------------------------------------\n",
      "Max loss: 0.08085940033197403\n",
      "Min loss: 0.013349227607250214\n",
      "Mean loss: 0.049265818670392036\n",
      "Std loss: 0.020568965767040773\n",
      "Total Loss: 0.2955949120223522\n",
      "------------------------------------ epoch 1236 (7410 steps) ------------------------------------\n",
      "Max loss: 0.059990476816892624\n",
      "Min loss: 0.02277310937643051\n",
      "Mean loss: 0.04064954879383246\n",
      "Std loss: 0.012617902424027563\n",
      "Total Loss: 0.24389729276299477\n",
      "------------------------------------ epoch 1237 (7416 steps) ------------------------------------\n",
      "Max loss: 0.041775837540626526\n",
      "Min loss: 0.020906701683998108\n",
      "Mean loss: 0.0309108371535937\n",
      "Std loss: 0.007150956655401969\n",
      "Total Loss: 0.1854650229215622\n",
      "------------------------------------ epoch 1238 (7422 steps) ------------------------------------\n",
      "Max loss: 0.055215924978256226\n",
      "Min loss: 0.03485198691487312\n",
      "Mean loss: 0.044135864824056625\n",
      "Std loss: 0.006807579006018923\n",
      "Total Loss: 0.26481518894433975\n",
      "------------------------------------ epoch 1239 (7428 steps) ------------------------------------\n",
      "Max loss: 0.06768369674682617\n",
      "Min loss: 0.025639085099101067\n",
      "Mean loss: 0.042635925424595676\n",
      "Std loss: 0.015912859129398393\n",
      "Total Loss: 0.25581555254757404\n",
      "------------------------------------ epoch 1240 (7434 steps) ------------------------------------\n",
      "Max loss: 0.0927412360906601\n",
      "Min loss: 0.020128462463617325\n",
      "Mean loss: 0.04242830413083235\n",
      "Std loss: 0.023453194116675652\n",
      "Total Loss: 0.2545698247849941\n",
      "------------------------------------ epoch 1241 (7440 steps) ------------------------------------\n",
      "Max loss: 0.05968433618545532\n",
      "Min loss: 0.020013632252812386\n",
      "Mean loss: 0.03436059784144163\n",
      "Std loss: 0.013379665497919385\n",
      "Total Loss: 0.2061635870486498\n",
      "------------------------------------ epoch 1242 (7446 steps) ------------------------------------\n",
      "Max loss: 0.04821526259183884\n",
      "Min loss: 0.022174015641212463\n",
      "Mean loss: 0.034255585012336574\n",
      "Std loss: 0.0080335133133782\n",
      "Total Loss: 0.20553351007401943\n",
      "------------------------------------ epoch 1243 (7452 steps) ------------------------------------\n",
      "Max loss: 0.07624846696853638\n",
      "Min loss: 0.027925435453653336\n",
      "Mean loss: 0.04573927819728851\n",
      "Std loss: 0.018544831795682375\n",
      "Total Loss: 0.2744356691837311\n",
      "------------------------------------ epoch 1244 (7458 steps) ------------------------------------\n",
      "Max loss: 0.05274857208132744\n",
      "Min loss: 0.025830639526247978\n",
      "Mean loss: 0.03823443595319986\n",
      "Std loss: 0.008278521343221755\n",
      "Total Loss: 0.22940661571919918\n",
      "------------------------------------ epoch 1245 (7464 steps) ------------------------------------\n",
      "Max loss: 0.055691421031951904\n",
      "Min loss: 0.026111451908946037\n",
      "Mean loss: 0.035983643804987274\n",
      "Std loss: 0.010230236360187156\n",
      "Total Loss: 0.21590186282992363\n",
      "------------------------------------ epoch 1246 (7470 steps) ------------------------------------\n",
      "Max loss: 0.04922400042414665\n",
      "Min loss: 0.02739090844988823\n",
      "Mean loss: 0.041298496847351394\n",
      "Std loss: 0.007339138731904358\n",
      "Total Loss: 0.24779098108410835\n",
      "------------------------------------ epoch 1247 (7476 steps) ------------------------------------\n",
      "Max loss: 0.053924184292554855\n",
      "Min loss: 0.025972526520490646\n",
      "Mean loss: 0.0375657125065724\n",
      "Std loss: 0.009063377153315472\n",
      "Total Loss: 0.22539427503943443\n",
      "------------------------------------ epoch 1248 (7482 steps) ------------------------------------\n",
      "Max loss: 0.04792238399386406\n",
      "Min loss: 0.017425550147891045\n",
      "Mean loss: 0.031185056703786056\n",
      "Std loss: 0.010417418822939239\n",
      "Total Loss: 0.18711034022271633\n",
      "------------------------------------ epoch 1249 (7488 steps) ------------------------------------\n",
      "Max loss: 0.08075430989265442\n",
      "Min loss: 0.02725098840892315\n",
      "Mean loss: 0.053295047643284\n",
      "Std loss: 0.017213023866492635\n",
      "Total Loss: 0.319770285859704\n",
      "------------------------------------ epoch 1250 (7494 steps) ------------------------------------\n",
      "Max loss: 0.04814677685499191\n",
      "Min loss: 0.025137662887573242\n",
      "Mean loss: 0.03878802806138992\n",
      "Std loss: 0.00806291971091848\n",
      "Total Loss: 0.23272816836833954\n",
      "------------------------------------ epoch 1251 (7500 steps) ------------------------------------\n",
      "Max loss: 0.0883629322052002\n",
      "Min loss: 0.027375590056180954\n",
      "Mean loss: 0.04937058314681053\n",
      "Std loss: 0.019540856772151064\n",
      "Total Loss: 0.2962234988808632\n",
      "------------------------------------ epoch 1252 (7506 steps) ------------------------------------\n",
      "Max loss: 0.05162328481674194\n",
      "Min loss: 0.02430492267012596\n",
      "Mean loss: 0.03837084180365006\n",
      "Std loss: 0.010617003993751194\n",
      "Total Loss: 0.23022505082190037\n",
      "------------------------------------ epoch 1253 (7512 steps) ------------------------------------\n",
      "Max loss: 0.07301062345504761\n",
      "Min loss: 0.020439907908439636\n",
      "Mean loss: 0.04049149248749018\n",
      "Std loss: 0.019075027832028812\n",
      "Total Loss: 0.24294895492494106\n",
      "------------------------------------ epoch 1254 (7518 steps) ------------------------------------\n",
      "Max loss: 0.08221294730901718\n",
      "Min loss: 0.024637527763843536\n",
      "Mean loss: 0.0402114267150561\n",
      "Std loss: 0.019769266383538805\n",
      "Total Loss: 0.2412685602903366\n",
      "------------------------------------ epoch 1255 (7524 steps) ------------------------------------\n",
      "Max loss: 0.08242490887641907\n",
      "Min loss: 0.024606075137853622\n",
      "Mean loss: 0.044182091330488525\n",
      "Std loss: 0.019234945134213328\n",
      "Total Loss: 0.26509254798293114\n",
      "------------------------------------ epoch 1256 (7530 steps) ------------------------------------\n",
      "Max loss: 0.04123654216527939\n",
      "Min loss: 0.01728210225701332\n",
      "Mean loss: 0.033069717697799206\n",
      "Std loss: 0.008801375512867778\n",
      "Total Loss: 0.19841830618679523\n",
      "------------------------------------ epoch 1257 (7536 steps) ------------------------------------\n",
      "Max loss: 0.08052431792020798\n",
      "Min loss: 0.027132004499435425\n",
      "Mean loss: 0.04317281519373258\n",
      "Std loss: 0.017637247834110973\n",
      "Total Loss: 0.2590368911623955\n",
      "------------------------------------ epoch 1258 (7542 steps) ------------------------------------\n",
      "Max loss: 0.08979647606611252\n",
      "Min loss: 0.022328585386276245\n",
      "Mean loss: 0.044520425299803414\n",
      "Std loss: 0.02235937939055986\n",
      "Total Loss: 0.2671225517988205\n",
      "------------------------------------ epoch 1259 (7548 steps) ------------------------------------\n",
      "Max loss: 0.08805538713932037\n",
      "Min loss: 0.022547302767634392\n",
      "Mean loss: 0.05082819020996491\n",
      "Std loss: 0.021764939118962507\n",
      "Total Loss: 0.30496914125978947\n",
      "------------------------------------ epoch 1260 (7554 steps) ------------------------------------\n",
      "Max loss: 0.06412379443645477\n",
      "Min loss: 0.02651350200176239\n",
      "Mean loss: 0.04244003879527251\n",
      "Std loss: 0.013224397640671055\n",
      "Total Loss: 0.25464023277163506\n",
      "------------------------------------ epoch 1261 (7560 steps) ------------------------------------\n",
      "Max loss: 0.07473208010196686\n",
      "Min loss: 0.02876264415681362\n",
      "Mean loss: 0.04653264923642079\n",
      "Std loss: 0.01628221686581909\n",
      "Total Loss: 0.27919589541852474\n",
      "------------------------------------ epoch 1262 (7566 steps) ------------------------------------\n",
      "Max loss: 0.070752814412117\n",
      "Min loss: 0.038959622383117676\n",
      "Mean loss: 0.05426729346315066\n",
      "Std loss: 0.011170915378230555\n",
      "Total Loss: 0.32560376077890396\n",
      "------------------------------------ epoch 1263 (7572 steps) ------------------------------------\n",
      "Max loss: 0.05272328481078148\n",
      "Min loss: 0.02493242733180523\n",
      "Mean loss: 0.04053528824200233\n",
      "Std loss: 0.009912541095794905\n",
      "Total Loss: 0.24321172945201397\n",
      "------------------------------------ epoch 1264 (7578 steps) ------------------------------------\n",
      "Max loss: 0.06304935365915298\n",
      "Min loss: 0.025940101593732834\n",
      "Mean loss: 0.040470799741645656\n",
      "Std loss: 0.01352723111923574\n",
      "Total Loss: 0.24282479844987392\n",
      "------------------------------------ epoch 1265 (7584 steps) ------------------------------------\n",
      "Max loss: 0.047388773411512375\n",
      "Min loss: 0.023082410916686058\n",
      "Mean loss: 0.03649729397147894\n",
      "Std loss: 0.007450630018142191\n",
      "Total Loss: 0.21898376382887363\n",
      "------------------------------------ epoch 1266 (7590 steps) ------------------------------------\n",
      "Max loss: 0.054572053253650665\n",
      "Min loss: 0.02379097044467926\n",
      "Mean loss: 0.03840133703003327\n",
      "Std loss: 0.011027405537053586\n",
      "Total Loss: 0.23040802218019962\n",
      "------------------------------------ epoch 1267 (7596 steps) ------------------------------------\n",
      "Max loss: 0.06499959528446198\n",
      "Min loss: 0.03607200086116791\n",
      "Mean loss: 0.048160514483849205\n",
      "Std loss: 0.009647632136634023\n",
      "Total Loss: 0.28896308690309525\n",
      "------------------------------------ epoch 1268 (7602 steps) ------------------------------------\n",
      "Max loss: 0.046679481863975525\n",
      "Min loss: 0.023436661809682846\n",
      "Mean loss: 0.031920245538155236\n",
      "Std loss: 0.007657117721421349\n",
      "Total Loss: 0.19152147322893143\n",
      "------------------------------------ epoch 1269 (7608 steps) ------------------------------------\n",
      "Max loss: 0.07932952046394348\n",
      "Min loss: 0.01996779814362526\n",
      "Mean loss: 0.046903761103749275\n",
      "Std loss: 0.02129755502453018\n",
      "Total Loss: 0.28142256662249565\n",
      "------------------------------------ epoch 1270 (7614 steps) ------------------------------------\n",
      "Max loss: 0.0495905801653862\n",
      "Min loss: 0.026938578113913536\n",
      "Mean loss: 0.038079384403924145\n",
      "Std loss: 0.009040665617736872\n",
      "Total Loss: 0.22847630642354488\n",
      "------------------------------------ epoch 1271 (7620 steps) ------------------------------------\n",
      "Max loss: 0.04671230539679527\n",
      "Min loss: 0.02224831096827984\n",
      "Mean loss: 0.03392333133767048\n",
      "Std loss: 0.008921580370054605\n",
      "Total Loss: 0.2035399880260229\n",
      "------------------------------------ epoch 1272 (7626 steps) ------------------------------------\n",
      "Max loss: 0.08996459096670151\n",
      "Min loss: 0.025177177041769028\n",
      "Mean loss: 0.049689832143485546\n",
      "Std loss: 0.023302054440532193\n",
      "Total Loss: 0.2981389928609133\n",
      "------------------------------------ epoch 1273 (7632 steps) ------------------------------------\n",
      "Max loss: 0.05181069299578667\n",
      "Min loss: 0.022436056286096573\n",
      "Mean loss: 0.037846721398333706\n",
      "Std loss: 0.010760359031900953\n",
      "Total Loss: 0.22708032839000225\n",
      "------------------------------------ epoch 1274 (7638 steps) ------------------------------------\n",
      "Max loss: 0.06423777341842651\n",
      "Min loss: 0.023697419092059135\n",
      "Mean loss: 0.04106589499861002\n",
      "Std loss: 0.013718237888310923\n",
      "Total Loss: 0.24639536999166012\n",
      "------------------------------------ epoch 1275 (7644 steps) ------------------------------------\n",
      "Max loss: 0.06634965538978577\n",
      "Min loss: 0.03377487510442734\n",
      "Mean loss: 0.04795291895667712\n",
      "Std loss: 0.01169725620104723\n",
      "Total Loss: 0.2877175137400627\n",
      "------------------------------------ epoch 1276 (7650 steps) ------------------------------------\n",
      "Max loss: 0.04955562204122543\n",
      "Min loss: 0.02347041666507721\n",
      "Mean loss: 0.03954735522468885\n",
      "Std loss: 0.008638188053064998\n",
      "Total Loss: 0.2372841313481331\n",
      "------------------------------------ epoch 1277 (7656 steps) ------------------------------------\n",
      "Max loss: 0.05651189014315605\n",
      "Min loss: 0.018714388832449913\n",
      "Mean loss: 0.04063739317158858\n",
      "Std loss: 0.015228954311611809\n",
      "Total Loss: 0.24382435902953148\n",
      "------------------------------------ epoch 1278 (7662 steps) ------------------------------------\n",
      "Max loss: 0.08331095427274704\n",
      "Min loss: 0.013825757429003716\n",
      "Mean loss: 0.04441720154136419\n",
      "Std loss: 0.026690422265148787\n",
      "Total Loss: 0.26650320924818516\n",
      "------------------------------------ epoch 1279 (7668 steps) ------------------------------------\n",
      "Max loss: 0.04096699133515358\n",
      "Min loss: 0.022307273000478745\n",
      "Mean loss: 0.029448076461752255\n",
      "Std loss: 0.005651587495894647\n",
      "Total Loss: 0.17668845877051353\n",
      "------------------------------------ epoch 1280 (7674 steps) ------------------------------------\n",
      "Max loss: 0.05307994410395622\n",
      "Min loss: 0.0235171876847744\n",
      "Mean loss: 0.03579598106443882\n",
      "Std loss: 0.01031140701756427\n",
      "Total Loss: 0.21477588638663292\n",
      "------------------------------------ epoch 1281 (7680 steps) ------------------------------------\n",
      "Max loss: 0.08756232261657715\n",
      "Min loss: 0.018539052456617355\n",
      "Mean loss: 0.04066533098618189\n",
      "Std loss: 0.022279222576132664\n",
      "Total Loss: 0.24399198591709137\n",
      "------------------------------------ epoch 1282 (7686 steps) ------------------------------------\n",
      "Max loss: 0.05059310421347618\n",
      "Min loss: 0.021248972043395042\n",
      "Mean loss: 0.03247104833523432\n",
      "Std loss: 0.010620549725788209\n",
      "Total Loss: 0.19482629001140594\n",
      "------------------------------------ epoch 1283 (7692 steps) ------------------------------------\n",
      "Max loss: 0.042573925107717514\n",
      "Min loss: 0.025767609477043152\n",
      "Mean loss: 0.03512956264118353\n",
      "Std loss: 0.006305967996359287\n",
      "Total Loss: 0.2107773758471012\n",
      "------------------------------------ epoch 1284 (7698 steps) ------------------------------------\n",
      "Max loss: 0.06470783054828644\n",
      "Min loss: 0.021084671840071678\n",
      "Mean loss: 0.037677046532432236\n",
      "Std loss: 0.014614599538228145\n",
      "Total Loss: 0.22606227919459343\n",
      "------------------------------------ epoch 1285 (7704 steps) ------------------------------------\n",
      "Max loss: 0.07230756431818008\n",
      "Min loss: 0.02056664042174816\n",
      "Mean loss: 0.041663263303538166\n",
      "Std loss: 0.018944976581308465\n",
      "Total Loss: 0.24997957982122898\n",
      "------------------------------------ epoch 1286 (7710 steps) ------------------------------------\n",
      "Max loss: 0.057303156703710556\n",
      "Min loss: 0.021446876227855682\n",
      "Mean loss: 0.0373353585600853\n",
      "Std loss: 0.010814344487873127\n",
      "Total Loss: 0.22401215136051178\n",
      "------------------------------------ epoch 1287 (7716 steps) ------------------------------------\n",
      "Max loss: 0.07956013083457947\n",
      "Min loss: 0.020531900227069855\n",
      "Mean loss: 0.04233869475622972\n",
      "Std loss: 0.018256753510826867\n",
      "Total Loss: 0.2540321685373783\n",
      "------------------------------------ epoch 1288 (7722 steps) ------------------------------------\n",
      "Max loss: 0.05697604641318321\n",
      "Min loss: 0.018306735903024673\n",
      "Mean loss: 0.03666221164166927\n",
      "Std loss: 0.012083335132918302\n",
      "Total Loss: 0.21997326985001564\n",
      "------------------------------------ epoch 1289 (7728 steps) ------------------------------------\n",
      "Max loss: 0.08851542323827744\n",
      "Min loss: 0.02738204412162304\n",
      "Mean loss: 0.040358537497619786\n",
      "Std loss: 0.02164271401059178\n",
      "Total Loss: 0.24215122498571873\n",
      "------------------------------------ epoch 1290 (7734 steps) ------------------------------------\n",
      "Max loss: 0.05543337017297745\n",
      "Min loss: 0.0223676897585392\n",
      "Mean loss: 0.040160175412893295\n",
      "Std loss: 0.012466282339714663\n",
      "Total Loss: 0.24096105247735977\n",
      "------------------------------------ epoch 1291 (7740 steps) ------------------------------------\n",
      "Max loss: 0.04292629659175873\n",
      "Min loss: 0.029073571786284447\n",
      "Mean loss: 0.03688853203008572\n",
      "Std loss: 0.005382813358496648\n",
      "Total Loss: 0.22133119218051434\n",
      "------------------------------------ epoch 1292 (7746 steps) ------------------------------------\n",
      "Max loss: 0.0769188180565834\n",
      "Min loss: 0.027317941188812256\n",
      "Mean loss: 0.04437322666247686\n",
      "Std loss: 0.01720406564817785\n",
      "Total Loss: 0.26623935997486115\n",
      "------------------------------------ epoch 1293 (7752 steps) ------------------------------------\n",
      "Max loss: 0.07721482962369919\n",
      "Min loss: 0.028699412941932678\n",
      "Mean loss: 0.0471207561592261\n",
      "Std loss: 0.016792814197767843\n",
      "Total Loss: 0.2827245369553566\n",
      "------------------------------------ epoch 1294 (7758 steps) ------------------------------------\n",
      "Max loss: 0.08055491745471954\n",
      "Min loss: 0.0181584395468235\n",
      "Mean loss: 0.03681484175225099\n",
      "Std loss: 0.021770871273241173\n",
      "Total Loss: 0.22088905051350594\n",
      "------------------------------------ epoch 1295 (7764 steps) ------------------------------------\n",
      "Max loss: 0.05358915776014328\n",
      "Min loss: 0.02271309122443199\n",
      "Mean loss: 0.03448413902272781\n",
      "Std loss: 0.01066221194055924\n",
      "Total Loss: 0.20690483413636684\n",
      "------------------------------------ epoch 1296 (7770 steps) ------------------------------------\n",
      "Max loss: 0.03681080415844917\n",
      "Min loss: 0.016365066170692444\n",
      "Mean loss: 0.025621605416138966\n",
      "Std loss: 0.007528344750573048\n",
      "Total Loss: 0.1537296324968338\n",
      "------------------------------------ epoch 1297 (7776 steps) ------------------------------------\n",
      "Max loss: 0.05313235521316528\n",
      "Min loss: 0.014258920215070248\n",
      "Mean loss: 0.03232091246172786\n",
      "Std loss: 0.012616990732850157\n",
      "Total Loss: 0.19392547477036715\n",
      "------------------------------------ epoch 1298 (7782 steps) ------------------------------------\n",
      "Max loss: 0.061307065188884735\n",
      "Min loss: 0.02093685232102871\n",
      "Mean loss: 0.041151703956226506\n",
      "Std loss: 0.012551559165698155\n",
      "Total Loss: 0.24691022373735905\n",
      "------------------------------------ epoch 1299 (7788 steps) ------------------------------------\n",
      "Max loss: 0.05586341768503189\n",
      "Min loss: 0.017778761684894562\n",
      "Mean loss: 0.028951122735937435\n",
      "Std loss: 0.012798788914639939\n",
      "Total Loss: 0.17370673641562462\n",
      "------------------------------------ epoch 1300 (7794 steps) ------------------------------------\n",
      "Max loss: 0.054073452949523926\n",
      "Min loss: 0.021473359316587448\n",
      "Mean loss: 0.0399266400684913\n",
      "Std loss: 0.010458865224227737\n",
      "Total Loss: 0.2395598404109478\n",
      "------------------------------------ epoch 1301 (7800 steps) ------------------------------------\n",
      "Max loss: 0.047777414321899414\n",
      "Min loss: 0.02244589850306511\n",
      "Mean loss: 0.03339097400506338\n",
      "Std loss: 0.009018856045983329\n",
      "Total Loss: 0.20034584403038025\n",
      "saved model at ./weights/model_1301.pth\n",
      "------------------------------------ epoch 1302 (7806 steps) ------------------------------------\n",
      "Max loss: 0.05224965140223503\n",
      "Min loss: 0.020637374371290207\n",
      "Mean loss: 0.03175147157162428\n",
      "Std loss: 0.010284993711492918\n",
      "Total Loss: 0.19050882942974567\n",
      "------------------------------------ epoch 1303 (7812 steps) ------------------------------------\n",
      "Max loss: 0.06088073179125786\n",
      "Min loss: 0.02189039997756481\n",
      "Mean loss: 0.04237995824466149\n",
      "Std loss: 0.013287540606192945\n",
      "Total Loss: 0.25427974946796894\n",
      "------------------------------------ epoch 1304 (7818 steps) ------------------------------------\n",
      "Max loss: 0.06863196194171906\n",
      "Min loss: 0.021722868084907532\n",
      "Mean loss: 0.03563245479017496\n",
      "Std loss: 0.015559330476984216\n",
      "Total Loss: 0.21379472874104977\n",
      "------------------------------------ epoch 1305 (7824 steps) ------------------------------------\n",
      "Max loss: 0.041995786130428314\n",
      "Min loss: 0.016838857904076576\n",
      "Mean loss: 0.026003630210955937\n",
      "Std loss: 0.008287115051901571\n",
      "Total Loss: 0.15602178126573563\n",
      "------------------------------------ epoch 1306 (7830 steps) ------------------------------------\n",
      "Max loss: 0.08954080194234848\n",
      "Min loss: 0.016929423436522484\n",
      "Mean loss: 0.04566079657524824\n",
      "Std loss: 0.023456150020163654\n",
      "Total Loss: 0.27396477945148945\n",
      "------------------------------------ epoch 1307 (7836 steps) ------------------------------------\n",
      "Max loss: 0.03861594200134277\n",
      "Min loss: 0.01580209843814373\n",
      "Mean loss: 0.03026828169822693\n",
      "Std loss: 0.008139487071364662\n",
      "Total Loss: 0.18160969018936157\n",
      "------------------------------------ epoch 1308 (7842 steps) ------------------------------------\n",
      "Max loss: 0.05044589564204216\n",
      "Min loss: 0.02170022949576378\n",
      "Mean loss: 0.03469697075585524\n",
      "Std loss: 0.011138867572721999\n",
      "Total Loss: 0.20818182453513145\n",
      "------------------------------------ epoch 1309 (7848 steps) ------------------------------------\n",
      "Max loss: 0.05194845423102379\n",
      "Min loss: 0.024505427107214928\n",
      "Mean loss: 0.03825190011411905\n",
      "Std loss: 0.009065630670658478\n",
      "Total Loss: 0.22951140068471432\n",
      "------------------------------------ epoch 1310 (7854 steps) ------------------------------------\n",
      "Max loss: 0.05565214157104492\n",
      "Min loss: 0.01906634494662285\n",
      "Mean loss: 0.03866531575719515\n",
      "Std loss: 0.012360252883817965\n",
      "Total Loss: 0.23199189454317093\n",
      "------------------------------------ epoch 1311 (7860 steps) ------------------------------------\n",
      "Max loss: 0.06922547519207001\n",
      "Min loss: 0.01874915324151516\n",
      "Mean loss: 0.038256220829983555\n",
      "Std loss: 0.02121166073818772\n",
      "Total Loss: 0.2295373249799013\n",
      "------------------------------------ epoch 1312 (7866 steps) ------------------------------------\n",
      "Max loss: 0.06815910339355469\n",
      "Min loss: 0.02157706767320633\n",
      "Mean loss: 0.03483137860894203\n",
      "Std loss: 0.016320528440289445\n",
      "Total Loss: 0.2089882716536522\n",
      "------------------------------------ epoch 1313 (7872 steps) ------------------------------------\n",
      "Max loss: 0.07852522283792496\n",
      "Min loss: 0.019979525357484818\n",
      "Mean loss: 0.03928527639557918\n",
      "Std loss: 0.020602959328734433\n",
      "Total Loss: 0.23571165837347507\n",
      "------------------------------------ epoch 1314 (7878 steps) ------------------------------------\n",
      "Max loss: 0.06111550331115723\n",
      "Min loss: 0.02188156358897686\n",
      "Mean loss: 0.035171947441995144\n",
      "Std loss: 0.01354440366013324\n",
      "Total Loss: 0.21103168465197086\n",
      "------------------------------------ epoch 1315 (7884 steps) ------------------------------------\n",
      "Max loss: 0.05601419135928154\n",
      "Min loss: 0.022793879732489586\n",
      "Mean loss: 0.038052888587117195\n",
      "Std loss: 0.011432889481373476\n",
      "Total Loss: 0.22831733152270317\n",
      "------------------------------------ epoch 1316 (7890 steps) ------------------------------------\n",
      "Max loss: 0.034013211727142334\n",
      "Min loss: 0.021226298063993454\n",
      "Mean loss: 0.02882236707955599\n",
      "Std loss: 0.004645761786926668\n",
      "Total Loss: 0.17293420247733593\n",
      "------------------------------------ epoch 1317 (7896 steps) ------------------------------------\n",
      "Max loss: 0.05705488845705986\n",
      "Min loss: 0.02294207736849785\n",
      "Mean loss: 0.040435263266166054\n",
      "Std loss: 0.013826000904284179\n",
      "Total Loss: 0.2426115795969963\n",
      "------------------------------------ epoch 1318 (7902 steps) ------------------------------------\n",
      "Max loss: 0.10570807754993439\n",
      "Min loss: 0.03454604744911194\n",
      "Mean loss: 0.05695481846729914\n",
      "Std loss: 0.02287559117403163\n",
      "Total Loss: 0.34172891080379486\n",
      "------------------------------------ epoch 1319 (7908 steps) ------------------------------------\n",
      "Max loss: 0.05638420581817627\n",
      "Min loss: 0.026098962873220444\n",
      "Mean loss: 0.03716077686597904\n",
      "Std loss: 0.010329836255506071\n",
      "Total Loss: 0.22296466119587421\n",
      "------------------------------------ epoch 1320 (7914 steps) ------------------------------------\n",
      "Max loss: 0.05421146750450134\n",
      "Min loss: 0.0242201816290617\n",
      "Mean loss: 0.035173757933080196\n",
      "Std loss: 0.009236173797918239\n",
      "Total Loss: 0.21104254759848118\n",
      "------------------------------------ epoch 1321 (7920 steps) ------------------------------------\n",
      "Max loss: 0.05819246172904968\n",
      "Min loss: 0.021038329228758812\n",
      "Mean loss: 0.034507704277833305\n",
      "Std loss: 0.012172774664302685\n",
      "Total Loss: 0.20704622566699982\n",
      "------------------------------------ epoch 1322 (7926 steps) ------------------------------------\n",
      "Max loss: 0.04579295217990875\n",
      "Min loss: 0.020398886874318123\n",
      "Mean loss: 0.03260111281027397\n",
      "Std loss: 0.010178867129553742\n",
      "Total Loss: 0.1956066768616438\n",
      "------------------------------------ epoch 1323 (7932 steps) ------------------------------------\n",
      "Max loss: 0.05003245174884796\n",
      "Min loss: 0.016277259215712547\n",
      "Mean loss: 0.029865235090255737\n",
      "Std loss: 0.010698629807594359\n",
      "Total Loss: 0.17919141054153442\n",
      "------------------------------------ epoch 1324 (7938 steps) ------------------------------------\n",
      "Max loss: 0.054485172033309937\n",
      "Min loss: 0.0303039588034153\n",
      "Mean loss: 0.03754397047062715\n",
      "Std loss: 0.008339691040057448\n",
      "Total Loss: 0.2252638228237629\n",
      "------------------------------------ epoch 1325 (7944 steps) ------------------------------------\n",
      "Max loss: 0.07763034850358963\n",
      "Min loss: 0.022107914090156555\n",
      "Mean loss: 0.03776322087893883\n",
      "Std loss: 0.018482864054196323\n",
      "Total Loss: 0.226579325273633\n",
      "------------------------------------ epoch 1326 (7950 steps) ------------------------------------\n",
      "Max loss: 0.05374276638031006\n",
      "Min loss: 0.026524726301431656\n",
      "Mean loss: 0.03680393844842911\n",
      "Std loss: 0.009270987663349025\n",
      "Total Loss: 0.22082363069057465\n",
      "------------------------------------ epoch 1327 (7956 steps) ------------------------------------\n",
      "Max loss: 0.05374623462557793\n",
      "Min loss: 0.015326018445193768\n",
      "Mean loss: 0.03179144222910205\n",
      "Std loss: 0.013872820010982275\n",
      "Total Loss: 0.19074865337461233\n",
      "------------------------------------ epoch 1328 (7962 steps) ------------------------------------\n",
      "Max loss: 0.068900927901268\n",
      "Min loss: 0.018739579245448112\n",
      "Mean loss: 0.04207397997379303\n",
      "Std loss: 0.01676675315613032\n",
      "Total Loss: 0.2524438798427582\n",
      "------------------------------------ epoch 1329 (7968 steps) ------------------------------------\n",
      "Max loss: 0.05232323333621025\n",
      "Min loss: 0.022948049008846283\n",
      "Mean loss: 0.035298967733979225\n",
      "Std loss: 0.011503383642867498\n",
      "Total Loss: 0.21179380640387535\n",
      "------------------------------------ epoch 1330 (7974 steps) ------------------------------------\n",
      "Max loss: 0.07335694134235382\n",
      "Min loss: 0.03434566780924797\n",
      "Mean loss: 0.046341147273778915\n",
      "Std loss: 0.012516935025942656\n",
      "Total Loss: 0.2780468836426735\n",
      "------------------------------------ epoch 1331 (7980 steps) ------------------------------------\n",
      "Max loss: 0.06239832937717438\n",
      "Min loss: 0.016874060034751892\n",
      "Mean loss: 0.03337028964112202\n",
      "Std loss: 0.015145592829124688\n",
      "Total Loss: 0.20022173784673214\n",
      "------------------------------------ epoch 1332 (7986 steps) ------------------------------------\n",
      "Max loss: 0.058364495635032654\n",
      "Min loss: 0.025300346314907074\n",
      "Mean loss: 0.03839673815915982\n",
      "Std loss: 0.014051996728280483\n",
      "Total Loss: 0.23038042895495892\n",
      "------------------------------------ epoch 1333 (7992 steps) ------------------------------------\n",
      "Max loss: 0.05662265419960022\n",
      "Min loss: 0.019131876528263092\n",
      "Mean loss: 0.03236069157719612\n",
      "Std loss: 0.014049688444595584\n",
      "Total Loss: 0.19416414946317673\n",
      "------------------------------------ epoch 1334 (7998 steps) ------------------------------------\n",
      "Max loss: 0.07203177362680435\n",
      "Min loss: 0.01908084563910961\n",
      "Mean loss: 0.03189564310014248\n",
      "Std loss: 0.018631947721549465\n",
      "Total Loss: 0.19137385860085487\n",
      "------------------------------------ epoch 1335 (8004 steps) ------------------------------------\n",
      "Max loss: 0.048635564744472504\n",
      "Min loss: 0.02347935363650322\n",
      "Mean loss: 0.03323465803017219\n",
      "Std loss: 0.008778263079834078\n",
      "Total Loss: 0.19940794818103313\n",
      "------------------------------------ epoch 1336 (8010 steps) ------------------------------------\n",
      "Max loss: 0.05631084367632866\n",
      "Min loss: 0.02645290642976761\n",
      "Mean loss: 0.04052481738229593\n",
      "Std loss: 0.0113586852432514\n",
      "Total Loss: 0.24314890429377556\n",
      "------------------------------------ epoch 1337 (8016 steps) ------------------------------------\n",
      "Max loss: 0.04936692863702774\n",
      "Min loss: 0.02786226198077202\n",
      "Mean loss: 0.03533824719488621\n",
      "Std loss: 0.00785695108133351\n",
      "Total Loss: 0.21202948316931725\n",
      "------------------------------------ epoch 1338 (8022 steps) ------------------------------------\n",
      "Max loss: 0.0680214986205101\n",
      "Min loss: 0.03371194005012512\n",
      "Mean loss: 0.04938630759716034\n",
      "Std loss: 0.011600563550830967\n",
      "Total Loss: 0.29631784558296204\n",
      "------------------------------------ epoch 1339 (8028 steps) ------------------------------------\n",
      "Max loss: 0.06091180443763733\n",
      "Min loss: 0.026422960683703423\n",
      "Mean loss: 0.04351283765087525\n",
      "Std loss: 0.011822319761306129\n",
      "Total Loss: 0.2610770259052515\n",
      "------------------------------------ epoch 1340 (8034 steps) ------------------------------------\n",
      "Max loss: 0.054215144366025925\n",
      "Min loss: 0.01889471523463726\n",
      "Mean loss: 0.0360317121570309\n",
      "Std loss: 0.012866598794657064\n",
      "Total Loss: 0.2161902729421854\n",
      "------------------------------------ epoch 1341 (8040 steps) ------------------------------------\n",
      "Max loss: 0.0900583565235138\n",
      "Min loss: 0.019844533875584602\n",
      "Mean loss: 0.049876210279762745\n",
      "Std loss: 0.02126121486838865\n",
      "Total Loss: 0.29925726167857647\n",
      "------------------------------------ epoch 1342 (8046 steps) ------------------------------------\n",
      "Max loss: 0.053496815264225006\n",
      "Min loss: 0.026459798216819763\n",
      "Mean loss: 0.04029702271024386\n",
      "Std loss: 0.008569750124741142\n",
      "Total Loss: 0.24178213626146317\n",
      "------------------------------------ epoch 1343 (8052 steps) ------------------------------------\n",
      "Max loss: 0.05621828883886337\n",
      "Min loss: 0.02580784261226654\n",
      "Mean loss: 0.03872312853733698\n",
      "Std loss: 0.0117610600528719\n",
      "Total Loss: 0.2323387712240219\n",
      "------------------------------------ epoch 1344 (8058 steps) ------------------------------------\n",
      "Max loss: 0.037172332406044006\n",
      "Min loss: 0.017061475664377213\n",
      "Mean loss: 0.026689245365560055\n",
      "Std loss: 0.007105422697810508\n",
      "Total Loss: 0.16013547219336033\n",
      "------------------------------------ epoch 1345 (8064 steps) ------------------------------------\n",
      "Max loss: 0.0398402102291584\n",
      "Min loss: 0.022809281945228577\n",
      "Mean loss: 0.03128797902415196\n",
      "Std loss: 0.00555631494045258\n",
      "Total Loss: 0.18772787414491177\n",
      "------------------------------------ epoch 1346 (8070 steps) ------------------------------------\n",
      "Max loss: 0.04066437855362892\n",
      "Min loss: 0.016228362917900085\n",
      "Mean loss: 0.027163905402024586\n",
      "Std loss: 0.008138363028010865\n",
      "Total Loss: 0.16298343241214752\n",
      "------------------------------------ epoch 1347 (8076 steps) ------------------------------------\n",
      "Max loss: 0.04922271519899368\n",
      "Min loss: 0.019732890650629997\n",
      "Mean loss: 0.03505666429797808\n",
      "Std loss: 0.010020111422174556\n",
      "Total Loss: 0.2103399857878685\n",
      "------------------------------------ epoch 1348 (8082 steps) ------------------------------------\n",
      "Max loss: 0.03628219664096832\n",
      "Min loss: 0.017621049657464027\n",
      "Mean loss: 0.023450861995418865\n",
      "Std loss: 0.006038171863178916\n",
      "Total Loss: 0.1407051719725132\n",
      "------------------------------------ epoch 1349 (8088 steps) ------------------------------------\n",
      "Max loss: 0.06700506806373596\n",
      "Min loss: 0.02795678935945034\n",
      "Mean loss: 0.04079676512628794\n",
      "Std loss: 0.012879889652261424\n",
      "Total Loss: 0.24478059075772762\n",
      "------------------------------------ epoch 1350 (8094 steps) ------------------------------------\n",
      "Max loss: 0.03654561936855316\n",
      "Min loss: 0.022072404623031616\n",
      "Mean loss: 0.028108364902436733\n",
      "Std loss: 0.004583860860798224\n",
      "Total Loss: 0.1686501894146204\n",
      "------------------------------------ epoch 1351 (8100 steps) ------------------------------------\n",
      "Max loss: 0.05356394499540329\n",
      "Min loss: 0.018147721886634827\n",
      "Mean loss: 0.03561218952139219\n",
      "Std loss: 0.012696201004322196\n",
      "Total Loss: 0.21367313712835312\n",
      "------------------------------------ epoch 1352 (8106 steps) ------------------------------------\n",
      "Max loss: 0.06025012582540512\n",
      "Min loss: 0.030090292915701866\n",
      "Mean loss: 0.04277885798364878\n",
      "Std loss: 0.011307267694292676\n",
      "Total Loss: 0.25667314790189266\n",
      "------------------------------------ epoch 1353 (8112 steps) ------------------------------------\n",
      "Max loss: 0.050724562257528305\n",
      "Min loss: 0.017741113901138306\n",
      "Mean loss: 0.040439268574118614\n",
      "Std loss: 0.011468002210400475\n",
      "Total Loss: 0.24263561144471169\n",
      "------------------------------------ epoch 1354 (8118 steps) ------------------------------------\n",
      "Max loss: 0.05932804197072983\n",
      "Min loss: 0.023324575275182724\n",
      "Mean loss: 0.0332951337719957\n",
      "Std loss: 0.012663162282610954\n",
      "Total Loss: 0.19977080263197422\n",
      "------------------------------------ epoch 1355 (8124 steps) ------------------------------------\n",
      "Max loss: 0.028280243277549744\n",
      "Min loss: 0.015052691102027893\n",
      "Mean loss: 0.022084257565438747\n",
      "Std loss: 0.0038998460717954223\n",
      "Total Loss: 0.13250554539263248\n",
      "------------------------------------ epoch 1356 (8130 steps) ------------------------------------\n",
      "Max loss: 0.05773928388953209\n",
      "Min loss: 0.019774047657847404\n",
      "Mean loss: 0.03478523871550957\n",
      "Std loss: 0.01303646606650716\n",
      "Total Loss: 0.20871143229305744\n",
      "------------------------------------ epoch 1357 (8136 steps) ------------------------------------\n",
      "Max loss: 0.07478176057338715\n",
      "Min loss: 0.023044709116220474\n",
      "Mean loss: 0.04475627032419046\n",
      "Std loss: 0.015955829368686014\n",
      "Total Loss: 0.26853762194514275\n",
      "------------------------------------ epoch 1358 (8142 steps) ------------------------------------\n",
      "Max loss: 0.03673389181494713\n",
      "Min loss: 0.024922668933868408\n",
      "Mean loss: 0.030111657145122688\n",
      "Std loss: 0.004818168913141018\n",
      "Total Loss: 0.18066994287073612\n",
      "------------------------------------ epoch 1359 (8148 steps) ------------------------------------\n",
      "Max loss: 0.04542815685272217\n",
      "Min loss: 0.019481059163808823\n",
      "Mean loss: 0.029659756148854893\n",
      "Std loss: 0.00881061813712952\n",
      "Total Loss: 0.17795853689312935\n",
      "------------------------------------ epoch 1360 (8154 steps) ------------------------------------\n",
      "Max loss: 0.0524635910987854\n",
      "Min loss: 0.012814677320420742\n",
      "Mean loss: 0.03975712554529309\n",
      "Std loss: 0.013897258703977865\n",
      "Total Loss: 0.23854275327175856\n",
      "------------------------------------ epoch 1361 (8160 steps) ------------------------------------\n",
      "Max loss: 0.06291820108890533\n",
      "Min loss: 0.020451778545975685\n",
      "Mean loss: 0.04446152566621701\n",
      "Std loss: 0.013739653781133197\n",
      "Total Loss: 0.26676915399730206\n",
      "------------------------------------ epoch 1362 (8166 steps) ------------------------------------\n",
      "Max loss: 0.0679534524679184\n",
      "Min loss: 0.021102622151374817\n",
      "Mean loss: 0.03771023452281952\n",
      "Std loss: 0.015410099504993287\n",
      "Total Loss: 0.22626140713691711\n",
      "------------------------------------ epoch 1363 (8172 steps) ------------------------------------\n",
      "Max loss: 0.08023692667484283\n",
      "Min loss: 0.01729256846010685\n",
      "Mean loss: 0.044416566068927445\n",
      "Std loss: 0.021815367300154362\n",
      "Total Loss: 0.2664993964135647\n",
      "------------------------------------ epoch 1364 (8178 steps) ------------------------------------\n",
      "Max loss: 0.07901497930288315\n",
      "Min loss: 0.0211354847997427\n",
      "Mean loss: 0.04544550646096468\n",
      "Std loss: 0.019436431523992277\n",
      "Total Loss: 0.2726730387657881\n",
      "------------------------------------ epoch 1365 (8184 steps) ------------------------------------\n",
      "Max loss: 0.05741960555315018\n",
      "Min loss: 0.019402630627155304\n",
      "Mean loss: 0.03747387292484442\n",
      "Std loss: 0.014508454299693609\n",
      "Total Loss: 0.22484323754906654\n",
      "------------------------------------ epoch 1366 (8190 steps) ------------------------------------\n",
      "Max loss: 0.04881782457232475\n",
      "Min loss: 0.014428241178393364\n",
      "Mean loss: 0.030570698902010918\n",
      "Std loss: 0.011478973647251542\n",
      "Total Loss: 0.1834241934120655\n",
      "------------------------------------ epoch 1367 (8196 steps) ------------------------------------\n",
      "Max loss: 0.0588473379611969\n",
      "Min loss: 0.018876703456044197\n",
      "Mean loss: 0.0371742220595479\n",
      "Std loss: 0.015298875508326749\n",
      "Total Loss: 0.2230453323572874\n",
      "------------------------------------ epoch 1368 (8202 steps) ------------------------------------\n",
      "Max loss: 0.041009292006492615\n",
      "Min loss: 0.015193343162536621\n",
      "Mean loss: 0.02175253815948963\n",
      "Std loss: 0.00883779957098629\n",
      "Total Loss: 0.1305152289569378\n",
      "------------------------------------ epoch 1369 (8208 steps) ------------------------------------\n",
      "Max loss: 0.058810483664274216\n",
      "Min loss: 0.01955074816942215\n",
      "Mean loss: 0.04278591523567835\n",
      "Std loss: 0.013725408073568503\n",
      "Total Loss: 0.25671549141407013\n",
      "------------------------------------ epoch 1370 (8214 steps) ------------------------------------\n",
      "Max loss: 0.06168770045042038\n",
      "Min loss: 0.01974133402109146\n",
      "Mean loss: 0.03685111987094084\n",
      "Std loss: 0.014487922107371572\n",
      "Total Loss: 0.22110671922564507\n",
      "------------------------------------ epoch 1371 (8220 steps) ------------------------------------\n",
      "Max loss: 0.06467285007238388\n",
      "Min loss: 0.03255484253168106\n",
      "Mean loss: 0.05111836331586043\n",
      "Std loss: 0.010635569197772544\n",
      "Total Loss: 0.3067101798951626\n",
      "------------------------------------ epoch 1372 (8226 steps) ------------------------------------\n",
      "Max loss: 0.07641187310218811\n",
      "Min loss: 0.02439180575311184\n",
      "Mean loss: 0.03765750707437595\n",
      "Std loss: 0.01890015356594312\n",
      "Total Loss: 0.22594504244625568\n",
      "------------------------------------ epoch 1373 (8232 steps) ------------------------------------\n",
      "Max loss: 0.04448245093226433\n",
      "Min loss: 0.025322478264570236\n",
      "Mean loss: 0.032909770496189594\n",
      "Std loss: 0.007426453746782681\n",
      "Total Loss: 0.19745862297713757\n",
      "------------------------------------ epoch 1374 (8238 steps) ------------------------------------\n",
      "Max loss: 0.05386706441640854\n",
      "Min loss: 0.016436127945780754\n",
      "Mean loss: 0.03571324950704972\n",
      "Std loss: 0.013820924678464592\n",
      "Total Loss: 0.21427949704229832\n",
      "------------------------------------ epoch 1375 (8244 steps) ------------------------------------\n",
      "Max loss: 0.05429637432098389\n",
      "Min loss: 0.019220426678657532\n",
      "Mean loss: 0.03488220771153768\n",
      "Std loss: 0.011149729143877775\n",
      "Total Loss: 0.20929324626922607\n",
      "------------------------------------ epoch 1376 (8250 steps) ------------------------------------\n",
      "Max loss: 0.05158686637878418\n",
      "Min loss: 0.023812774568796158\n",
      "Mean loss: 0.0370507917056481\n",
      "Std loss: 0.008499967026337393\n",
      "Total Loss: 0.22230475023388863\n",
      "------------------------------------ epoch 1377 (8256 steps) ------------------------------------\n",
      "Max loss: 0.0601501390337944\n",
      "Min loss: 0.017030127346515656\n",
      "Mean loss: 0.03991615027189255\n",
      "Std loss: 0.015322816576528182\n",
      "Total Loss: 0.23949690163135529\n",
      "------------------------------------ epoch 1378 (8262 steps) ------------------------------------\n",
      "Max loss: 0.05599009990692139\n",
      "Min loss: 0.017802296206355095\n",
      "Mean loss: 0.044043573550879955\n",
      "Std loss: 0.012813866564739325\n",
      "Total Loss: 0.26426144130527973\n",
      "------------------------------------ epoch 1379 (8268 steps) ------------------------------------\n",
      "Max loss: 0.05351422354578972\n",
      "Min loss: 0.018245160579681396\n",
      "Mean loss: 0.032563433051109314\n",
      "Std loss: 0.012332817511816026\n",
      "Total Loss: 0.19538059830665588\n",
      "------------------------------------ epoch 1380 (8274 steps) ------------------------------------\n",
      "Max loss: 0.07996560633182526\n",
      "Min loss: 0.021570105105638504\n",
      "Mean loss: 0.04014353702465693\n",
      "Std loss: 0.020152332550805327\n",
      "Total Loss: 0.2408612221479416\n",
      "------------------------------------ epoch 1381 (8280 steps) ------------------------------------\n",
      "Max loss: 0.057201698422431946\n",
      "Min loss: 0.02586732804775238\n",
      "Mean loss: 0.039516677459081016\n",
      "Std loss: 0.011288062255132666\n",
      "Total Loss: 0.23710006475448608\n",
      "------------------------------------ epoch 1382 (8286 steps) ------------------------------------\n",
      "Max loss: 0.0314180925488472\n",
      "Min loss: 0.015961263328790665\n",
      "Mean loss: 0.023177584322790306\n",
      "Std loss: 0.004652630983337637\n",
      "Total Loss: 0.13906550593674183\n",
      "------------------------------------ epoch 1383 (8292 steps) ------------------------------------\n",
      "Max loss: 0.07532964646816254\n",
      "Min loss: 0.025567345321178436\n",
      "Mean loss: 0.05009105863670508\n",
      "Std loss: 0.01886352507641353\n",
      "Total Loss: 0.3005463518202305\n",
      "------------------------------------ epoch 1384 (8298 steps) ------------------------------------\n",
      "Max loss: 0.0602605938911438\n",
      "Min loss: 0.01616555079817772\n",
      "Mean loss: 0.04284656730790933\n",
      "Std loss: 0.01375972518023156\n",
      "Total Loss: 0.257079403847456\n",
      "------------------------------------ epoch 1385 (8304 steps) ------------------------------------\n",
      "Max loss: 0.05809733271598816\n",
      "Min loss: 0.018269630149006844\n",
      "Mean loss: 0.03202888462692499\n",
      "Std loss: 0.013864395234829546\n",
      "Total Loss: 0.19217330776154995\n",
      "------------------------------------ epoch 1386 (8310 steps) ------------------------------------\n",
      "Max loss: 0.04301085323095322\n",
      "Min loss: 0.019175086170434952\n",
      "Mean loss: 0.03330309161295494\n",
      "Std loss: 0.0077557172526480185\n",
      "Total Loss: 0.1998185496777296\n",
      "------------------------------------ epoch 1387 (8316 steps) ------------------------------------\n",
      "Max loss: 0.04685159772634506\n",
      "Min loss: 0.021187279373407364\n",
      "Mean loss: 0.02955297225465377\n",
      "Std loss: 0.009729500893667038\n",
      "Total Loss: 0.17731783352792263\n",
      "------------------------------------ epoch 1388 (8322 steps) ------------------------------------\n",
      "Max loss: 0.05658591911196709\n",
      "Min loss: 0.014842642471194267\n",
      "Mean loss: 0.03539685159921646\n",
      "Std loss: 0.013006990953503194\n",
      "Total Loss: 0.21238110959529877\n",
      "------------------------------------ epoch 1389 (8328 steps) ------------------------------------\n",
      "Max loss: 0.049838703125715256\n",
      "Min loss: 0.01692279987037182\n",
      "Mean loss: 0.03661165603746971\n",
      "Std loss: 0.010998572618727288\n",
      "Total Loss: 0.21966993622481823\n",
      "------------------------------------ epoch 1390 (8334 steps) ------------------------------------\n",
      "Max loss: 0.045497652143239975\n",
      "Min loss: 0.019061259925365448\n",
      "Mean loss: 0.029188683877388637\n",
      "Std loss: 0.00960764111988933\n",
      "Total Loss: 0.17513210326433182\n",
      "------------------------------------ epoch 1391 (8340 steps) ------------------------------------\n",
      "Max loss: 0.06864120811223984\n",
      "Min loss: 0.022477954626083374\n",
      "Mean loss: 0.03611196049799522\n",
      "Std loss: 0.015581136404886096\n",
      "Total Loss: 0.2166717629879713\n",
      "------------------------------------ epoch 1392 (8346 steps) ------------------------------------\n",
      "Max loss: 0.057299286127090454\n",
      "Min loss: 0.0186383705586195\n",
      "Mean loss: 0.032411777724822365\n",
      "Std loss: 0.01262023553400628\n",
      "Total Loss: 0.19447066634893417\n",
      "------------------------------------ epoch 1393 (8352 steps) ------------------------------------\n",
      "Max loss: 0.05382927507162094\n",
      "Min loss: 0.015998870134353638\n",
      "Mean loss: 0.030043511961897213\n",
      "Std loss: 0.012105965354993224\n",
      "Total Loss: 0.18026107177138329\n",
      "------------------------------------ epoch 1394 (8358 steps) ------------------------------------\n",
      "Max loss: 0.07194273173809052\n",
      "Min loss: 0.014799206517636776\n",
      "Mean loss: 0.039169918900976576\n",
      "Std loss: 0.0205888680187276\n",
      "Total Loss: 0.23501951340585947\n",
      "------------------------------------ epoch 1395 (8364 steps) ------------------------------------\n",
      "Max loss: 0.0428268238902092\n",
      "Min loss: 0.017170820385217667\n",
      "Mean loss: 0.0295924236997962\n",
      "Std loss: 0.008303127014436936\n",
      "Total Loss: 0.1775545421987772\n",
      "------------------------------------ epoch 1396 (8370 steps) ------------------------------------\n",
      "Max loss: 0.09266600012779236\n",
      "Min loss: 0.018889330327510834\n",
      "Mean loss: 0.042960903296868004\n",
      "Std loss: 0.025934196815232867\n",
      "Total Loss: 0.25776541978120804\n",
      "------------------------------------ epoch 1397 (8376 steps) ------------------------------------\n",
      "Max loss: 0.0776718407869339\n",
      "Min loss: 0.022861195728182793\n",
      "Mean loss: 0.03906346453974644\n",
      "Std loss: 0.017884632885541648\n",
      "Total Loss: 0.23438078723847866\n",
      "------------------------------------ epoch 1398 (8382 steps) ------------------------------------\n",
      "Max loss: 0.06242663785815239\n",
      "Min loss: 0.024462586268782616\n",
      "Mean loss: 0.0382219065601627\n",
      "Std loss: 0.013231462065088085\n",
      "Total Loss: 0.22933143936097622\n",
      "------------------------------------ epoch 1399 (8388 steps) ------------------------------------\n",
      "Max loss: 0.09156517684459686\n",
      "Min loss: 0.020175909623503685\n",
      "Mean loss: 0.04735924986501535\n",
      "Std loss: 0.022344388720057187\n",
      "Total Loss: 0.2841554991900921\n",
      "------------------------------------ epoch 1400 (8394 steps) ------------------------------------\n",
      "Max loss: 0.0775182917714119\n",
      "Min loss: 0.017731864005327225\n",
      "Mean loss: 0.04236172636349996\n",
      "Std loss: 0.02164506542224246\n",
      "Total Loss: 0.25417035818099976\n",
      "------------------------------------ epoch 1401 (8400 steps) ------------------------------------\n",
      "Max loss: 0.05200480669736862\n",
      "Min loss: 0.018319016322493553\n",
      "Mean loss: 0.031403359957039356\n",
      "Std loss: 0.010430695231354075\n",
      "Total Loss: 0.18842015974223614\n",
      "saved model at ./weights/model_1401.pth\n",
      "------------------------------------ epoch 1402 (8406 steps) ------------------------------------\n",
      "Max loss: 0.04995564743876457\n",
      "Min loss: 0.016935529187321663\n",
      "Mean loss: 0.03385128267109394\n",
      "Std loss: 0.01274568796446034\n",
      "Total Loss: 0.20310769602656364\n",
      "------------------------------------ epoch 1403 (8412 steps) ------------------------------------\n",
      "Max loss: 0.04184005782008171\n",
      "Min loss: 0.02682517096400261\n",
      "Mean loss: 0.035444388166069984\n",
      "Std loss: 0.006400820909981267\n",
      "Total Loss: 0.2126663289964199\n",
      "------------------------------------ epoch 1404 (8418 steps) ------------------------------------\n",
      "Max loss: 0.049720488488674164\n",
      "Min loss: 0.026803184300661087\n",
      "Mean loss: 0.039866374184687935\n",
      "Std loss: 0.00802816547686246\n",
      "Total Loss: 0.2391982451081276\n",
      "------------------------------------ epoch 1405 (8424 steps) ------------------------------------\n",
      "Max loss: 0.036695510149002075\n",
      "Min loss: 0.015591020695865154\n",
      "Mean loss: 0.024070169311016798\n",
      "Std loss: 0.007968256468523423\n",
      "Total Loss: 0.1444210158661008\n",
      "------------------------------------ epoch 1406 (8430 steps) ------------------------------------\n",
      "Max loss: 0.07292227447032928\n",
      "Min loss: 0.016644811257719994\n",
      "Mean loss: 0.037295473739504814\n",
      "Std loss: 0.01876972057674905\n",
      "Total Loss: 0.22377284243702888\n",
      "------------------------------------ epoch 1407 (8436 steps) ------------------------------------\n",
      "Max loss: 0.040843844413757324\n",
      "Min loss: 0.01331619918346405\n",
      "Mean loss: 0.028028953199585278\n",
      "Std loss: 0.009719819570090117\n",
      "Total Loss: 0.16817371919751167\n",
      "------------------------------------ epoch 1408 (8442 steps) ------------------------------------\n",
      "Max loss: 0.0466415099799633\n",
      "Min loss: 0.019155820831656456\n",
      "Mean loss: 0.030923180282115936\n",
      "Std loss: 0.009665044844281781\n",
      "Total Loss: 0.18553908169269562\n",
      "------------------------------------ epoch 1409 (8448 steps) ------------------------------------\n",
      "Max loss: 0.03646528720855713\n",
      "Min loss: 0.016642263159155846\n",
      "Mean loss: 0.026864104283352692\n",
      "Std loss: 0.0063943476794459135\n",
      "Total Loss: 0.16118462570011616\n",
      "------------------------------------ epoch 1410 (8454 steps) ------------------------------------\n",
      "Max loss: 0.05481661483645439\n",
      "Min loss: 0.025922682136297226\n",
      "Mean loss: 0.04047956131398678\n",
      "Std loss: 0.01021961225501617\n",
      "Total Loss: 0.24287736788392067\n",
      "------------------------------------ epoch 1411 (8460 steps) ------------------------------------\n",
      "Max loss: 0.04813980311155319\n",
      "Min loss: 0.016916412860155106\n",
      "Mean loss: 0.03144074355562528\n",
      "Std loss: 0.01148939047952535\n",
      "Total Loss: 0.18864446133375168\n",
      "------------------------------------ epoch 1412 (8466 steps) ------------------------------------\n",
      "Max loss: 0.056921765208244324\n",
      "Min loss: 0.019560333341360092\n",
      "Mean loss: 0.03796881685654322\n",
      "Std loss: 0.014207150153314906\n",
      "Total Loss: 0.22781290113925934\n",
      "------------------------------------ epoch 1413 (8472 steps) ------------------------------------\n",
      "Max loss: 0.05427713692188263\n",
      "Min loss: 0.022707801312208176\n",
      "Mean loss: 0.03580908135821422\n",
      "Std loss: 0.0101449996843839\n",
      "Total Loss: 0.21485448814928532\n",
      "------------------------------------ epoch 1414 (8478 steps) ------------------------------------\n",
      "Max loss: 0.07162310183048248\n",
      "Min loss: 0.02061508223414421\n",
      "Mean loss: 0.045769390339652695\n",
      "Std loss: 0.015784525112228654\n",
      "Total Loss: 0.2746163420379162\n",
      "------------------------------------ epoch 1415 (8484 steps) ------------------------------------\n",
      "Max loss: 0.0671006292104721\n",
      "Min loss: 0.019339097663760185\n",
      "Mean loss: 0.03746047460784515\n",
      "Std loss: 0.016276025165803544\n",
      "Total Loss: 0.22476284764707088\n",
      "------------------------------------ epoch 1416 (8490 steps) ------------------------------------\n",
      "Max loss: 0.0978119894862175\n",
      "Min loss: 0.017559409141540527\n",
      "Mean loss: 0.05071273073554039\n",
      "Std loss: 0.030470918585663025\n",
      "Total Loss: 0.30427638441324234\n",
      "------------------------------------ epoch 1417 (8496 steps) ------------------------------------\n",
      "Max loss: 0.06400348991155624\n",
      "Min loss: 0.015435612760484219\n",
      "Mean loss: 0.031608742816994585\n",
      "Std loss: 0.01754264734684657\n",
      "Total Loss: 0.18965245690196753\n",
      "------------------------------------ epoch 1418 (8502 steps) ------------------------------------\n",
      "Max loss: 0.04874425753951073\n",
      "Min loss: 0.023995649069547653\n",
      "Mean loss: 0.03481301044424375\n",
      "Std loss: 0.008877604953469719\n",
      "Total Loss: 0.2088780626654625\n",
      "------------------------------------ epoch 1419 (8508 steps) ------------------------------------\n",
      "Max loss: 0.08527517318725586\n",
      "Min loss: 0.027586234733462334\n",
      "Mean loss: 0.050244345950583615\n",
      "Std loss: 0.024005230670119625\n",
      "Total Loss: 0.3014660757035017\n",
      "------------------------------------ epoch 1420 (8514 steps) ------------------------------------\n",
      "Max loss: 0.04534135386347771\n",
      "Min loss: 0.021496346220374107\n",
      "Mean loss: 0.030815097813804943\n",
      "Std loss: 0.007860709403606782\n",
      "Total Loss: 0.18489058688282967\n",
      "------------------------------------ epoch 1421 (8520 steps) ------------------------------------\n",
      "Max loss: 0.059516411274671555\n",
      "Min loss: 0.018062662333250046\n",
      "Mean loss: 0.034053535821537174\n",
      "Std loss: 0.013763412742571453\n",
      "Total Loss: 0.20432121492922306\n",
      "------------------------------------ epoch 1422 (8526 steps) ------------------------------------\n",
      "Max loss: 0.0785154327750206\n",
      "Min loss: 0.013598315417766571\n",
      "Mean loss: 0.044189417424301304\n",
      "Std loss: 0.023047481439420345\n",
      "Total Loss: 0.26513650454580784\n",
      "------------------------------------ epoch 1423 (8532 steps) ------------------------------------\n",
      "Max loss: 0.03759060055017471\n",
      "Min loss: 0.014553360641002655\n",
      "Mean loss: 0.026318049368758995\n",
      "Std loss: 0.007733151615212473\n",
      "Total Loss: 0.15790829621255398\n",
      "------------------------------------ epoch 1424 (8538 steps) ------------------------------------\n",
      "Max loss: 0.0439508780837059\n",
      "Min loss: 0.023950885981321335\n",
      "Mean loss: 0.03479054321845373\n",
      "Std loss: 0.006130202167016632\n",
      "Total Loss: 0.20874325931072235\n",
      "------------------------------------ epoch 1425 (8544 steps) ------------------------------------\n",
      "Max loss: 0.0741611123085022\n",
      "Min loss: 0.027608126401901245\n",
      "Mean loss: 0.046604933527608715\n",
      "Std loss: 0.01645084944562694\n",
      "Total Loss: 0.2796296011656523\n",
      "------------------------------------ epoch 1426 (8550 steps) ------------------------------------\n",
      "Max loss: 0.0400872603058815\n",
      "Min loss: 0.0161310825496912\n",
      "Mean loss: 0.030467861021558445\n",
      "Std loss: 0.0080132535883049\n",
      "Total Loss: 0.18280716612935066\n",
      "------------------------------------ epoch 1427 (8556 steps) ------------------------------------\n",
      "Max loss: 0.03657374531030655\n",
      "Min loss: 0.02467302978038788\n",
      "Mean loss: 0.028664444262782734\n",
      "Std loss: 0.0037825500700860125\n",
      "Total Loss: 0.1719866655766964\n",
      "------------------------------------ epoch 1428 (8562 steps) ------------------------------------\n",
      "Max loss: 0.05558415502309799\n",
      "Min loss: 0.01720348373055458\n",
      "Mean loss: 0.026864984072744846\n",
      "Std loss: 0.0138438416313905\n",
      "Total Loss: 0.16118990443646908\n",
      "------------------------------------ epoch 1429 (8568 steps) ------------------------------------\n",
      "Max loss: 0.06656742095947266\n",
      "Min loss: 0.02033277042210102\n",
      "Mean loss: 0.0402048546820879\n",
      "Std loss: 0.01663695703691011\n",
      "Total Loss: 0.2412291280925274\n",
      "------------------------------------ epoch 1430 (8574 steps) ------------------------------------\n",
      "Max loss: 0.0680726170539856\n",
      "Min loss: 0.017868151888251305\n",
      "Mean loss: 0.04136260754118363\n",
      "Std loss: 0.017409290488608176\n",
      "Total Loss: 0.24817564524710178\n",
      "------------------------------------ epoch 1431 (8580 steps) ------------------------------------\n",
      "Max loss: 0.06060563772916794\n",
      "Min loss: 0.02353159710764885\n",
      "Mean loss: 0.03821379505097866\n",
      "Std loss: 0.01447339274552774\n",
      "Total Loss: 0.22928277030587196\n",
      "------------------------------------ epoch 1432 (8586 steps) ------------------------------------\n",
      "Max loss: 0.049925994127988815\n",
      "Min loss: 0.019033752381801605\n",
      "Mean loss: 0.02999271856000026\n",
      "Std loss: 0.012857556459914899\n",
      "Total Loss: 0.17995631136000156\n",
      "------------------------------------ epoch 1433 (8592 steps) ------------------------------------\n",
      "Max loss: 0.057958923280239105\n",
      "Min loss: 0.02669411152601242\n",
      "Mean loss: 0.04174664802849293\n",
      "Std loss: 0.010779995182726182\n",
      "Total Loss: 0.25047988817095757\n",
      "------------------------------------ epoch 1434 (8598 steps) ------------------------------------\n",
      "Max loss: 0.05103103816509247\n",
      "Min loss: 0.032641008496284485\n",
      "Mean loss: 0.044073984026908875\n",
      "Std loss: 0.006600148535624805\n",
      "Total Loss: 0.26444390416145325\n",
      "------------------------------------ epoch 1435 (8604 steps) ------------------------------------\n",
      "Max loss: 0.055511049926280975\n",
      "Min loss: 0.020433183759450912\n",
      "Mean loss: 0.03327308129519224\n",
      "Std loss: 0.011972112722492902\n",
      "Total Loss: 0.19963848777115345\n",
      "------------------------------------ epoch 1436 (8610 steps) ------------------------------------\n",
      "Max loss: 0.04451350122690201\n",
      "Min loss: 0.01728133298456669\n",
      "Mean loss: 0.034555092143515743\n",
      "Std loss: 0.00917364963619527\n",
      "Total Loss: 0.20733055286109447\n",
      "------------------------------------ epoch 1437 (8616 steps) ------------------------------------\n",
      "Max loss: 0.034006547182798386\n",
      "Min loss: 0.019111668691039085\n",
      "Mean loss: 0.027388653407494228\n",
      "Std loss: 0.005133866499544942\n",
      "Total Loss: 0.16433192044496536\n",
      "------------------------------------ epoch 1438 (8622 steps) ------------------------------------\n",
      "Max loss: 0.04248761758208275\n",
      "Min loss: 0.02556675300002098\n",
      "Mean loss: 0.033451566472649574\n",
      "Std loss: 0.005852914436007241\n",
      "Total Loss: 0.20070939883589745\n",
      "------------------------------------ epoch 1439 (8628 steps) ------------------------------------\n",
      "Max loss: 0.034561414271593094\n",
      "Min loss: 0.020744729787111282\n",
      "Mean loss: 0.025398515164852142\n",
      "Std loss: 0.004610936889279053\n",
      "Total Loss: 0.15239109098911285\n",
      "------------------------------------ epoch 1440 (8634 steps) ------------------------------------\n",
      "Max loss: 0.05487735569477081\n",
      "Min loss: 0.02091396227478981\n",
      "Mean loss: 0.032521956600248814\n",
      "Std loss: 0.011871628027573486\n",
      "Total Loss: 0.19513173960149288\n",
      "------------------------------------ epoch 1441 (8640 steps) ------------------------------------\n",
      "Max loss: 0.05729950591921806\n",
      "Min loss: 0.013618355616927147\n",
      "Mean loss: 0.02804184177269538\n",
      "Std loss: 0.014851459625565855\n",
      "Total Loss: 0.1682510506361723\n",
      "------------------------------------ epoch 1442 (8646 steps) ------------------------------------\n",
      "Max loss: 0.04665307328104973\n",
      "Min loss: 0.019349172711372375\n",
      "Mean loss: 0.027307624307771523\n",
      "Std loss: 0.010152558598921849\n",
      "Total Loss: 0.16384574584662914\n",
      "------------------------------------ epoch 1443 (8652 steps) ------------------------------------\n",
      "Max loss: 0.05980516970157623\n",
      "Min loss: 0.019491305574774742\n",
      "Mean loss: 0.03058774893482526\n",
      "Std loss: 0.013810394763158502\n",
      "Total Loss: 0.18352649360895157\n",
      "------------------------------------ epoch 1444 (8658 steps) ------------------------------------\n",
      "Max loss: 0.03602520376443863\n",
      "Min loss: 0.024284210056066513\n",
      "Mean loss: 0.029863301043709118\n",
      "Std loss: 0.003414795273692937\n",
      "Total Loss: 0.17917980626225471\n",
      "------------------------------------ epoch 1445 (8664 steps) ------------------------------------\n",
      "Max loss: 0.038951367139816284\n",
      "Min loss: 0.01949281059205532\n",
      "Mean loss: 0.031458211752275624\n",
      "Std loss: 0.006470403722744479\n",
      "Total Loss: 0.18874927051365376\n",
      "------------------------------------ epoch 1446 (8670 steps) ------------------------------------\n",
      "Max loss: 0.04843069612979889\n",
      "Min loss: 0.017876943573355675\n",
      "Mean loss: 0.03295514111717542\n",
      "Std loss: 0.01099655975135018\n",
      "Total Loss: 0.19773084670305252\n",
      "------------------------------------ epoch 1447 (8676 steps) ------------------------------------\n",
      "Max loss: 0.05874892696738243\n",
      "Min loss: 0.01466665230691433\n",
      "Mean loss: 0.026046917773783207\n",
      "Std loss: 0.015523992535715887\n",
      "Total Loss: 0.15628150664269924\n",
      "------------------------------------ epoch 1448 (8682 steps) ------------------------------------\n",
      "Max loss: 0.06685584038496017\n",
      "Min loss: 0.013624775223433971\n",
      "Mean loss: 0.03911619742090503\n",
      "Std loss: 0.01875823447481089\n",
      "Total Loss: 0.2346971845254302\n",
      "------------------------------------ epoch 1449 (8688 steps) ------------------------------------\n",
      "Max loss: 0.06311427056789398\n",
      "Min loss: 0.016850298270583153\n",
      "Mean loss: 0.03695947707941135\n",
      "Std loss: 0.014509721785370735\n",
      "Total Loss: 0.2217568624764681\n",
      "------------------------------------ epoch 1450 (8694 steps) ------------------------------------\n",
      "Max loss: 0.038276784121990204\n",
      "Min loss: 0.015213917940855026\n",
      "Mean loss: 0.025929991155862808\n",
      "Std loss: 0.007637046876715862\n",
      "Total Loss: 0.15557994693517685\n",
      "------------------------------------ epoch 1451 (8700 steps) ------------------------------------\n",
      "Max loss: 0.0546330064535141\n",
      "Min loss: 0.013870361261069775\n",
      "Mean loss: 0.03662770660594106\n",
      "Std loss: 0.013178971300119533\n",
      "Total Loss: 0.21976623963564634\n",
      "------------------------------------ epoch 1452 (8706 steps) ------------------------------------\n",
      "Max loss: 0.050289884209632874\n",
      "Min loss: 0.013691088184714317\n",
      "Mean loss: 0.03177856747061014\n",
      "Std loss: 0.011249952487923872\n",
      "Total Loss: 0.19067140482366085\n",
      "------------------------------------ epoch 1453 (8712 steps) ------------------------------------\n",
      "Max loss: 0.04723958671092987\n",
      "Min loss: 0.01866188272833824\n",
      "Mean loss: 0.029626035131514072\n",
      "Std loss: 0.010555605230647731\n",
      "Total Loss: 0.17775621078908443\n",
      "------------------------------------ epoch 1454 (8718 steps) ------------------------------------\n",
      "Max loss: 0.07856481522321701\n",
      "Min loss: 0.02274475246667862\n",
      "Mean loss: 0.04571011972924074\n",
      "Std loss: 0.02292269966768164\n",
      "Total Loss: 0.2742607183754444\n",
      "------------------------------------ epoch 1455 (8724 steps) ------------------------------------\n",
      "Max loss: 0.07947506755590439\n",
      "Min loss: 0.02923128567636013\n",
      "Mean loss: 0.047018203573922314\n",
      "Std loss: 0.016765181389267865\n",
      "Total Loss: 0.2821092214435339\n",
      "------------------------------------ epoch 1456 (8730 steps) ------------------------------------\n",
      "Max loss: 0.06795591861009598\n",
      "Min loss: 0.018583126366138458\n",
      "Mean loss: 0.037073840387165546\n",
      "Std loss: 0.01728122183923678\n",
      "Total Loss: 0.22244304232299328\n",
      "------------------------------------ epoch 1457 (8736 steps) ------------------------------------\n",
      "Max loss: 0.06687766313552856\n",
      "Min loss: 0.016343697905540466\n",
      "Mean loss: 0.03215589871009191\n",
      "Std loss: 0.016151729960533597\n",
      "Total Loss: 0.19293539226055145\n",
      "------------------------------------ epoch 1458 (8742 steps) ------------------------------------\n",
      "Max loss: 0.05086374282836914\n",
      "Min loss: 0.019587304443120956\n",
      "Mean loss: 0.0328374362240235\n",
      "Std loss: 0.01056355236151407\n",
      "Total Loss: 0.197024617344141\n",
      "------------------------------------ epoch 1459 (8748 steps) ------------------------------------\n",
      "Max loss: 0.042614564299583435\n",
      "Min loss: 0.017996778711676598\n",
      "Mean loss: 0.031044915008048218\n",
      "Std loss: 0.00881696324530627\n",
      "Total Loss: 0.1862694900482893\n",
      "------------------------------------ epoch 1460 (8754 steps) ------------------------------------\n",
      "Max loss: 0.07778263092041016\n",
      "Min loss: 0.01586320996284485\n",
      "Mean loss: 0.042715088774760566\n",
      "Std loss: 0.021295224454972984\n",
      "Total Loss: 0.2562905326485634\n",
      "------------------------------------ epoch 1461 (8760 steps) ------------------------------------\n",
      "Max loss: 0.05397474020719528\n",
      "Min loss: 0.0270062442868948\n",
      "Mean loss: 0.03881452356775602\n",
      "Std loss: 0.011184610895153194\n",
      "Total Loss: 0.2328871414065361\n",
      "------------------------------------ epoch 1462 (8766 steps) ------------------------------------\n",
      "Max loss: 0.06579224765300751\n",
      "Min loss: 0.023540638387203217\n",
      "Mean loss: 0.03441269230097532\n",
      "Std loss: 0.014624396956622376\n",
      "Total Loss: 0.20647615380585194\n",
      "------------------------------------ epoch 1463 (8772 steps) ------------------------------------\n",
      "Max loss: 0.04632209241390228\n",
      "Min loss: 0.018589738756418228\n",
      "Mean loss: 0.03389318473637104\n",
      "Std loss: 0.009380878457144998\n",
      "Total Loss: 0.20335910841822624\n",
      "------------------------------------ epoch 1464 (8778 steps) ------------------------------------\n",
      "Max loss: 0.090067557990551\n",
      "Min loss: 0.01810441166162491\n",
      "Mean loss: 0.04527061618864536\n",
      "Std loss: 0.026296448298305467\n",
      "Total Loss: 0.2716236971318722\n",
      "------------------------------------ epoch 1465 (8784 steps) ------------------------------------\n",
      "Max loss: 0.059163905680179596\n",
      "Min loss: 0.020685045048594475\n",
      "Mean loss: 0.03348848409950733\n",
      "Std loss: 0.012712697475910664\n",
      "Total Loss: 0.200930904597044\n",
      "------------------------------------ epoch 1466 (8790 steps) ------------------------------------\n",
      "Max loss: 0.05787758529186249\n",
      "Min loss: 0.02101273462176323\n",
      "Mean loss: 0.04223851580172777\n",
      "Std loss: 0.015546342971473861\n",
      "Total Loss: 0.25343109481036663\n",
      "------------------------------------ epoch 1467 (8796 steps) ------------------------------------\n",
      "Max loss: 0.061616867780685425\n",
      "Min loss: 0.01765475794672966\n",
      "Mean loss: 0.04262061634411415\n",
      "Std loss: 0.015866556981240825\n",
      "Total Loss: 0.25572369806468487\n",
      "------------------------------------ epoch 1468 (8802 steps) ------------------------------------\n",
      "Max loss: 0.0597374401986599\n",
      "Min loss: 0.02017860859632492\n",
      "Mean loss: 0.03405977009485165\n",
      "Std loss: 0.012689198806666089\n",
      "Total Loss: 0.20435862056910992\n",
      "------------------------------------ epoch 1469 (8808 steps) ------------------------------------\n",
      "Max loss: 0.0666150152683258\n",
      "Min loss: 0.013596436008810997\n",
      "Mean loss: 0.029437646890679996\n",
      "Std loss: 0.01754213502671668\n",
      "Total Loss: 0.17662588134407997\n",
      "------------------------------------ epoch 1470 (8814 steps) ------------------------------------\n",
      "Max loss: 0.07379915565252304\n",
      "Min loss: 0.017873546108603477\n",
      "Mean loss: 0.0335065775240461\n",
      "Std loss: 0.019353712006434882\n",
      "Total Loss: 0.20103946514427662\n",
      "------------------------------------ epoch 1471 (8820 steps) ------------------------------------\n",
      "Max loss: 0.04653477296233177\n",
      "Min loss: 0.016024066135287285\n",
      "Mean loss: 0.03040055340776841\n",
      "Std loss: 0.01000743276869157\n",
      "Total Loss: 0.18240332044661045\n",
      "------------------------------------ epoch 1472 (8826 steps) ------------------------------------\n",
      "Max loss: 0.05500528961420059\n",
      "Min loss: 0.026631450280547142\n",
      "Mean loss: 0.04204703184465567\n",
      "Std loss: 0.011862539481455605\n",
      "Total Loss: 0.25228219106793404\n",
      "------------------------------------ epoch 1473 (8832 steps) ------------------------------------\n",
      "Max loss: 0.06254959106445312\n",
      "Min loss: 0.013730190694332123\n",
      "Mean loss: 0.035498996575673424\n",
      "Std loss: 0.017466057680716355\n",
      "Total Loss: 0.21299397945404053\n",
      "------------------------------------ epoch 1474 (8838 steps) ------------------------------------\n",
      "Max loss: 0.05029433220624924\n",
      "Min loss: 0.017262475565075874\n",
      "Mean loss: 0.027964302028218906\n",
      "Std loss: 0.011042421170811466\n",
      "Total Loss: 0.16778581216931343\n",
      "------------------------------------ epoch 1475 (8844 steps) ------------------------------------\n",
      "Max loss: 0.07189592719078064\n",
      "Min loss: 0.013868136331439018\n",
      "Mean loss: 0.034163618770738445\n",
      "Std loss: 0.01953898637632224\n",
      "Total Loss: 0.20498171262443066\n",
      "------------------------------------ epoch 1476 (8850 steps) ------------------------------------\n",
      "Max loss: 0.0330590158700943\n",
      "Min loss: 0.018722478300333023\n",
      "Mean loss: 0.025570801459252834\n",
      "Std loss: 0.005360872491234914\n",
      "Total Loss: 0.153424808755517\n",
      "------------------------------------ epoch 1477 (8856 steps) ------------------------------------\n",
      "Max loss: 0.047960519790649414\n",
      "Min loss: 0.015639450401067734\n",
      "Mean loss: 0.031129861250519753\n",
      "Std loss: 0.009610598273788039\n",
      "Total Loss: 0.18677916750311852\n",
      "------------------------------------ epoch 1478 (8862 steps) ------------------------------------\n",
      "Max loss: 0.05151154845952988\n",
      "Min loss: 0.02009226381778717\n",
      "Mean loss: 0.03164310660213232\n",
      "Std loss: 0.011419159450607867\n",
      "Total Loss: 0.18985863961279392\n",
      "------------------------------------ epoch 1479 (8868 steps) ------------------------------------\n",
      "Max loss: 0.06031063199043274\n",
      "Min loss: 0.01944804936647415\n",
      "Mean loss: 0.03434935584664345\n",
      "Std loss: 0.013083117494835787\n",
      "Total Loss: 0.2060961350798607\n",
      "------------------------------------ epoch 1480 (8874 steps) ------------------------------------\n",
      "Max loss: 0.050493739545345306\n",
      "Min loss: 0.013580742292106152\n",
      "Mean loss: 0.02619534932697813\n",
      "Std loss: 0.013707051308677594\n",
      "Total Loss: 0.15717209596186876\n",
      "------------------------------------ epoch 1481 (8880 steps) ------------------------------------\n",
      "Max loss: 0.04314756393432617\n",
      "Min loss: 0.018019605427980423\n",
      "Mean loss: 0.030922379965583485\n",
      "Std loss: 0.00856605825735804\n",
      "Total Loss: 0.1855342797935009\n",
      "------------------------------------ epoch 1482 (8886 steps) ------------------------------------\n",
      "Max loss: 0.07470859587192535\n",
      "Min loss: 0.013396767899394035\n",
      "Mean loss: 0.039150720151762165\n",
      "Std loss: 0.020021613474055133\n",
      "Total Loss: 0.234904320910573\n",
      "------------------------------------ epoch 1483 (8892 steps) ------------------------------------\n",
      "Max loss: 0.08561025559902191\n",
      "Min loss: 0.025414537638425827\n",
      "Mean loss: 0.046859271824359894\n",
      "Std loss: 0.023703463991476388\n",
      "Total Loss: 0.28115563094615936\n",
      "------------------------------------ epoch 1484 (8898 steps) ------------------------------------\n",
      "Max loss: 0.04818759113550186\n",
      "Min loss: 0.02215736173093319\n",
      "Mean loss: 0.03465002899368604\n",
      "Std loss: 0.009819523540457918\n",
      "Total Loss: 0.20790017396211624\n",
      "------------------------------------ epoch 1485 (8904 steps) ------------------------------------\n",
      "Max loss: 0.039056241512298584\n",
      "Min loss: 0.015762411057949066\n",
      "Mean loss: 0.02557053913672765\n",
      "Std loss: 0.007958751815875825\n",
      "Total Loss: 0.1534232348203659\n",
      "------------------------------------ epoch 1486 (8910 steps) ------------------------------------\n",
      "Max loss: 0.047576747834682465\n",
      "Min loss: 0.023570282384753227\n",
      "Mean loss: 0.033410901203751564\n",
      "Std loss: 0.008846357011725514\n",
      "Total Loss: 0.20046540722250938\n",
      "------------------------------------ epoch 1487 (8916 steps) ------------------------------------\n",
      "Max loss: 0.06491109728813171\n",
      "Min loss: 0.03269472345709801\n",
      "Mean loss: 0.04528428986668587\n",
      "Std loss: 0.010842455265978587\n",
      "Total Loss: 0.2717057392001152\n",
      "------------------------------------ epoch 1488 (8922 steps) ------------------------------------\n",
      "Max loss: 0.06306518614292145\n",
      "Min loss: 0.026328589767217636\n",
      "Mean loss: 0.04802279112239679\n",
      "Std loss: 0.013984051394869022\n",
      "Total Loss: 0.2881367467343807\n",
      "------------------------------------ epoch 1489 (8928 steps) ------------------------------------\n",
      "Max loss: 0.08474358171224594\n",
      "Min loss: 0.016583677381277084\n",
      "Mean loss: 0.034762708470225334\n",
      "Std loss: 0.023981097589712813\n",
      "Total Loss: 0.208576250821352\n",
      "------------------------------------ epoch 1490 (8934 steps) ------------------------------------\n",
      "Max loss: 0.051983222365379333\n",
      "Min loss: 0.028051678091287613\n",
      "Mean loss: 0.03796059203644594\n",
      "Std loss: 0.008741125234573648\n",
      "Total Loss: 0.2277635522186756\n",
      "------------------------------------ epoch 1491 (8940 steps) ------------------------------------\n",
      "Max loss: 0.05180780217051506\n",
      "Min loss: 0.01717320643365383\n",
      "Mean loss: 0.03482063580304384\n",
      "Std loss: 0.013509541205103205\n",
      "Total Loss: 0.20892381481826305\n",
      "------------------------------------ epoch 1492 (8946 steps) ------------------------------------\n",
      "Max loss: 0.060691799968481064\n",
      "Min loss: 0.01771942526102066\n",
      "Mean loss: 0.03142477137347063\n",
      "Std loss: 0.014798263154017657\n",
      "Total Loss: 0.18854862824082375\n",
      "------------------------------------ epoch 1493 (8952 steps) ------------------------------------\n",
      "Max loss: 0.04188188165426254\n",
      "Min loss: 0.014734546653926373\n",
      "Mean loss: 0.026778791565448046\n",
      "Std loss: 0.00820405777232881\n",
      "Total Loss: 0.16067274939268827\n",
      "------------------------------------ epoch 1494 (8958 steps) ------------------------------------\n",
      "Max loss: 0.056091517210006714\n",
      "Min loss: 0.024180974811315536\n",
      "Mean loss: 0.03191451169550419\n",
      "Std loss: 0.011120101992074425\n",
      "Total Loss: 0.19148707017302513\n",
      "------------------------------------ epoch 1495 (8964 steps) ------------------------------------\n",
      "Max loss: 0.04546574503183365\n",
      "Min loss: 0.031699951738119125\n",
      "Mean loss: 0.039149571831027664\n",
      "Std loss: 0.005120628745680273\n",
      "Total Loss: 0.234897430986166\n",
      "------------------------------------ epoch 1496 (8970 steps) ------------------------------------\n",
      "Max loss: 0.0489417165517807\n",
      "Min loss: 0.013463906943798065\n",
      "Mean loss: 0.03132452288021644\n",
      "Std loss: 0.01297047941420187\n",
      "Total Loss: 0.18794713728129864\n",
      "------------------------------------ epoch 1497 (8976 steps) ------------------------------------\n",
      "Max loss: 0.05452127382159233\n",
      "Min loss: 0.014488181099295616\n",
      "Mean loss: 0.03300529873619477\n",
      "Std loss: 0.01281838447053007\n",
      "Total Loss: 0.19803179241716862\n",
      "------------------------------------ epoch 1498 (8982 steps) ------------------------------------\n",
      "Max loss: 0.04332710802555084\n",
      "Min loss: 0.015901964157819748\n",
      "Mean loss: 0.02787611385186513\n",
      "Std loss: 0.009114720875353156\n",
      "Total Loss: 0.1672566831111908\n",
      "------------------------------------ epoch 1499 (8988 steps) ------------------------------------\n",
      "Max loss: 0.03475669026374817\n",
      "Min loss: 0.01515396498143673\n",
      "Mean loss: 0.021858893955747288\n",
      "Std loss: 0.007789029546298414\n",
      "Total Loss: 0.13115336373448372\n",
      "------------------------------------ epoch 1500 (8994 steps) ------------------------------------\n",
      "Max loss: 0.043263085186481476\n",
      "Min loss: 0.019236914813518524\n",
      "Mean loss: 0.02960724476724863\n",
      "Std loss: 0.008479838123713232\n",
      "Total Loss: 0.17764346860349178\n",
      "------------------------------------ epoch 1501 (9000 steps) ------------------------------------\n",
      "Max loss: 0.06274351477622986\n",
      "Min loss: 0.01778329722583294\n",
      "Mean loss: 0.03910513346393903\n",
      "Std loss: 0.01624596072793076\n",
      "Total Loss: 0.23463080078363419\n",
      "saved model at ./weights/model_1501.pth\n",
      "------------------------------------ epoch 1502 (9006 steps) ------------------------------------\n",
      "Max loss: 0.039227571338415146\n",
      "Min loss: 0.015539579093456268\n",
      "Mean loss: 0.026098744943737984\n",
      "Std loss: 0.008233740530293042\n",
      "Total Loss: 0.1565924696624279\n",
      "------------------------------------ epoch 1503 (9012 steps) ------------------------------------\n",
      "Max loss: 0.03282203525304794\n",
      "Min loss: 0.013490504585206509\n",
      "Mean loss: 0.024634563208868105\n",
      "Std loss: 0.006164412984651788\n",
      "Total Loss: 0.14780737925320864\n",
      "------------------------------------ epoch 1504 (9018 steps) ------------------------------------\n",
      "Max loss: 0.05318804085254669\n",
      "Min loss: 0.022347431629896164\n",
      "Mean loss: 0.035601100573937096\n",
      "Std loss: 0.011497000424520301\n",
      "Total Loss: 0.2136066034436226\n",
      "------------------------------------ epoch 1505 (9024 steps) ------------------------------------\n",
      "Max loss: 0.03811481595039368\n",
      "Min loss: 0.02044840343296528\n",
      "Mean loss: 0.029197045291463535\n",
      "Std loss: 0.006407942600259849\n",
      "Total Loss: 0.1751822717487812\n",
      "------------------------------------ epoch 1506 (9030 steps) ------------------------------------\n",
      "Max loss: 0.08324360847473145\n",
      "Min loss: 0.018542218953371048\n",
      "Mean loss: 0.03512105563034614\n",
      "Std loss: 0.021862704588978907\n",
      "Total Loss: 0.21072633378207684\n",
      "------------------------------------ epoch 1507 (9036 steps) ------------------------------------\n",
      "Max loss: 0.052338577806949615\n",
      "Min loss: 0.019902657717466354\n",
      "Mean loss: 0.034455882695813976\n",
      "Std loss: 0.013704691741623443\n",
      "Total Loss: 0.20673529617488384\n",
      "------------------------------------ epoch 1508 (9042 steps) ------------------------------------\n",
      "Max loss: 0.07225096225738525\n",
      "Min loss: 0.01814829558134079\n",
      "Mean loss: 0.038293760269880295\n",
      "Std loss: 0.017520891725378983\n",
      "Total Loss: 0.22976256161928177\n",
      "------------------------------------ epoch 1509 (9048 steps) ------------------------------------\n",
      "Max loss: 0.05268775671720505\n",
      "Min loss: 0.015130194835364819\n",
      "Mean loss: 0.03184367607658108\n",
      "Std loss: 0.011661926961103459\n",
      "Total Loss: 0.19106205645948648\n",
      "------------------------------------ epoch 1510 (9054 steps) ------------------------------------\n",
      "Max loss: 0.04019502177834511\n",
      "Min loss: 0.018093964084982872\n",
      "Mean loss: 0.028874100496371586\n",
      "Std loss: 0.007729691073627228\n",
      "Total Loss: 0.17324460297822952\n",
      "------------------------------------ epoch 1511 (9060 steps) ------------------------------------\n",
      "Max loss: 0.04322994872927666\n",
      "Min loss: 0.021043479442596436\n",
      "Mean loss: 0.033944930881261826\n",
      "Std loss: 0.0075168555946032055\n",
      "Total Loss: 0.20366958528757095\n",
      "------------------------------------ epoch 1512 (9066 steps) ------------------------------------\n",
      "Max loss: 0.03190150111913681\n",
      "Min loss: 0.017293253913521767\n",
      "Mean loss: 0.025896073939899605\n",
      "Std loss: 0.005523387482884435\n",
      "Total Loss: 0.15537644363939762\n",
      "------------------------------------ epoch 1513 (9072 steps) ------------------------------------\n",
      "Max loss: 0.03891034424304962\n",
      "Min loss: 0.016591977328062057\n",
      "Mean loss: 0.02532132218281428\n",
      "Std loss: 0.007873676326193794\n",
      "Total Loss: 0.15192793309688568\n",
      "------------------------------------ epoch 1514 (9078 steps) ------------------------------------\n",
      "Max loss: 0.06376951932907104\n",
      "Min loss: 0.023874541744589806\n",
      "Mean loss: 0.0445076764250795\n",
      "Std loss: 0.015506432960621649\n",
      "Total Loss: 0.26704605855047703\n",
      "------------------------------------ epoch 1515 (9084 steps) ------------------------------------\n",
      "Max loss: 0.05421338975429535\n",
      "Min loss: 0.020429957658052444\n",
      "Mean loss: 0.039339253678917885\n",
      "Std loss: 0.012917741263530791\n",
      "Total Loss: 0.2360355220735073\n",
      "------------------------------------ epoch 1516 (9090 steps) ------------------------------------\n",
      "Max loss: 0.14251002669334412\n",
      "Min loss: 0.026639342308044434\n",
      "Mean loss: 0.05167828065653642\n",
      "Std loss: 0.04123586218594692\n",
      "Total Loss: 0.3100696839392185\n",
      "------------------------------------ epoch 1517 (9096 steps) ------------------------------------\n",
      "Max loss: 0.054048966616392136\n",
      "Min loss: 0.013348197564482689\n",
      "Mean loss: 0.032855560071766376\n",
      "Std loss: 0.014247209312994908\n",
      "Total Loss: 0.19713336043059826\n",
      "------------------------------------ epoch 1518 (9102 steps) ------------------------------------\n",
      "Max loss: 0.048678528517484665\n",
      "Min loss: 0.019510403275489807\n",
      "Mean loss: 0.03371624959011873\n",
      "Std loss: 0.008971910004068172\n",
      "Total Loss: 0.20229749754071236\n",
      "------------------------------------ epoch 1519 (9108 steps) ------------------------------------\n",
      "Max loss: 0.05577503889799118\n",
      "Min loss: 0.022291770204901695\n",
      "Mean loss: 0.03373367649813493\n",
      "Std loss: 0.010543503679392819\n",
      "Total Loss: 0.20240205898880959\n",
      "------------------------------------ epoch 1520 (9114 steps) ------------------------------------\n",
      "Max loss: 0.03554495424032211\n",
      "Min loss: 0.020083090290427208\n",
      "Mean loss: 0.029316639838119347\n",
      "Std loss: 0.005420155185337071\n",
      "Total Loss: 0.1758998390287161\n",
      "------------------------------------ epoch 1521 (9120 steps) ------------------------------------\n",
      "Max loss: 0.08717381954193115\n",
      "Min loss: 0.0312962681055069\n",
      "Mean loss: 0.04486661031842232\n",
      "Std loss: 0.019283809267748586\n",
      "Total Loss: 0.2691996619105339\n",
      "------------------------------------ epoch 1522 (9126 steps) ------------------------------------\n",
      "Max loss: 0.04060114920139313\n",
      "Min loss: 0.018626872450113297\n",
      "Mean loss: 0.026332758367061615\n",
      "Std loss: 0.007044714482022604\n",
      "Total Loss: 0.1579965502023697\n",
      "------------------------------------ epoch 1523 (9132 steps) ------------------------------------\n",
      "Max loss: 0.04082944616675377\n",
      "Min loss: 0.01893031597137451\n",
      "Mean loss: 0.027506761873761814\n",
      "Std loss: 0.008096905422687376\n",
      "Total Loss: 0.16504057124257088\n",
      "------------------------------------ epoch 1524 (9138 steps) ------------------------------------\n",
      "Max loss: 0.056321170181035995\n",
      "Min loss: 0.017223935574293137\n",
      "Mean loss: 0.03188751575847467\n",
      "Std loss: 0.01315587547283655\n",
      "Total Loss: 0.191325094550848\n",
      "------------------------------------ epoch 1525 (9144 steps) ------------------------------------\n",
      "Max loss: 0.04101565480232239\n",
      "Min loss: 0.0179017074406147\n",
      "Mean loss: 0.026592266745865345\n",
      "Std loss: 0.008002615223286248\n",
      "Total Loss: 0.15955360047519207\n",
      "------------------------------------ epoch 1526 (9150 steps) ------------------------------------\n",
      "Max loss: 0.0705694854259491\n",
      "Min loss: 0.01739579066634178\n",
      "Mean loss: 0.042861039439837136\n",
      "Std loss: 0.01744961308636978\n",
      "Total Loss: 0.2571662366390228\n",
      "------------------------------------ epoch 1527 (9156 steps) ------------------------------------\n",
      "Max loss: 0.07449112832546234\n",
      "Min loss: 0.018803037703037262\n",
      "Mean loss: 0.034913149662315845\n",
      "Std loss: 0.019579753424682385\n",
      "Total Loss: 0.20947889797389507\n",
      "------------------------------------ epoch 1528 (9162 steps) ------------------------------------\n",
      "Max loss: 0.038203489035367966\n",
      "Min loss: 0.0199311263859272\n",
      "Mean loss: 0.028247986920177937\n",
      "Std loss: 0.007040756462592074\n",
      "Total Loss: 0.16948792152106762\n",
      "------------------------------------ epoch 1529 (9168 steps) ------------------------------------\n",
      "Max loss: 0.03418855369091034\n",
      "Min loss: 0.015228820964694023\n",
      "Mean loss: 0.0245969050253431\n",
      "Std loss: 0.0065485279834051014\n",
      "Total Loss: 0.1475814301520586\n",
      "------------------------------------ epoch 1530 (9174 steps) ------------------------------------\n",
      "Max loss: 0.03495488688349724\n",
      "Min loss: 0.014213230460882187\n",
      "Mean loss: 0.026931671425700188\n",
      "Std loss: 0.008041950320286734\n",
      "Total Loss: 0.16159002855420113\n",
      "------------------------------------ epoch 1531 (9180 steps) ------------------------------------\n",
      "Max loss: 0.049955420196056366\n",
      "Min loss: 0.016434337943792343\n",
      "Mean loss: 0.032717583080132805\n",
      "Std loss: 0.013384784813663456\n",
      "Total Loss: 0.19630549848079681\n",
      "------------------------------------ epoch 1532 (9186 steps) ------------------------------------\n",
      "Max loss: 0.05861344188451767\n",
      "Min loss: 0.016526974737644196\n",
      "Mean loss: 0.02944305259734392\n",
      "Std loss: 0.01497370257559561\n",
      "Total Loss: 0.17665831558406353\n",
      "------------------------------------ epoch 1533 (9192 steps) ------------------------------------\n",
      "Max loss: 0.05556444078683853\n",
      "Min loss: 0.021131541579961777\n",
      "Mean loss: 0.035533692377309\n",
      "Std loss: 0.01099293627923575\n",
      "Total Loss: 0.21320215426385403\n",
      "------------------------------------ epoch 1534 (9198 steps) ------------------------------------\n",
      "Max loss: 0.056816231459379196\n",
      "Min loss: 0.012869589030742645\n",
      "Mean loss: 0.03562786181767782\n",
      "Std loss: 0.015209531592526597\n",
      "Total Loss: 0.2137671709060669\n",
      "------------------------------------ epoch 1535 (9204 steps) ------------------------------------\n",
      "Max loss: 0.03147635981440544\n",
      "Min loss: 0.015596171841025352\n",
      "Mean loss: 0.022501363108555477\n",
      "Std loss: 0.006011731626257048\n",
      "Total Loss: 0.13500817865133286\n",
      "------------------------------------ epoch 1536 (9210 steps) ------------------------------------\n",
      "Max loss: 0.05982294678688049\n",
      "Min loss: 0.023419078439474106\n",
      "Mean loss: 0.03932522858182589\n",
      "Std loss: 0.012474596781533398\n",
      "Total Loss: 0.23595137149095535\n",
      "------------------------------------ epoch 1537 (9216 steps) ------------------------------------\n",
      "Max loss: 0.04069563001394272\n",
      "Min loss: 0.015680979937314987\n",
      "Mean loss: 0.028402031088868778\n",
      "Std loss: 0.0076007412512015485\n",
      "Total Loss: 0.17041218653321266\n",
      "------------------------------------ epoch 1538 (9222 steps) ------------------------------------\n",
      "Max loss: 0.039054471999406815\n",
      "Min loss: 0.025036148726940155\n",
      "Mean loss: 0.03472444228827953\n",
      "Std loss: 0.004996643389980895\n",
      "Total Loss: 0.2083466537296772\n",
      "------------------------------------ epoch 1539 (9228 steps) ------------------------------------\n",
      "Max loss: 0.03376498073339462\n",
      "Min loss: 0.015914233401417732\n",
      "Mean loss: 0.02553668028364579\n",
      "Std loss: 0.007837453094902197\n",
      "Total Loss: 0.15322008170187473\n",
      "------------------------------------ epoch 1540 (9234 steps) ------------------------------------\n",
      "Max loss: 0.0880821943283081\n",
      "Min loss: 0.014409351162612438\n",
      "Mean loss: 0.04690820025280118\n",
      "Std loss: 0.025165395240977013\n",
      "Total Loss: 0.2814492015168071\n",
      "------------------------------------ epoch 1541 (9240 steps) ------------------------------------\n",
      "Max loss: 0.04985645413398743\n",
      "Min loss: 0.016646824777126312\n",
      "Mean loss: 0.030774102546274662\n",
      "Std loss: 0.013532659123473258\n",
      "Total Loss: 0.18464461527764797\n",
      "------------------------------------ epoch 1542 (9246 steps) ------------------------------------\n",
      "Max loss: 0.04800170660018921\n",
      "Min loss: 0.022229861468076706\n",
      "Mean loss: 0.04033631583054861\n",
      "Std loss: 0.00856637742399179\n",
      "Total Loss: 0.24201789498329163\n",
      "------------------------------------ epoch 1543 (9252 steps) ------------------------------------\n",
      "Max loss: 0.050363462418317795\n",
      "Min loss: 0.02605113945901394\n",
      "Mean loss: 0.03425564710050821\n",
      "Std loss: 0.008043433985107014\n",
      "Total Loss: 0.20553388260304928\n",
      "------------------------------------ epoch 1544 (9258 steps) ------------------------------------\n",
      "Max loss: 0.03618822246789932\n",
      "Min loss: 0.017932748422026634\n",
      "Mean loss: 0.0267419187972943\n",
      "Std loss: 0.005719123883505515\n",
      "Total Loss: 0.1604515127837658\n",
      "------------------------------------ epoch 1545 (9264 steps) ------------------------------------\n",
      "Max loss: 0.049434468150138855\n",
      "Min loss: 0.012721991166472435\n",
      "Mean loss: 0.03201793289432923\n",
      "Std loss: 0.013579039526901\n",
      "Total Loss: 0.19210759736597538\n",
      "------------------------------------ epoch 1546 (9270 steps) ------------------------------------\n",
      "Max loss: 0.03874128311872482\n",
      "Min loss: 0.020676668733358383\n",
      "Mean loss: 0.029059988756974537\n",
      "Std loss: 0.008015586830741613\n",
      "Total Loss: 0.17435993254184723\n",
      "------------------------------------ epoch 1547 (9276 steps) ------------------------------------\n",
      "Max loss: 0.0450529083609581\n",
      "Min loss: 0.022576352581381798\n",
      "Mean loss: 0.034574936454494797\n",
      "Std loss: 0.007061266099501589\n",
      "Total Loss: 0.20744961872696877\n",
      "------------------------------------ epoch 1548 (9282 steps) ------------------------------------\n",
      "Max loss: 0.06068289279937744\n",
      "Min loss: 0.024741053581237793\n",
      "Mean loss: 0.04153567863007387\n",
      "Std loss: 0.01121367416290528\n",
      "Total Loss: 0.2492140717804432\n",
      "------------------------------------ epoch 1549 (9288 steps) ------------------------------------\n",
      "Max loss: 0.04200776666402817\n",
      "Min loss: 0.013152990490198135\n",
      "Mean loss: 0.02593995040903489\n",
      "Std loss: 0.011655008835141574\n",
      "Total Loss: 0.15563970245420933\n",
      "------------------------------------ epoch 1550 (9294 steps) ------------------------------------\n",
      "Max loss: 0.0542195700109005\n",
      "Min loss: 0.02456657588481903\n",
      "Mean loss: 0.03782453822592894\n",
      "Std loss: 0.009811955754392102\n",
      "Total Loss: 0.22694722935557365\n",
      "------------------------------------ epoch 1551 (9300 steps) ------------------------------------\n",
      "Max loss: 0.03919493407011032\n",
      "Min loss: 0.026231689378619194\n",
      "Mean loss: 0.03200416949888071\n",
      "Std loss: 0.005040435847956579\n",
      "Total Loss: 0.19202501699328423\n",
      "------------------------------------ epoch 1552 (9306 steps) ------------------------------------\n",
      "Max loss: 0.05220866948366165\n",
      "Min loss: 0.03023838996887207\n",
      "Mean loss: 0.04376713496943315\n",
      "Std loss: 0.00796429823714297\n",
      "Total Loss: 0.2626028098165989\n",
      "------------------------------------ epoch 1553 (9312 steps) ------------------------------------\n",
      "Max loss: 0.056710436940193176\n",
      "Min loss: 0.021719086915254593\n",
      "Mean loss: 0.04107580644388994\n",
      "Std loss: 0.011830958918666862\n",
      "Total Loss: 0.24645483866333961\n",
      "------------------------------------ epoch 1554 (9318 steps) ------------------------------------\n",
      "Max loss: 0.06972026079893112\n",
      "Min loss: 0.01383393257856369\n",
      "Mean loss: 0.0338248402501146\n",
      "Std loss: 0.01902131074041839\n",
      "Total Loss: 0.2029490415006876\n",
      "------------------------------------ epoch 1555 (9324 steps) ------------------------------------\n",
      "Max loss: 0.058017194271087646\n",
      "Min loss: 0.018743038177490234\n",
      "Mean loss: 0.032075152111550174\n",
      "Std loss: 0.013556016468982862\n",
      "Total Loss: 0.19245091266930103\n",
      "------------------------------------ epoch 1556 (9330 steps) ------------------------------------\n",
      "Max loss: 0.0671580359339714\n",
      "Min loss: 0.013343244791030884\n",
      "Mean loss: 0.039300357922911644\n",
      "Std loss: 0.01873665240040495\n",
      "Total Loss: 0.23580214753746986\n",
      "------------------------------------ epoch 1557 (9336 steps) ------------------------------------\n",
      "Max loss: 0.057030290365219116\n",
      "Min loss: 0.01711583510041237\n",
      "Mean loss: 0.03623776199916998\n",
      "Std loss: 0.013612097606894144\n",
      "Total Loss: 0.2174265719950199\n",
      "------------------------------------ epoch 1558 (9342 steps) ------------------------------------\n",
      "Max loss: 0.03630761429667473\n",
      "Min loss: 0.019851170480251312\n",
      "Mean loss: 0.02595233265310526\n",
      "Std loss: 0.006511407370743377\n",
      "Total Loss: 0.15571399591863155\n",
      "------------------------------------ epoch 1559 (9348 steps) ------------------------------------\n",
      "Max loss: 0.07038736343383789\n",
      "Min loss: 0.028948843479156494\n",
      "Mean loss: 0.04767960806687673\n",
      "Std loss: 0.015358544810379791\n",
      "Total Loss: 0.2860776484012604\n",
      "------------------------------------ epoch 1560 (9354 steps) ------------------------------------\n",
      "Max loss: 0.08669941127300262\n",
      "Min loss: 0.019675221294164658\n",
      "Mean loss: 0.03758510916183392\n",
      "Std loss: 0.02276853629997377\n",
      "Total Loss: 0.22551065497100353\n",
      "------------------------------------ epoch 1561 (9360 steps) ------------------------------------\n",
      "Max loss: 0.040074221789836884\n",
      "Min loss: 0.013548320159316063\n",
      "Mean loss: 0.026953698446353275\n",
      "Std loss: 0.008926448308504108\n",
      "Total Loss: 0.16172219067811966\n",
      "------------------------------------ epoch 1562 (9366 steps) ------------------------------------\n",
      "Max loss: 0.04957495629787445\n",
      "Min loss: 0.018662666901946068\n",
      "Mean loss: 0.03515844698995352\n",
      "Std loss: 0.012904732495976536\n",
      "Total Loss: 0.2109506819397211\n",
      "------------------------------------ epoch 1563 (9372 steps) ------------------------------------\n",
      "Max loss: 0.0639086589217186\n",
      "Min loss: 0.01767227053642273\n",
      "Mean loss: 0.03348965694506963\n",
      "Std loss: 0.015519470784721774\n",
      "Total Loss: 0.20093794167041779\n",
      "------------------------------------ epoch 1564 (9378 steps) ------------------------------------\n",
      "Max loss: 0.04068995267152786\n",
      "Min loss: 0.021942127496004105\n",
      "Mean loss: 0.029924949010213215\n",
      "Std loss: 0.0059945466472387\n",
      "Total Loss: 0.1795496940612793\n",
      "------------------------------------ epoch 1565 (9384 steps) ------------------------------------\n",
      "Max loss: 0.05700431391596794\n",
      "Min loss: 0.019275855273008347\n",
      "Mean loss: 0.038698131839434304\n",
      "Std loss: 0.01406055702420428\n",
      "Total Loss: 0.23218879103660583\n",
      "------------------------------------ epoch 1566 (9390 steps) ------------------------------------\n",
      "Max loss: 0.04122863709926605\n",
      "Min loss: 0.012967894785106182\n",
      "Mean loss: 0.02696573967114091\n",
      "Std loss: 0.00973009068218255\n",
      "Total Loss: 0.16179443802684546\n",
      "------------------------------------ epoch 1567 (9396 steps) ------------------------------------\n",
      "Max loss: 0.049454446882009506\n",
      "Min loss: 0.02049911394715309\n",
      "Mean loss: 0.03134062234312296\n",
      "Std loss: 0.009017976812996582\n",
      "Total Loss: 0.18804373405873775\n",
      "------------------------------------ epoch 1568 (9402 steps) ------------------------------------\n",
      "Max loss: 0.05348179489374161\n",
      "Min loss: 0.020033933222293854\n",
      "Mean loss: 0.03550484497100115\n",
      "Std loss: 0.012066352887539909\n",
      "Total Loss: 0.2130290698260069\n",
      "------------------------------------ epoch 1569 (9408 steps) ------------------------------------\n",
      "Max loss: 0.037691857665777206\n",
      "Min loss: 0.01361018605530262\n",
      "Mean loss: 0.025801619825263817\n",
      "Std loss: 0.009080164860289695\n",
      "Total Loss: 0.1548097189515829\n",
      "------------------------------------ epoch 1570 (9414 steps) ------------------------------------\n",
      "Max loss: 0.03660167381167412\n",
      "Min loss: 0.01689940132200718\n",
      "Mean loss: 0.02915291270862023\n",
      "Std loss: 0.006930568271234308\n",
      "Total Loss: 0.17491747625172138\n",
      "------------------------------------ epoch 1571 (9420 steps) ------------------------------------\n",
      "Max loss: 0.047175776213407516\n",
      "Min loss: 0.014871954917907715\n",
      "Mean loss: 0.026816468685865402\n",
      "Std loss: 0.01311079292402363\n",
      "Total Loss: 0.1608988121151924\n",
      "------------------------------------ epoch 1572 (9426 steps) ------------------------------------\n",
      "Max loss: 0.03710823506116867\n",
      "Min loss: 0.018719693645834923\n",
      "Mean loss: 0.02634324102352063\n",
      "Std loss: 0.006080533611990357\n",
      "Total Loss: 0.15805944614112377\n",
      "------------------------------------ epoch 1573 (9432 steps) ------------------------------------\n",
      "Max loss: 0.05308827757835388\n",
      "Min loss: 0.016449637711048126\n",
      "Mean loss: 0.02891587869574626\n",
      "Std loss: 0.011519448504100662\n",
      "Total Loss: 0.17349527217447758\n",
      "------------------------------------ epoch 1574 (9438 steps) ------------------------------------\n",
      "Max loss: 0.10223101079463959\n",
      "Min loss: 0.020114561542868614\n",
      "Mean loss: 0.043667103784779705\n",
      "Std loss: 0.027058278518978192\n",
      "Total Loss: 0.26200262270867825\n",
      "------------------------------------ epoch 1575 (9444 steps) ------------------------------------\n",
      "Max loss: 0.04739345237612724\n",
      "Min loss: 0.01816081814467907\n",
      "Mean loss: 0.032388883642852306\n",
      "Std loss: 0.008616433840438332\n",
      "Total Loss: 0.19433330185711384\n",
      "------------------------------------ epoch 1576 (9450 steps) ------------------------------------\n",
      "Max loss: 0.055699609220027924\n",
      "Min loss: 0.01839914731681347\n",
      "Mean loss: 0.03438118534783522\n",
      "Std loss: 0.012993132537195312\n",
      "Total Loss: 0.20628711208701134\n",
      "------------------------------------ epoch 1577 (9456 steps) ------------------------------------\n",
      "Max loss: 0.0811900943517685\n",
      "Min loss: 0.01711592823266983\n",
      "Mean loss: 0.035516829850773014\n",
      "Std loss: 0.021026910654917665\n",
      "Total Loss: 0.2131009791046381\n",
      "------------------------------------ epoch 1578 (9462 steps) ------------------------------------\n",
      "Max loss: 0.05064716190099716\n",
      "Min loss: 0.017655937001109123\n",
      "Mean loss: 0.027280086651444435\n",
      "Std loss: 0.01118595180098392\n",
      "Total Loss: 0.1636805199086666\n",
      "------------------------------------ epoch 1579 (9468 steps) ------------------------------------\n",
      "Max loss: 0.07497107982635498\n",
      "Min loss: 0.01178465411067009\n",
      "Mean loss: 0.031762355317672096\n",
      "Std loss: 0.020921196556254344\n",
      "Total Loss: 0.19057413190603256\n",
      "------------------------------------ epoch 1580 (9474 steps) ------------------------------------\n",
      "Max loss: 0.0650516152381897\n",
      "Min loss: 0.022718969732522964\n",
      "Mean loss: 0.03371242992579937\n",
      "Std loss: 0.014551942001426824\n",
      "Total Loss: 0.20227457955479622\n",
      "------------------------------------ epoch 1581 (9480 steps) ------------------------------------\n",
      "Max loss: 0.029033228754997253\n",
      "Min loss: 0.017690779641270638\n",
      "Mean loss: 0.024057086867590744\n",
      "Std loss: 0.004244228623760307\n",
      "Total Loss: 0.14434252120554447\n",
      "------------------------------------ epoch 1582 (9486 steps) ------------------------------------\n",
      "Max loss: 0.04467926174402237\n",
      "Min loss: 0.012246277183294296\n",
      "Mean loss: 0.028429629902044933\n",
      "Std loss: 0.011279113353152802\n",
      "Total Loss: 0.1705777794122696\n",
      "------------------------------------ epoch 1583 (9492 steps) ------------------------------------\n",
      "Max loss: 0.04730060696601868\n",
      "Min loss: 0.02294645458459854\n",
      "Mean loss: 0.03180216594288746\n",
      "Std loss: 0.008448109678896854\n",
      "Total Loss: 0.1908129956573248\n",
      "------------------------------------ epoch 1584 (9498 steps) ------------------------------------\n",
      "Max loss: 0.04722157120704651\n",
      "Min loss: 0.017316490411758423\n",
      "Mean loss: 0.027411436662077904\n",
      "Std loss: 0.00960813452127365\n",
      "Total Loss: 0.16446861997246742\n",
      "------------------------------------ epoch 1585 (9504 steps) ------------------------------------\n",
      "Max loss: 0.041956789791584015\n",
      "Min loss: 0.013628097251057625\n",
      "Mean loss: 0.028639012947678566\n",
      "Std loss: 0.01016738146420882\n",
      "Total Loss: 0.1718340776860714\n",
      "------------------------------------ epoch 1586 (9510 steps) ------------------------------------\n",
      "Max loss: 0.045217081904411316\n",
      "Min loss: 0.019308967515826225\n",
      "Mean loss: 0.03143407963216305\n",
      "Std loss: 0.00874460476889142\n",
      "Total Loss: 0.1886044777929783\n",
      "------------------------------------ epoch 1587 (9516 steps) ------------------------------------\n",
      "Max loss: 0.08211369812488556\n",
      "Min loss: 0.013886353932321072\n",
      "Mean loss: 0.036317454961438976\n",
      "Std loss: 0.025119481003333367\n",
      "Total Loss: 0.21790472976863384\n",
      "------------------------------------ epoch 1588 (9522 steps) ------------------------------------\n",
      "Max loss: 0.08967696130275726\n",
      "Min loss: 0.01973215863108635\n",
      "Mean loss: 0.04144916217774153\n",
      "Std loss: 0.026754898865166275\n",
      "Total Loss: 0.24869497306644917\n",
      "------------------------------------ epoch 1589 (9528 steps) ------------------------------------\n",
      "Max loss: 0.03913644701242447\n",
      "Min loss: 0.019706841558218002\n",
      "Mean loss: 0.027526805487771828\n",
      "Std loss: 0.00679263663602389\n",
      "Total Loss: 0.16516083292663097\n",
      "------------------------------------ epoch 1590 (9534 steps) ------------------------------------\n",
      "Max loss: 0.04200832545757294\n",
      "Min loss: 0.018402624875307083\n",
      "Mean loss: 0.02656054776161909\n",
      "Std loss: 0.007955587010187035\n",
      "Total Loss: 0.15936328656971455\n",
      "------------------------------------ epoch 1591 (9540 steps) ------------------------------------\n",
      "Max loss: 0.06582974642515182\n",
      "Min loss: 0.02471259981393814\n",
      "Mean loss: 0.0412974851205945\n",
      "Std loss: 0.013635183443984275\n",
      "Total Loss: 0.247784910723567\n",
      "------------------------------------ epoch 1592 (9546 steps) ------------------------------------\n",
      "Max loss: 0.06889481097459793\n",
      "Min loss: 0.022514939308166504\n",
      "Mean loss: 0.042467501324911915\n",
      "Std loss: 0.014325950278820599\n",
      "Total Loss: 0.2548050079494715\n",
      "------------------------------------ epoch 1593 (9552 steps) ------------------------------------\n",
      "Max loss: 0.03514880686998367\n",
      "Min loss: 0.014971244148910046\n",
      "Mean loss: 0.024354735855013132\n",
      "Std loss: 0.007043188415412963\n",
      "Total Loss: 0.1461284151300788\n",
      "------------------------------------ epoch 1594 (9558 steps) ------------------------------------\n",
      "Max loss: 0.08271896094083786\n",
      "Min loss: 0.01379344891756773\n",
      "Mean loss: 0.033140003215521574\n",
      "Std loss: 0.022773928089085405\n",
      "Total Loss: 0.19884001929312944\n",
      "------------------------------------ epoch 1595 (9564 steps) ------------------------------------\n",
      "Max loss: 0.039920907467603683\n",
      "Min loss: 0.01860910654067993\n",
      "Mean loss: 0.0299734678119421\n",
      "Std loss: 0.007776574568224723\n",
      "Total Loss: 0.1798408068716526\n",
      "------------------------------------ epoch 1596 (9570 steps) ------------------------------------\n",
      "Max loss: 0.05626299977302551\n",
      "Min loss: 0.01622866652905941\n",
      "Mean loss: 0.04090039463092884\n",
      "Std loss: 0.013014944944379386\n",
      "Total Loss: 0.245402367785573\n",
      "------------------------------------ epoch 1597 (9576 steps) ------------------------------------\n",
      "Max loss: 0.061023831367492676\n",
      "Min loss: 0.0131184421479702\n",
      "Mean loss: 0.0380492378026247\n",
      "Std loss: 0.017658182961489636\n",
      "Total Loss: 0.22829542681574821\n",
      "------------------------------------ epoch 1598 (9582 steps) ------------------------------------\n",
      "Max loss: 0.049422845244407654\n",
      "Min loss: 0.020686542615294456\n",
      "Mean loss: 0.032250175873438515\n",
      "Std loss: 0.011899211583211963\n",
      "Total Loss: 0.1935010552406311\n",
      "------------------------------------ epoch 1599 (9588 steps) ------------------------------------\n",
      "Max loss: 0.03317803516983986\n",
      "Min loss: 0.017915934324264526\n",
      "Mean loss: 0.026565689283112686\n",
      "Std loss: 0.0056715128153635086\n",
      "Total Loss: 0.1593941356986761\n",
      "------------------------------------ epoch 1600 (9594 steps) ------------------------------------\n",
      "Max loss: 0.05217694491147995\n",
      "Min loss: 0.021610578522086143\n",
      "Mean loss: 0.033554501831531525\n",
      "Std loss: 0.012238187373711525\n",
      "Total Loss: 0.20132701098918915\n",
      "------------------------------------ epoch 1601 (9600 steps) ------------------------------------\n",
      "Max loss: 0.05986357480287552\n",
      "Min loss: 0.020794881507754326\n",
      "Mean loss: 0.034950925037264824\n",
      "Std loss: 0.013307767425897397\n",
      "Total Loss: 0.20970555022358894\n",
      "saved model at ./weights/model_1601.pth\n",
      "------------------------------------ epoch 1602 (9606 steps) ------------------------------------\n",
      "Max loss: 0.05604865401983261\n",
      "Min loss: 0.016849834471940994\n",
      "Mean loss: 0.03466687351465225\n",
      "Std loss: 0.013027088515118595\n",
      "Total Loss: 0.2080012410879135\n",
      "------------------------------------ epoch 1603 (9612 steps) ------------------------------------\n",
      "Max loss: 0.050692617893218994\n",
      "Min loss: 0.01840076595544815\n",
      "Mean loss: 0.032072282706697784\n",
      "Std loss: 0.010882353710924253\n",
      "Total Loss: 0.1924336962401867\n",
      "------------------------------------ epoch 1604 (9618 steps) ------------------------------------\n",
      "Max loss: 0.046647362411022186\n",
      "Min loss: 0.01948429085314274\n",
      "Mean loss: 0.03169417226066192\n",
      "Std loss: 0.009095540306675275\n",
      "Total Loss: 0.19016503356397152\n",
      "------------------------------------ epoch 1605 (9624 steps) ------------------------------------\n",
      "Max loss: 0.05202263221144676\n",
      "Min loss: 0.019124403595924377\n",
      "Mean loss: 0.03943414303163687\n",
      "Std loss: 0.010253255519575627\n",
      "Total Loss: 0.23660485818982124\n",
      "------------------------------------ epoch 1606 (9630 steps) ------------------------------------\n",
      "Max loss: 0.027819273993372917\n",
      "Min loss: 0.012448789551854134\n",
      "Mean loss: 0.02215395960956812\n",
      "Std loss: 0.0046918990040652996\n",
      "Total Loss: 0.13292375765740871\n",
      "------------------------------------ epoch 1607 (9636 steps) ------------------------------------\n",
      "Max loss: 0.04203061759471893\n",
      "Min loss: 0.016455214470624924\n",
      "Mean loss: 0.029424352881809075\n",
      "Std loss: 0.00806962340812128\n",
      "Total Loss: 0.17654611729085445\n",
      "------------------------------------ epoch 1608 (9642 steps) ------------------------------------\n",
      "Max loss: 0.06661273539066315\n",
      "Min loss: 0.022039752453565598\n",
      "Mean loss: 0.03438335470855236\n",
      "Std loss: 0.015233096499512714\n",
      "Total Loss: 0.20630012825131416\n",
      "------------------------------------ epoch 1609 (9648 steps) ------------------------------------\n",
      "Max loss: 0.06482608616352081\n",
      "Min loss: 0.017038816586136818\n",
      "Mean loss: 0.02922750326494376\n",
      "Std loss: 0.016900438996995178\n",
      "Total Loss: 0.17536501958966255\n",
      "------------------------------------ epoch 1610 (9654 steps) ------------------------------------\n",
      "Max loss: 0.04505489021539688\n",
      "Min loss: 0.015624841675162315\n",
      "Mean loss: 0.0274703623726964\n",
      "Std loss: 0.00937534760403446\n",
      "Total Loss: 0.1648221742361784\n",
      "------------------------------------ epoch 1611 (9660 steps) ------------------------------------\n",
      "Max loss: 0.06219787150621414\n",
      "Min loss: 0.020101498812437057\n",
      "Mean loss: 0.034501929146548115\n",
      "Std loss: 0.014457019098850854\n",
      "Total Loss: 0.20701157487928867\n",
      "------------------------------------ epoch 1612 (9666 steps) ------------------------------------\n",
      "Max loss: 0.02836136519908905\n",
      "Min loss: 0.01723252423107624\n",
      "Mean loss: 0.023648175410926342\n",
      "Std loss: 0.0037781825858265697\n",
      "Total Loss: 0.14188905246555805\n",
      "------------------------------------ epoch 1613 (9672 steps) ------------------------------------\n",
      "Max loss: 0.044430337846279144\n",
      "Min loss: 0.015586882829666138\n",
      "Mean loss: 0.029787500388920307\n",
      "Std loss: 0.011195419696325488\n",
      "Total Loss: 0.17872500233352184\n",
      "------------------------------------ epoch 1614 (9678 steps) ------------------------------------\n",
      "Max loss: 0.035627249628305435\n",
      "Min loss: 0.01767703890800476\n",
      "Mean loss: 0.024651112345357735\n",
      "Std loss: 0.006152390602821557\n",
      "Total Loss: 0.14790667407214642\n",
      "------------------------------------ epoch 1615 (9684 steps) ------------------------------------\n",
      "Max loss: 0.03398818522691727\n",
      "Min loss: 0.017286628484725952\n",
      "Mean loss: 0.025025764480233192\n",
      "Std loss: 0.005986248598364205\n",
      "Total Loss: 0.15015458688139915\n",
      "------------------------------------ epoch 1616 (9690 steps) ------------------------------------\n",
      "Max loss: 0.06022034212946892\n",
      "Min loss: 0.01171326544135809\n",
      "Mean loss: 0.031049571000039577\n",
      "Std loss: 0.01692837470275273\n",
      "Total Loss: 0.18629742600023746\n",
      "------------------------------------ epoch 1617 (9696 steps) ------------------------------------\n",
      "Max loss: 0.032980646938085556\n",
      "Min loss: 0.017247449606657028\n",
      "Mean loss: 0.023046182778974373\n",
      "Std loss: 0.005157356737045673\n",
      "Total Loss: 0.13827709667384624\n",
      "------------------------------------ epoch 1618 (9702 steps) ------------------------------------\n",
      "Max loss: 0.06225046515464783\n",
      "Min loss: 0.015029994770884514\n",
      "Mean loss: 0.035868383012712\n",
      "Std loss: 0.015201658370124077\n",
      "Total Loss: 0.215210298076272\n",
      "------------------------------------ epoch 1619 (9708 steps) ------------------------------------\n",
      "Max loss: 0.04975203797221184\n",
      "Min loss: 0.01896331086754799\n",
      "Mean loss: 0.031767842980722584\n",
      "Std loss: 0.010654240558988849\n",
      "Total Loss: 0.19060705788433552\n",
      "------------------------------------ epoch 1620 (9714 steps) ------------------------------------\n",
      "Max loss: 0.05843241512775421\n",
      "Min loss: 0.015088187530636787\n",
      "Mean loss: 0.02584310496846835\n",
      "Std loss: 0.015008305076374896\n",
      "Total Loss: 0.1550586298108101\n",
      "------------------------------------ epoch 1621 (9720 steps) ------------------------------------\n",
      "Max loss: 0.05961634963750839\n",
      "Min loss: 0.01997826248407364\n",
      "Mean loss: 0.03552326963593563\n",
      "Std loss: 0.013910228478145052\n",
      "Total Loss: 0.21313961781561375\n",
      "------------------------------------ epoch 1622 (9726 steps) ------------------------------------\n",
      "Max loss: 0.07362353801727295\n",
      "Min loss: 0.019018888473510742\n",
      "Mean loss: 0.03237255569547415\n",
      "Std loss: 0.019145726981499966\n",
      "Total Loss: 0.1942353341728449\n",
      "------------------------------------ epoch 1623 (9732 steps) ------------------------------------\n",
      "Max loss: 0.03055189922451973\n",
      "Min loss: 0.011621884070336819\n",
      "Mean loss: 0.02219596489643057\n",
      "Std loss: 0.007907251564692458\n",
      "Total Loss: 0.13317578937858343\n",
      "------------------------------------ epoch 1624 (9738 steps) ------------------------------------\n",
      "Max loss: 0.04723742604255676\n",
      "Min loss: 0.0146404430270195\n",
      "Mean loss: 0.029983885896702606\n",
      "Std loss: 0.01294135135513797\n",
      "Total Loss: 0.17990331538021564\n",
      "------------------------------------ epoch 1625 (9744 steps) ------------------------------------\n",
      "Max loss: 0.05852155387401581\n",
      "Min loss: 0.015903137624263763\n",
      "Mean loss: 0.03327622854461273\n",
      "Std loss: 0.0127479637130559\n",
      "Total Loss: 0.19965737126767635\n",
      "------------------------------------ epoch 1626 (9750 steps) ------------------------------------\n",
      "Max loss: 0.054975785315036774\n",
      "Min loss: 0.01745806634426117\n",
      "Mean loss: 0.032356238613526024\n",
      "Std loss: 0.012188553169419646\n",
      "Total Loss: 0.19413743168115616\n",
      "------------------------------------ epoch 1627 (9756 steps) ------------------------------------\n",
      "Max loss: 0.04642673581838608\n",
      "Min loss: 0.017083918675780296\n",
      "Mean loss: 0.03207486433287462\n",
      "Std loss: 0.009996513712965279\n",
      "Total Loss: 0.1924491859972477\n",
      "------------------------------------ epoch 1628 (9762 steps) ------------------------------------\n",
      "Max loss: 0.0264179315418005\n",
      "Min loss: 0.013534897938370705\n",
      "Mean loss: 0.019918497341374557\n",
      "Std loss: 0.005117156923299232\n",
      "Total Loss: 0.11951098404824734\n",
      "------------------------------------ epoch 1629 (9768 steps) ------------------------------------\n",
      "Max loss: 0.04390221834182739\n",
      "Min loss: 0.019006503745913506\n",
      "Mean loss: 0.027754062476257484\n",
      "Std loss: 0.008524746012235254\n",
      "Total Loss: 0.1665243748575449\n",
      "------------------------------------ epoch 1630 (9774 steps) ------------------------------------\n",
      "Max loss: 0.056982167065143585\n",
      "Min loss: 0.012928061187267303\n",
      "Mean loss: 0.030245596542954445\n",
      "Std loss: 0.016701336530174568\n",
      "Total Loss: 0.18147357925772667\n",
      "------------------------------------ epoch 1631 (9780 steps) ------------------------------------\n",
      "Max loss: 0.07268176972866058\n",
      "Min loss: 0.01707930862903595\n",
      "Mean loss: 0.030323981617887814\n",
      "Std loss: 0.019532328826606726\n",
      "Total Loss: 0.1819438897073269\n",
      "------------------------------------ epoch 1632 (9786 steps) ------------------------------------\n",
      "Max loss: 0.046767257153987885\n",
      "Min loss: 0.019220974296331406\n",
      "Mean loss: 0.02975097205489874\n",
      "Std loss: 0.012122220955433579\n",
      "Total Loss: 0.17850583232939243\n",
      "------------------------------------ epoch 1633 (9792 steps) ------------------------------------\n",
      "Max loss: 0.06571652740240097\n",
      "Min loss: 0.013763240538537502\n",
      "Mean loss: 0.03109787544235587\n",
      "Std loss: 0.017680360581010624\n",
      "Total Loss: 0.18658725265413523\n",
      "------------------------------------ epoch 1634 (9798 steps) ------------------------------------\n",
      "Max loss: 0.054967381060123444\n",
      "Min loss: 0.014184891246259212\n",
      "Mean loss: 0.034998617911090456\n",
      "Std loss: 0.012103053430882877\n",
      "Total Loss: 0.20999170746654272\n",
      "------------------------------------ epoch 1635 (9804 steps) ------------------------------------\n",
      "Max loss: 0.06704830378293991\n",
      "Min loss: 0.013294907286763191\n",
      "Mean loss: 0.03252252067128817\n",
      "Std loss: 0.017585222689393068\n",
      "Total Loss: 0.19513512402772903\n",
      "------------------------------------ epoch 1636 (9810 steps) ------------------------------------\n",
      "Max loss: 0.036387212574481964\n",
      "Min loss: 0.018379410728812218\n",
      "Mean loss: 0.027321743468443554\n",
      "Std loss: 0.0060893388335974035\n",
      "Total Loss: 0.16393046081066132\n",
      "------------------------------------ epoch 1637 (9816 steps) ------------------------------------\n",
      "Max loss: 0.04514523968100548\n",
      "Min loss: 0.014220908284187317\n",
      "Mean loss: 0.02667972957715392\n",
      "Std loss: 0.01164353950662947\n",
      "Total Loss: 0.16007837746292353\n",
      "------------------------------------ epoch 1638 (9822 steps) ------------------------------------\n",
      "Max loss: 0.0413973405957222\n",
      "Min loss: 0.014972856268286705\n",
      "Mean loss: 0.021749830183883507\n",
      "Std loss: 0.009615031024293496\n",
      "Total Loss: 0.13049898110330105\n",
      "------------------------------------ epoch 1639 (9828 steps) ------------------------------------\n",
      "Max loss: 0.03642221912741661\n",
      "Min loss: 0.0194068755954504\n",
      "Mean loss: 0.02915634245922168\n",
      "Std loss: 0.00580016144172035\n",
      "Total Loss: 0.17493805475533009\n",
      "------------------------------------ epoch 1640 (9834 steps) ------------------------------------\n",
      "Max loss: 0.05360569804906845\n",
      "Min loss: 0.012004426680505276\n",
      "Mean loss: 0.036490104937305055\n",
      "Std loss: 0.012630189574359458\n",
      "Total Loss: 0.21894062962383032\n",
      "------------------------------------ epoch 1641 (9840 steps) ------------------------------------\n",
      "Max loss: 0.035324692726135254\n",
      "Min loss: 0.022487126290798187\n",
      "Mean loss: 0.02785688576598962\n",
      "Std loss: 0.0046900377763591135\n",
      "Total Loss: 0.16714131459593773\n",
      "------------------------------------ epoch 1642 (9846 steps) ------------------------------------\n",
      "Max loss: 0.03795235604047775\n",
      "Min loss: 0.01782314106822014\n",
      "Mean loss: 0.028835080564022064\n",
      "Std loss: 0.007847695754983109\n",
      "Total Loss: 0.17301048338413239\n",
      "------------------------------------ epoch 1643 (9852 steps) ------------------------------------\n",
      "Max loss: 0.025820400565862656\n",
      "Min loss: 0.015707457438111305\n",
      "Mean loss: 0.021653451025485992\n",
      "Std loss: 0.0031401046654571186\n",
      "Total Loss: 0.12992070615291595\n",
      "------------------------------------ epoch 1644 (9858 steps) ------------------------------------\n",
      "Max loss: 0.06128969043493271\n",
      "Min loss: 0.018249481916427612\n",
      "Mean loss: 0.029085684878130753\n",
      "Std loss: 0.015445122896877972\n",
      "Total Loss: 0.17451410926878452\n",
      "------------------------------------ epoch 1645 (9864 steps) ------------------------------------\n",
      "Max loss: 0.036706920713186264\n",
      "Min loss: 0.01719319447875023\n",
      "Mean loss: 0.02637218652913968\n",
      "Std loss: 0.00699252767954847\n",
      "Total Loss: 0.15823311917483807\n",
      "------------------------------------ epoch 1646 (9870 steps) ------------------------------------\n",
      "Max loss: 0.04221078380942345\n",
      "Min loss: 0.015150526538491249\n",
      "Mean loss: 0.029635642965634663\n",
      "Std loss: 0.008777283670081798\n",
      "Total Loss: 0.17781385779380798\n",
      "------------------------------------ epoch 1647 (9876 steps) ------------------------------------\n",
      "Max loss: 0.038359738886356354\n",
      "Min loss: 0.012615303508937359\n",
      "Mean loss: 0.024683775535474222\n",
      "Std loss: 0.009107494569425747\n",
      "Total Loss: 0.14810265321284533\n",
      "------------------------------------ epoch 1648 (9882 steps) ------------------------------------\n",
      "Max loss: 0.04578259587287903\n",
      "Min loss: 0.01520856749266386\n",
      "Mean loss: 0.03102300176396966\n",
      "Std loss: 0.009655151943349979\n",
      "Total Loss: 0.18613801058381796\n",
      "------------------------------------ epoch 1649 (9888 steps) ------------------------------------\n",
      "Max loss: 0.031066101044416428\n",
      "Min loss: 0.012672106735408306\n",
      "Mean loss: 0.024279710991928976\n",
      "Std loss: 0.0062396659520573975\n",
      "Total Loss: 0.14567826595157385\n",
      "------------------------------------ epoch 1650 (9894 steps) ------------------------------------\n",
      "Max loss: 0.057733215391635895\n",
      "Min loss: 0.014514155685901642\n",
      "Mean loss: 0.03843581459174553\n",
      "Std loss: 0.014627866956879398\n",
      "Total Loss: 0.2306148875504732\n",
      "------------------------------------ epoch 1651 (9900 steps) ------------------------------------\n",
      "Max loss: 0.047336362302303314\n",
      "Min loss: 0.016087763011455536\n",
      "Mean loss: 0.02735960328330596\n",
      "Std loss: 0.01087671242128905\n",
      "Total Loss: 0.16415761969983578\n",
      "------------------------------------ epoch 1652 (9906 steps) ------------------------------------\n",
      "Max loss: 0.05098019167780876\n",
      "Min loss: 0.01994449459016323\n",
      "Mean loss: 0.028374680317938328\n",
      "Std loss: 0.010368654726461883\n",
      "Total Loss: 0.17024808190762997\n",
      "------------------------------------ epoch 1653 (9912 steps) ------------------------------------\n",
      "Max loss: 0.04809033125638962\n",
      "Min loss: 0.01432684063911438\n",
      "Mean loss: 0.02814553181330363\n",
      "Std loss: 0.011824561191946572\n",
      "Total Loss: 0.16887319087982178\n",
      "------------------------------------ epoch 1654 (9918 steps) ------------------------------------\n",
      "Max loss: 0.04851282760500908\n",
      "Min loss: 0.029575664550065994\n",
      "Mean loss: 0.03872506351520618\n",
      "Std loss: 0.007462046886298452\n",
      "Total Loss: 0.23235038109123707\n",
      "------------------------------------ epoch 1655 (9924 steps) ------------------------------------\n",
      "Max loss: 0.0371280238032341\n",
      "Min loss: 0.013080800883471966\n",
      "Mean loss: 0.02344009916608532\n",
      "Std loss: 0.007939460782748502\n",
      "Total Loss: 0.14064059499651194\n",
      "------------------------------------ epoch 1656 (9930 steps) ------------------------------------\n",
      "Max loss: 0.04283764585852623\n",
      "Min loss: 0.013462411239743233\n",
      "Mean loss: 0.024934366966287296\n",
      "Std loss: 0.009737017103841736\n",
      "Total Loss: 0.14960620179772377\n",
      "------------------------------------ epoch 1657 (9936 steps) ------------------------------------\n",
      "Max loss: 0.048403508961200714\n",
      "Min loss: 0.01519415806978941\n",
      "Mean loss: 0.03014599299058318\n",
      "Std loss: 0.0114985417182513\n",
      "Total Loss: 0.1808759579434991\n",
      "------------------------------------ epoch 1658 (9942 steps) ------------------------------------\n",
      "Max loss: 0.06198466941714287\n",
      "Min loss: 0.01684989407658577\n",
      "Mean loss: 0.03556704117606083\n",
      "Std loss: 0.013575393825773706\n",
      "Total Loss: 0.213402247056365\n",
      "------------------------------------ epoch 1659 (9948 steps) ------------------------------------\n",
      "Max loss: 0.02756565622985363\n",
      "Min loss: 0.01679028756916523\n",
      "Mean loss: 0.021987037733197212\n",
      "Std loss: 0.003434067014898228\n",
      "Total Loss: 0.13192222639918327\n",
      "------------------------------------ epoch 1660 (9954 steps) ------------------------------------\n",
      "Max loss: 0.09617799520492554\n",
      "Min loss: 0.013074307702481747\n",
      "Mean loss: 0.03240443595374624\n",
      "Std loss: 0.028967111588510022\n",
      "Total Loss: 0.19442661572247744\n",
      "------------------------------------ epoch 1661 (9960 steps) ------------------------------------\n",
      "Max loss: 0.05229286476969719\n",
      "Min loss: 0.02480756863951683\n",
      "Mean loss: 0.03922340211768945\n",
      "Std loss: 0.009188576091566435\n",
      "Total Loss: 0.2353404127061367\n",
      "------------------------------------ epoch 1662 (9966 steps) ------------------------------------\n",
      "Max loss: 0.039484553039073944\n",
      "Min loss: 0.015093494206666946\n",
      "Mean loss: 0.02733371251573165\n",
      "Std loss: 0.009124791435258222\n",
      "Total Loss: 0.16400227509438992\n",
      "------------------------------------ epoch 1663 (9972 steps) ------------------------------------\n",
      "Max loss: 0.04460752010345459\n",
      "Min loss: 0.01591326668858528\n",
      "Mean loss: 0.02918325737118721\n",
      "Std loss: 0.010906363447969645\n",
      "Total Loss: 0.17509954422712326\n",
      "------------------------------------ epoch 1664 (9978 steps) ------------------------------------\n",
      "Max loss: 0.07534383982419968\n",
      "Min loss: 0.017818545922636986\n",
      "Mean loss: 0.039509123812119164\n",
      "Std loss: 0.018732118537347464\n",
      "Total Loss: 0.237054742872715\n",
      "------------------------------------ epoch 1665 (9984 steps) ------------------------------------\n",
      "Max loss: 0.05479170009493828\n",
      "Min loss: 0.029710441827774048\n",
      "Mean loss: 0.04171378227571646\n",
      "Std loss: 0.008648086660905992\n",
      "Total Loss: 0.2502826936542988\n",
      "------------------------------------ epoch 1666 (9990 steps) ------------------------------------\n",
      "Max loss: 0.15267711877822876\n",
      "Min loss: 0.06620993465185165\n",
      "Mean loss: 0.09099185342590015\n",
      "Std loss: 0.029315899845163994\n",
      "Total Loss: 0.5459511205554008\n",
      "------------------------------------ epoch 1667 (9996 steps) ------------------------------------\n",
      "Max loss: 0.08269581943750381\n",
      "Min loss: 0.045454107224941254\n",
      "Mean loss: 0.06397740170359612\n",
      "Std loss: 0.011314939607521588\n",
      "Total Loss: 0.3838644102215767\n",
      "------------------------------------ epoch 1668 (10002 steps) ------------------------------------\n",
      "Max loss: 0.06548477709293365\n",
      "Min loss: 0.0400349386036396\n",
      "Mean loss: 0.053752245381474495\n",
      "Std loss: 0.00880951194991044\n",
      "Total Loss: 0.32251347228884697\n",
      "------------------------------------ epoch 1669 (10008 steps) ------------------------------------\n",
      "Max loss: 0.06532222777605057\n",
      "Min loss: 0.027604594826698303\n",
      "Mean loss: 0.04208095744252205\n",
      "Std loss: 0.012605411664262066\n",
      "Total Loss: 0.2524857446551323\n",
      "------------------------------------ epoch 1670 (10014 steps) ------------------------------------\n",
      "Max loss: 0.05844463035464287\n",
      "Min loss: 0.027280792593955994\n",
      "Mean loss: 0.044664040207862854\n",
      "Std loss: 0.010821028303083343\n",
      "Total Loss: 0.2679842412471771\n",
      "------------------------------------ epoch 1671 (10020 steps) ------------------------------------\n",
      "Max loss: 0.0798192247748375\n",
      "Min loss: 0.022823384031653404\n",
      "Mean loss: 0.04086804141600927\n",
      "Std loss: 0.01938068965207094\n",
      "Total Loss: 0.2452082484960556\n",
      "------------------------------------ epoch 1672 (10026 steps) ------------------------------------\n",
      "Max loss: 0.057806890457868576\n",
      "Min loss: 0.024268588051199913\n",
      "Mean loss: 0.040554254626234375\n",
      "Std loss: 0.01363990536609138\n",
      "Total Loss: 0.24332552775740623\n",
      "------------------------------------ epoch 1673 (10032 steps) ------------------------------------\n",
      "Max loss: 0.05527934432029724\n",
      "Min loss: 0.024807726964354515\n",
      "Mean loss: 0.03660511691123247\n",
      "Std loss: 0.009404233796223328\n",
      "Total Loss: 0.21963070146739483\n",
      "------------------------------------ epoch 1674 (10038 steps) ------------------------------------\n",
      "Max loss: 0.04313778504729271\n",
      "Min loss: 0.020655635744333267\n",
      "Mean loss: 0.033123441661397614\n",
      "Std loss: 0.008928183035471306\n",
      "Total Loss: 0.1987406499683857\n",
      "------------------------------------ epoch 1675 (10044 steps) ------------------------------------\n",
      "Max loss: 0.07439042627811432\n",
      "Min loss: 0.016550574451684952\n",
      "Mean loss: 0.03412898598859707\n",
      "Std loss: 0.019383078521167416\n",
      "Total Loss: 0.20477391593158245\n",
      "------------------------------------ epoch 1676 (10050 steps) ------------------------------------\n",
      "Max loss: 0.06678915023803711\n",
      "Min loss: 0.014518586918711662\n",
      "Mean loss: 0.03338923087964455\n",
      "Std loss: 0.017821032748509637\n",
      "Total Loss: 0.20033538527786732\n",
      "------------------------------------ epoch 1677 (10056 steps) ------------------------------------\n",
      "Max loss: 0.052548691630363464\n",
      "Min loss: 0.016632601618766785\n",
      "Mean loss: 0.02761260513216257\n",
      "Std loss: 0.011845076324676953\n",
      "Total Loss: 0.16567563079297543\n",
      "------------------------------------ epoch 1678 (10062 steps) ------------------------------------\n",
      "Max loss: 0.023945074528455734\n",
      "Min loss: 0.015482179820537567\n",
      "Mean loss: 0.01982861819366614\n",
      "Std loss: 0.003442532915353613\n",
      "Total Loss: 0.11897170916199684\n",
      "------------------------------------ epoch 1679 (10068 steps) ------------------------------------\n",
      "Max loss: 0.09134216606616974\n",
      "Min loss: 0.016597699373960495\n",
      "Mean loss: 0.03536473028361797\n",
      "Std loss: 0.026235574464401088\n",
      "Total Loss: 0.21218838170170784\n",
      "------------------------------------ epoch 1680 (10074 steps) ------------------------------------\n",
      "Max loss: 0.04807281494140625\n",
      "Min loss: 0.013229750096797943\n",
      "Mean loss: 0.027861887278656166\n",
      "Std loss: 0.01061812991222646\n",
      "Total Loss: 0.167171323671937\n",
      "------------------------------------ epoch 1681 (10080 steps) ------------------------------------\n",
      "Max loss: 0.048419564962387085\n",
      "Min loss: 0.022465914487838745\n",
      "Mean loss: 0.03371461356679598\n",
      "Std loss: 0.010858422273788308\n",
      "Total Loss: 0.2022876814007759\n",
      "------------------------------------ epoch 1682 (10086 steps) ------------------------------------\n",
      "Max loss: 0.05937886983156204\n",
      "Min loss: 0.01681503653526306\n",
      "Mean loss: 0.033677914179861546\n",
      "Std loss: 0.014541292066345819\n",
      "Total Loss: 0.20206748507916927\n",
      "------------------------------------ epoch 1683 (10092 steps) ------------------------------------\n",
      "Max loss: 0.0385294072329998\n",
      "Min loss: 0.014509948901832104\n",
      "Mean loss: 0.02263405593112111\n",
      "Std loss: 0.007570558653945608\n",
      "Total Loss: 0.13580433558672667\n",
      "------------------------------------ epoch 1684 (10098 steps) ------------------------------------\n",
      "Max loss: 0.07157357037067413\n",
      "Min loss: 0.023332174867391586\n",
      "Mean loss: 0.044372703259189926\n",
      "Std loss: 0.014565771421873569\n",
      "Total Loss: 0.26623621955513954\n",
      "------------------------------------ epoch 1685 (10104 steps) ------------------------------------\n",
      "Max loss: 0.030094534158706665\n",
      "Min loss: 0.018658790737390518\n",
      "Mean loss: 0.024390570508937042\n",
      "Std loss: 0.0038184599741252615\n",
      "Total Loss: 0.14634342305362225\n",
      "------------------------------------ epoch 1686 (10110 steps) ------------------------------------\n",
      "Max loss: 0.05053011700510979\n",
      "Min loss: 0.011998842470347881\n",
      "Mean loss: 0.030122024783243734\n",
      "Std loss: 0.012866512384398447\n",
      "Total Loss: 0.1807321486994624\n",
      "------------------------------------ epoch 1687 (10116 steps) ------------------------------------\n",
      "Max loss: 0.08452880382537842\n",
      "Min loss: 0.012563961558043957\n",
      "Mean loss: 0.03434902258838216\n",
      "Std loss: 0.02393701534400348\n",
      "Total Loss: 0.206094135530293\n",
      "------------------------------------ epoch 1688 (10122 steps) ------------------------------------\n",
      "Max loss: 0.066648468375206\n",
      "Min loss: 0.026197653263807297\n",
      "Mean loss: 0.05178725719451904\n",
      "Std loss: 0.015510631985984576\n",
      "Total Loss: 0.31072354316711426\n",
      "------------------------------------ epoch 1689 (10128 steps) ------------------------------------\n",
      "Max loss: 0.0731658861041069\n",
      "Min loss: 0.019044682383537292\n",
      "Mean loss: 0.03253714647144079\n",
      "Std loss: 0.018576726952309882\n",
      "Total Loss: 0.19522287882864475\n",
      "------------------------------------ epoch 1690 (10134 steps) ------------------------------------\n",
      "Max loss: 0.07417856156826019\n",
      "Min loss: 0.020286403596401215\n",
      "Mean loss: 0.04090553025404612\n",
      "Std loss: 0.017280460372884825\n",
      "Total Loss: 0.24543318152427673\n",
      "------------------------------------ epoch 1691 (10140 steps) ------------------------------------\n",
      "Max loss: 0.037430472671985626\n",
      "Min loss: 0.014909712597727776\n",
      "Mean loss: 0.02613461824754874\n",
      "Std loss: 0.00805499233953781\n",
      "Total Loss: 0.15680770948529243\n",
      "------------------------------------ epoch 1692 (10146 steps) ------------------------------------\n",
      "Max loss: 0.043056707829236984\n",
      "Min loss: 0.015466755256056786\n",
      "Mean loss: 0.031062885498007137\n",
      "Std loss: 0.010355979228109621\n",
      "Total Loss: 0.18637731298804283\n",
      "------------------------------------ epoch 1693 (10152 steps) ------------------------------------\n",
      "Max loss: 0.03703983128070831\n",
      "Min loss: 0.014773322269320488\n",
      "Mean loss: 0.02769699351241191\n",
      "Std loss: 0.008103788589265831\n",
      "Total Loss: 0.16618196107447147\n",
      "------------------------------------ epoch 1694 (10158 steps) ------------------------------------\n",
      "Max loss: 0.053775202482938766\n",
      "Min loss: 0.017459403723478317\n",
      "Mean loss: 0.034280056754748024\n",
      "Std loss: 0.0140853566302993\n",
      "Total Loss: 0.20568034052848816\n",
      "------------------------------------ epoch 1695 (10164 steps) ------------------------------------\n",
      "Max loss: 0.04061213880777359\n",
      "Min loss: 0.022437309846282005\n",
      "Mean loss: 0.03026528221865495\n",
      "Std loss: 0.006176137993854019\n",
      "Total Loss: 0.1815916933119297\n",
      "------------------------------------ epoch 1696 (10170 steps) ------------------------------------\n",
      "Max loss: 0.030481426045298576\n",
      "Min loss: 0.017819244414567947\n",
      "Mean loss: 0.02538946581383546\n",
      "Std loss: 0.004333419575940786\n",
      "Total Loss: 0.15233679488301277\n",
      "------------------------------------ epoch 1697 (10176 steps) ------------------------------------\n",
      "Max loss: 0.06336194276809692\n",
      "Min loss: 0.01377793401479721\n",
      "Mean loss: 0.040154307459791504\n",
      "Std loss: 0.018353804969659916\n",
      "Total Loss: 0.240925844758749\n",
      "------------------------------------ epoch 1698 (10182 steps) ------------------------------------\n",
      "Max loss: 0.0369773730635643\n",
      "Min loss: 0.01664567179977894\n",
      "Mean loss: 0.028316195122897625\n",
      "Std loss: 0.007299187979467204\n",
      "Total Loss: 0.16989717073738575\n",
      "------------------------------------ epoch 1699 (10188 steps) ------------------------------------\n",
      "Max loss: 0.09114746749401093\n",
      "Min loss: 0.02589929662644863\n",
      "Mean loss: 0.047009123799701534\n",
      "Std loss: 0.021424318898048085\n",
      "Total Loss: 0.2820547427982092\n",
      "------------------------------------ epoch 1700 (10194 steps) ------------------------------------\n",
      "Max loss: 0.06056724488735199\n",
      "Min loss: 0.01944718137383461\n",
      "Mean loss: 0.03558008745312691\n",
      "Std loss: 0.013224586191883774\n",
      "Total Loss: 0.21348052471876144\n",
      "------------------------------------ epoch 1701 (10200 steps) ------------------------------------\n",
      "Max loss: 0.05171607434749603\n",
      "Min loss: 0.022272445261478424\n",
      "Mean loss: 0.029206580792864163\n",
      "Std loss: 0.010219036328365745\n",
      "Total Loss: 0.17523948475718498\n",
      "saved model at ./weights/model_1701.pth\n",
      "------------------------------------ epoch 1702 (10206 steps) ------------------------------------\n",
      "Max loss: 0.04929640144109726\n",
      "Min loss: 0.018071142956614494\n",
      "Mean loss: 0.028814711297551792\n",
      "Std loss: 0.01012539017419189\n",
      "Total Loss: 0.17288826778531075\n",
      "------------------------------------ epoch 1703 (10212 steps) ------------------------------------\n",
      "Max loss: 0.09323474019765854\n",
      "Min loss: 0.02300507389008999\n",
      "Mean loss: 0.049362821194032826\n",
      "Std loss: 0.026296662512020428\n",
      "Total Loss: 0.29617692716419697\n",
      "------------------------------------ epoch 1704 (10218 steps) ------------------------------------\n",
      "Max loss: 0.04528716951608658\n",
      "Min loss: 0.02646361105144024\n",
      "Mean loss: 0.035798447827498116\n",
      "Std loss: 0.006917892966942404\n",
      "Total Loss: 0.2147906869649887\n",
      "------------------------------------ epoch 1705 (10224 steps) ------------------------------------\n",
      "Max loss: 0.05257515609264374\n",
      "Min loss: 0.02053717151284218\n",
      "Mean loss: 0.03342266070346037\n",
      "Std loss: 0.011130030586392914\n",
      "Total Loss: 0.20053596422076225\n",
      "------------------------------------ epoch 1706 (10230 steps) ------------------------------------\n",
      "Max loss: 0.04826047271490097\n",
      "Min loss: 0.016925208270549774\n",
      "Mean loss: 0.032902391937871776\n",
      "Std loss: 0.010285639413133279\n",
      "Total Loss: 0.19741435162723064\n",
      "------------------------------------ epoch 1707 (10236 steps) ------------------------------------\n",
      "Max loss: 0.04233983904123306\n",
      "Min loss: 0.015968231484293938\n",
      "Mean loss: 0.027012291364371777\n",
      "Std loss: 0.010358959223654289\n",
      "Total Loss: 0.16207374818623066\n",
      "------------------------------------ epoch 1708 (10242 steps) ------------------------------------\n",
      "Max loss: 0.04071810841560364\n",
      "Min loss: 0.015182361006736755\n",
      "Mean loss: 0.027322406880557537\n",
      "Std loss: 0.008522113205414598\n",
      "Total Loss: 0.16393444128334522\n",
      "------------------------------------ epoch 1709 (10248 steps) ------------------------------------\n",
      "Max loss: 0.05041484162211418\n",
      "Min loss: 0.01857469603419304\n",
      "Mean loss: 0.03502069506794214\n",
      "Std loss: 0.012574611534142289\n",
      "Total Loss: 0.21012417040765285\n",
      "------------------------------------ epoch 1710 (10254 steps) ------------------------------------\n",
      "Max loss: 0.05528757721185684\n",
      "Min loss: 0.012007041834294796\n",
      "Mean loss: 0.029248014092445374\n",
      "Std loss: 0.014985582374746405\n",
      "Total Loss: 0.17548808455467224\n",
      "------------------------------------ epoch 1711 (10260 steps) ------------------------------------\n",
      "Max loss: 0.03391613811254501\n",
      "Min loss: 0.014011813327670097\n",
      "Mean loss: 0.025205842219293118\n",
      "Std loss: 0.007852048142887275\n",
      "Total Loss: 0.1512350533157587\n",
      "------------------------------------ epoch 1712 (10266 steps) ------------------------------------\n",
      "Max loss: 0.03991329297423363\n",
      "Min loss: 0.01906682178378105\n",
      "Mean loss: 0.028844917193055153\n",
      "Std loss: 0.006592248574428698\n",
      "Total Loss: 0.17306950315833092\n",
      "------------------------------------ epoch 1713 (10272 steps) ------------------------------------\n",
      "Max loss: 0.046779122203588486\n",
      "Min loss: 0.029825866222381592\n",
      "Mean loss: 0.03893269101778666\n",
      "Std loss: 0.006003757497798786\n",
      "Total Loss: 0.23359614610671997\n",
      "------------------------------------ epoch 1714 (10278 steps) ------------------------------------\n",
      "Max loss: 0.04480975121259689\n",
      "Min loss: 0.015199868008494377\n",
      "Mean loss: 0.02683512183527152\n",
      "Std loss: 0.010568945893359534\n",
      "Total Loss: 0.1610107310116291\n",
      "------------------------------------ epoch 1715 (10284 steps) ------------------------------------\n",
      "Max loss: 0.034463364630937576\n",
      "Min loss: 0.015336751006543636\n",
      "Mean loss: 0.024992368028809626\n",
      "Std loss: 0.007076517726772621\n",
      "Total Loss: 0.14995420817285776\n",
      "------------------------------------ epoch 1716 (10290 steps) ------------------------------------\n",
      "Max loss: 0.030769599601626396\n",
      "Min loss: 0.015784267336130142\n",
      "Mean loss: 0.023967894725501537\n",
      "Std loss: 0.005682176705749413\n",
      "Total Loss: 0.14380736835300922\n",
      "------------------------------------ epoch 1717 (10296 steps) ------------------------------------\n",
      "Max loss: 0.053770795464515686\n",
      "Min loss: 0.019490733742713928\n",
      "Mean loss: 0.03465772016594807\n",
      "Std loss: 0.013674147755503697\n",
      "Total Loss: 0.20794632099568844\n",
      "------------------------------------ epoch 1718 (10302 steps) ------------------------------------\n",
      "Max loss: 0.03213819861412048\n",
      "Min loss: 0.01538442075252533\n",
      "Mean loss: 0.02278856684764226\n",
      "Std loss: 0.005285818908832963\n",
      "Total Loss: 0.13673140108585358\n",
      "------------------------------------ epoch 1719 (10308 steps) ------------------------------------\n",
      "Max loss: 0.03056170605123043\n",
      "Min loss: 0.01493290439248085\n",
      "Mean loss: 0.02165305366118749\n",
      "Std loss: 0.005899757084790894\n",
      "Total Loss: 0.12991832196712494\n",
      "------------------------------------ epoch 1720 (10314 steps) ------------------------------------\n",
      "Max loss: 0.024304524064064026\n",
      "Min loss: 0.011043496429920197\n",
      "Mean loss: 0.01642574369907379\n",
      "Std loss: 0.004691377998101516\n",
      "Total Loss: 0.09855446219444275\n",
      "------------------------------------ epoch 1721 (10320 steps) ------------------------------------\n",
      "Max loss: 0.02734992653131485\n",
      "Min loss: 0.013343061320483685\n",
      "Mean loss: 0.022390855010598898\n",
      "Std loss: 0.004623537723532467\n",
      "Total Loss: 0.1343451300635934\n",
      "------------------------------------ epoch 1722 (10326 steps) ------------------------------------\n",
      "Max loss: 0.04248986393213272\n",
      "Min loss: 0.014913039281964302\n",
      "Mean loss: 0.027281001520653565\n",
      "Std loss: 0.00926329151831961\n",
      "Total Loss: 0.1636860091239214\n",
      "------------------------------------ epoch 1723 (10332 steps) ------------------------------------\n",
      "Max loss: 0.033961813896894455\n",
      "Min loss: 0.018451232463121414\n",
      "Mean loss: 0.0242294588436683\n",
      "Std loss: 0.005652527689695839\n",
      "Total Loss: 0.1453767530620098\n",
      "------------------------------------ epoch 1724 (10338 steps) ------------------------------------\n",
      "Max loss: 0.04313720762729645\n",
      "Min loss: 0.024190930649638176\n",
      "Mean loss: 0.031106580669681232\n",
      "Std loss: 0.005951220143837835\n",
      "Total Loss: 0.1866394840180874\n",
      "------------------------------------ epoch 1725 (10344 steps) ------------------------------------\n",
      "Max loss: 0.08925268054008484\n",
      "Min loss: 0.01516387052834034\n",
      "Mean loss: 0.03166043292731047\n",
      "Std loss: 0.025923667723444746\n",
      "Total Loss: 0.1899625975638628\n",
      "------------------------------------ epoch 1726 (10350 steps) ------------------------------------\n",
      "Max loss: 0.048836808651685715\n",
      "Min loss: 0.01277121715247631\n",
      "Mean loss: 0.028058914778133232\n",
      "Std loss: 0.013020969212229033\n",
      "Total Loss: 0.1683534886687994\n",
      "------------------------------------ epoch 1727 (10356 steps) ------------------------------------\n",
      "Max loss: 0.05514872074127197\n",
      "Min loss: 0.014507098123431206\n",
      "Mean loss: 0.03230084323634704\n",
      "Std loss: 0.014504765874774263\n",
      "Total Loss: 0.19380505941808224\n",
      "------------------------------------ epoch 1728 (10362 steps) ------------------------------------\n",
      "Max loss: 0.06500270962715149\n",
      "Min loss: 0.036842796951532364\n",
      "Mean loss: 0.046116466944416366\n",
      "Std loss: 0.01024215500770579\n",
      "Total Loss: 0.2766988016664982\n",
      "------------------------------------ epoch 1729 (10368 steps) ------------------------------------\n",
      "Max loss: 0.03137287497520447\n",
      "Min loss: 0.01655822992324829\n",
      "Mean loss: 0.025881717291971047\n",
      "Std loss: 0.005250596656633102\n",
      "Total Loss: 0.1552903037518263\n",
      "------------------------------------ epoch 1730 (10374 steps) ------------------------------------\n",
      "Max loss: 0.03599732741713524\n",
      "Min loss: 0.01490530651062727\n",
      "Mean loss: 0.024992193871488173\n",
      "Std loss: 0.008055853619815435\n",
      "Total Loss: 0.14995316322892904\n",
      "------------------------------------ epoch 1731 (10380 steps) ------------------------------------\n",
      "Max loss: 0.043253134936094284\n",
      "Min loss: 0.02258993498980999\n",
      "Mean loss: 0.028663587135573227\n",
      "Std loss: 0.006950429377274194\n",
      "Total Loss: 0.17198152281343937\n",
      "------------------------------------ epoch 1732 (10386 steps) ------------------------------------\n",
      "Max loss: 0.03075791895389557\n",
      "Min loss: 0.013021953403949738\n",
      "Mean loss: 0.0220488424723347\n",
      "Std loss: 0.006031111505325175\n",
      "Total Loss: 0.13229305483400822\n",
      "------------------------------------ epoch 1733 (10392 steps) ------------------------------------\n",
      "Max loss: 0.05757339298725128\n",
      "Min loss: 0.017636723816394806\n",
      "Mean loss: 0.03579889206836621\n",
      "Std loss: 0.014788355283126718\n",
      "Total Loss: 0.21479335241019726\n",
      "------------------------------------ epoch 1734 (10398 steps) ------------------------------------\n",
      "Max loss: 0.040208540856838226\n",
      "Min loss: 0.018662212416529655\n",
      "Mean loss: 0.025745018074909847\n",
      "Std loss: 0.007768785556806581\n",
      "Total Loss: 0.15447010844945908\n",
      "------------------------------------ epoch 1735 (10404 steps) ------------------------------------\n",
      "Max loss: 0.07015763968229294\n",
      "Min loss: 0.015694934874773026\n",
      "Mean loss: 0.038764151434103646\n",
      "Std loss: 0.017398983585272847\n",
      "Total Loss: 0.2325849086046219\n",
      "------------------------------------ epoch 1736 (10410 steps) ------------------------------------\n",
      "Max loss: 0.09272047132253647\n",
      "Min loss: 0.01658177562057972\n",
      "Mean loss: 0.0339696683610479\n",
      "Std loss: 0.02646057507746289\n",
      "Total Loss: 0.20381801016628742\n",
      "------------------------------------ epoch 1737 (10416 steps) ------------------------------------\n",
      "Max loss: 0.067227803170681\n",
      "Min loss: 0.019523007795214653\n",
      "Mean loss: 0.04462846275418997\n",
      "Std loss: 0.01683770136700363\n",
      "Total Loss: 0.2677707765251398\n",
      "------------------------------------ epoch 1738 (10422 steps) ------------------------------------\n",
      "Max loss: 0.04261336475610733\n",
      "Min loss: 0.015041254460811615\n",
      "Mean loss: 0.024322357960045338\n",
      "Std loss: 0.009388955148085099\n",
      "Total Loss: 0.14593414776027203\n",
      "------------------------------------ epoch 1739 (10428 steps) ------------------------------------\n",
      "Max loss: 0.0392116978764534\n",
      "Min loss: 0.01736927404999733\n",
      "Mean loss: 0.02829667491217454\n",
      "Std loss: 0.008388685580564225\n",
      "Total Loss: 0.16978004947304726\n",
      "------------------------------------ epoch 1740 (10434 steps) ------------------------------------\n",
      "Max loss: 0.07743912190198898\n",
      "Min loss: 0.016616113483905792\n",
      "Mean loss: 0.03664655021081368\n",
      "Std loss: 0.020860782666606594\n",
      "Total Loss: 0.2198793012648821\n",
      "------------------------------------ epoch 1741 (10440 steps) ------------------------------------\n",
      "Max loss: 0.03585191071033478\n",
      "Min loss: 0.0191630981862545\n",
      "Mean loss: 0.026085267464319866\n",
      "Std loss: 0.00508311170671562\n",
      "Total Loss: 0.1565116047859192\n",
      "------------------------------------ epoch 1742 (10446 steps) ------------------------------------\n",
      "Max loss: 0.038074541836977005\n",
      "Min loss: 0.014652838930487633\n",
      "Mean loss: 0.02216494580109914\n",
      "Std loss: 0.007943779411729412\n",
      "Total Loss: 0.13298967480659485\n",
      "------------------------------------ epoch 1743 (10452 steps) ------------------------------------\n",
      "Max loss: 0.026804644614458084\n",
      "Min loss: 0.01859278976917267\n",
      "Mean loss: 0.022750420185426872\n",
      "Std loss: 0.0031107307188225197\n",
      "Total Loss: 0.13650252111256123\n",
      "------------------------------------ epoch 1744 (10458 steps) ------------------------------------\n",
      "Max loss: 0.029168829321861267\n",
      "Min loss: 0.010703089646995068\n",
      "Mean loss: 0.01732672533641259\n",
      "Std loss: 0.005944879778319808\n",
      "Total Loss: 0.10396035201847553\n",
      "------------------------------------ epoch 1745 (10464 steps) ------------------------------------\n",
      "Max loss: 0.039424292743206024\n",
      "Min loss: 0.012484394013881683\n",
      "Mean loss: 0.02343691559508443\n",
      "Std loss: 0.010963776468256912\n",
      "Total Loss: 0.14062149357050657\n",
      "------------------------------------ epoch 1746 (10470 steps) ------------------------------------\n",
      "Max loss: 0.023129627108573914\n",
      "Min loss: 0.01659662090241909\n",
      "Mean loss: 0.020454926726718742\n",
      "Std loss: 0.002298739975174988\n",
      "Total Loss: 0.12272956036031246\n",
      "------------------------------------ epoch 1747 (10476 steps) ------------------------------------\n",
      "Max loss: 0.06714913249015808\n",
      "Min loss: 0.012828987091779709\n",
      "Mean loss: 0.025841818501551945\n",
      "Std loss: 0.018864147923071433\n",
      "Total Loss: 0.15505091100931168\n",
      "------------------------------------ epoch 1748 (10482 steps) ------------------------------------\n",
      "Max loss: 0.047662731260061264\n",
      "Min loss: 0.01259689312428236\n",
      "Mean loss: 0.024548977768669527\n",
      "Std loss: 0.011840847390904487\n",
      "Total Loss: 0.14729386661201715\n",
      "------------------------------------ epoch 1749 (10488 steps) ------------------------------------\n",
      "Max loss: 0.08486970514059067\n",
      "Min loss: 0.013609356246888638\n",
      "Mean loss: 0.03788142759973804\n",
      "Std loss: 0.023958874199776797\n",
      "Total Loss: 0.22728856559842825\n",
      "------------------------------------ epoch 1750 (10494 steps) ------------------------------------\n",
      "Max loss: 0.04065008834004402\n",
      "Min loss: 0.02136071026325226\n",
      "Mean loss: 0.028476092033088207\n",
      "Std loss: 0.006997752778398709\n",
      "Total Loss: 0.17085655219852924\n",
      "------------------------------------ epoch 1751 (10500 steps) ------------------------------------\n",
      "Max loss: 0.044324617832899094\n",
      "Min loss: 0.013600066304206848\n",
      "Mean loss: 0.024537323663632076\n",
      "Std loss: 0.009662570332123912\n",
      "Total Loss: 0.14722394198179245\n",
      "------------------------------------ epoch 1752 (10506 steps) ------------------------------------\n",
      "Max loss: 0.044238265603780746\n",
      "Min loss: 0.01718398742377758\n",
      "Mean loss: 0.03240319838126501\n",
      "Std loss: 0.009445939233118594\n",
      "Total Loss: 0.19441919028759003\n",
      "------------------------------------ epoch 1753 (10512 steps) ------------------------------------\n",
      "Max loss: 0.07463018596172333\n",
      "Min loss: 0.013935602270066738\n",
      "Mean loss: 0.03016500749314825\n",
      "Std loss: 0.02036310137444192\n",
      "Total Loss: 0.18099004495888948\n",
      "------------------------------------ epoch 1754 (10518 steps) ------------------------------------\n",
      "Max loss: 0.047690924257040024\n",
      "Min loss: 0.011586557142436504\n",
      "Mean loss: 0.028003387929250795\n",
      "Std loss: 0.012819159729948529\n",
      "Total Loss: 0.16802032757550478\n",
      "------------------------------------ epoch 1755 (10524 steps) ------------------------------------\n",
      "Max loss: 0.04761805385351181\n",
      "Min loss: 0.014394688419997692\n",
      "Mean loss: 0.03180248088513812\n",
      "Std loss: 0.011583607240593808\n",
      "Total Loss: 0.19081488531082869\n",
      "------------------------------------ epoch 1756 (10530 steps) ------------------------------------\n",
      "Max loss: 0.049316540360450745\n",
      "Min loss: 0.01860150694847107\n",
      "Mean loss: 0.03310124141474565\n",
      "Std loss: 0.010257724847351206\n",
      "Total Loss: 0.1986074484884739\n",
      "------------------------------------ epoch 1757 (10536 steps) ------------------------------------\n",
      "Max loss: 0.042290546000003815\n",
      "Min loss: 0.014274796470999718\n",
      "Mean loss: 0.023314339729646843\n",
      "Std loss: 0.009191628378700318\n",
      "Total Loss: 0.13988603837788105\n",
      "------------------------------------ epoch 1758 (10542 steps) ------------------------------------\n",
      "Max loss: 0.05432192236185074\n",
      "Min loss: 0.011654808185994625\n",
      "Mean loss: 0.031408638848612704\n",
      "Std loss: 0.013640173364625467\n",
      "Total Loss: 0.18845183309167624\n",
      "------------------------------------ epoch 1759 (10548 steps) ------------------------------------\n",
      "Max loss: 0.020421667024493217\n",
      "Min loss: 0.015339887700974941\n",
      "Mean loss: 0.018362684020151693\n",
      "Std loss: 0.0018631680576907757\n",
      "Total Loss: 0.11017610412091017\n",
      "------------------------------------ epoch 1760 (10554 steps) ------------------------------------\n",
      "Max loss: 0.04906567186117172\n",
      "Min loss: 0.016489695757627487\n",
      "Mean loss: 0.025754850047330063\n",
      "Std loss: 0.01135699192711096\n",
      "Total Loss: 0.15452910028398037\n",
      "------------------------------------ epoch 1761 (10560 steps) ------------------------------------\n",
      "Max loss: 0.0749528557062149\n",
      "Min loss: 0.015607822686433792\n",
      "Mean loss: 0.03382676374167204\n",
      "Std loss: 0.019337370389467225\n",
      "Total Loss: 0.20296058245003223\n",
      "------------------------------------ epoch 1762 (10566 steps) ------------------------------------\n",
      "Max loss: 0.061408013105392456\n",
      "Min loss: 0.012294895946979523\n",
      "Mean loss: 0.0317209226389726\n",
      "Std loss: 0.016614376132681206\n",
      "Total Loss: 0.1903255358338356\n",
      "------------------------------------ epoch 1763 (10572 steps) ------------------------------------\n",
      "Max loss: 0.07278483361005783\n",
      "Min loss: 0.012491874396800995\n",
      "Mean loss: 0.027206805534660816\n",
      "Std loss: 0.021063962428139717\n",
      "Total Loss: 0.1632408332079649\n",
      "------------------------------------ epoch 1764 (10578 steps) ------------------------------------\n",
      "Max loss: 0.06786991655826569\n",
      "Min loss: 0.012843720614910126\n",
      "Mean loss: 0.03731211585303148\n",
      "Std loss: 0.017020419202714583\n",
      "Total Loss: 0.22387269511818886\n",
      "------------------------------------ epoch 1765 (10584 steps) ------------------------------------\n",
      "Max loss: 0.09303277730941772\n",
      "Min loss: 0.014121385291218758\n",
      "Mean loss: 0.0391956881309549\n",
      "Std loss: 0.027479114561251985\n",
      "Total Loss: 0.2351741287857294\n",
      "------------------------------------ epoch 1766 (10590 steps) ------------------------------------\n",
      "Max loss: 0.059287600219249725\n",
      "Min loss: 0.023167796432971954\n",
      "Mean loss: 0.03672776433328787\n",
      "Std loss: 0.015400169108335804\n",
      "Total Loss: 0.22036658599972725\n",
      "------------------------------------ epoch 1767 (10596 steps) ------------------------------------\n",
      "Max loss: 0.04120936244726181\n",
      "Min loss: 0.022315379232168198\n",
      "Mean loss: 0.03037512395530939\n",
      "Std loss: 0.006261175540041105\n",
      "Total Loss: 0.18225074373185635\n",
      "------------------------------------ epoch 1768 (10602 steps) ------------------------------------\n",
      "Max loss: 0.03641069307923317\n",
      "Min loss: 0.025546491146087646\n",
      "Mean loss: 0.03109721715251605\n",
      "Std loss: 0.004301197509504759\n",
      "Total Loss: 0.18658330291509628\n",
      "------------------------------------ epoch 1769 (10608 steps) ------------------------------------\n",
      "Max loss: 0.046024154871702194\n",
      "Min loss: 0.025592561811208725\n",
      "Mean loss: 0.03500846835474173\n",
      "Std loss: 0.0069604985560461985\n",
      "Total Loss: 0.2100508101284504\n",
      "------------------------------------ epoch 1770 (10614 steps) ------------------------------------\n",
      "Max loss: 0.08457718789577484\n",
      "Min loss: 0.01782013289630413\n",
      "Mean loss: 0.03780021487424771\n",
      "Std loss: 0.023146945236115518\n",
      "Total Loss: 0.22680128924548626\n",
      "------------------------------------ epoch 1771 (10620 steps) ------------------------------------\n",
      "Max loss: 0.05722701549530029\n",
      "Min loss: 0.0176556333899498\n",
      "Mean loss: 0.029380987708767254\n",
      "Std loss: 0.013827654630028316\n",
      "Total Loss: 0.17628592625260353\n",
      "------------------------------------ epoch 1772 (10626 steps) ------------------------------------\n",
      "Max loss: 0.053698863834142685\n",
      "Min loss: 0.014117422513663769\n",
      "Mean loss: 0.0347413447064658\n",
      "Std loss: 0.014599964777415948\n",
      "Total Loss: 0.2084480682387948\n",
      "------------------------------------ epoch 1773 (10632 steps) ------------------------------------\n",
      "Max loss: 0.04867176711559296\n",
      "Min loss: 0.015291894786059856\n",
      "Mean loss: 0.02743709444378813\n",
      "Std loss: 0.011975044636364195\n",
      "Total Loss: 0.1646225666627288\n",
      "------------------------------------ epoch 1774 (10638 steps) ------------------------------------\n",
      "Max loss: 0.03602059930562973\n",
      "Min loss: 0.015748023986816406\n",
      "Mean loss: 0.024757777030269306\n",
      "Std loss: 0.006896032260273174\n",
      "Total Loss: 0.14854666218161583\n",
      "------------------------------------ epoch 1775 (10644 steps) ------------------------------------\n",
      "Max loss: 0.039794307202100754\n",
      "Min loss: 0.014906692318618298\n",
      "Mean loss: 0.023282779070238274\n",
      "Std loss: 0.009032342636600348\n",
      "Total Loss: 0.13969667442142963\n",
      "------------------------------------ epoch 1776 (10650 steps) ------------------------------------\n",
      "Max loss: 0.043848566710948944\n",
      "Min loss: 0.0230377446860075\n",
      "Mean loss: 0.033549343856672444\n",
      "Std loss: 0.0074524952851032595\n",
      "Total Loss: 0.20129606314003468\n",
      "------------------------------------ epoch 1777 (10656 steps) ------------------------------------\n",
      "Max loss: 0.03294374793767929\n",
      "Min loss: 0.014662052504718304\n",
      "Mean loss: 0.023587042310585577\n",
      "Std loss: 0.007601072938032585\n",
      "Total Loss: 0.14152225386351347\n",
      "------------------------------------ epoch 1778 (10662 steps) ------------------------------------\n",
      "Max loss: 0.040816184133291245\n",
      "Min loss: 0.013912133872509003\n",
      "Mean loss: 0.026317677771051724\n",
      "Std loss: 0.011315671888614898\n",
      "Total Loss: 0.15790606662631035\n",
      "------------------------------------ epoch 1779 (10668 steps) ------------------------------------\n",
      "Max loss: 0.048400577157735825\n",
      "Min loss: 0.01404464989900589\n",
      "Mean loss: 0.030401617288589478\n",
      "Std loss: 0.013343042511168994\n",
      "Total Loss: 0.18240970373153687\n",
      "------------------------------------ epoch 1780 (10674 steps) ------------------------------------\n",
      "Max loss: 0.03932753950357437\n",
      "Min loss: 0.011167541146278381\n",
      "Mean loss: 0.022166760948797066\n",
      "Std loss: 0.009066674949750211\n",
      "Total Loss: 0.1330005656927824\n",
      "------------------------------------ epoch 1781 (10680 steps) ------------------------------------\n",
      "Max loss: 0.07016539573669434\n",
      "Min loss: 0.012837315909564495\n",
      "Mean loss: 0.029863471165299416\n",
      "Std loss: 0.019560451541818763\n",
      "Total Loss: 0.1791808269917965\n",
      "------------------------------------ epoch 1782 (10686 steps) ------------------------------------\n",
      "Max loss: 0.05173782631754875\n",
      "Min loss: 0.013944278471171856\n",
      "Mean loss: 0.026139469041178625\n",
      "Std loss: 0.013482176262886012\n",
      "Total Loss: 0.15683681424707174\n",
      "------------------------------------ epoch 1783 (10692 steps) ------------------------------------\n",
      "Max loss: 0.07404264062643051\n",
      "Min loss: 0.014669476076960564\n",
      "Mean loss: 0.0416323027263085\n",
      "Std loss: 0.019186131744229413\n",
      "Total Loss: 0.24979381635785103\n",
      "------------------------------------ epoch 1784 (10698 steps) ------------------------------------\n",
      "Max loss: 0.04362204670906067\n",
      "Min loss: 0.0139569491147995\n",
      "Mean loss: 0.0269660201544563\n",
      "Std loss: 0.009553144447194612\n",
      "Total Loss: 0.16179612092673779\n",
      "------------------------------------ epoch 1785 (10704 steps) ------------------------------------\n",
      "Max loss: 0.03594018518924713\n",
      "Min loss: 0.015329517424106598\n",
      "Mean loss: 0.021968755560616653\n",
      "Std loss: 0.00685156057481127\n",
      "Total Loss: 0.1318125333636999\n",
      "------------------------------------ epoch 1786 (10710 steps) ------------------------------------\n",
      "Max loss: 0.05838969349861145\n",
      "Min loss: 0.013616561889648438\n",
      "Mean loss: 0.034598240007956825\n",
      "Std loss: 0.016277532498770685\n",
      "Total Loss: 0.20758944004774094\n",
      "------------------------------------ epoch 1787 (10716 steps) ------------------------------------\n",
      "Max loss: 0.0379432737827301\n",
      "Min loss: 0.014162621460855007\n",
      "Mean loss: 0.025689547415822744\n",
      "Std loss: 0.010061533330342874\n",
      "Total Loss: 0.15413728449493647\n",
      "------------------------------------ epoch 1788 (10722 steps) ------------------------------------\n",
      "Max loss: 0.04411616921424866\n",
      "Min loss: 0.011571509763598442\n",
      "Mean loss: 0.021546352344254654\n",
      "Std loss: 0.011156136469322094\n",
      "Total Loss: 0.12927811406552792\n",
      "------------------------------------ epoch 1789 (10728 steps) ------------------------------------\n",
      "Max loss: 0.044962696731090546\n",
      "Min loss: 0.010837417095899582\n",
      "Mean loss: 0.026194128518303234\n",
      "Std loss: 0.011927652734086922\n",
      "Total Loss: 0.1571647711098194\n",
      "------------------------------------ epoch 1790 (10734 steps) ------------------------------------\n",
      "Max loss: 0.045188065618276596\n",
      "Min loss: 0.017389681190252304\n",
      "Mean loss: 0.024853733989099663\n",
      "Std loss: 0.009548122743652816\n",
      "Total Loss: 0.14912240393459797\n",
      "------------------------------------ epoch 1791 (10740 steps) ------------------------------------\n",
      "Max loss: 0.03424952179193497\n",
      "Min loss: 0.021470455452799797\n",
      "Mean loss: 0.024946626896659534\n",
      "Std loss: 0.0043493037179208795\n",
      "Total Loss: 0.1496797613799572\n",
      "------------------------------------ epoch 1792 (10746 steps) ------------------------------------\n",
      "Max loss: 0.05031992122530937\n",
      "Min loss: 0.020569078624248505\n",
      "Mean loss: 0.027759912734230358\n",
      "Std loss: 0.010430053901424888\n",
      "Total Loss: 0.16655947640538216\n",
      "------------------------------------ epoch 1793 (10752 steps) ------------------------------------\n",
      "Max loss: 0.03631102293729782\n",
      "Min loss: 0.013941559009253979\n",
      "Mean loss: 0.02239665435627103\n",
      "Std loss: 0.007530722472091748\n",
      "Total Loss: 0.13437992613762617\n",
      "------------------------------------ epoch 1794 (10758 steps) ------------------------------------\n",
      "Max loss: 0.07468229532241821\n",
      "Min loss: 0.011298276484012604\n",
      "Mean loss: 0.032075706558922924\n",
      "Std loss: 0.020470064392395033\n",
      "Total Loss: 0.19245423935353756\n",
      "------------------------------------ epoch 1795 (10764 steps) ------------------------------------\n",
      "Max loss: 0.046061791479587555\n",
      "Min loss: 0.014508510008454323\n",
      "Mean loss: 0.026890386672069628\n",
      "Std loss: 0.011291116825005847\n",
      "Total Loss: 0.16134232003241777\n",
      "------------------------------------ epoch 1796 (10770 steps) ------------------------------------\n",
      "Max loss: 0.047875188291072845\n",
      "Min loss: 0.013394258916378021\n",
      "Mean loss: 0.028309832016626995\n",
      "Std loss: 0.011380928996709571\n",
      "Total Loss: 0.16985899209976196\n",
      "------------------------------------ epoch 1797 (10776 steps) ------------------------------------\n",
      "Max loss: 0.030338918790221214\n",
      "Min loss: 0.018090251833200455\n",
      "Mean loss: 0.02292189033081134\n",
      "Std loss: 0.0040821744723069245\n",
      "Total Loss: 0.13753134198486805\n",
      "------------------------------------ epoch 1798 (10782 steps) ------------------------------------\n",
      "Max loss: 0.034568022936582565\n",
      "Min loss: 0.014034907333552837\n",
      "Mean loss: 0.019861402766158182\n",
      "Std loss: 0.006998917829912914\n",
      "Total Loss: 0.1191684165969491\n",
      "------------------------------------ epoch 1799 (10788 steps) ------------------------------------\n",
      "Max loss: 0.06773898005485535\n",
      "Min loss: 0.01720578595995903\n",
      "Mean loss: 0.02996768367787202\n",
      "Std loss: 0.017432031518652108\n",
      "Total Loss: 0.17980610206723213\n",
      "------------------------------------ epoch 1800 (10794 steps) ------------------------------------\n",
      "Max loss: 0.05071505904197693\n",
      "Min loss: 0.015167497098445892\n",
      "Mean loss: 0.028296369748810928\n",
      "Std loss: 0.01183486794021404\n",
      "Total Loss: 0.16977821849286556\n",
      "------------------------------------ epoch 1801 (10800 steps) ------------------------------------\n",
      "Max loss: 0.038680389523506165\n",
      "Min loss: 0.015181152150034904\n",
      "Mean loss: 0.02363671517620484\n",
      "Std loss: 0.008467087826046424\n",
      "Total Loss: 0.14182029105722904\n",
      "saved model at ./weights/model_1801.pth\n",
      "------------------------------------ epoch 1802 (10806 steps) ------------------------------------\n",
      "Max loss: 0.021094102412462234\n",
      "Min loss: 0.010852585546672344\n",
      "Mean loss: 0.015453131093333164\n",
      "Std loss: 0.0037531501315340946\n",
      "Total Loss: 0.09271878655999899\n",
      "------------------------------------ epoch 1803 (10812 steps) ------------------------------------\n",
      "Max loss: 0.03444896265864372\n",
      "Min loss: 0.015402335673570633\n",
      "Mean loss: 0.023011304748555023\n",
      "Std loss: 0.0060336124004932105\n",
      "Total Loss: 0.13806782849133015\n",
      "------------------------------------ epoch 1804 (10818 steps) ------------------------------------\n",
      "Max loss: 0.028352074325084686\n",
      "Min loss: 0.01646539568901062\n",
      "Mean loss: 0.022082653517524403\n",
      "Std loss: 0.004038065223430809\n",
      "Total Loss: 0.1324959211051464\n",
      "------------------------------------ epoch 1805 (10824 steps) ------------------------------------\n",
      "Max loss: 0.04003303498029709\n",
      "Min loss: 0.013762358576059341\n",
      "Mean loss: 0.026913979401191074\n",
      "Std loss: 0.008578401466895898\n",
      "Total Loss: 0.16148387640714645\n",
      "------------------------------------ epoch 1806 (10830 steps) ------------------------------------\n",
      "Max loss: 0.03110901638865471\n",
      "Min loss: 0.011958522722125053\n",
      "Mean loss: 0.022552343861510355\n",
      "Std loss: 0.007310624906252494\n",
      "Total Loss: 0.13531406316906214\n",
      "------------------------------------ epoch 1807 (10836 steps) ------------------------------------\n",
      "Max loss: 0.032614655792713165\n",
      "Min loss: 0.015902956947684288\n",
      "Mean loss: 0.021327557042241096\n",
      "Std loss: 0.005724125710835132\n",
      "Total Loss: 0.12796534225344658\n",
      "------------------------------------ epoch 1808 (10842 steps) ------------------------------------\n",
      "Max loss: 0.03750322759151459\n",
      "Min loss: 0.014039079658687115\n",
      "Mean loss: 0.02395412279292941\n",
      "Std loss: 0.00817032805527047\n",
      "Total Loss: 0.14372473675757647\n",
      "------------------------------------ epoch 1809 (10848 steps) ------------------------------------\n",
      "Max loss: 0.06687679886817932\n",
      "Min loss: 0.01226024515926838\n",
      "Mean loss: 0.030956674522409838\n",
      "Std loss: 0.01813624967888445\n",
      "Total Loss: 0.18574004713445902\n",
      "------------------------------------ epoch 1810 (10854 steps) ------------------------------------\n",
      "Max loss: 0.04169455170631409\n",
      "Min loss: 0.01928016170859337\n",
      "Mean loss: 0.031616535037755966\n",
      "Std loss: 0.006951305559086697\n",
      "Total Loss: 0.1896992102265358\n",
      "------------------------------------ epoch 1811 (10860 steps) ------------------------------------\n",
      "Max loss: 0.06631384789943695\n",
      "Min loss: 0.01642729714512825\n",
      "Mean loss: 0.03141285416980585\n",
      "Std loss: 0.01693706909117072\n",
      "Total Loss: 0.18847712501883507\n",
      "------------------------------------ epoch 1812 (10866 steps) ------------------------------------\n",
      "Max loss: 0.03034890815615654\n",
      "Min loss: 0.019971193745732307\n",
      "Mean loss: 0.025411767264207203\n",
      "Std loss: 0.003337736672755261\n",
      "Total Loss: 0.15247060358524323\n",
      "------------------------------------ epoch 1813 (10872 steps) ------------------------------------\n",
      "Max loss: 0.04767189547419548\n",
      "Min loss: 0.014855381101369858\n",
      "Mean loss: 0.0253714332357049\n",
      "Std loss: 0.010584405952946225\n",
      "Total Loss: 0.1522285994142294\n",
      "------------------------------------ epoch 1814 (10878 steps) ------------------------------------\n",
      "Max loss: 0.04597832262516022\n",
      "Min loss: 0.022543471306562424\n",
      "Mean loss: 0.035503568748633065\n",
      "Std loss: 0.008255646811723487\n",
      "Total Loss: 0.2130214124917984\n",
      "------------------------------------ epoch 1815 (10884 steps) ------------------------------------\n",
      "Max loss: 0.033767811954021454\n",
      "Min loss: 0.012869542464613914\n",
      "Mean loss: 0.027881669501463573\n",
      "Std loss: 0.007088065455038795\n",
      "Total Loss: 0.16729001700878143\n",
      "------------------------------------ epoch 1816 (10890 steps) ------------------------------------\n",
      "Max loss: 0.04571027308702469\n",
      "Min loss: 0.025396618992090225\n",
      "Mean loss: 0.03371347642193238\n",
      "Std loss: 0.008119685290265664\n",
      "Total Loss: 0.20228085853159428\n",
      "------------------------------------ epoch 1817 (10896 steps) ------------------------------------\n",
      "Max loss: 0.04706772416830063\n",
      "Min loss: 0.015659205615520477\n",
      "Mean loss: 0.026765669696033\n",
      "Std loss: 0.010657756854031844\n",
      "Total Loss: 0.160594018176198\n",
      "------------------------------------ epoch 1818 (10902 steps) ------------------------------------\n",
      "Max loss: 0.03273366391658783\n",
      "Min loss: 0.011885728687047958\n",
      "Mean loss: 0.02522084303200245\n",
      "Std loss: 0.006654634236688672\n",
      "Total Loss: 0.1513250581920147\n",
      "------------------------------------ epoch 1819 (10908 steps) ------------------------------------\n",
      "Max loss: 0.03145475685596466\n",
      "Min loss: 0.01255621574819088\n",
      "Mean loss: 0.02431809778014819\n",
      "Std loss: 0.007163783473438109\n",
      "Total Loss: 0.14590858668088913\n",
      "------------------------------------ epoch 1820 (10914 steps) ------------------------------------\n",
      "Max loss: 0.02753710374236107\n",
      "Min loss: 0.01232045516371727\n",
      "Mean loss: 0.01950811641290784\n",
      "Std loss: 0.005825948965206935\n",
      "Total Loss: 0.11704869847744703\n",
      "------------------------------------ epoch 1821 (10920 steps) ------------------------------------\n",
      "Max loss: 0.042162053287029266\n",
      "Min loss: 0.016880476847290993\n",
      "Mean loss: 0.027587896212935448\n",
      "Std loss: 0.007889376066055791\n",
      "Total Loss: 0.1655273772776127\n",
      "------------------------------------ epoch 1822 (10926 steps) ------------------------------------\n",
      "Max loss: 0.0660477876663208\n",
      "Min loss: 0.017995674163103104\n",
      "Mean loss: 0.033649769301215805\n",
      "Std loss: 0.016746573952925867\n",
      "Total Loss: 0.20189861580729485\n",
      "------------------------------------ epoch 1823 (10932 steps) ------------------------------------\n",
      "Max loss: 0.06509670615196228\n",
      "Min loss: 0.01638743281364441\n",
      "Mean loss: 0.032927838464577995\n",
      "Std loss: 0.017206533272062285\n",
      "Total Loss: 0.19756703078746796\n",
      "------------------------------------ epoch 1824 (10938 steps) ------------------------------------\n",
      "Max loss: 0.03405247628688812\n",
      "Min loss: 0.012024502269923687\n",
      "Mean loss: 0.02344034720833103\n",
      "Std loss: 0.008478163496539277\n",
      "Total Loss: 0.14064208324998617\n",
      "------------------------------------ epoch 1825 (10944 steps) ------------------------------------\n",
      "Max loss: 0.02968105673789978\n",
      "Min loss: 0.014571319334208965\n",
      "Mean loss: 0.02389001799747348\n",
      "Std loss: 0.005607796331417515\n",
      "Total Loss: 0.14334010798484087\n",
      "------------------------------------ epoch 1826 (10950 steps) ------------------------------------\n",
      "Max loss: 0.02383565902709961\n",
      "Min loss: 0.01018393225967884\n",
      "Mean loss: 0.01910007232800126\n",
      "Std loss: 0.005699713178379783\n",
      "Total Loss: 0.11460043396800756\n",
      "------------------------------------ epoch 1827 (10956 steps) ------------------------------------\n",
      "Max loss: 0.045439109206199646\n",
      "Min loss: 0.010910685174167156\n",
      "Mean loss: 0.02578296683107813\n",
      "Std loss: 0.011537091699476577\n",
      "Total Loss: 0.1546978009864688\n",
      "------------------------------------ epoch 1828 (10962 steps) ------------------------------------\n",
      "Max loss: 0.06787454336881638\n",
      "Min loss: 0.011849979870021343\n",
      "Mean loss: 0.032081339973956347\n",
      "Std loss: 0.017999199605350253\n",
      "Total Loss: 0.19248803984373808\n",
      "------------------------------------ epoch 1829 (10968 steps) ------------------------------------\n",
      "Max loss: 0.02999080717563629\n",
      "Min loss: 0.012423209846019745\n",
      "Mean loss: 0.02262348650644223\n",
      "Std loss: 0.0059057039888883705\n",
      "Total Loss: 0.13574091903865337\n",
      "------------------------------------ epoch 1830 (10974 steps) ------------------------------------\n",
      "Max loss: 0.032576002180576324\n",
      "Min loss: 0.016827194020152092\n",
      "Mean loss: 0.024874549669524033\n",
      "Std loss: 0.006059670183608426\n",
      "Total Loss: 0.1492472980171442\n",
      "------------------------------------ epoch 1831 (10980 steps) ------------------------------------\n",
      "Max loss: 0.08246178925037384\n",
      "Min loss: 0.02114352397620678\n",
      "Mean loss: 0.042187533962229885\n",
      "Std loss: 0.01951827405859572\n",
      "Total Loss: 0.2531252037733793\n",
      "------------------------------------ epoch 1832 (10986 steps) ------------------------------------\n",
      "Max loss: 0.047713786363601685\n",
      "Min loss: 0.013048413209617138\n",
      "Mean loss: 0.02802873852973183\n",
      "Std loss: 0.011936477361741377\n",
      "Total Loss: 0.16817243117839098\n",
      "------------------------------------ epoch 1833 (10992 steps) ------------------------------------\n",
      "Max loss: 0.03237162157893181\n",
      "Min loss: 0.012607203796505928\n",
      "Mean loss: 0.021680691900352638\n",
      "Std loss: 0.006598162705057585\n",
      "Total Loss: 0.13008415140211582\n",
      "------------------------------------ epoch 1834 (10998 steps) ------------------------------------\n",
      "Max loss: 0.038290053606033325\n",
      "Min loss: 0.012339289300143719\n",
      "Mean loss: 0.026064237114042044\n",
      "Std loss: 0.008876223400667663\n",
      "Total Loss: 0.15638542268425226\n",
      "------------------------------------ epoch 1835 (11004 steps) ------------------------------------\n",
      "Max loss: 0.03800145909190178\n",
      "Min loss: 0.021403566002845764\n",
      "Mean loss: 0.02740643545985222\n",
      "Std loss: 0.005668273840318768\n",
      "Total Loss: 0.1644386127591133\n",
      "------------------------------------ epoch 1836 (11010 steps) ------------------------------------\n",
      "Max loss: 0.039593473076820374\n",
      "Min loss: 0.016501184552907944\n",
      "Mean loss: 0.02777729369699955\n",
      "Std loss: 0.008767027719922979\n",
      "Total Loss: 0.1666637621819973\n",
      "------------------------------------ epoch 1837 (11016 steps) ------------------------------------\n",
      "Max loss: 0.04298014938831329\n",
      "Min loss: 0.01264585554599762\n",
      "Mean loss: 0.02567046321928501\n",
      "Std loss: 0.010698891674180292\n",
      "Total Loss: 0.15402277931571007\n",
      "------------------------------------ epoch 1838 (11022 steps) ------------------------------------\n",
      "Max loss: 0.04144071415066719\n",
      "Min loss: 0.014361320994794369\n",
      "Mean loss: 0.022496833310772974\n",
      "Std loss: 0.00882113133957649\n",
      "Total Loss: 0.13498099986463785\n",
      "------------------------------------ epoch 1839 (11028 steps) ------------------------------------\n",
      "Max loss: 0.0398869626224041\n",
      "Min loss: 0.021180232986807823\n",
      "Mean loss: 0.028430636661748093\n",
      "Std loss: 0.006944610935181898\n",
      "Total Loss: 0.17058381997048855\n",
      "------------------------------------ epoch 1840 (11034 steps) ------------------------------------\n",
      "Max loss: 0.045575615018606186\n",
      "Min loss: 0.018206235021352768\n",
      "Mean loss: 0.028440476084748905\n",
      "Std loss: 0.009352323226343951\n",
      "Total Loss: 0.17064285650849342\n",
      "------------------------------------ epoch 1841 (11040 steps) ------------------------------------\n",
      "Max loss: 0.05401293560862541\n",
      "Min loss: 0.010870168916881084\n",
      "Mean loss: 0.021362779351572197\n",
      "Std loss: 0.015368210111229727\n",
      "Total Loss: 0.12817667610943317\n",
      "------------------------------------ epoch 1842 (11046 steps) ------------------------------------\n",
      "Max loss: 0.05472555011510849\n",
      "Min loss: 0.01737144961953163\n",
      "Mean loss: 0.02668891257296006\n",
      "Std loss: 0.013098337861318792\n",
      "Total Loss: 0.16013347543776035\n",
      "------------------------------------ epoch 1843 (11052 steps) ------------------------------------\n",
      "Max loss: 0.08439624309539795\n",
      "Min loss: 0.014408002607524395\n",
      "Mean loss: 0.03223350690677762\n",
      "Std loss: 0.024392897305436298\n",
      "Total Loss: 0.19340104144066572\n",
      "------------------------------------ epoch 1844 (11058 steps) ------------------------------------\n",
      "Max loss: 0.029595082625746727\n",
      "Min loss: 0.01708712987601757\n",
      "Mean loss: 0.021978437279661495\n",
      "Std loss: 0.003950166483925781\n",
      "Total Loss: 0.13187062367796898\n",
      "------------------------------------ epoch 1845 (11064 steps) ------------------------------------\n",
      "Max loss: 0.0527629554271698\n",
      "Min loss: 0.02183021977543831\n",
      "Mean loss: 0.036494885881741844\n",
      "Std loss: 0.01230419454098069\n",
      "Total Loss: 0.21896931529045105\n",
      "------------------------------------ epoch 1846 (11070 steps) ------------------------------------\n",
      "Max loss: 0.06988725066184998\n",
      "Min loss: 0.011825412511825562\n",
      "Mean loss: 0.03150762741764387\n",
      "Std loss: 0.018519326724959637\n",
      "Total Loss: 0.1890457645058632\n",
      "------------------------------------ epoch 1847 (11076 steps) ------------------------------------\n",
      "Max loss: 0.03272637724876404\n",
      "Min loss: 0.01365627907216549\n",
      "Mean loss: 0.02366579199830691\n",
      "Std loss: 0.006179594583503724\n",
      "Total Loss: 0.14199475198984146\n",
      "------------------------------------ epoch 1848 (11082 steps) ------------------------------------\n",
      "Max loss: 0.03876039385795593\n",
      "Min loss: 0.013730993494391441\n",
      "Mean loss: 0.024043220716218155\n",
      "Std loss: 0.009551904220673134\n",
      "Total Loss: 0.14425932429730892\n",
      "------------------------------------ epoch 1849 (11088 steps) ------------------------------------\n",
      "Max loss: 0.04529057443141937\n",
      "Min loss: 0.02580234408378601\n",
      "Mean loss: 0.03166391979902983\n",
      "Std loss: 0.006519605008944796\n",
      "Total Loss: 0.18998351879417896\n",
      "------------------------------------ epoch 1850 (11094 steps) ------------------------------------\n",
      "Max loss: 0.03436726704239845\n",
      "Min loss: 0.013303975574672222\n",
      "Mean loss: 0.02013576729223132\n",
      "Std loss: 0.007244207080677097\n",
      "Total Loss: 0.12081460375338793\n",
      "------------------------------------ epoch 1851 (11100 steps) ------------------------------------\n",
      "Max loss: 0.07211478054523468\n",
      "Min loss: 0.020807258784770966\n",
      "Mean loss: 0.038423399130503334\n",
      "Std loss: 0.01659338568579104\n",
      "Total Loss: 0.23054039478302002\n",
      "------------------------------------ epoch 1852 (11106 steps) ------------------------------------\n",
      "Max loss: 0.05591155216097832\n",
      "Min loss: 0.018218213692307472\n",
      "Mean loss: 0.031057143894334633\n",
      "Std loss: 0.013418096274710465\n",
      "Total Loss: 0.1863428633660078\n",
      "------------------------------------ epoch 1853 (11112 steps) ------------------------------------\n",
      "Max loss: 0.0422913059592247\n",
      "Min loss: 0.015266982838511467\n",
      "Mean loss: 0.02786160446703434\n",
      "Std loss: 0.009959624655811595\n",
      "Total Loss: 0.16716962680220604\n",
      "------------------------------------ epoch 1854 (11118 steps) ------------------------------------\n",
      "Max loss: 0.04102709889411926\n",
      "Min loss: 0.013344971463084221\n",
      "Mean loss: 0.029247220916052658\n",
      "Std loss: 0.009850504414162695\n",
      "Total Loss: 0.17548332549631596\n",
      "------------------------------------ epoch 1855 (11124 steps) ------------------------------------\n",
      "Max loss: 0.03302890807390213\n",
      "Min loss: 0.013876980170607567\n",
      "Mean loss: 0.02278777988006671\n",
      "Std loss: 0.006903769452625219\n",
      "Total Loss: 0.13672667928040028\n",
      "------------------------------------ epoch 1856 (11130 steps) ------------------------------------\n",
      "Max loss: 0.07848294079303741\n",
      "Min loss: 0.013314376585185528\n",
      "Mean loss: 0.029872789047658443\n",
      "Std loss: 0.023971813411862317\n",
      "Total Loss: 0.17923673428595066\n",
      "------------------------------------ epoch 1857 (11136 steps) ------------------------------------\n",
      "Max loss: 0.04236776754260063\n",
      "Min loss: 0.015613826923072338\n",
      "Mean loss: 0.02717737155035138\n",
      "Std loss: 0.00820000986227526\n",
      "Total Loss: 0.1630642293021083\n",
      "------------------------------------ epoch 1858 (11142 steps) ------------------------------------\n",
      "Max loss: 0.03898148983716965\n",
      "Min loss: 0.013672934845089912\n",
      "Mean loss: 0.02569423895329237\n",
      "Std loss: 0.009413258742379103\n",
      "Total Loss: 0.15416543371975422\n",
      "------------------------------------ epoch 1859 (11148 steps) ------------------------------------\n",
      "Max loss: 0.04914914071559906\n",
      "Min loss: 0.014407417736947536\n",
      "Mean loss: 0.029883555912723143\n",
      "Std loss: 0.01315623666374849\n",
      "Total Loss: 0.17930133547633886\n",
      "------------------------------------ epoch 1860 (11154 steps) ------------------------------------\n",
      "Max loss: 0.036642443388700485\n",
      "Min loss: 0.012911397032439709\n",
      "Mean loss: 0.026646755170077085\n",
      "Std loss: 0.007054662115978275\n",
      "Total Loss: 0.1598805310204625\n",
      "------------------------------------ epoch 1861 (11160 steps) ------------------------------------\n",
      "Max loss: 0.04293135553598404\n",
      "Min loss: 0.013735152781009674\n",
      "Mean loss: 0.024047027342021465\n",
      "Std loss: 0.010064216058785951\n",
      "Total Loss: 0.1442821640521288\n",
      "------------------------------------ epoch 1862 (11166 steps) ------------------------------------\n",
      "Max loss: 0.036055296659469604\n",
      "Min loss: 0.011164764873683453\n",
      "Mean loss: 0.020907045652468998\n",
      "Std loss: 0.00907707388070064\n",
      "Total Loss: 0.125442273914814\n",
      "------------------------------------ epoch 1863 (11172 steps) ------------------------------------\n",
      "Max loss: 0.05702384561300278\n",
      "Min loss: 0.01173730194568634\n",
      "Mean loss: 0.031454540168245636\n",
      "Std loss: 0.014378708560892866\n",
      "Total Loss: 0.1887272410094738\n",
      "------------------------------------ epoch 1864 (11178 steps) ------------------------------------\n",
      "Max loss: 0.049136724323034286\n",
      "Min loss: 0.0181199349462986\n",
      "Mean loss: 0.03451163632174333\n",
      "Std loss: 0.011223650589451068\n",
      "Total Loss: 0.20706981793045998\n",
      "------------------------------------ epoch 1865 (11184 steps) ------------------------------------\n",
      "Max loss: 0.06173375993967056\n",
      "Min loss: 0.015129531733691692\n",
      "Mean loss: 0.03091479791328311\n",
      "Std loss: 0.01643176456481185\n",
      "Total Loss: 0.18548878747969866\n",
      "------------------------------------ epoch 1866 (11190 steps) ------------------------------------\n",
      "Max loss: 0.03923612833023071\n",
      "Min loss: 0.012223395518958569\n",
      "Mean loss: 0.022705663771679003\n",
      "Std loss: 0.009009578659849776\n",
      "Total Loss: 0.13623398263007402\n",
      "------------------------------------ epoch 1867 (11196 steps) ------------------------------------\n",
      "Max loss: 0.04571794718503952\n",
      "Min loss: 0.012735489755868912\n",
      "Mean loss: 0.026397244383891422\n",
      "Std loss: 0.011829577098665618\n",
      "Total Loss: 0.15838346630334854\n",
      "------------------------------------ epoch 1868 (11202 steps) ------------------------------------\n",
      "Max loss: 0.03567736595869064\n",
      "Min loss: 0.012735113501548767\n",
      "Mean loss: 0.019984710185478132\n",
      "Std loss: 0.008043306998149086\n",
      "Total Loss: 0.11990826111286879\n",
      "------------------------------------ epoch 1869 (11208 steps) ------------------------------------\n",
      "Max loss: 0.0297323577105999\n",
      "Min loss: 0.01469703670591116\n",
      "Mean loss: 0.019896784331649542\n",
      "Std loss: 0.0051841270542670945\n",
      "Total Loss: 0.11938070598989725\n",
      "------------------------------------ epoch 1870 (11214 steps) ------------------------------------\n",
      "Max loss: 0.03346967697143555\n",
      "Min loss: 0.010593007318675518\n",
      "Mean loss: 0.017977719971289236\n",
      "Std loss: 0.009105372636577314\n",
      "Total Loss: 0.10786631982773542\n",
      "------------------------------------ epoch 1871 (11220 steps) ------------------------------------\n",
      "Max loss: 0.04579794034361839\n",
      "Min loss: 0.01490170881152153\n",
      "Mean loss: 0.022258243678758543\n",
      "Std loss: 0.010805698787944103\n",
      "Total Loss: 0.13354946207255125\n",
      "------------------------------------ epoch 1872 (11226 steps) ------------------------------------\n",
      "Max loss: 0.03648058697581291\n",
      "Min loss: 0.015385810285806656\n",
      "Mean loss: 0.020804830826818943\n",
      "Std loss: 0.007130492455917303\n",
      "Total Loss: 0.12482898496091366\n",
      "------------------------------------ epoch 1873 (11232 steps) ------------------------------------\n",
      "Max loss: 0.039892636239528656\n",
      "Min loss: 0.015336673706769943\n",
      "Mean loss: 0.02541282121092081\n",
      "Std loss: 0.008401650270324135\n",
      "Total Loss: 0.15247692726552486\n",
      "------------------------------------ epoch 1874 (11238 steps) ------------------------------------\n",
      "Max loss: 0.08125555515289307\n",
      "Min loss: 0.013503359630703926\n",
      "Mean loss: 0.03031705381969611\n",
      "Std loss: 0.023417672276862367\n",
      "Total Loss: 0.18190232291817665\n",
      "------------------------------------ epoch 1875 (11244 steps) ------------------------------------\n",
      "Max loss: 0.054231077432632446\n",
      "Min loss: 0.019422784447669983\n",
      "Mean loss: 0.03210846520960331\n",
      "Std loss: 0.011602336786826423\n",
      "Total Loss: 0.19265079125761986\n",
      "------------------------------------ epoch 1876 (11250 steps) ------------------------------------\n",
      "Max loss: 0.03289438411593437\n",
      "Min loss: 0.011212780140340328\n",
      "Mean loss: 0.023925390870620806\n",
      "Std loss: 0.008356660319532375\n",
      "Total Loss: 0.14355234522372484\n",
      "------------------------------------ epoch 1877 (11256 steps) ------------------------------------\n",
      "Max loss: 0.027202609926462173\n",
      "Min loss: 0.01217146497219801\n",
      "Mean loss: 0.020501116445908945\n",
      "Std loss: 0.00521706376621648\n",
      "Total Loss: 0.12300669867545366\n",
      "------------------------------------ epoch 1878 (11262 steps) ------------------------------------\n",
      "Max loss: 0.02767302840948105\n",
      "Min loss: 0.011965278536081314\n",
      "Mean loss: 0.017895449108133715\n",
      "Std loss: 0.004802325969898664\n",
      "Total Loss: 0.10737269464880228\n",
      "------------------------------------ epoch 1879 (11268 steps) ------------------------------------\n",
      "Max loss: 0.02390020340681076\n",
      "Min loss: 0.01174114178866148\n",
      "Mean loss: 0.018422262898335855\n",
      "Std loss: 0.0037713859008853276\n",
      "Total Loss: 0.11053357739001513\n",
      "------------------------------------ epoch 1880 (11274 steps) ------------------------------------\n",
      "Max loss: 0.04842408746480942\n",
      "Min loss: 0.013229236006736755\n",
      "Mean loss: 0.024685721844434738\n",
      "Std loss: 0.013493274232010369\n",
      "Total Loss: 0.14811433106660843\n",
      "------------------------------------ epoch 1881 (11280 steps) ------------------------------------\n",
      "Max loss: 0.04149152711033821\n",
      "Min loss: 0.01509328093379736\n",
      "Mean loss: 0.02319862398629387\n",
      "Std loss: 0.008645043983769798\n",
      "Total Loss: 0.13919174391776323\n",
      "------------------------------------ epoch 1882 (11286 steps) ------------------------------------\n",
      "Max loss: 0.05305423587560654\n",
      "Min loss: 0.010476373136043549\n",
      "Mean loss: 0.0202209932419161\n",
      "Std loss: 0.015038283617615392\n",
      "Total Loss: 0.1213259594514966\n",
      "------------------------------------ epoch 1883 (11292 steps) ------------------------------------\n",
      "Max loss: 0.03753095865249634\n",
      "Min loss: 0.012491796165704727\n",
      "Mean loss: 0.02358238057543834\n",
      "Std loss: 0.008765135352073026\n",
      "Total Loss: 0.14149428345263004\n",
      "------------------------------------ epoch 1884 (11298 steps) ------------------------------------\n",
      "Max loss: 0.07591458410024643\n",
      "Min loss: 0.013539629988372326\n",
      "Mean loss: 0.026756317354738712\n",
      "Std loss: 0.022098043534106954\n",
      "Total Loss: 0.16053790412843227\n",
      "------------------------------------ epoch 1885 (11304 steps) ------------------------------------\n",
      "Max loss: 0.04944198578596115\n",
      "Min loss: 0.01430213451385498\n",
      "Mean loss: 0.02574546014269193\n",
      "Std loss: 0.011372559514065048\n",
      "Total Loss: 0.15447276085615158\n",
      "------------------------------------ epoch 1886 (11310 steps) ------------------------------------\n",
      "Max loss: 0.06318151950836182\n",
      "Min loss: 0.01117223035544157\n",
      "Mean loss: 0.028762494834760826\n",
      "Std loss: 0.01702180356035877\n",
      "Total Loss: 0.17257496900856495\n",
      "------------------------------------ epoch 1887 (11316 steps) ------------------------------------\n",
      "Max loss: 0.05903513729572296\n",
      "Min loss: 0.014757382683455944\n",
      "Mean loss: 0.02599907557790478\n",
      "Std loss: 0.015042441726411787\n",
      "Total Loss: 0.15599445346742868\n",
      "------------------------------------ epoch 1888 (11322 steps) ------------------------------------\n",
      "Max loss: 0.050580523908138275\n",
      "Min loss: 0.012108966708183289\n",
      "Mean loss: 0.027688296201328438\n",
      "Std loss: 0.014032068196101423\n",
      "Total Loss: 0.16612977720797062\n",
      "------------------------------------ epoch 1889 (11328 steps) ------------------------------------\n",
      "Max loss: 0.06476014852523804\n",
      "Min loss: 0.01555989682674408\n",
      "Mean loss: 0.03477901003013054\n",
      "Std loss: 0.01691376377814558\n",
      "Total Loss: 0.20867406018078327\n",
      "------------------------------------ epoch 1890 (11334 steps) ------------------------------------\n",
      "Max loss: 0.05373420566320419\n",
      "Min loss: 0.013029633089900017\n",
      "Mean loss: 0.0327687036866943\n",
      "Std loss: 0.014517340333692952\n",
      "Total Loss: 0.19661222212016582\n",
      "------------------------------------ epoch 1891 (11340 steps) ------------------------------------\n",
      "Max loss: 0.026807047426700592\n",
      "Min loss: 0.016924206167459488\n",
      "Mean loss: 0.02272482557843129\n",
      "Std loss: 0.0030226969490099124\n",
      "Total Loss: 0.13634895347058773\n",
      "------------------------------------ epoch 1892 (11346 steps) ------------------------------------\n",
      "Max loss: 0.05235094577074051\n",
      "Min loss: 0.014667896553874016\n",
      "Mean loss: 0.025546135691305\n",
      "Std loss: 0.01267914245551512\n",
      "Total Loss: 0.15327681414783\n",
      "------------------------------------ epoch 1893 (11352 steps) ------------------------------------\n",
      "Max loss: 0.0657590851187706\n",
      "Min loss: 0.016261745244264603\n",
      "Mean loss: 0.031261581306656204\n",
      "Std loss: 0.017228178074793786\n",
      "Total Loss: 0.1875694878399372\n",
      "------------------------------------ epoch 1894 (11358 steps) ------------------------------------\n",
      "Max loss: 0.042054735124111176\n",
      "Min loss: 0.016408629715442657\n",
      "Mean loss: 0.030946113790074985\n",
      "Std loss: 0.009023879117357872\n",
      "Total Loss: 0.1856766827404499\n",
      "------------------------------------ epoch 1895 (11364 steps) ------------------------------------\n",
      "Max loss: 0.024440599605441093\n",
      "Min loss: 0.014193115755915642\n",
      "Mean loss: 0.01957459095865488\n",
      "Std loss: 0.0043769641325343105\n",
      "Total Loss: 0.11744754575192928\n",
      "------------------------------------ epoch 1896 (11370 steps) ------------------------------------\n",
      "Max loss: 0.0762588158249855\n",
      "Min loss: 0.013273386284708977\n",
      "Mean loss: 0.032990179335077606\n",
      "Std loss: 0.020394206523147714\n",
      "Total Loss: 0.19794107601046562\n",
      "------------------------------------ epoch 1897 (11376 steps) ------------------------------------\n",
      "Max loss: 0.038788966834545135\n",
      "Min loss: 0.011540815234184265\n",
      "Mean loss: 0.02201429009437561\n",
      "Std loss: 0.008726129315353557\n",
      "Total Loss: 0.13208574056625366\n",
      "------------------------------------ epoch 1898 (11382 steps) ------------------------------------\n",
      "Max loss: 0.04167727380990982\n",
      "Min loss: 0.011580515652894974\n",
      "Mean loss: 0.026628074546655018\n",
      "Std loss: 0.008828297381629562\n",
      "Total Loss: 0.15976844727993011\n",
      "------------------------------------ epoch 1899 (11388 steps) ------------------------------------\n",
      "Max loss: 0.03898654878139496\n",
      "Min loss: 0.010101290419697762\n",
      "Mean loss: 0.025265083958705265\n",
      "Std loss: 0.00930965098595676\n",
      "Total Loss: 0.1515905037522316\n",
      "------------------------------------ epoch 1900 (11394 steps) ------------------------------------\n",
      "Max loss: 0.035446710884571075\n",
      "Min loss: 0.011503386311233044\n",
      "Mean loss: 0.02116953224564592\n",
      "Std loss: 0.008039460446544766\n",
      "Total Loss: 0.12701719347387552\n",
      "------------------------------------ epoch 1901 (11400 steps) ------------------------------------\n",
      "Max loss: 0.05080553889274597\n",
      "Min loss: 0.018540553748607635\n",
      "Mean loss: 0.03162705587844054\n",
      "Std loss: 0.013568564675206911\n",
      "Total Loss: 0.18976233527064323\n",
      "saved model at ./weights/model_1901.pth\n",
      "------------------------------------ epoch 1902 (11406 steps) ------------------------------------\n",
      "Max loss: 0.03828659653663635\n",
      "Min loss: 0.020330414175987244\n",
      "Mean loss: 0.030894110289712746\n",
      "Std loss: 0.006707848662065256\n",
      "Total Loss: 0.18536466173827648\n",
      "------------------------------------ epoch 1903 (11412 steps) ------------------------------------\n",
      "Max loss: 0.03939436376094818\n",
      "Min loss: 0.011286105960607529\n",
      "Mean loss: 0.020832317881286144\n",
      "Std loss: 0.009434614531112096\n",
      "Total Loss: 0.12499390728771687\n",
      "------------------------------------ epoch 1904 (11418 steps) ------------------------------------\n",
      "Max loss: 0.02695581316947937\n",
      "Min loss: 0.01522323489189148\n",
      "Mean loss: 0.0204851267238458\n",
      "Std loss: 0.00348961354789003\n",
      "Total Loss: 0.1229107603430748\n",
      "------------------------------------ epoch 1905 (11424 steps) ------------------------------------\n",
      "Max loss: 0.08289361745119095\n",
      "Min loss: 0.013530932366847992\n",
      "Mean loss: 0.03374311296890179\n",
      "Std loss: 0.023641414230803945\n",
      "Total Loss: 0.20245867781341076\n",
      "------------------------------------ epoch 1906 (11430 steps) ------------------------------------\n",
      "Max loss: 0.04670393466949463\n",
      "Min loss: 0.01456642895936966\n",
      "Mean loss: 0.02256159686173002\n",
      "Std loss: 0.011554832032292123\n",
      "Total Loss: 0.13536958117038012\n",
      "------------------------------------ epoch 1907 (11436 steps) ------------------------------------\n",
      "Max loss: 0.037879351526498795\n",
      "Min loss: 0.017425015568733215\n",
      "Mean loss: 0.024307778105139732\n",
      "Std loss: 0.0070519736519229275\n",
      "Total Loss: 0.1458466686308384\n",
      "------------------------------------ epoch 1908 (11442 steps) ------------------------------------\n",
      "Max loss: 0.03369702026247978\n",
      "Min loss: 0.013416023924946785\n",
      "Mean loss: 0.021658845866719883\n",
      "Std loss: 0.006486502698630104\n",
      "Total Loss: 0.1299530752003193\n",
      "------------------------------------ epoch 1909 (11448 steps) ------------------------------------\n",
      "Max loss: 0.06199110671877861\n",
      "Min loss: 0.019508851692080498\n",
      "Mean loss: 0.031054009993871052\n",
      "Std loss: 0.01489613634924267\n",
      "Total Loss: 0.18632405996322632\n",
      "------------------------------------ epoch 1910 (11454 steps) ------------------------------------\n",
      "Max loss: 0.04903624951839447\n",
      "Min loss: 0.011388893239200115\n",
      "Mean loss: 0.027436754821489256\n",
      "Std loss: 0.011188971624964541\n",
      "Total Loss: 0.16462052892893553\n",
      "------------------------------------ epoch 1911 (11460 steps) ------------------------------------\n",
      "Max loss: 0.05067798122763634\n",
      "Min loss: 0.012413924559950829\n",
      "Mean loss: 0.02407055317113797\n",
      "Std loss: 0.012524669768592284\n",
      "Total Loss: 0.1444233190268278\n",
      "------------------------------------ epoch 1912 (11466 steps) ------------------------------------\n",
      "Max loss: 0.04330579936504364\n",
      "Min loss: 0.01105990819633007\n",
      "Mean loss: 0.024129540814707678\n",
      "Std loss: 0.01031507403027174\n",
      "Total Loss: 0.14477724488824606\n",
      "------------------------------------ epoch 1913 (11472 steps) ------------------------------------\n",
      "Max loss: 0.0399460569024086\n",
      "Min loss: 0.011610304936766624\n",
      "Mean loss: 0.02222299762070179\n",
      "Std loss: 0.00868801538270456\n",
      "Total Loss: 0.13333798572421074\n",
      "------------------------------------ epoch 1914 (11478 steps) ------------------------------------\n",
      "Max loss: 0.05284824222326279\n",
      "Min loss: 0.010764582082629204\n",
      "Mean loss: 0.022145032572249573\n",
      "Std loss: 0.014088419388552011\n",
      "Total Loss: 0.13287019543349743\n",
      "------------------------------------ epoch 1915 (11484 steps) ------------------------------------\n",
      "Max loss: 0.039292141795158386\n",
      "Min loss: 0.010163369588553905\n",
      "Mean loss: 0.01872016939645012\n",
      "Std loss: 0.009909118732240794\n",
      "Total Loss: 0.11232101637870073\n",
      "------------------------------------ epoch 1916 (11490 steps) ------------------------------------\n",
      "Max loss: 0.04117849841713905\n",
      "Min loss: 0.011070560663938522\n",
      "Mean loss: 0.02742624143138528\n",
      "Std loss: 0.01191712124461321\n",
      "Total Loss: 0.16455744858831167\n",
      "------------------------------------ epoch 1917 (11496 steps) ------------------------------------\n",
      "Max loss: 0.0516003780066967\n",
      "Min loss: 0.013012507930397987\n",
      "Mean loss: 0.03227923127512137\n",
      "Std loss: 0.01148188192186687\n",
      "Total Loss: 0.19367538765072823\n",
      "------------------------------------ epoch 1918 (11502 steps) ------------------------------------\n",
      "Max loss: 0.04149813577532768\n",
      "Min loss: 0.014142921194434166\n",
      "Mean loss: 0.02273155904064576\n",
      "Std loss: 0.008864601403651097\n",
      "Total Loss: 0.13638935424387455\n",
      "------------------------------------ epoch 1919 (11508 steps) ------------------------------------\n",
      "Max loss: 0.03641798719763756\n",
      "Min loss: 0.014566175639629364\n",
      "Mean loss: 0.019741473409036796\n",
      "Std loss: 0.007595241598897654\n",
      "Total Loss: 0.11844884045422077\n",
      "------------------------------------ epoch 1920 (11514 steps) ------------------------------------\n",
      "Max loss: 0.03215250000357628\n",
      "Min loss: 0.012693388387560844\n",
      "Mean loss: 0.02098994484792153\n",
      "Std loss: 0.0063984379884836394\n",
      "Total Loss: 0.12593966908752918\n",
      "------------------------------------ epoch 1921 (11520 steps) ------------------------------------\n",
      "Max loss: 0.02598247304558754\n",
      "Min loss: 0.0122637078166008\n",
      "Mean loss: 0.018712295219302177\n",
      "Std loss: 0.00451625712696423\n",
      "Total Loss: 0.11227377131581306\n",
      "------------------------------------ epoch 1922 (11526 steps) ------------------------------------\n",
      "Max loss: 0.04150193929672241\n",
      "Min loss: 0.010997479781508446\n",
      "Mean loss: 0.024582228623330593\n",
      "Std loss: 0.009539098485401902\n",
      "Total Loss: 0.14749337173998356\n",
      "------------------------------------ epoch 1923 (11532 steps) ------------------------------------\n",
      "Max loss: 0.03861266374588013\n",
      "Min loss: 0.01383377518504858\n",
      "Mean loss: 0.023726010229438543\n",
      "Std loss: 0.007654816712940771\n",
      "Total Loss: 0.14235606137663126\n",
      "------------------------------------ epoch 1924 (11538 steps) ------------------------------------\n",
      "Max loss: 0.027414124459028244\n",
      "Min loss: 0.012787748128175735\n",
      "Mean loss: 0.01964806749795874\n",
      "Std loss: 0.00505708169670533\n",
      "Total Loss: 0.11788840498775244\n",
      "------------------------------------ epoch 1925 (11544 steps) ------------------------------------\n",
      "Max loss: 0.03509117662906647\n",
      "Min loss: 0.015612570568919182\n",
      "Mean loss: 0.02045348131408294\n",
      "Std loss: 0.006839715457595814\n",
      "Total Loss: 0.12272088788449764\n",
      "------------------------------------ epoch 1926 (11550 steps) ------------------------------------\n",
      "Max loss: 0.04499659687280655\n",
      "Min loss: 0.011017382144927979\n",
      "Mean loss: 0.022985998541116714\n",
      "Std loss: 0.010995146251627286\n",
      "Total Loss: 0.1379159912467003\n",
      "------------------------------------ epoch 1927 (11556 steps) ------------------------------------\n",
      "Max loss: 0.06960383802652359\n",
      "Min loss: 0.01242805551737547\n",
      "Mean loss: 0.035890046041458845\n",
      "Std loss: 0.01939724718926687\n",
      "Total Loss: 0.21534027624875307\n",
      "------------------------------------ epoch 1928 (11562 steps) ------------------------------------\n",
      "Max loss: 0.04962915927171707\n",
      "Min loss: 0.013247771188616753\n",
      "Mean loss: 0.02125597031166156\n",
      "Std loss: 0.01274671754253964\n",
      "Total Loss: 0.12753582186996937\n",
      "------------------------------------ epoch 1929 (11568 steps) ------------------------------------\n",
      "Max loss: 0.12560483813285828\n",
      "Min loss: 0.017798282206058502\n",
      "Mean loss: 0.048921353494127594\n",
      "Std loss: 0.03670062484582829\n",
      "Total Loss: 0.29352812096476555\n",
      "------------------------------------ epoch 1930 (11574 steps) ------------------------------------\n",
      "Max loss: 0.060785092413425446\n",
      "Min loss: 0.017955541610717773\n",
      "Mean loss: 0.038919853356977306\n",
      "Std loss: 0.013823586587083827\n",
      "Total Loss: 0.23351912014186382\n",
      "------------------------------------ epoch 1931 (11580 steps) ------------------------------------\n",
      "Max loss: 0.09877143800258636\n",
      "Min loss: 0.01528503280133009\n",
      "Mean loss: 0.03608227319394549\n",
      "Std loss: 0.0284304030029826\n",
      "Total Loss: 0.21649363916367292\n",
      "------------------------------------ epoch 1932 (11586 steps) ------------------------------------\n",
      "Max loss: 0.06616498529911041\n",
      "Min loss: 0.017484566196799278\n",
      "Mean loss: 0.03754821481804053\n",
      "Std loss: 0.01827636204052565\n",
      "Total Loss: 0.22528928890824318\n",
      "------------------------------------ epoch 1933 (11592 steps) ------------------------------------\n",
      "Max loss: 0.05254867300391197\n",
      "Min loss: 0.01873422972857952\n",
      "Mean loss: 0.03712160357584556\n",
      "Std loss: 0.01134807138906386\n",
      "Total Loss: 0.22272962145507336\n",
      "------------------------------------ epoch 1934 (11598 steps) ------------------------------------\n",
      "Max loss: 0.060837388038635254\n",
      "Min loss: 0.01344984769821167\n",
      "Mean loss: 0.02945336823662122\n",
      "Std loss: 0.015968121725630082\n",
      "Total Loss: 0.17672020941972733\n",
      "------------------------------------ epoch 1935 (11604 steps) ------------------------------------\n",
      "Max loss: 0.05217437073588371\n",
      "Min loss: 0.015831321477890015\n",
      "Mean loss: 0.027370878495275974\n",
      "Std loss: 0.012913285588753582\n",
      "Total Loss: 0.16422527097165585\n",
      "------------------------------------ epoch 1936 (11610 steps) ------------------------------------\n",
      "Max loss: 0.056536655873060226\n",
      "Min loss: 0.015102572739124298\n",
      "Mean loss: 0.031930520199239254\n",
      "Std loss: 0.012663549926742845\n",
      "Total Loss: 0.19158312119543552\n",
      "------------------------------------ epoch 1937 (11616 steps) ------------------------------------\n",
      "Max loss: 0.09944000840187073\n",
      "Min loss: 0.02485007606446743\n",
      "Mean loss: 0.047500052178899445\n",
      "Std loss: 0.028532969369544206\n",
      "Total Loss: 0.2850003130733967\n",
      "------------------------------------ epoch 1938 (11622 steps) ------------------------------------\n",
      "Max loss: 0.04402841627597809\n",
      "Min loss: 0.014889707788825035\n",
      "Mean loss: 0.029355438115696113\n",
      "Std loss: 0.010957966967341509\n",
      "Total Loss: 0.17613262869417667\n",
      "------------------------------------ epoch 1939 (11628 steps) ------------------------------------\n",
      "Max loss: 0.031816594302654266\n",
      "Min loss: 0.0165815819054842\n",
      "Mean loss: 0.02134755626320839\n",
      "Std loss: 0.005506369068139943\n",
      "Total Loss: 0.12808533757925034\n",
      "------------------------------------ epoch 1940 (11634 steps) ------------------------------------\n",
      "Max loss: 0.04156649112701416\n",
      "Min loss: 0.01686740107834339\n",
      "Mean loss: 0.027155534674723942\n",
      "Std loss: 0.008541538135078377\n",
      "Total Loss: 0.16293320804834366\n",
      "------------------------------------ epoch 1941 (11640 steps) ------------------------------------\n",
      "Max loss: 0.036777183413505554\n",
      "Min loss: 0.013640617951750755\n",
      "Mean loss: 0.021132750436663628\n",
      "Std loss: 0.008138716339747916\n",
      "Total Loss: 0.12679650261998177\n",
      "------------------------------------ epoch 1942 (11646 steps) ------------------------------------\n",
      "Max loss: 0.0369538813829422\n",
      "Min loss: 0.0161893367767334\n",
      "Mean loss: 0.022936795217295487\n",
      "Std loss: 0.008153808586225017\n",
      "Total Loss: 0.13762077130377293\n",
      "------------------------------------ epoch 1943 (11652 steps) ------------------------------------\n",
      "Max loss: 0.03815923631191254\n",
      "Min loss: 0.009905483573675156\n",
      "Mean loss: 0.01890808840592702\n",
      "Std loss: 0.009045853374529135\n",
      "Total Loss: 0.11344853043556213\n",
      "------------------------------------ epoch 1944 (11658 steps) ------------------------------------\n",
      "Max loss: 0.03349308297038078\n",
      "Min loss: 0.016292046755552292\n",
      "Mean loss: 0.023437388241291046\n",
      "Std loss: 0.005570202011258152\n",
      "Total Loss: 0.14062432944774628\n",
      "------------------------------------ epoch 1945 (11664 steps) ------------------------------------\n",
      "Max loss: 0.044891294091939926\n",
      "Min loss: 0.012174922972917557\n",
      "Mean loss: 0.028303065337240696\n",
      "Std loss: 0.010597052350371483\n",
      "Total Loss: 0.16981839202344418\n",
      "------------------------------------ epoch 1946 (11670 steps) ------------------------------------\n",
      "Max loss: 0.017698675394058228\n",
      "Min loss: 0.011350835673511028\n",
      "Mean loss: 0.014388859582444033\n",
      "Std loss: 0.002440595439388281\n",
      "Total Loss: 0.08633315749466419\n",
      "------------------------------------ epoch 1947 (11676 steps) ------------------------------------\n",
      "Max loss: 0.0456414669752121\n",
      "Min loss: 0.012833597138524055\n",
      "Mean loss: 0.024914246207724016\n",
      "Std loss: 0.011395458440687466\n",
      "Total Loss: 0.1494854772463441\n",
      "------------------------------------ epoch 1948 (11682 steps) ------------------------------------\n",
      "Max loss: 0.049039822071790695\n",
      "Min loss: 0.010837508365511894\n",
      "Mean loss: 0.026917205502589543\n",
      "Std loss: 0.012508363176932092\n",
      "Total Loss: 0.16150323301553726\n",
      "------------------------------------ epoch 1949 (11688 steps) ------------------------------------\n",
      "Max loss: 0.031992897391319275\n",
      "Min loss: 0.013859088532626629\n",
      "Mean loss: 0.022620034869760275\n",
      "Std loss: 0.006004588411886837\n",
      "Total Loss: 0.13572020921856165\n",
      "------------------------------------ epoch 1950 (11694 steps) ------------------------------------\n",
      "Max loss: 0.09871503710746765\n",
      "Min loss: 0.013814969919621944\n",
      "Mean loss: 0.039899935480207205\n",
      "Std loss: 0.030784781364655046\n",
      "Total Loss: 0.23939961288124323\n",
      "------------------------------------ epoch 1951 (11700 steps) ------------------------------------\n",
      "Max loss: 0.04481898993253708\n",
      "Min loss: 0.013841301202774048\n",
      "Mean loss: 0.025242753326892853\n",
      "Std loss: 0.011852054640602867\n",
      "Total Loss: 0.15145651996135712\n",
      "------------------------------------ epoch 1952 (11706 steps) ------------------------------------\n",
      "Max loss: 0.05114436894655228\n",
      "Min loss: 0.011532887816429138\n",
      "Mean loss: 0.025608773808926344\n",
      "Std loss: 0.014313403454418892\n",
      "Total Loss: 0.15365264285355806\n",
      "------------------------------------ epoch 1953 (11712 steps) ------------------------------------\n",
      "Max loss: 0.046638112515211105\n",
      "Min loss: 0.01669212058186531\n",
      "Mean loss: 0.026546773811181385\n",
      "Std loss: 0.009674731945471084\n",
      "Total Loss: 0.15928064286708832\n",
      "------------------------------------ epoch 1954 (11718 steps) ------------------------------------\n",
      "Max loss: 0.037774913012981415\n",
      "Min loss: 0.01285147201269865\n",
      "Mean loss: 0.02402936186020573\n",
      "Std loss: 0.009985203668622366\n",
      "Total Loss: 0.14417617116123438\n",
      "------------------------------------ epoch 1955 (11724 steps) ------------------------------------\n",
      "Max loss: 0.03633829206228256\n",
      "Min loss: 0.017300132662057877\n",
      "Mean loss: 0.026484142988920212\n",
      "Std loss: 0.006610231400576654\n",
      "Total Loss: 0.15890485793352127\n",
      "------------------------------------ epoch 1956 (11730 steps) ------------------------------------\n",
      "Max loss: 0.07828870415687561\n",
      "Min loss: 0.01311038713902235\n",
      "Mean loss: 0.02732936277364691\n",
      "Std loss: 0.022963528257963786\n",
      "Total Loss: 0.16397617664188147\n",
      "------------------------------------ epoch 1957 (11736 steps) ------------------------------------\n",
      "Max loss: 0.038233112543821335\n",
      "Min loss: 0.01600368693470955\n",
      "Mean loss: 0.02379834434638421\n",
      "Std loss: 0.0076740325443320065\n",
      "Total Loss: 0.14279006607830524\n",
      "------------------------------------ epoch 1958 (11742 steps) ------------------------------------\n",
      "Max loss: 0.03953080251812935\n",
      "Min loss: 0.011171245947480202\n",
      "Mean loss: 0.02883878070861101\n",
      "Std loss: 0.00922044430056876\n",
      "Total Loss: 0.17303268425166607\n",
      "------------------------------------ epoch 1959 (11748 steps) ------------------------------------\n",
      "Max loss: 0.026683365926146507\n",
      "Min loss: 0.017067456617951393\n",
      "Mean loss: 0.02160150868197282\n",
      "Std loss: 0.003546426449607783\n",
      "Total Loss: 0.12960905209183693\n",
      "------------------------------------ epoch 1960 (11754 steps) ------------------------------------\n",
      "Max loss: 0.0297459214925766\n",
      "Min loss: 0.010850872844457626\n",
      "Mean loss: 0.022412878771622975\n",
      "Std loss: 0.006467283576062173\n",
      "Total Loss: 0.13447727262973785\n",
      "------------------------------------ epoch 1961 (11760 steps) ------------------------------------\n",
      "Max loss: 0.054061081260442734\n",
      "Min loss: 0.012933994643390179\n",
      "Mean loss: 0.026544141738365095\n",
      "Std loss: 0.013718205471498655\n",
      "Total Loss: 0.15926485043019056\n",
      "------------------------------------ epoch 1962 (11766 steps) ------------------------------------\n",
      "Max loss: 0.05706712603569031\n",
      "Min loss: 0.013920615427196026\n",
      "Mean loss: 0.02473706239834428\n",
      "Std loss: 0.014811001988089248\n",
      "Total Loss: 0.14842237439006567\n",
      "------------------------------------ epoch 1963 (11772 steps) ------------------------------------\n",
      "Max loss: 0.03535079210996628\n",
      "Min loss: 0.013819048181176186\n",
      "Mean loss: 0.021610058688869078\n",
      "Std loss: 0.008756680832479017\n",
      "Total Loss: 0.12966035213321447\n",
      "------------------------------------ epoch 1964 (11778 steps) ------------------------------------\n",
      "Max loss: 0.019773313775658607\n",
      "Min loss: 0.010494980961084366\n",
      "Mean loss: 0.015680461811522644\n",
      "Std loss: 0.003018647465558742\n",
      "Total Loss: 0.09408277086913586\n",
      "------------------------------------ epoch 1965 (11784 steps) ------------------------------------\n",
      "Max loss: 0.02576175145804882\n",
      "Min loss: 0.010391155257821083\n",
      "Mean loss: 0.021331441588699818\n",
      "Std loss: 0.005155191175996839\n",
      "Total Loss: 0.1279886495321989\n",
      "------------------------------------ epoch 1966 (11790 steps) ------------------------------------\n",
      "Max loss: 0.03845155984163284\n",
      "Min loss: 0.01911209151148796\n",
      "Mean loss: 0.030156799281636875\n",
      "Std loss: 0.006798837950653345\n",
      "Total Loss: 0.18094079568982124\n",
      "------------------------------------ epoch 1967 (11796 steps) ------------------------------------\n",
      "Max loss: 0.06244870647788048\n",
      "Min loss: 0.017846927046775818\n",
      "Mean loss: 0.03097995836287737\n",
      "Std loss: 0.016793023391415276\n",
      "Total Loss: 0.1858797501772642\n",
      "------------------------------------ epoch 1968 (11802 steps) ------------------------------------\n",
      "Max loss: 0.05489470809698105\n",
      "Min loss: 0.028077509254217148\n",
      "Mean loss: 0.03905651345849037\n",
      "Std loss: 0.009137432093589208\n",
      "Total Loss: 0.23433908075094223\n",
      "------------------------------------ epoch 1969 (11808 steps) ------------------------------------\n",
      "Max loss: 0.04228030890226364\n",
      "Min loss: 0.013976257294416428\n",
      "Mean loss: 0.027957771439105272\n",
      "Std loss: 0.010450268677532544\n",
      "Total Loss: 0.16774662863463163\n",
      "------------------------------------ epoch 1970 (11814 steps) ------------------------------------\n",
      "Max loss: 0.030275844037532806\n",
      "Min loss: 0.014609435573220253\n",
      "Mean loss: 0.021757556435962517\n",
      "Std loss: 0.004725444226832014\n",
      "Total Loss: 0.1305453386157751\n",
      "------------------------------------ epoch 1971 (11820 steps) ------------------------------------\n",
      "Max loss: 0.04427053779363632\n",
      "Min loss: 0.014153818599879742\n",
      "Mean loss: 0.02901241664464275\n",
      "Std loss: 0.012410447247841138\n",
      "Total Loss: 0.1740744998678565\n",
      "------------------------------------ epoch 1972 (11826 steps) ------------------------------------\n",
      "Max loss: 0.0545104444026947\n",
      "Min loss: 0.01947340928018093\n",
      "Mean loss: 0.029119019707043965\n",
      "Std loss: 0.01214528807842405\n",
      "Total Loss: 0.1747141182422638\n",
      "------------------------------------ epoch 1973 (11832 steps) ------------------------------------\n",
      "Max loss: 0.024319294840097427\n",
      "Min loss: 0.013369633816182613\n",
      "Mean loss: 0.020320018598188955\n",
      "Std loss: 0.003665127695826867\n",
      "Total Loss: 0.12192011158913374\n",
      "------------------------------------ epoch 1974 (11838 steps) ------------------------------------\n",
      "Max loss: 0.04433450847864151\n",
      "Min loss: 0.012702777981758118\n",
      "Mean loss: 0.02237932151183486\n",
      "Std loss: 0.010575507001240407\n",
      "Total Loss: 0.13427592907100916\n",
      "------------------------------------ epoch 1975 (11844 steps) ------------------------------------\n",
      "Max loss: 0.059014514088630676\n",
      "Min loss: 0.012423696927726269\n",
      "Mean loss: 0.025519509334117174\n",
      "Std loss: 0.015721643231561038\n",
      "Total Loss: 0.15311705600470304\n",
      "------------------------------------ epoch 1976 (11850 steps) ------------------------------------\n",
      "Max loss: 0.05398629233241081\n",
      "Min loss: 0.021244633942842484\n",
      "Mean loss: 0.03346251416951418\n",
      "Std loss: 0.012407318675030445\n",
      "Total Loss: 0.20077508501708508\n",
      "------------------------------------ epoch 1977 (11856 steps) ------------------------------------\n",
      "Max loss: 0.049567531794309616\n",
      "Min loss: 0.01706165447831154\n",
      "Mean loss: 0.034833585222562156\n",
      "Std loss: 0.013847462048609403\n",
      "Total Loss: 0.20900151133537292\n",
      "------------------------------------ epoch 1978 (11862 steps) ------------------------------------\n",
      "Max loss: 0.04122209548950195\n",
      "Min loss: 0.011251944117248058\n",
      "Mean loss: 0.026307300198823214\n",
      "Std loss: 0.010853879868143136\n",
      "Total Loss: 0.15784380119293928\n",
      "------------------------------------ epoch 1979 (11868 steps) ------------------------------------\n",
      "Max loss: 0.03666205704212189\n",
      "Min loss: 0.01583893410861492\n",
      "Mean loss: 0.026407128510375816\n",
      "Std loss: 0.007653848335351257\n",
      "Total Loss: 0.1584427710622549\n",
      "------------------------------------ epoch 1980 (11874 steps) ------------------------------------\n",
      "Max loss: 0.0522623211145401\n",
      "Min loss: 0.011370456777513027\n",
      "Mean loss: 0.020929171703755856\n",
      "Std loss: 0.014248285585054562\n",
      "Total Loss: 0.12557503022253513\n",
      "------------------------------------ epoch 1981 (11880 steps) ------------------------------------\n",
      "Max loss: 0.027453279122710228\n",
      "Min loss: 0.01052191760390997\n",
      "Mean loss: 0.01813256631915768\n",
      "Std loss: 0.0063153882599583655\n",
      "Total Loss: 0.10879539791494608\n",
      "------------------------------------ epoch 1982 (11886 steps) ------------------------------------\n",
      "Max loss: 0.04243115335702896\n",
      "Min loss: 0.016952751204371452\n",
      "Mean loss: 0.02462450818469127\n",
      "Std loss: 0.008489986486134669\n",
      "Total Loss: 0.14774704910814762\n",
      "------------------------------------ epoch 1983 (11892 steps) ------------------------------------\n",
      "Max loss: 0.03823013976216316\n",
      "Min loss: 0.012776091694831848\n",
      "Mean loss: 0.02521293020496766\n",
      "Std loss: 0.008204501858328368\n",
      "Total Loss: 0.15127758122980595\n",
      "------------------------------------ epoch 1984 (11898 steps) ------------------------------------\n",
      "Max loss: 0.03193926438689232\n",
      "Min loss: 0.011684129945933819\n",
      "Mean loss: 0.023377406876534224\n",
      "Std loss: 0.0068732356548668995\n",
      "Total Loss: 0.14026444125920534\n",
      "------------------------------------ epoch 1985 (11904 steps) ------------------------------------\n",
      "Max loss: 0.0440957248210907\n",
      "Min loss: 0.012940633110702038\n",
      "Mean loss: 0.023536518526573975\n",
      "Std loss: 0.010281135740013072\n",
      "Total Loss: 0.14121911115944386\n",
      "------------------------------------ epoch 1986 (11910 steps) ------------------------------------\n",
      "Max loss: 0.06490714102983475\n",
      "Min loss: 0.01096160989254713\n",
      "Mean loss: 0.02449661275992791\n",
      "Std loss: 0.018369907584989364\n",
      "Total Loss: 0.14697967655956745\n",
      "------------------------------------ epoch 1987 (11916 steps) ------------------------------------\n",
      "Max loss: 0.07349790632724762\n",
      "Min loss: 0.010477639734745026\n",
      "Mean loss: 0.02899891681348284\n",
      "Std loss: 0.022927362986082833\n",
      "Total Loss: 0.17399350088089705\n",
      "------------------------------------ epoch 1988 (11922 steps) ------------------------------------\n",
      "Max loss: 0.06788869202136993\n",
      "Min loss: 0.010433688759803772\n",
      "Mean loss: 0.028741558858503897\n",
      "Std loss: 0.019507469603426867\n",
      "Total Loss: 0.1724493531510234\n",
      "------------------------------------ epoch 1989 (11928 steps) ------------------------------------\n",
      "Max loss: 0.03292989358305931\n",
      "Min loss: 0.010713150724768639\n",
      "Mean loss: 0.02069834029922883\n",
      "Std loss: 0.008244532169496027\n",
      "Total Loss: 0.12419004179537296\n",
      "------------------------------------ epoch 1990 (11934 steps) ------------------------------------\n",
      "Max loss: 0.04534495994448662\n",
      "Min loss: 0.012165648862719536\n",
      "Mean loss: 0.024557899373273056\n",
      "Std loss: 0.010821762422509906\n",
      "Total Loss: 0.14734739623963833\n",
      "------------------------------------ epoch 1991 (11940 steps) ------------------------------------\n",
      "Max loss: 0.023897459730505943\n",
      "Min loss: 0.010418886318802834\n",
      "Mean loss: 0.018220284022390842\n",
      "Std loss: 0.004636414950798337\n",
      "Total Loss: 0.10932170413434505\n",
      "------------------------------------ epoch 1992 (11946 steps) ------------------------------------\n",
      "Max loss: 0.03712700679898262\n",
      "Min loss: 0.013125753030180931\n",
      "Mean loss: 0.02026733352492253\n",
      "Std loss: 0.008172277247302892\n",
      "Total Loss: 0.12160400114953518\n",
      "------------------------------------ epoch 1993 (11952 steps) ------------------------------------\n",
      "Max loss: 0.03769576922059059\n",
      "Min loss: 0.011531641706824303\n",
      "Mean loss: 0.01913812089090546\n",
      "Std loss: 0.008871003310206068\n",
      "Total Loss: 0.11482872534543276\n",
      "------------------------------------ epoch 1994 (11958 steps) ------------------------------------\n",
      "Max loss: 0.04326466843485832\n",
      "Min loss: 0.014770222827792168\n",
      "Mean loss: 0.02651158223549525\n",
      "Std loss: 0.01099177349625514\n",
      "Total Loss: 0.1590694934129715\n",
      "------------------------------------ epoch 1995 (11964 steps) ------------------------------------\n",
      "Max loss: 0.03867439180612564\n",
      "Min loss: 0.01168120838701725\n",
      "Mean loss: 0.022606732323765755\n",
      "Std loss: 0.010474757889305545\n",
      "Total Loss: 0.13564039394259453\n",
      "------------------------------------ epoch 1996 (11970 steps) ------------------------------------\n",
      "Max loss: 0.035725511610507965\n",
      "Min loss: 0.015252036973834038\n",
      "Mean loss: 0.02336656612654527\n",
      "Std loss: 0.006457915374231348\n",
      "Total Loss: 0.14019939675927162\n",
      "------------------------------------ epoch 1997 (11976 steps) ------------------------------------\n",
      "Max loss: 0.028576191514730453\n",
      "Min loss: 0.011936619877815247\n",
      "Mean loss: 0.018699500244110823\n",
      "Std loss: 0.005430262568041804\n",
      "Total Loss: 0.11219700146466494\n",
      "------------------------------------ epoch 1998 (11982 steps) ------------------------------------\n",
      "Max loss: 0.02494763396680355\n",
      "Min loss: 0.010126492008566856\n",
      "Mean loss: 0.015873294168462355\n",
      "Std loss: 0.004671129207163973\n",
      "Total Loss: 0.09523976501077414\n",
      "------------------------------------ epoch 1999 (11988 steps) ------------------------------------\n",
      "Max loss: 0.031629808247089386\n",
      "Min loss: 0.009312452748417854\n",
      "Mean loss: 0.017192821018397808\n",
      "Std loss: 0.006907230761442057\n",
      "Total Loss: 0.10315692611038685\n",
      "------------------------------------ epoch 2000 (11994 steps) ------------------------------------\n",
      "Max loss: 0.02865408919751644\n",
      "Min loss: 0.015773095190525055\n",
      "Mean loss: 0.021957722182075184\n",
      "Std loss: 0.0042084324127409315\n",
      "Total Loss: 0.1317463330924511\n",
      "------------------------------------ epoch 2001 (12000 steps) ------------------------------------\n",
      "Max loss: 0.02390335127711296\n",
      "Min loss: 0.010777600109577179\n",
      "Mean loss: 0.018112875521183014\n",
      "Std loss: 0.003961428588369778\n",
      "Total Loss: 0.10867725312709808\n",
      "saved model at ./weights/model_2001.pth\n",
      "------------------------------------ epoch 2002 (12006 steps) ------------------------------------\n",
      "Max loss: 0.0504319965839386\n",
      "Min loss: 0.0091183390468359\n",
      "Mean loss: 0.02209794397155444\n",
      "Std loss: 0.014287095507888694\n",
      "Total Loss: 0.13258766382932663\n",
      "------------------------------------ epoch 2003 (12012 steps) ------------------------------------\n",
      "Max loss: 0.029287558048963547\n",
      "Min loss: 0.011672340333461761\n",
      "Mean loss: 0.018831796788920958\n",
      "Std loss: 0.005577693571560229\n",
      "Total Loss: 0.11299078073352575\n",
      "------------------------------------ epoch 2004 (12018 steps) ------------------------------------\n",
      "Max loss: 0.01753760688006878\n",
      "Min loss: 0.00965174287557602\n",
      "Mean loss: 0.014890481407443682\n",
      "Std loss: 0.00269523361677562\n",
      "Total Loss: 0.0893428884446621\n",
      "------------------------------------ epoch 2005 (12024 steps) ------------------------------------\n",
      "Max loss: 0.043545059859752655\n",
      "Min loss: 0.010575056076049805\n",
      "Mean loss: 0.023153036832809448\n",
      "Std loss: 0.011588318495826071\n",
      "Total Loss: 0.1389182209968567\n",
      "------------------------------------ epoch 2006 (12030 steps) ------------------------------------\n",
      "Max loss: 0.03356238454580307\n",
      "Min loss: 0.010531785897910595\n",
      "Mean loss: 0.019402588872859877\n",
      "Std loss: 0.008846763013906786\n",
      "Total Loss: 0.11641553323715925\n",
      "------------------------------------ epoch 2007 (12036 steps) ------------------------------------\n",
      "Max loss: 0.03420628607273102\n",
      "Min loss: 0.012450396083295345\n",
      "Mean loss: 0.02106604678556323\n",
      "Std loss: 0.007563878763892832\n",
      "Total Loss: 0.12639628071337938\n",
      "------------------------------------ epoch 2008 (12042 steps) ------------------------------------\n",
      "Max loss: 0.0641610398888588\n",
      "Min loss: 0.012470496818423271\n",
      "Mean loss: 0.02688021259382367\n",
      "Std loss: 0.017198758759439516\n",
      "Total Loss: 0.16128127556294203\n",
      "------------------------------------ epoch 2009 (12048 steps) ------------------------------------\n",
      "Max loss: 0.04500611871480942\n",
      "Min loss: 0.013593224808573723\n",
      "Mean loss: 0.02709905430674553\n",
      "Std loss: 0.0129597566073924\n",
      "Total Loss: 0.16259432584047318\n",
      "------------------------------------ epoch 2010 (12054 steps) ------------------------------------\n",
      "Max loss: 0.03667313605546951\n",
      "Min loss: 0.013945436105132103\n",
      "Mean loss: 0.022839535648624103\n",
      "Std loss: 0.008489092279440974\n",
      "Total Loss: 0.1370372138917446\n",
      "------------------------------------ epoch 2011 (12060 steps) ------------------------------------\n",
      "Max loss: 0.08729730546474457\n",
      "Min loss: 0.010736510157585144\n",
      "Mean loss: 0.03278222059210142\n",
      "Std loss: 0.025875602845368596\n",
      "Total Loss: 0.1966933235526085\n",
      "------------------------------------ epoch 2012 (12066 steps) ------------------------------------\n",
      "Max loss: 0.04449082538485527\n",
      "Min loss: 0.015450343489646912\n",
      "Mean loss: 0.02567349684735139\n",
      "Std loss: 0.01093270488659964\n",
      "Total Loss: 0.15404098108410835\n",
      "------------------------------------ epoch 2013 (12072 steps) ------------------------------------\n",
      "Max loss: 0.025688936933875084\n",
      "Min loss: 0.016455402597784996\n",
      "Mean loss: 0.02141269575804472\n",
      "Std loss: 0.0031664544880408573\n",
      "Total Loss: 0.12847617454826832\n",
      "------------------------------------ epoch 2014 (12078 steps) ------------------------------------\n",
      "Max loss: 0.05335184559226036\n",
      "Min loss: 0.017060868442058563\n",
      "Mean loss: 0.03457495601226886\n",
      "Std loss: 0.013675353374078669\n",
      "Total Loss: 0.20744973607361317\n",
      "------------------------------------ epoch 2015 (12084 steps) ------------------------------------\n",
      "Max loss: 0.042110227048397064\n",
      "Min loss: 0.024973584339022636\n",
      "Mean loss: 0.03259008129437765\n",
      "Std loss: 0.006822852973737879\n",
      "Total Loss: 0.19554048776626587\n",
      "------------------------------------ epoch 2016 (12090 steps) ------------------------------------\n",
      "Max loss: 0.0641310065984726\n",
      "Min loss: 0.016434038057923317\n",
      "Mean loss: 0.037457251300414406\n",
      "Std loss: 0.01577169499673748\n",
      "Total Loss: 0.22474350780248642\n",
      "------------------------------------ epoch 2017 (12096 steps) ------------------------------------\n",
      "Max loss: 0.061242759227752686\n",
      "Min loss: 0.012700664810836315\n",
      "Mean loss: 0.025838928141941626\n",
      "Std loss: 0.01664120400522942\n",
      "Total Loss: 0.15503356885164976\n",
      "------------------------------------ epoch 2018 (12102 steps) ------------------------------------\n",
      "Max loss: 0.037406377494335175\n",
      "Min loss: 0.024271946400403976\n",
      "Mean loss: 0.02980581484735012\n",
      "Std loss: 0.004285254305645862\n",
      "Total Loss: 0.17883488908410072\n",
      "------------------------------------ epoch 2019 (12108 steps) ------------------------------------\n",
      "Max loss: 0.022297464311122894\n",
      "Min loss: 0.015457578003406525\n",
      "Mean loss: 0.0198444373284777\n",
      "Std loss: 0.002317163335944964\n",
      "Total Loss: 0.1190666239708662\n",
      "------------------------------------ epoch 2020 (12114 steps) ------------------------------------\n",
      "Max loss: 0.03202778846025467\n",
      "Min loss: 0.011134721338748932\n",
      "Mean loss: 0.023895741750796635\n",
      "Std loss: 0.006788370266956797\n",
      "Total Loss: 0.14337445050477982\n",
      "------------------------------------ epoch 2021 (12120 steps) ------------------------------------\n",
      "Max loss: 0.03873589262366295\n",
      "Min loss: 0.012803493067622185\n",
      "Mean loss: 0.020551088886956375\n",
      "Std loss: 0.008498564888249632\n",
      "Total Loss: 0.12330653332173824\n",
      "------------------------------------ epoch 2022 (12126 steps) ------------------------------------\n",
      "Max loss: 0.026628151535987854\n",
      "Min loss: 0.012343354523181915\n",
      "Mean loss: 0.01807828402767579\n",
      "Std loss: 0.0044977282131404926\n",
      "Total Loss: 0.10846970416605473\n",
      "------------------------------------ epoch 2023 (12132 steps) ------------------------------------\n",
      "Max loss: 0.04853799194097519\n",
      "Min loss: 0.018054094165563583\n",
      "Mean loss: 0.03158467604468266\n",
      "Std loss: 0.00977009337298058\n",
      "Total Loss: 0.18950805626809597\n",
      "------------------------------------ epoch 2024 (12138 steps) ------------------------------------\n",
      "Max loss: 0.04429049417376518\n",
      "Min loss: 0.010829242877662182\n",
      "Mean loss: 0.020904666278511286\n",
      "Std loss: 0.0115481809277504\n",
      "Total Loss: 0.12542799767106771\n",
      "------------------------------------ epoch 2025 (12144 steps) ------------------------------------\n",
      "Max loss: 0.05120693892240524\n",
      "Min loss: 0.009567495435476303\n",
      "Mean loss: 0.020802038876960676\n",
      "Std loss: 0.014063047285403689\n",
      "Total Loss: 0.12481223326176405\n",
      "------------------------------------ epoch 2026 (12150 steps) ------------------------------------\n",
      "Max loss: 0.03559030592441559\n",
      "Min loss: 0.01721065118908882\n",
      "Mean loss: 0.02450672785441081\n",
      "Std loss: 0.006365940517281035\n",
      "Total Loss: 0.14704036712646484\n",
      "------------------------------------ epoch 2027 (12156 steps) ------------------------------------\n",
      "Max loss: 0.06187225505709648\n",
      "Min loss: 0.015169370919466019\n",
      "Mean loss: 0.029715064602593582\n",
      "Std loss: 0.0162322323850302\n",
      "Total Loss: 0.17829038761556149\n",
      "------------------------------------ epoch 2028 (12162 steps) ------------------------------------\n",
      "Max loss: 0.036957114934921265\n",
      "Min loss: 0.012835398316383362\n",
      "Mean loss: 0.02064523519948125\n",
      "Std loss: 0.010243232427549206\n",
      "Total Loss: 0.1238714111968875\n",
      "------------------------------------ epoch 2029 (12168 steps) ------------------------------------\n",
      "Max loss: 0.055097632110118866\n",
      "Min loss: 0.02203092724084854\n",
      "Mean loss: 0.03135052261253198\n",
      "Std loss: 0.011274016762564933\n",
      "Total Loss: 0.18810313567519188\n",
      "------------------------------------ epoch 2030 (12174 steps) ------------------------------------\n",
      "Max loss: 0.058751530945301056\n",
      "Min loss: 0.025092018768191338\n",
      "Mean loss: 0.03678709485878547\n",
      "Std loss: 0.013196606033810737\n",
      "Total Loss: 0.22072256915271282\n",
      "------------------------------------ epoch 2031 (12180 steps) ------------------------------------\n",
      "Max loss: 0.04028354585170746\n",
      "Min loss: 0.014426006004214287\n",
      "Mean loss: 0.02686251824100812\n",
      "Std loss: 0.00791993588964836\n",
      "Total Loss: 0.16117510944604874\n",
      "------------------------------------ epoch 2032 (12186 steps) ------------------------------------\n",
      "Max loss: 0.03472812473773956\n",
      "Min loss: 0.017921514809131622\n",
      "Mean loss: 0.023636906407773495\n",
      "Std loss: 0.005666109600664038\n",
      "Total Loss: 0.14182143844664097\n",
      "------------------------------------ epoch 2033 (12192 steps) ------------------------------------\n",
      "Max loss: 0.04672401398420334\n",
      "Min loss: 0.013898531906306744\n",
      "Mean loss: 0.0239423424936831\n",
      "Std loss: 0.010635059837015553\n",
      "Total Loss: 0.1436540549620986\n",
      "------------------------------------ epoch 2034 (12198 steps) ------------------------------------\n",
      "Max loss: 0.02644484117627144\n",
      "Min loss: 0.011629879474639893\n",
      "Mean loss: 0.020330779254436493\n",
      "Std loss: 0.005390244974130636\n",
      "Total Loss: 0.12198467552661896\n",
      "------------------------------------ epoch 2035 (12204 steps) ------------------------------------\n",
      "Max loss: 0.0503087192773819\n",
      "Min loss: 0.019244147464632988\n",
      "Mean loss: 0.03370196962108215\n",
      "Std loss: 0.010077407468814187\n",
      "Total Loss: 0.20221181772649288\n",
      "------------------------------------ epoch 2036 (12210 steps) ------------------------------------\n",
      "Max loss: 0.04429513216018677\n",
      "Min loss: 0.009939588606357574\n",
      "Mean loss: 0.019031828735023737\n",
      "Std loss: 0.011641425951492753\n",
      "Total Loss: 0.11419097241014242\n",
      "------------------------------------ epoch 2037 (12216 steps) ------------------------------------\n",
      "Max loss: 0.028277723118662834\n",
      "Min loss: 0.012326618656516075\n",
      "Mean loss: 0.020378482683251303\n",
      "Std loss: 0.006203591354529401\n",
      "Total Loss: 0.12227089609950781\n",
      "------------------------------------ epoch 2038 (12222 steps) ------------------------------------\n",
      "Max loss: 0.05202348530292511\n",
      "Min loss: 0.018483035266399384\n",
      "Mean loss: 0.03446464582035939\n",
      "Std loss: 0.012581511767395989\n",
      "Total Loss: 0.20678787492215633\n",
      "------------------------------------ epoch 2039 (12228 steps) ------------------------------------\n",
      "Max loss: 0.034899190068244934\n",
      "Min loss: 0.015811799094080925\n",
      "Mean loss: 0.023147920457025368\n",
      "Std loss: 0.00815136341069375\n",
      "Total Loss: 0.13888752274215221\n",
      "------------------------------------ epoch 2040 (12234 steps) ------------------------------------\n",
      "Max loss: 0.0461374968290329\n",
      "Min loss: 0.014003503136336803\n",
      "Mean loss: 0.023446547333151102\n",
      "Std loss: 0.01056299444282241\n",
      "Total Loss: 0.1406792839989066\n",
      "------------------------------------ epoch 2041 (12240 steps) ------------------------------------\n",
      "Max loss: 0.028251156210899353\n",
      "Min loss: 0.0166705921292305\n",
      "Mean loss: 0.02265791626026233\n",
      "Std loss: 0.004255836907463882\n",
      "Total Loss: 0.13594749756157398\n",
      "------------------------------------ epoch 2042 (12246 steps) ------------------------------------\n",
      "Max loss: 0.027084149420261383\n",
      "Min loss: 0.010860126465559006\n",
      "Mean loss: 0.01966604155798753\n",
      "Std loss: 0.006194226836961692\n",
      "Total Loss: 0.11799624934792519\n",
      "------------------------------------ epoch 2043 (12252 steps) ------------------------------------\n",
      "Max loss: 0.03505832701921463\n",
      "Min loss: 0.018637895584106445\n",
      "Mean loss: 0.02591362874954939\n",
      "Std loss: 0.00660065482213554\n",
      "Total Loss: 0.15548177249729633\n",
      "------------------------------------ epoch 2044 (12258 steps) ------------------------------------\n",
      "Max loss: 0.04126986116170883\n",
      "Min loss: 0.015331603586673737\n",
      "Mean loss: 0.026377199528117973\n",
      "Std loss: 0.00922176902136694\n",
      "Total Loss: 0.15826319716870785\n",
      "------------------------------------ epoch 2045 (12264 steps) ------------------------------------\n",
      "Max loss: 0.02142869494855404\n",
      "Min loss: 0.010614318773150444\n",
      "Mean loss: 0.015224208900084099\n",
      "Std loss: 0.003471303976838545\n",
      "Total Loss: 0.09134525340050459\n",
      "------------------------------------ epoch 2046 (12270 steps) ------------------------------------\n",
      "Max loss: 0.022371023893356323\n",
      "Min loss: 0.013154114596545696\n",
      "Mean loss: 0.017902327701449394\n",
      "Std loss: 0.0036905970263678978\n",
      "Total Loss: 0.10741396620869637\n",
      "------------------------------------ epoch 2047 (12276 steps) ------------------------------------\n",
      "Max loss: 0.028462138026952744\n",
      "Min loss: 0.013376541435718536\n",
      "Mean loss: 0.020545983066161472\n",
      "Std loss: 0.006047213326884744\n",
      "Total Loss: 0.12327589839696884\n",
      "------------------------------------ epoch 2048 (12282 steps) ------------------------------------\n",
      "Max loss: 0.0399763248860836\n",
      "Min loss: 0.011211074888706207\n",
      "Mean loss: 0.020733536686748266\n",
      "Std loss: 0.010148176976288953\n",
      "Total Loss: 0.1244012201204896\n",
      "------------------------------------ epoch 2049 (12288 steps) ------------------------------------\n",
      "Max loss: 0.05205528438091278\n",
      "Min loss: 0.011674320325255394\n",
      "Mean loss: 0.022401527191201847\n",
      "Std loss: 0.014475800844762656\n",
      "Total Loss: 0.13440916314721107\n",
      "------------------------------------ epoch 2050 (12294 steps) ------------------------------------\n",
      "Max loss: 0.06884315609931946\n",
      "Min loss: 0.015635190531611443\n",
      "Mean loss: 0.03190216266860565\n",
      "Std loss: 0.018237131466229586\n",
      "Total Loss: 0.19141297601163387\n",
      "------------------------------------ epoch 2051 (12300 steps) ------------------------------------\n",
      "Max loss: 0.025404376909136772\n",
      "Min loss: 0.01264269184321165\n",
      "Mean loss: 0.02104951146369179\n",
      "Std loss: 0.004766828455984522\n",
      "Total Loss: 0.12629706878215075\n",
      "------------------------------------ epoch 2052 (12306 steps) ------------------------------------\n",
      "Max loss: 0.0728711485862732\n",
      "Min loss: 0.01247567217797041\n",
      "Mean loss: 0.033130598409722246\n",
      "Std loss: 0.023172176990610815\n",
      "Total Loss: 0.1987835904583335\n",
      "------------------------------------ epoch 2053 (12312 steps) ------------------------------------\n",
      "Max loss: 0.02535158209502697\n",
      "Min loss: 0.012314612977206707\n",
      "Mean loss: 0.021048778823266428\n",
      "Std loss: 0.004737933811763731\n",
      "Total Loss: 0.12629267293959856\n",
      "------------------------------------ epoch 2054 (12318 steps) ------------------------------------\n",
      "Max loss: 0.03800978511571884\n",
      "Min loss: 0.00992392748594284\n",
      "Mean loss: 0.019873258657753468\n",
      "Std loss: 0.010621296152845738\n",
      "Total Loss: 0.1192395519465208\n",
      "------------------------------------ epoch 2055 (12324 steps) ------------------------------------\n",
      "Max loss: 0.041908226907253265\n",
      "Min loss: 0.010679563507437706\n",
      "Mean loss: 0.024218439124524593\n",
      "Std loss: 0.010004176478766275\n",
      "Total Loss: 0.14531063474714756\n",
      "------------------------------------ epoch 2056 (12330 steps) ------------------------------------\n",
      "Max loss: 0.02807513438165188\n",
      "Min loss: 0.015566367655992508\n",
      "Mean loss: 0.021258605333666008\n",
      "Std loss: 0.004388889701197806\n",
      "Total Loss: 0.12755163200199604\n",
      "------------------------------------ epoch 2057 (12336 steps) ------------------------------------\n",
      "Max loss: 0.04639414697885513\n",
      "Min loss: 0.010642364621162415\n",
      "Mean loss: 0.02143986119578282\n",
      "Std loss: 0.011651476088265769\n",
      "Total Loss: 0.12863916717469692\n",
      "------------------------------------ epoch 2058 (12342 steps) ------------------------------------\n",
      "Max loss: 0.03798436373472214\n",
      "Min loss: 0.011573481373488903\n",
      "Mean loss: 0.021163597237318754\n",
      "Std loss: 0.009226585546405318\n",
      "Total Loss: 0.12698158342391253\n",
      "------------------------------------ epoch 2059 (12348 steps) ------------------------------------\n",
      "Max loss: 0.05692067742347717\n",
      "Min loss: 0.011552458629012108\n",
      "Mean loss: 0.025899745523929596\n",
      "Std loss: 0.01613632556736368\n",
      "Total Loss: 0.15539847314357758\n",
      "------------------------------------ epoch 2060 (12354 steps) ------------------------------------\n",
      "Max loss: 0.041768789291381836\n",
      "Min loss: 0.012921011075377464\n",
      "Mean loss: 0.027512808951238792\n",
      "Std loss: 0.011420650501657014\n",
      "Total Loss: 0.16507685370743275\n",
      "------------------------------------ epoch 2061 (12360 steps) ------------------------------------\n",
      "Max loss: 0.028496693819761276\n",
      "Min loss: 0.012320160865783691\n",
      "Mean loss: 0.02056201531862219\n",
      "Std loss: 0.006235195626091701\n",
      "Total Loss: 0.12337209191173315\n",
      "------------------------------------ epoch 2062 (12366 steps) ------------------------------------\n",
      "Max loss: 0.08022601902484894\n",
      "Min loss: 0.010383641347289085\n",
      "Mean loss: 0.03670753942181667\n",
      "Std loss: 0.022482830671484447\n",
      "Total Loss: 0.2202452365309\n",
      "------------------------------------ epoch 2063 (12372 steps) ------------------------------------\n",
      "Max loss: 0.0907079204916954\n",
      "Min loss: 0.030773965641856194\n",
      "Mean loss: 0.048432618690033756\n",
      "Std loss: 0.020326370674512775\n",
      "Total Loss: 0.2905957121402025\n",
      "------------------------------------ epoch 2064 (12378 steps) ------------------------------------\n",
      "Max loss: 0.10929131507873535\n",
      "Min loss: 0.02761870250105858\n",
      "Mean loss: 0.04767993247757355\n",
      "Std loss: 0.02867592371982006\n",
      "Total Loss: 0.2860795948654413\n",
      "------------------------------------ epoch 2065 (12384 steps) ------------------------------------\n",
      "Max loss: 0.06327836215496063\n",
      "Min loss: 0.020141873508691788\n",
      "Mean loss: 0.04131261631846428\n",
      "Std loss: 0.015024576469933356\n",
      "Total Loss: 0.24787569791078568\n",
      "------------------------------------ epoch 2066 (12390 steps) ------------------------------------\n",
      "Max loss: 0.042388513684272766\n",
      "Min loss: 0.022790495306253433\n",
      "Mean loss: 0.03131946145246426\n",
      "Std loss: 0.00793897934247212\n",
      "Total Loss: 0.18791676871478558\n",
      "------------------------------------ epoch 2067 (12396 steps) ------------------------------------\n",
      "Max loss: 0.06200500950217247\n",
      "Min loss: 0.02057524025440216\n",
      "Mean loss: 0.034933590019742645\n",
      "Std loss: 0.01379336413495175\n",
      "Total Loss: 0.2096015401184559\n",
      "------------------------------------ epoch 2068 (12402 steps) ------------------------------------\n",
      "Max loss: 0.036612071096897125\n",
      "Min loss: 0.016763176769018173\n",
      "Mean loss: 0.02418332112332185\n",
      "Std loss: 0.0069291247493812325\n",
      "Total Loss: 0.1450999267399311\n",
      "------------------------------------ epoch 2069 (12408 steps) ------------------------------------\n",
      "Max loss: 0.03785979747772217\n",
      "Min loss: 0.014316143468022346\n",
      "Mean loss: 0.027236871731777985\n",
      "Std loss: 0.008930140592769137\n",
      "Total Loss: 0.16342123039066792\n",
      "------------------------------------ epoch 2070 (12414 steps) ------------------------------------\n",
      "Max loss: 0.04948864132165909\n",
      "Min loss: 0.012400751933455467\n",
      "Mean loss: 0.029749628466864426\n",
      "Std loss: 0.014090402164136394\n",
      "Total Loss: 0.17849777080118656\n",
      "------------------------------------ epoch 2071 (12420 steps) ------------------------------------\n",
      "Max loss: 0.03208639472723007\n",
      "Min loss: 0.013027369976043701\n",
      "Mean loss: 0.021045241504907608\n",
      "Std loss: 0.006696433858615581\n",
      "Total Loss: 0.12627144902944565\n",
      "------------------------------------ epoch 2072 (12426 steps) ------------------------------------\n",
      "Max loss: 0.0323493666946888\n",
      "Min loss: 0.016193848103284836\n",
      "Mean loss: 0.021076902747154236\n",
      "Std loss: 0.006232626599307003\n",
      "Total Loss: 0.12646141648292542\n",
      "------------------------------------ epoch 2073 (12432 steps) ------------------------------------\n",
      "Max loss: 0.035231225192546844\n",
      "Min loss: 0.015156835317611694\n",
      "Mean loss: 0.022514208530386288\n",
      "Std loss: 0.007739031250736158\n",
      "Total Loss: 0.13508525118231773\n",
      "------------------------------------ epoch 2074 (12438 steps) ------------------------------------\n",
      "Max loss: 0.027697313576936722\n",
      "Min loss: 0.013674361631274223\n",
      "Mean loss: 0.01973519551878174\n",
      "Std loss: 0.005013222064832582\n",
      "Total Loss: 0.11841117311269045\n",
      "------------------------------------ epoch 2075 (12444 steps) ------------------------------------\n",
      "Max loss: 0.027871444821357727\n",
      "Min loss: 0.0112418532371521\n",
      "Mean loss: 0.020918905424575012\n",
      "Std loss: 0.005507439851776403\n",
      "Total Loss: 0.12551343254745007\n",
      "------------------------------------ epoch 2076 (12450 steps) ------------------------------------\n",
      "Max loss: 0.02031860128045082\n",
      "Min loss: 0.012543144635856152\n",
      "Mean loss: 0.015214902348816395\n",
      "Std loss: 0.0029382960341008016\n",
      "Total Loss: 0.09128941409289837\n",
      "------------------------------------ epoch 2077 (12456 steps) ------------------------------------\n",
      "Max loss: 0.0827794075012207\n",
      "Min loss: 0.010070695541799068\n",
      "Mean loss: 0.032251610731085144\n",
      "Std loss: 0.025196776654887548\n",
      "Total Loss: 0.19350966438651085\n",
      "------------------------------------ epoch 2078 (12462 steps) ------------------------------------\n",
      "Max loss: 0.06366585195064545\n",
      "Min loss: 0.010626185685396194\n",
      "Mean loss: 0.022715818136930466\n",
      "Std loss: 0.018501732659165982\n",
      "Total Loss: 0.1362949088215828\n",
      "------------------------------------ epoch 2079 (12468 steps) ------------------------------------\n",
      "Max loss: 0.042062219232320786\n",
      "Min loss: 0.011755287647247314\n",
      "Mean loss: 0.02154269302263856\n",
      "Std loss: 0.01213826955758184\n",
      "Total Loss: 0.12925615813583136\n",
      "------------------------------------ epoch 2080 (12474 steps) ------------------------------------\n",
      "Max loss: 0.039754316210746765\n",
      "Min loss: 0.024309493601322174\n",
      "Mean loss: 0.03253631200641394\n",
      "Std loss: 0.005664049032990877\n",
      "Total Loss: 0.19521787203848362\n",
      "------------------------------------ epoch 2081 (12480 steps) ------------------------------------\n",
      "Max loss: 0.045575615018606186\n",
      "Min loss: 0.015097277238965034\n",
      "Mean loss: 0.028446173916260403\n",
      "Std loss: 0.010363743444330957\n",
      "Total Loss: 0.1706770434975624\n",
      "------------------------------------ epoch 2082 (12486 steps) ------------------------------------\n",
      "Max loss: 0.05232444033026695\n",
      "Min loss: 0.012440767139196396\n",
      "Mean loss: 0.026185497020681698\n",
      "Std loss: 0.013140000731663276\n",
      "Total Loss: 0.1571129821240902\n",
      "------------------------------------ epoch 2083 (12492 steps) ------------------------------------\n",
      "Max loss: 0.0450601689517498\n",
      "Min loss: 0.015298734419047832\n",
      "Mean loss: 0.03039082931354642\n",
      "Std loss: 0.008661394115087345\n",
      "Total Loss: 0.18234497588127851\n",
      "------------------------------------ epoch 2084 (12498 steps) ------------------------------------\n",
      "Max loss: 0.043336208909749985\n",
      "Min loss: 0.016217414289712906\n",
      "Mean loss: 0.03055551244566838\n",
      "Std loss: 0.011920320491969378\n",
      "Total Loss: 0.18333307467401028\n",
      "------------------------------------ epoch 2085 (12504 steps) ------------------------------------\n",
      "Max loss: 0.01902896910905838\n",
      "Min loss: 0.010709392838180065\n",
      "Mean loss: 0.014898331680645546\n",
      "Std loss: 0.002913708752089001\n",
      "Total Loss: 0.08938999008387327\n",
      "------------------------------------ epoch 2086 (12510 steps) ------------------------------------\n",
      "Max loss: 0.05225088447332382\n",
      "Min loss: 0.009912407025694847\n",
      "Mean loss: 0.024229207697014015\n",
      "Std loss: 0.01424955059562679\n",
      "Total Loss: 0.14537524618208408\n",
      "------------------------------------ epoch 2087 (12516 steps) ------------------------------------\n",
      "Max loss: 0.0292523056268692\n",
      "Min loss: 0.009090661071240902\n",
      "Mean loss: 0.017835950944572687\n",
      "Std loss: 0.0071309294334825175\n",
      "Total Loss: 0.10701570566743612\n",
      "------------------------------------ epoch 2088 (12522 steps) ------------------------------------\n",
      "Max loss: 0.038438327610492706\n",
      "Min loss: 0.012129354290664196\n",
      "Mean loss: 0.023240902926772833\n",
      "Std loss: 0.008344996808512898\n",
      "Total Loss: 0.139445417560637\n",
      "------------------------------------ epoch 2089 (12528 steps) ------------------------------------\n",
      "Max loss: 0.05651238560676575\n",
      "Min loss: 0.011199797503650188\n",
      "Mean loss: 0.025069680996239185\n",
      "Std loss: 0.01578011485966496\n",
      "Total Loss: 0.1504180859774351\n",
      "------------------------------------ epoch 2090 (12534 steps) ------------------------------------\n",
      "Max loss: 0.03394214063882828\n",
      "Min loss: 0.010835040360689163\n",
      "Mean loss: 0.024672511654595535\n",
      "Std loss: 0.008013849826052372\n",
      "Total Loss: 0.1480350699275732\n",
      "------------------------------------ epoch 2091 (12540 steps) ------------------------------------\n",
      "Max loss: 0.028738748282194138\n",
      "Min loss: 0.014762680046260357\n",
      "Mean loss: 0.021348736404130857\n",
      "Std loss: 0.005150710375375304\n",
      "Total Loss: 0.12809241842478514\n",
      "------------------------------------ epoch 2092 (12546 steps) ------------------------------------\n",
      "Max loss: 0.04452640563249588\n",
      "Min loss: 0.010316502302885056\n",
      "Mean loss: 0.02254627738147974\n",
      "Std loss: 0.013360658963409853\n",
      "Total Loss: 0.13527766428887844\n",
      "------------------------------------ epoch 2093 (12552 steps) ------------------------------------\n",
      "Max loss: 0.039364829659461975\n",
      "Min loss: 0.010602042078971863\n",
      "Mean loss: 0.02307435606295864\n",
      "Std loss: 0.011552506224961285\n",
      "Total Loss: 0.13844613637775183\n",
      "------------------------------------ epoch 2094 (12558 steps) ------------------------------------\n",
      "Max loss: 0.03629421442747116\n",
      "Min loss: 0.019480740651488304\n",
      "Mean loss: 0.025392337081333\n",
      "Std loss: 0.006191737922609773\n",
      "Total Loss: 0.152354022487998\n",
      "------------------------------------ epoch 2095 (12564 steps) ------------------------------------\n",
      "Max loss: 0.024724379181861877\n",
      "Min loss: 0.015299167484045029\n",
      "Mean loss: 0.020248243895669777\n",
      "Std loss: 0.003162344172185759\n",
      "Total Loss: 0.12148946337401867\n",
      "------------------------------------ epoch 2096 (12570 steps) ------------------------------------\n",
      "Max loss: 0.047710999846458435\n",
      "Min loss: 0.011445547454059124\n",
      "Mean loss: 0.019353703130036592\n",
      "Std loss: 0.012887053474486277\n",
      "Total Loss: 0.11612221878021955\n",
      "------------------------------------ epoch 2097 (12576 steps) ------------------------------------\n",
      "Max loss: 0.026618357747793198\n",
      "Min loss: 0.00925183854997158\n",
      "Mean loss: 0.018831933848559856\n",
      "Std loss: 0.005840551446455942\n",
      "Total Loss: 0.11299160309135914\n",
      "------------------------------------ epoch 2098 (12582 steps) ------------------------------------\n",
      "Max loss: 0.02934175729751587\n",
      "Min loss: 0.009761825203895569\n",
      "Mean loss: 0.01842010176430146\n",
      "Std loss: 0.006526280298224254\n",
      "Total Loss: 0.11052061058580875\n",
      "------------------------------------ epoch 2099 (12588 steps) ------------------------------------\n",
      "Max loss: 0.03755977004766464\n",
      "Min loss: 0.010743099264800549\n",
      "Mean loss: 0.023149570915848017\n",
      "Std loss: 0.0088845377620479\n",
      "Total Loss: 0.1388974254950881\n",
      "------------------------------------ epoch 2100 (12594 steps) ------------------------------------\n",
      "Max loss: 0.05297330021858215\n",
      "Min loss: 0.01178450882434845\n",
      "Mean loss: 0.02498898624132077\n",
      "Std loss: 0.013361124540018393\n",
      "Total Loss: 0.14993391744792461\n",
      "------------------------------------ epoch 2101 (12600 steps) ------------------------------------\n",
      "Max loss: 0.04337012767791748\n",
      "Min loss: 0.009945075958967209\n",
      "Mean loss: 0.019622200013448794\n",
      "Std loss: 0.01277682043336988\n",
      "Total Loss: 0.11773320008069277\n",
      "saved model at ./weights/model_2101.pth\n",
      "------------------------------------ epoch 2102 (12606 steps) ------------------------------------\n",
      "Max loss: 0.03013552725315094\n",
      "Min loss: 0.011901555582880974\n",
      "Mean loss: 0.020960450793306034\n",
      "Std loss: 0.0082123554035115\n",
      "Total Loss: 0.1257627047598362\n",
      "------------------------------------ epoch 2103 (12612 steps) ------------------------------------\n",
      "Max loss: 0.03569389879703522\n",
      "Min loss: 0.014625882729887962\n",
      "Mean loss: 0.022725646539280813\n",
      "Std loss: 0.007616666912158433\n",
      "Total Loss: 0.13635387923568487\n",
      "------------------------------------ epoch 2104 (12618 steps) ------------------------------------\n",
      "Max loss: 0.03480622172355652\n",
      "Min loss: 0.014635104686021805\n",
      "Mean loss: 0.024642686049143474\n",
      "Std loss: 0.007329789743837087\n",
      "Total Loss: 0.14785611629486084\n",
      "------------------------------------ epoch 2105 (12624 steps) ------------------------------------\n",
      "Max loss: 0.035774845629930496\n",
      "Min loss: 0.011962255463004112\n",
      "Mean loss: 0.021486145288993914\n",
      "Std loss: 0.008875388119565945\n",
      "Total Loss: 0.1289168717339635\n",
      "------------------------------------ epoch 2106 (12630 steps) ------------------------------------\n",
      "Max loss: 0.027654465287923813\n",
      "Min loss: 0.012262660078704357\n",
      "Mean loss: 0.018891362473368645\n",
      "Std loss: 0.006187364210699159\n",
      "Total Loss: 0.11334817484021187\n",
      "------------------------------------ epoch 2107 (12636 steps) ------------------------------------\n",
      "Max loss: 0.05191708728671074\n",
      "Min loss: 0.010098441503942013\n",
      "Mean loss: 0.022073051737000544\n",
      "Std loss: 0.014084463187717465\n",
      "Total Loss: 0.13243831042200327\n",
      "------------------------------------ epoch 2108 (12642 steps) ------------------------------------\n",
      "Max loss: 0.041633471846580505\n",
      "Min loss: 0.01247299276292324\n",
      "Mean loss: 0.02433688861007492\n",
      "Std loss: 0.00987210303329697\n",
      "Total Loss: 0.1460213316604495\n",
      "------------------------------------ epoch 2109 (12648 steps) ------------------------------------\n",
      "Max loss: 0.033803343772888184\n",
      "Min loss: 0.017396822571754456\n",
      "Mean loss: 0.026435644055406254\n",
      "Std loss: 0.005654727220357947\n",
      "Total Loss: 0.15861386433243752\n",
      "------------------------------------ epoch 2110 (12654 steps) ------------------------------------\n",
      "Max loss: 0.060473404824733734\n",
      "Min loss: 0.016070805490016937\n",
      "Mean loss: 0.03080851553628842\n",
      "Std loss: 0.014513623467246268\n",
      "Total Loss: 0.18485109321773052\n",
      "------------------------------------ epoch 2111 (12660 steps) ------------------------------------\n",
      "Max loss: 0.036545634269714355\n",
      "Min loss: 0.011665796861052513\n",
      "Mean loss: 0.02293211966753006\n",
      "Std loss: 0.007304123804446731\n",
      "Total Loss: 0.13759271800518036\n",
      "------------------------------------ epoch 2112 (12666 steps) ------------------------------------\n",
      "Max loss: 0.04035129025578499\n",
      "Min loss: 0.012934049591422081\n",
      "Mean loss: 0.02351949953784545\n",
      "Std loss: 0.011195439049589667\n",
      "Total Loss: 0.14111699722707272\n",
      "------------------------------------ epoch 2113 (12672 steps) ------------------------------------\n",
      "Max loss: 0.02112695202231407\n",
      "Min loss: 0.00985623337328434\n",
      "Mean loss: 0.017015912104398012\n",
      "Std loss: 0.0038682614469872836\n",
      "Total Loss: 0.10209547262638807\n",
      "------------------------------------ epoch 2114 (12678 steps) ------------------------------------\n",
      "Max loss: 0.027397528290748596\n",
      "Min loss: 0.011311067268252373\n",
      "Mean loss: 0.017900359351187944\n",
      "Std loss: 0.005192139766161709\n",
      "Total Loss: 0.10740215610712767\n",
      "------------------------------------ epoch 2115 (12684 steps) ------------------------------------\n",
      "Max loss: 0.038905709981918335\n",
      "Min loss: 0.013874652795493603\n",
      "Mean loss: 0.02553431053335468\n",
      "Std loss: 0.00966632596754865\n",
      "Total Loss: 0.15320586320012808\n",
      "------------------------------------ epoch 2116 (12690 steps) ------------------------------------\n",
      "Max loss: 0.038084596395492554\n",
      "Min loss: 0.009849929250776768\n",
      "Mean loss: 0.019190322142094374\n",
      "Std loss: 0.009074941013957838\n",
      "Total Loss: 0.11514193285256624\n",
      "------------------------------------ epoch 2117 (12696 steps) ------------------------------------\n",
      "Max loss: 0.05126309022307396\n",
      "Min loss: 0.010715827345848083\n",
      "Mean loss: 0.02852045310040315\n",
      "Std loss: 0.015536932788684019\n",
      "Total Loss: 0.1711227186024189\n",
      "------------------------------------ epoch 2118 (12702 steps) ------------------------------------\n",
      "Max loss: 0.05057615041732788\n",
      "Min loss: 0.011420516297221184\n",
      "Mean loss: 0.022210041992366314\n",
      "Std loss: 0.013135281541371457\n",
      "Total Loss: 0.13326025195419788\n",
      "------------------------------------ epoch 2119 (12708 steps) ------------------------------------\n",
      "Max loss: 0.040112193673849106\n",
      "Min loss: 0.014102891087532043\n",
      "Mean loss: 0.021611336153000593\n",
      "Std loss: 0.008782433248064629\n",
      "Total Loss: 0.12966801691800356\n",
      "------------------------------------ epoch 2120 (12714 steps) ------------------------------------\n",
      "Max loss: 0.03241033852100372\n",
      "Min loss: 0.015733957290649414\n",
      "Mean loss: 0.022685127643247444\n",
      "Std loss: 0.0050738941260705704\n",
      "Total Loss: 0.13611076585948467\n",
      "------------------------------------ epoch 2121 (12720 steps) ------------------------------------\n",
      "Max loss: 0.06731165945529938\n",
      "Min loss: 0.009933797642588615\n",
      "Mean loss: 0.028788323514163494\n",
      "Std loss: 0.01867428492202949\n",
      "Total Loss: 0.17272994108498096\n",
      "------------------------------------ epoch 2122 (12726 steps) ------------------------------------\n",
      "Max loss: 0.024351030588150024\n",
      "Min loss: 0.013414096087217331\n",
      "Mean loss: 0.019785830440620582\n",
      "Std loss: 0.003514257176155636\n",
      "Total Loss: 0.11871498264372349\n",
      "------------------------------------ epoch 2123 (12732 steps) ------------------------------------\n",
      "Max loss: 0.07983364909887314\n",
      "Min loss: 0.01642877608537674\n",
      "Mean loss: 0.028620855572323006\n",
      "Std loss: 0.023005890789049663\n",
      "Total Loss: 0.17172513343393803\n",
      "------------------------------------ epoch 2124 (12738 steps) ------------------------------------\n",
      "Max loss: 0.04648211598396301\n",
      "Min loss: 0.011123216710984707\n",
      "Mean loss: 0.021029875924189884\n",
      "Std loss: 0.012502032848221505\n",
      "Total Loss: 0.1261792555451393\n",
      "------------------------------------ epoch 2125 (12744 steps) ------------------------------------\n",
      "Max loss: 0.05469280481338501\n",
      "Min loss: 0.011787783354520798\n",
      "Mean loss: 0.022699545758465927\n",
      "Std loss: 0.014827212730721765\n",
      "Total Loss: 0.13619727455079556\n",
      "------------------------------------ epoch 2126 (12750 steps) ------------------------------------\n",
      "Max loss: 0.05754503607749939\n",
      "Min loss: 0.01096303015947342\n",
      "Mean loss: 0.021615018292019766\n",
      "Std loss: 0.01634150046572194\n",
      "Total Loss: 0.1296901097521186\n",
      "------------------------------------ epoch 2127 (12756 steps) ------------------------------------\n",
      "Max loss: 0.03347896412014961\n",
      "Min loss: 0.01598646678030491\n",
      "Mean loss: 0.02329089492559433\n",
      "Std loss: 0.006316932001189255\n",
      "Total Loss: 0.13974536955356598\n",
      "------------------------------------ epoch 2128 (12762 steps) ------------------------------------\n",
      "Max loss: 0.047753311693668365\n",
      "Min loss: 0.013173467479646206\n",
      "Mean loss: 0.026054763700813055\n",
      "Std loss: 0.012171074928406616\n",
      "Total Loss: 0.15632858220487833\n",
      "------------------------------------ epoch 2129 (12768 steps) ------------------------------------\n",
      "Max loss: 0.020115867257118225\n",
      "Min loss: 0.009839596226811409\n",
      "Mean loss: 0.013784775044769049\n",
      "Std loss: 0.0034450611766521913\n",
      "Total Loss: 0.08270865026861429\n",
      "------------------------------------ epoch 2130 (12774 steps) ------------------------------------\n",
      "Max loss: 0.046944182366132736\n",
      "Min loss: 0.012833191081881523\n",
      "Mean loss: 0.025009300870199997\n",
      "Std loss: 0.012438552962178492\n",
      "Total Loss: 0.1500558052212\n",
      "------------------------------------ epoch 2131 (12780 steps) ------------------------------------\n",
      "Max loss: 0.04741434007883072\n",
      "Min loss: 0.021502580493688583\n",
      "Mean loss: 0.0319515106578668\n",
      "Std loss: 0.008847540882868859\n",
      "Total Loss: 0.19170906394720078\n",
      "------------------------------------ epoch 2132 (12786 steps) ------------------------------------\n",
      "Max loss: 0.023412255570292473\n",
      "Min loss: 0.00981204118579626\n",
      "Mean loss: 0.01727019064128399\n",
      "Std loss: 0.005256316028761172\n",
      "Total Loss: 0.10362114384770393\n",
      "------------------------------------ epoch 2133 (12792 steps) ------------------------------------\n",
      "Max loss: 0.049666475504636765\n",
      "Min loss: 0.01205085963010788\n",
      "Mean loss: 0.022991038858890533\n",
      "Std loss: 0.01273433546839393\n",
      "Total Loss: 0.1379462331533432\n",
      "------------------------------------ epoch 2134 (12798 steps) ------------------------------------\n",
      "Max loss: 0.029066096991300583\n",
      "Min loss: 0.01453617587685585\n",
      "Mean loss: 0.021665791670481365\n",
      "Std loss: 0.005323394328235315\n",
      "Total Loss: 0.12999475002288818\n",
      "------------------------------------ epoch 2135 (12804 steps) ------------------------------------\n",
      "Max loss: 0.03110061213374138\n",
      "Min loss: 0.008515801280736923\n",
      "Mean loss: 0.019410100920746725\n",
      "Std loss: 0.008147714985129203\n",
      "Total Loss: 0.11646060552448034\n",
      "------------------------------------ epoch 2136 (12810 steps) ------------------------------------\n",
      "Max loss: 0.029710393399000168\n",
      "Min loss: 0.014502562582492828\n",
      "Mean loss: 0.02324069757014513\n",
      "Std loss: 0.0054707929129036844\n",
      "Total Loss: 0.13944418542087078\n",
      "------------------------------------ epoch 2137 (12816 steps) ------------------------------------\n",
      "Max loss: 0.04354088753461838\n",
      "Min loss: 0.013260896317660809\n",
      "Mean loss: 0.02479236898943782\n",
      "Std loss: 0.009909122457836636\n",
      "Total Loss: 0.1487542139366269\n",
      "------------------------------------ epoch 2138 (12822 steps) ------------------------------------\n",
      "Max loss: 0.022490233182907104\n",
      "Min loss: 0.009619733318686485\n",
      "Mean loss: 0.01550891064107418\n",
      "Std loss: 0.0047007600075305005\n",
      "Total Loss: 0.09305346384644508\n",
      "------------------------------------ epoch 2139 (12828 steps) ------------------------------------\n",
      "Max loss: 0.03622071072459221\n",
      "Min loss: 0.014545932412147522\n",
      "Mean loss: 0.022751901609202225\n",
      "Std loss: 0.00761229376625695\n",
      "Total Loss: 0.13651140965521336\n",
      "------------------------------------ epoch 2140 (12834 steps) ------------------------------------\n",
      "Max loss: 0.08693929016590118\n",
      "Min loss: 0.009998939000070095\n",
      "Mean loss: 0.03239317967866858\n",
      "Std loss: 0.027854655167901147\n",
      "Total Loss: 0.19435907807201147\n",
      "------------------------------------ epoch 2141 (12840 steps) ------------------------------------\n",
      "Max loss: 0.02897409163415432\n",
      "Min loss: 0.01230164896696806\n",
      "Mean loss: 0.020376091667761404\n",
      "Std loss: 0.005708909280511345\n",
      "Total Loss: 0.12225655000656843\n",
      "------------------------------------ epoch 2142 (12846 steps) ------------------------------------\n",
      "Max loss: 0.0354171022772789\n",
      "Min loss: 0.012276302091777325\n",
      "Mean loss: 0.02133602990458409\n",
      "Std loss: 0.00814784163093853\n",
      "Total Loss: 0.12801617942750454\n",
      "------------------------------------ epoch 2143 (12852 steps) ------------------------------------\n",
      "Max loss: 0.0340014323592186\n",
      "Min loss: 0.010510174557566643\n",
      "Mean loss: 0.02226147335022688\n",
      "Std loss: 0.00855050047399842\n",
      "Total Loss: 0.13356884010136127\n",
      "------------------------------------ epoch 2144 (12858 steps) ------------------------------------\n",
      "Max loss: 0.05648062378168106\n",
      "Min loss: 0.01051851361989975\n",
      "Mean loss: 0.02568119888504346\n",
      "Std loss: 0.01504238029273907\n",
      "Total Loss: 0.15408719331026077\n",
      "------------------------------------ epoch 2145 (12864 steps) ------------------------------------\n",
      "Max loss: 0.02829955890774727\n",
      "Min loss: 0.009755699895322323\n",
      "Mean loss: 0.017079940531402826\n",
      "Std loss: 0.006587606063496828\n",
      "Total Loss: 0.10247964318841696\n",
      "------------------------------------ epoch 2146 (12870 steps) ------------------------------------\n",
      "Max loss: 0.04713541269302368\n",
      "Min loss: 0.013154740445315838\n",
      "Mean loss: 0.021955907810479403\n",
      "Std loss: 0.01185682714831572\n",
      "Total Loss: 0.13173544686287642\n",
      "------------------------------------ epoch 2147 (12876 steps) ------------------------------------\n",
      "Max loss: 0.021570753306150436\n",
      "Min loss: 0.009573342278599739\n",
      "Mean loss: 0.014407177300502857\n",
      "Std loss: 0.004739762970666755\n",
      "Total Loss: 0.08644306380301714\n",
      "------------------------------------ epoch 2148 (12882 steps) ------------------------------------\n",
      "Max loss: 0.03155267983675003\n",
      "Min loss: 0.010634660720825195\n",
      "Mean loss: 0.020547477528452873\n",
      "Std loss: 0.007313622614197236\n",
      "Total Loss: 0.12328486517071724\n",
      "------------------------------------ epoch 2149 (12888 steps) ------------------------------------\n",
      "Max loss: 0.040155909955501556\n",
      "Min loss: 0.010106420144438744\n",
      "Mean loss: 0.024418023725350697\n",
      "Std loss: 0.009755565068234202\n",
      "Total Loss: 0.1465081423521042\n",
      "------------------------------------ epoch 2150 (12894 steps) ------------------------------------\n",
      "Max loss: 0.04207075759768486\n",
      "Min loss: 0.011494050733745098\n",
      "Mean loss: 0.018704780687888462\n",
      "Std loss: 0.010618768384334308\n",
      "Total Loss: 0.11222868412733078\n",
      "------------------------------------ epoch 2151 (12900 steps) ------------------------------------\n",
      "Max loss: 0.029533706605434418\n",
      "Min loss: 0.012310913763940334\n",
      "Mean loss: 0.019983043273289997\n",
      "Std loss: 0.007089654900175425\n",
      "Total Loss: 0.11989825963973999\n",
      "------------------------------------ epoch 2152 (12906 steps) ------------------------------------\n",
      "Max loss: 0.03652488440275192\n",
      "Min loss: 0.012585131451487541\n",
      "Mean loss: 0.02358854655176401\n",
      "Std loss: 0.008863627354245915\n",
      "Total Loss: 0.14153127931058407\n",
      "------------------------------------ epoch 2153 (12912 steps) ------------------------------------\n",
      "Max loss: 0.04107549786567688\n",
      "Min loss: 0.012094886973500252\n",
      "Mean loss: 0.02570619930823644\n",
      "Std loss: 0.00989031776582626\n",
      "Total Loss: 0.15423719584941864\n",
      "------------------------------------ epoch 2154 (12918 steps) ------------------------------------\n",
      "Max loss: 0.032588981091976166\n",
      "Min loss: 0.012782864272594452\n",
      "Mean loss: 0.020532614861925442\n",
      "Std loss: 0.006694934708883903\n",
      "Total Loss: 0.12319568917155266\n",
      "------------------------------------ epoch 2155 (12924 steps) ------------------------------------\n",
      "Max loss: 0.026352059096097946\n",
      "Min loss: 0.012293100357055664\n",
      "Mean loss: 0.02003662132968505\n",
      "Std loss: 0.004314873894772282\n",
      "Total Loss: 0.12021972797811031\n",
      "------------------------------------ epoch 2156 (12930 steps) ------------------------------------\n",
      "Max loss: 0.0260997973382473\n",
      "Min loss: 0.012401243671774864\n",
      "Mean loss: 0.016922329707692068\n",
      "Std loss: 0.004361616771511825\n",
      "Total Loss: 0.1015339782461524\n",
      "------------------------------------ epoch 2157 (12936 steps) ------------------------------------\n",
      "Max loss: 0.0719037875533104\n",
      "Min loss: 0.010578091256320477\n",
      "Mean loss: 0.025051587416479986\n",
      "Std loss: 0.021455328863509458\n",
      "Total Loss: 0.1503095244988799\n",
      "------------------------------------ epoch 2158 (12942 steps) ------------------------------------\n",
      "Max loss: 0.04195941612124443\n",
      "Min loss: 0.011889664456248283\n",
      "Mean loss: 0.024002449121326208\n",
      "Std loss: 0.012224464167131131\n",
      "Total Loss: 0.14401469472795725\n",
      "------------------------------------ epoch 2159 (12948 steps) ------------------------------------\n",
      "Max loss: 0.025152793154120445\n",
      "Min loss: 0.010073313489556313\n",
      "Mean loss: 0.016854826516161363\n",
      "Std loss: 0.004947867662396068\n",
      "Total Loss: 0.10112895909696817\n",
      "------------------------------------ epoch 2160 (12954 steps) ------------------------------------\n",
      "Max loss: 0.03528954088687897\n",
      "Min loss: 0.011543887667357922\n",
      "Mean loss: 0.01804153760895133\n",
      "Std loss: 0.008166411145412146\n",
      "Total Loss: 0.10824922565370798\n",
      "------------------------------------ epoch 2161 (12960 steps) ------------------------------------\n",
      "Max loss: 0.03966958448290825\n",
      "Min loss: 0.015415430068969727\n",
      "Mean loss: 0.02371673999975125\n",
      "Std loss: 0.008137605499068694\n",
      "Total Loss: 0.1423004399985075\n",
      "------------------------------------ epoch 2162 (12966 steps) ------------------------------------\n",
      "Max loss: 0.020051904022693634\n",
      "Min loss: 0.012982710264623165\n",
      "Mean loss: 0.0166548575895528\n",
      "Std loss: 0.0021954829841035997\n",
      "Total Loss: 0.0999291455373168\n",
      "------------------------------------ epoch 2163 (12972 steps) ------------------------------------\n",
      "Max loss: 0.06375811249017715\n",
      "Min loss: 0.009206042625010014\n",
      "Mean loss: 0.031325062115987144\n",
      "Std loss: 0.021667579901399184\n",
      "Total Loss: 0.18795037269592285\n",
      "------------------------------------ epoch 2164 (12978 steps) ------------------------------------\n",
      "Max loss: 0.05142059922218323\n",
      "Min loss: 0.014966100454330444\n",
      "Mean loss: 0.024228632139662903\n",
      "Std loss: 0.012666597803722628\n",
      "Total Loss: 0.1453717928379774\n",
      "------------------------------------ epoch 2165 (12984 steps) ------------------------------------\n",
      "Max loss: 0.03786493465304375\n",
      "Min loss: 0.01437811553478241\n",
      "Mean loss: 0.02256072871387005\n",
      "Std loss: 0.008451172659873371\n",
      "Total Loss: 0.1353643722832203\n",
      "------------------------------------ epoch 2166 (12990 steps) ------------------------------------\n",
      "Max loss: 0.05099114775657654\n",
      "Min loss: 0.011638599447906017\n",
      "Mean loss: 0.024107821906606357\n",
      "Std loss: 0.013710035013900286\n",
      "Total Loss: 0.14464693143963814\n",
      "------------------------------------ epoch 2167 (12996 steps) ------------------------------------\n",
      "Max loss: 0.02534784935414791\n",
      "Min loss: 0.015013590455055237\n",
      "Mean loss: 0.019195604758958023\n",
      "Std loss: 0.00365822331807515\n",
      "Total Loss: 0.11517362855374813\n",
      "------------------------------------ epoch 2168 (13002 steps) ------------------------------------\n",
      "Max loss: 0.029218493029475212\n",
      "Min loss: 0.013452726416289806\n",
      "Mean loss: 0.01845790275062124\n",
      "Std loss: 0.005278895044764763\n",
      "Total Loss: 0.11074741650372744\n",
      "------------------------------------ epoch 2169 (13008 steps) ------------------------------------\n",
      "Max loss: 0.022492675110697746\n",
      "Min loss: 0.009190264157950878\n",
      "Mean loss: 0.01623460774620374\n",
      "Std loss: 0.005081326318314927\n",
      "Total Loss: 0.09740764647722244\n",
      "------------------------------------ epoch 2170 (13014 steps) ------------------------------------\n",
      "Max loss: 0.04514510557055473\n",
      "Min loss: 0.013702860102057457\n",
      "Mean loss: 0.028710702434182167\n",
      "Std loss: 0.010707536785844173\n",
      "Total Loss: 0.172264214605093\n",
      "------------------------------------ epoch 2171 (13020 steps) ------------------------------------\n",
      "Max loss: 0.025652535259723663\n",
      "Min loss: 0.010352024808526039\n",
      "Mean loss: 0.017959784095486004\n",
      "Std loss: 0.004612327464027006\n",
      "Total Loss: 0.10775870457291603\n",
      "------------------------------------ epoch 2172 (13026 steps) ------------------------------------\n",
      "Max loss: 0.0384632907807827\n",
      "Min loss: 0.015305454842746258\n",
      "Mean loss: 0.02395916311070323\n",
      "Std loss: 0.008175852101443844\n",
      "Total Loss: 0.14375497866421938\n",
      "------------------------------------ epoch 2173 (13032 steps) ------------------------------------\n",
      "Max loss: 0.06443332135677338\n",
      "Min loss: 0.013497794978320599\n",
      "Mean loss: 0.035507787484675646\n",
      "Std loss: 0.018052730378417734\n",
      "Total Loss: 0.21304672490805387\n",
      "------------------------------------ epoch 2174 (13038 steps) ------------------------------------\n",
      "Max loss: 0.02704499661922455\n",
      "Min loss: 0.011494603008031845\n",
      "Mean loss: 0.020837121332685154\n",
      "Std loss: 0.005027035921362677\n",
      "Total Loss: 0.12502272799611092\n",
      "------------------------------------ epoch 2175 (13044 steps) ------------------------------------\n",
      "Max loss: 0.03160496801137924\n",
      "Min loss: 0.010391268879175186\n",
      "Mean loss: 0.022617499344050884\n",
      "Std loss: 0.007824690158621079\n",
      "Total Loss: 0.1357049960643053\n",
      "------------------------------------ epoch 2176 (13050 steps) ------------------------------------\n",
      "Max loss: 0.046174608170986176\n",
      "Min loss: 0.010049371048808098\n",
      "Mean loss: 0.025845672314365704\n",
      "Std loss: 0.013909803302443777\n",
      "Total Loss: 0.15507403388619423\n",
      "------------------------------------ epoch 2177 (13056 steps) ------------------------------------\n",
      "Max loss: 0.022422006353735924\n",
      "Min loss: 0.012501990422606468\n",
      "Mean loss: 0.01768609555438161\n",
      "Std loss: 0.0037707572053974647\n",
      "Total Loss: 0.10611657332628965\n",
      "------------------------------------ epoch 2178 (13062 steps) ------------------------------------\n",
      "Max loss: 0.0262053944170475\n",
      "Min loss: 0.011466811411082745\n",
      "Mean loss: 0.019134048993388813\n",
      "Std loss: 0.0053274743664589366\n",
      "Total Loss: 0.11480429396033287\n",
      "------------------------------------ epoch 2179 (13068 steps) ------------------------------------\n",
      "Max loss: 0.04138226807117462\n",
      "Min loss: 0.01033909060060978\n",
      "Mean loss: 0.018438703070084255\n",
      "Std loss: 0.010812098015020864\n",
      "Total Loss: 0.11063221842050552\n",
      "------------------------------------ epoch 2180 (13074 steps) ------------------------------------\n",
      "Max loss: 0.02668987214565277\n",
      "Min loss: 0.010294410400092602\n",
      "Mean loss: 0.017477193381637335\n",
      "Std loss: 0.0059317748986403625\n",
      "Total Loss: 0.10486316028982401\n",
      "------------------------------------ epoch 2181 (13080 steps) ------------------------------------\n",
      "Max loss: 0.030650293454527855\n",
      "Min loss: 0.010115980170667171\n",
      "Mean loss: 0.020911337186892826\n",
      "Std loss: 0.007519697233849375\n",
      "Total Loss: 0.12546802312135696\n",
      "------------------------------------ epoch 2182 (13086 steps) ------------------------------------\n",
      "Max loss: 0.04241359233856201\n",
      "Min loss: 0.01097196340560913\n",
      "Mean loss: 0.02221204899251461\n",
      "Std loss: 0.010714492216246992\n",
      "Total Loss: 0.13327229395508766\n",
      "------------------------------------ epoch 2183 (13092 steps) ------------------------------------\n",
      "Max loss: 0.043316975235939026\n",
      "Min loss: 0.01158505491912365\n",
      "Mean loss: 0.025001405738294125\n",
      "Std loss: 0.011321065372120447\n",
      "Total Loss: 0.15000843442976475\n",
      "------------------------------------ epoch 2184 (13098 steps) ------------------------------------\n",
      "Max loss: 0.038940735161304474\n",
      "Min loss: 0.008560337126255035\n",
      "Mean loss: 0.02478021414329608\n",
      "Std loss: 0.012552324169090079\n",
      "Total Loss: 0.1486812848597765\n",
      "------------------------------------ epoch 2185 (13104 steps) ------------------------------------\n",
      "Max loss: 0.03214190900325775\n",
      "Min loss: 0.01217455044388771\n",
      "Mean loss: 0.019573192112147808\n",
      "Std loss: 0.006292577461182317\n",
      "Total Loss: 0.11743915267288685\n",
      "------------------------------------ epoch 2186 (13110 steps) ------------------------------------\n",
      "Max loss: 0.04996969550848007\n",
      "Min loss: 0.011627188883721828\n",
      "Mean loss: 0.023595814903577168\n",
      "Std loss: 0.01488742090368619\n",
      "Total Loss: 0.141574889421463\n",
      "------------------------------------ epoch 2187 (13116 steps) ------------------------------------\n",
      "Max loss: 0.03839150816202164\n",
      "Min loss: 0.010175703093409538\n",
      "Mean loss: 0.020953441970050335\n",
      "Std loss: 0.009074796088975443\n",
      "Total Loss: 0.125720651820302\n",
      "------------------------------------ epoch 2188 (13122 steps) ------------------------------------\n",
      "Max loss: 0.03643418475985527\n",
      "Min loss: 0.009357274509966373\n",
      "Mean loss: 0.020526194013655186\n",
      "Std loss: 0.010536825319870727\n",
      "Total Loss: 0.12315716408193111\n",
      "------------------------------------ epoch 2189 (13128 steps) ------------------------------------\n",
      "Max loss: 0.030731892213225365\n",
      "Min loss: 0.011742974631488323\n",
      "Mean loss: 0.019939287720868986\n",
      "Std loss: 0.0066899911155870505\n",
      "Total Loss: 0.11963572632521391\n",
      "------------------------------------ epoch 2190 (13134 steps) ------------------------------------\n",
      "Max loss: 0.061727821826934814\n",
      "Min loss: 0.01343517005443573\n",
      "Mean loss: 0.03106819372624159\n",
      "Std loss: 0.016687673895988197\n",
      "Total Loss: 0.18640916235744953\n",
      "------------------------------------ epoch 2191 (13140 steps) ------------------------------------\n",
      "Max loss: 0.026406634598970413\n",
      "Min loss: 0.01520075649023056\n",
      "Mean loss: 0.020615396089851856\n",
      "Std loss: 0.0037364637822642323\n",
      "Total Loss: 0.12369237653911114\n",
      "------------------------------------ epoch 2192 (13146 steps) ------------------------------------\n",
      "Max loss: 0.03183159977197647\n",
      "Min loss: 0.012739904224872589\n",
      "Mean loss: 0.019228576061626274\n",
      "Std loss: 0.0063773429830690205\n",
      "Total Loss: 0.11537145636975765\n",
      "------------------------------------ epoch 2193 (13152 steps) ------------------------------------\n",
      "Max loss: 0.045181915163993835\n",
      "Min loss: 0.014840802177786827\n",
      "Mean loss: 0.023311803738276165\n",
      "Std loss: 0.01044621626431199\n",
      "Total Loss: 0.13987082242965698\n",
      "------------------------------------ epoch 2194 (13158 steps) ------------------------------------\n",
      "Max loss: 0.030248185619711876\n",
      "Min loss: 0.01038709469139576\n",
      "Mean loss: 0.018288991879671812\n",
      "Std loss: 0.006804125530032627\n",
      "Total Loss: 0.10973395127803087\n",
      "------------------------------------ epoch 2195 (13164 steps) ------------------------------------\n",
      "Max loss: 0.02057655155658722\n",
      "Min loss: 0.010866306722164154\n",
      "Mean loss: 0.016043368416527908\n",
      "Std loss: 0.003997517626776698\n",
      "Total Loss: 0.09626021049916744\n",
      "------------------------------------ epoch 2196 (13170 steps) ------------------------------------\n",
      "Max loss: 0.024505464360117912\n",
      "Min loss: 0.012142333202064037\n",
      "Mean loss: 0.01753734067703287\n",
      "Std loss: 0.004320701043289472\n",
      "Total Loss: 0.10522404406219721\n",
      "------------------------------------ epoch 2197 (13176 steps) ------------------------------------\n",
      "Max loss: 0.060317814350128174\n",
      "Min loss: 0.011228710412979126\n",
      "Mean loss: 0.025929434069742758\n",
      "Std loss: 0.017368466462166297\n",
      "Total Loss: 0.15557660441845655\n",
      "------------------------------------ epoch 2198 (13182 steps) ------------------------------------\n",
      "Max loss: 0.024528421461582184\n",
      "Min loss: 0.010016528889536858\n",
      "Mean loss: 0.015778039892514546\n",
      "Std loss: 0.004684255944321692\n",
      "Total Loss: 0.09466823935508728\n",
      "------------------------------------ epoch 2199 (13188 steps) ------------------------------------\n",
      "Max loss: 0.034219030290842056\n",
      "Min loss: 0.012941244058310986\n",
      "Mean loss: 0.023779235625018675\n",
      "Std loss: 0.00858331107662993\n",
      "Total Loss: 0.14267541375011206\n",
      "------------------------------------ epoch 2200 (13194 steps) ------------------------------------\n",
      "Max loss: 0.03191564232110977\n",
      "Min loss: 0.009975194931030273\n",
      "Mean loss: 0.018665096101661522\n",
      "Std loss: 0.009281436027809932\n",
      "Total Loss: 0.11199057660996914\n",
      "------------------------------------ epoch 2201 (13200 steps) ------------------------------------\n",
      "Max loss: 0.02904866635799408\n",
      "Min loss: 0.011563105508685112\n",
      "Mean loss: 0.017251776220897835\n",
      "Std loss: 0.006737967627634134\n",
      "Total Loss: 0.103510657325387\n",
      "saved model at ./weights/model_2201.pth\n",
      "------------------------------------ epoch 2202 (13206 steps) ------------------------------------\n",
      "Max loss: 0.021515969187021255\n",
      "Min loss: 0.009439884684979916\n",
      "Mean loss: 0.012608707572023073\n",
      "Std loss: 0.004132452768918776\n",
      "Total Loss: 0.07565224543213844\n",
      "------------------------------------ epoch 2203 (13212 steps) ------------------------------------\n",
      "Max loss: 0.06000327691435814\n",
      "Min loss: 0.009402923285961151\n",
      "Mean loss: 0.024451041594147682\n",
      "Std loss: 0.01786189083168349\n",
      "Total Loss: 0.1467062495648861\n",
      "------------------------------------ epoch 2204 (13218 steps) ------------------------------------\n",
      "Max loss: 0.022379200905561447\n",
      "Min loss: 0.013902089558541775\n",
      "Mean loss: 0.018152816997220118\n",
      "Std loss: 0.0030911322865518637\n",
      "Total Loss: 0.10891690198332071\n",
      "------------------------------------ epoch 2205 (13224 steps) ------------------------------------\n",
      "Max loss: 0.02759188786149025\n",
      "Min loss: 0.008049875497817993\n",
      "Mean loss: 0.015844683318088453\n",
      "Std loss: 0.007090148168757341\n",
      "Total Loss: 0.09506809990853071\n",
      "------------------------------------ epoch 2206 (13230 steps) ------------------------------------\n",
      "Max loss: 0.02245764061808586\n",
      "Min loss: 0.010554056614637375\n",
      "Mean loss: 0.015352014917880297\n",
      "Std loss: 0.00402754242441333\n",
      "Total Loss: 0.09211208950728178\n",
      "------------------------------------ epoch 2207 (13236 steps) ------------------------------------\n",
      "Max loss: 0.02404084801673889\n",
      "Min loss: 0.009703777730464935\n",
      "Mean loss: 0.015846307234217722\n",
      "Std loss: 0.0047892188387003735\n",
      "Total Loss: 0.09507784340530634\n",
      "------------------------------------ epoch 2208 (13242 steps) ------------------------------------\n",
      "Max loss: 0.0391322560608387\n",
      "Min loss: 0.014126993715763092\n",
      "Mean loss: 0.025842631546159584\n",
      "Std loss: 0.009094714626838116\n",
      "Total Loss: 0.1550557892769575\n",
      "------------------------------------ epoch 2209 (13248 steps) ------------------------------------\n",
      "Max loss: 0.0506969690322876\n",
      "Min loss: 0.014768404886126518\n",
      "Mean loss: 0.026568659270803135\n",
      "Std loss: 0.012331617907217351\n",
      "Total Loss: 0.1594119556248188\n",
      "------------------------------------ epoch 2210 (13254 steps) ------------------------------------\n",
      "Max loss: 0.027184337377548218\n",
      "Min loss: 0.011763585731387138\n",
      "Mean loss: 0.017769064754247665\n",
      "Std loss: 0.005044784040716644\n",
      "Total Loss: 0.10661438852548599\n",
      "------------------------------------ epoch 2211 (13260 steps) ------------------------------------\n",
      "Max loss: 0.03588598221540451\n",
      "Min loss: 0.011155086569488049\n",
      "Mean loss: 0.024030794855207205\n",
      "Std loss: 0.008146242139687575\n",
      "Total Loss: 0.14418476913124323\n",
      "------------------------------------ epoch 2212 (13266 steps) ------------------------------------\n",
      "Max loss: 0.02777453325688839\n",
      "Min loss: 0.014463442377746105\n",
      "Mean loss: 0.020679393007109564\n",
      "Std loss: 0.004972429067051863\n",
      "Total Loss: 0.12407635804265738\n",
      "------------------------------------ epoch 2213 (13272 steps) ------------------------------------\n",
      "Max loss: 0.038662977516651154\n",
      "Min loss: 0.01340033020824194\n",
      "Mean loss: 0.022616153738150995\n",
      "Std loss: 0.00917148978430818\n",
      "Total Loss: 0.13569692242890596\n",
      "------------------------------------ epoch 2214 (13278 steps) ------------------------------------\n",
      "Max loss: 0.021040266379714012\n",
      "Min loss: 0.010359133593738079\n",
      "Mean loss: 0.01739265040184061\n",
      "Std loss: 0.003465762212216766\n",
      "Total Loss: 0.10435590241104364\n",
      "------------------------------------ epoch 2215 (13284 steps) ------------------------------------\n",
      "Max loss: 0.03149019181728363\n",
      "Min loss: 0.011268666945397854\n",
      "Mean loss: 0.018567748523006838\n",
      "Std loss: 0.006917836018399784\n",
      "Total Loss: 0.11140649113804102\n",
      "------------------------------------ epoch 2216 (13290 steps) ------------------------------------\n",
      "Max loss: 0.06602516770362854\n",
      "Min loss: 0.011140789836645126\n",
      "Mean loss: 0.0293507082387805\n",
      "Std loss: 0.0182804295402572\n",
      "Total Loss: 0.176104249432683\n",
      "------------------------------------ epoch 2217 (13296 steps) ------------------------------------\n",
      "Max loss: 0.038029350340366364\n",
      "Min loss: 0.009283177554607391\n",
      "Mean loss: 0.021999115124344826\n",
      "Std loss: 0.010558408648154277\n",
      "Total Loss: 0.13199469074606895\n",
      "------------------------------------ epoch 2218 (13302 steps) ------------------------------------\n",
      "Max loss: 0.054039955139160156\n",
      "Min loss: 0.011924299411475658\n",
      "Mean loss: 0.026886903680860996\n",
      "Std loss: 0.016559820987220104\n",
      "Total Loss: 0.16132142208516598\n",
      "------------------------------------ epoch 2219 (13308 steps) ------------------------------------\n",
      "Max loss: 0.05035094544291496\n",
      "Min loss: 0.011702170595526695\n",
      "Mean loss: 0.026704761510094006\n",
      "Std loss: 0.012616065967616387\n",
      "Total Loss: 0.16022856906056404\n",
      "------------------------------------ epoch 2220 (13314 steps) ------------------------------------\n",
      "Max loss: 0.03479604795575142\n",
      "Min loss: 0.01202436350286007\n",
      "Mean loss: 0.022329438477754593\n",
      "Std loss: 0.007808148561190678\n",
      "Total Loss: 0.13397663086652756\n",
      "------------------------------------ epoch 2221 (13320 steps) ------------------------------------\n",
      "Max loss: 0.020332656800746918\n",
      "Min loss: 0.012767551466822624\n",
      "Mean loss: 0.017219432008763153\n",
      "Std loss: 0.002813778403899915\n",
      "Total Loss: 0.10331659205257893\n",
      "------------------------------------ epoch 2222 (13326 steps) ------------------------------------\n",
      "Max loss: 0.09192278236150742\n",
      "Min loss: 0.0099087655544281\n",
      "Mean loss: 0.02864656566331784\n",
      "Std loss: 0.028998124829216303\n",
      "Total Loss: 0.17187939397990704\n",
      "------------------------------------ epoch 2223 (13332 steps) ------------------------------------\n",
      "Max loss: 0.0320417657494545\n",
      "Min loss: 0.012970857322216034\n",
      "Mean loss: 0.023367174590627354\n",
      "Std loss: 0.008331931193383796\n",
      "Total Loss: 0.14020304754376411\n",
      "------------------------------------ epoch 2224 (13338 steps) ------------------------------------\n",
      "Max loss: 0.04204631969332695\n",
      "Min loss: 0.011717317625880241\n",
      "Mean loss: 0.027053422449777525\n",
      "Std loss: 0.010846011285373535\n",
      "Total Loss: 0.16232053469866514\n",
      "------------------------------------ epoch 2225 (13344 steps) ------------------------------------\n",
      "Max loss: 0.021697312593460083\n",
      "Min loss: 0.01122540794312954\n",
      "Mean loss: 0.01741084363311529\n",
      "Std loss: 0.003502272585874988\n",
      "Total Loss: 0.10446506179869175\n",
      "------------------------------------ epoch 2226 (13350 steps) ------------------------------------\n",
      "Max loss: 0.04007021337747574\n",
      "Min loss: 0.009762848727405071\n",
      "Mean loss: 0.024836302269250154\n",
      "Std loss: 0.010794860574893185\n",
      "Total Loss: 0.14901781361550093\n",
      "------------------------------------ epoch 2227 (13356 steps) ------------------------------------\n",
      "Max loss: 0.02940904162824154\n",
      "Min loss: 0.013787075877189636\n",
      "Mean loss: 0.019870479125529528\n",
      "Std loss: 0.006446564478198906\n",
      "Total Loss: 0.11922287475317717\n",
      "------------------------------------ epoch 2228 (13362 steps) ------------------------------------\n",
      "Max loss: 0.024694886058568954\n",
      "Min loss: 0.008825061842799187\n",
      "Mean loss: 0.017531301050136488\n",
      "Std loss: 0.005955671995235005\n",
      "Total Loss: 0.10518780630081892\n",
      "------------------------------------ epoch 2229 (13368 steps) ------------------------------------\n",
      "Max loss: 0.027044527232646942\n",
      "Min loss: 0.02389812096953392\n",
      "Mean loss: 0.02549787610769272\n",
      "Std loss: 0.0012213052262093525\n",
      "Total Loss: 0.1529872566461563\n",
      "------------------------------------ epoch 2230 (13374 steps) ------------------------------------\n",
      "Max loss: 0.026763230562210083\n",
      "Min loss: 0.01162581518292427\n",
      "Mean loss: 0.019149831651399534\n",
      "Std loss: 0.0062178251495618566\n",
      "Total Loss: 0.1148989899083972\n",
      "------------------------------------ epoch 2231 (13380 steps) ------------------------------------\n",
      "Max loss: 0.08597966283559799\n",
      "Min loss: 0.017929380759596825\n",
      "Mean loss: 0.03657378411541382\n",
      "Std loss: 0.023706398807176586\n",
      "Total Loss: 0.21944270469248295\n",
      "------------------------------------ epoch 2232 (13386 steps) ------------------------------------\n",
      "Max loss: 0.02752028778195381\n",
      "Min loss: 0.01080330740660429\n",
      "Mean loss: 0.01860545901581645\n",
      "Std loss: 0.005302392459854347\n",
      "Total Loss: 0.1116327540948987\n",
      "------------------------------------ epoch 2233 (13392 steps) ------------------------------------\n",
      "Max loss: 0.020996730774641037\n",
      "Min loss: 0.012459827587008476\n",
      "Mean loss: 0.017089363497992355\n",
      "Std loss: 0.003249051049109012\n",
      "Total Loss: 0.10253618098795414\n",
      "------------------------------------ epoch 2234 (13398 steps) ------------------------------------\n",
      "Max loss: 0.03292643278837204\n",
      "Min loss: 0.010190410539507866\n",
      "Mean loss: 0.016593664263685543\n",
      "Std loss: 0.008164601059293421\n",
      "Total Loss: 0.09956198558211327\n",
      "------------------------------------ epoch 2235 (13404 steps) ------------------------------------\n",
      "Max loss: 0.03638786822557449\n",
      "Min loss: 0.013708445243537426\n",
      "Mean loss: 0.02109951913977663\n",
      "Std loss: 0.007533242188157626\n",
      "Total Loss: 0.12659711483865976\n",
      "------------------------------------ epoch 2236 (13410 steps) ------------------------------------\n",
      "Max loss: 0.07338426262140274\n",
      "Min loss: 0.01085178554058075\n",
      "Mean loss: 0.023806750153501827\n",
      "Std loss: 0.022267045072779365\n",
      "Total Loss: 0.14284050092101097\n",
      "------------------------------------ epoch 2237 (13416 steps) ------------------------------------\n",
      "Max loss: 0.05736137926578522\n",
      "Min loss: 0.010993020609021187\n",
      "Mean loss: 0.026186099275946617\n",
      "Std loss: 0.015579457923150256\n",
      "Total Loss: 0.1571165956556797\n",
      "------------------------------------ epoch 2238 (13422 steps) ------------------------------------\n",
      "Max loss: 0.022395335137844086\n",
      "Min loss: 0.009041182696819305\n",
      "Mean loss: 0.01646547221268217\n",
      "Std loss: 0.004035071748918878\n",
      "Total Loss: 0.098792833276093\n",
      "------------------------------------ epoch 2239 (13428 steps) ------------------------------------\n",
      "Max loss: 0.03418087959289551\n",
      "Min loss: 0.015487252734601498\n",
      "Mean loss: 0.025114429959406454\n",
      "Std loss: 0.006819205614645852\n",
      "Total Loss: 0.15068657975643873\n",
      "------------------------------------ epoch 2240 (13434 steps) ------------------------------------\n",
      "Max loss: 0.09914766252040863\n",
      "Min loss: 0.020715273916721344\n",
      "Mean loss: 0.05268003853658835\n",
      "Std loss: 0.025487599095842247\n",
      "Total Loss: 0.3160802312195301\n",
      "------------------------------------ epoch 2241 (13440 steps) ------------------------------------\n",
      "Max loss: 0.14426064491271973\n",
      "Min loss: 0.059315912425518036\n",
      "Mean loss: 0.09613678356011708\n",
      "Std loss: 0.03346719162263307\n",
      "Total Loss: 0.5768207013607025\n",
      "------------------------------------ epoch 2242 (13446 steps) ------------------------------------\n",
      "Max loss: 0.07997205853462219\n",
      "Min loss: 0.04571001976728439\n",
      "Mean loss: 0.06574084547658761\n",
      "Std loss: 0.013462584733876196\n",
      "Total Loss: 0.3944450728595257\n",
      "------------------------------------ epoch 2243 (13452 steps) ------------------------------------\n",
      "Max loss: 0.12291745841503143\n",
      "Min loss: 0.0389389842748642\n",
      "Mean loss: 0.06835590427120526\n",
      "Std loss: 0.02736121065153718\n",
      "Total Loss: 0.4101354256272316\n",
      "------------------------------------ epoch 2244 (13458 steps) ------------------------------------\n",
      "Max loss: 0.07061782479286194\n",
      "Min loss: 0.03637837618589401\n",
      "Mean loss: 0.05082577218612035\n",
      "Std loss: 0.012679500399654714\n",
      "Total Loss: 0.3049546331167221\n",
      "------------------------------------ epoch 2245 (13464 steps) ------------------------------------\n",
      "Max loss: 0.1114291399717331\n",
      "Min loss: 0.02867453172802925\n",
      "Mean loss: 0.052593449441095196\n",
      "Std loss: 0.03285976154021223\n",
      "Total Loss: 0.31556069664657116\n",
      "------------------------------------ epoch 2246 (13470 steps) ------------------------------------\n",
      "Max loss: 0.08215950429439545\n",
      "Min loss: 0.035630807280540466\n",
      "Mean loss: 0.04971746355295181\n",
      "Std loss: 0.016505742452284304\n",
      "Total Loss: 0.2983047813177109\n",
      "------------------------------------ epoch 2247 (13476 steps) ------------------------------------\n",
      "Max loss: 0.05522683262825012\n",
      "Min loss: 0.01940285414457321\n",
      "Mean loss: 0.039457845501601696\n",
      "Std loss: 0.012886420671282511\n",
      "Total Loss: 0.23674707300961018\n",
      "------------------------------------ epoch 2248 (13482 steps) ------------------------------------\n",
      "Max loss: 0.037437520921230316\n",
      "Min loss: 0.01935451291501522\n",
      "Mean loss: 0.02946042673041423\n",
      "Std loss: 0.005661237971459383\n",
      "Total Loss: 0.1767625603824854\n",
      "------------------------------------ epoch 2249 (13488 steps) ------------------------------------\n",
      "Max loss: 0.08223939687013626\n",
      "Min loss: 0.016010664403438568\n",
      "Mean loss: 0.032490878365933895\n",
      "Std loss: 0.02265962486583723\n",
      "Total Loss: 0.19494527019560337\n",
      "------------------------------------ epoch 2250 (13494 steps) ------------------------------------\n",
      "Max loss: 0.05461714416742325\n",
      "Min loss: 0.016122199594974518\n",
      "Mean loss: 0.03249118942767382\n",
      "Std loss: 0.01443291156099229\n",
      "Total Loss: 0.1949471365660429\n",
      "------------------------------------ epoch 2251 (13500 steps) ------------------------------------\n",
      "Max loss: 0.046867068856954575\n",
      "Min loss: 0.023064058274030685\n",
      "Mean loss: 0.03255943115800619\n",
      "Std loss: 0.009687981978511883\n",
      "Total Loss: 0.19535658694803715\n",
      "------------------------------------ epoch 2252 (13506 steps) ------------------------------------\n",
      "Max loss: 0.06860028207302094\n",
      "Min loss: 0.019105564802885056\n",
      "Mean loss: 0.03656716085970402\n",
      "Std loss: 0.017148942022288723\n",
      "Total Loss: 0.2194029651582241\n",
      "------------------------------------ epoch 2253 (13512 steps) ------------------------------------\n",
      "Max loss: 0.062298696488142014\n",
      "Min loss: 0.016556449234485626\n",
      "Mean loss: 0.028248794687290985\n",
      "Std loss: 0.015542847663914856\n",
      "Total Loss: 0.16949276812374592\n",
      "------------------------------------ epoch 2254 (13518 steps) ------------------------------------\n",
      "Max loss: 0.06329116225242615\n",
      "Min loss: 0.013099519535899162\n",
      "Mean loss: 0.024834600277245045\n",
      "Std loss: 0.017379181097537617\n",
      "Total Loss: 0.14900760166347027\n",
      "------------------------------------ epoch 2255 (13524 steps) ------------------------------------\n",
      "Max loss: 0.032723091542720795\n",
      "Min loss: 0.015664221718907356\n",
      "Mean loss: 0.02252690214663744\n",
      "Std loss: 0.006436122575967984\n",
      "Total Loss: 0.13516141287982464\n",
      "------------------------------------ epoch 2256 (13530 steps) ------------------------------------\n",
      "Max loss: 0.027221273630857468\n",
      "Min loss: 0.012951410375535488\n",
      "Mean loss: 0.016332699762036402\n",
      "Std loss: 0.004940167595987135\n",
      "Total Loss: 0.09799619857221842\n",
      "------------------------------------ epoch 2257 (13536 steps) ------------------------------------\n",
      "Max loss: 0.030252378433942795\n",
      "Min loss: 0.011737852357327938\n",
      "Mean loss: 0.017438698870440323\n",
      "Std loss: 0.006802091868753205\n",
      "Total Loss: 0.10463219322264194\n",
      "------------------------------------ epoch 2258 (13542 steps) ------------------------------------\n",
      "Max loss: 0.022261543199419975\n",
      "Min loss: 0.013680225238204002\n",
      "Mean loss: 0.01686421564469735\n",
      "Std loss: 0.0027955528027892016\n",
      "Total Loss: 0.10118529386818409\n",
      "------------------------------------ epoch 2259 (13548 steps) ------------------------------------\n",
      "Max loss: 0.04126988351345062\n",
      "Min loss: 0.012810367159545422\n",
      "Mean loss: 0.024250889817873638\n",
      "Std loss: 0.010104237572239872\n",
      "Total Loss: 0.14550533890724182\n",
      "------------------------------------ epoch 2260 (13554 steps) ------------------------------------\n",
      "Max loss: 0.023034369572997093\n",
      "Min loss: 0.011480150744318962\n",
      "Mean loss: 0.017291015790154535\n",
      "Std loss: 0.0035987696510311443\n",
      "Total Loss: 0.10374609474092722\n",
      "------------------------------------ epoch 2261 (13560 steps) ------------------------------------\n",
      "Max loss: 0.042632218450307846\n",
      "Min loss: 0.010606582276523113\n",
      "Mean loss: 0.02273346297442913\n",
      "Std loss: 0.010543520997128013\n",
      "Total Loss: 0.13640077784657478\n",
      "------------------------------------ epoch 2262 (13566 steps) ------------------------------------\n",
      "Max loss: 0.021001070737838745\n",
      "Min loss: 0.013063579797744751\n",
      "Mean loss: 0.016010244997839134\n",
      "Std loss: 0.003510706342684995\n",
      "Total Loss: 0.0960614699870348\n",
      "------------------------------------ epoch 2263 (13572 steps) ------------------------------------\n",
      "Max loss: 0.028122734278440475\n",
      "Min loss: 0.011119967326521873\n",
      "Mean loss: 0.018956552414844435\n",
      "Std loss: 0.006619693077078675\n",
      "Total Loss: 0.1137393144890666\n",
      "------------------------------------ epoch 2264 (13578 steps) ------------------------------------\n",
      "Max loss: 0.03336125984787941\n",
      "Min loss: 0.012635363265872002\n",
      "Mean loss: 0.020979612755278747\n",
      "Std loss: 0.007160103014709605\n",
      "Total Loss: 0.12587767653167248\n",
      "------------------------------------ epoch 2265 (13584 steps) ------------------------------------\n",
      "Max loss: 0.030108556151390076\n",
      "Min loss: 0.009958088397979736\n",
      "Mean loss: 0.022133448161184788\n",
      "Std loss: 0.007115718199968551\n",
      "Total Loss: 0.13280068896710873\n",
      "------------------------------------ epoch 2266 (13590 steps) ------------------------------------\n",
      "Max loss: 0.03678444027900696\n",
      "Min loss: 0.010084934532642365\n",
      "Mean loss: 0.018529778501639765\n",
      "Std loss: 0.008887437410368825\n",
      "Total Loss: 0.11117867100983858\n",
      "------------------------------------ epoch 2267 (13596 steps) ------------------------------------\n",
      "Max loss: 0.026902368292212486\n",
      "Min loss: 0.009743450209498405\n",
      "Mean loss: 0.01743798175205787\n",
      "Std loss: 0.005784761792619209\n",
      "Total Loss: 0.10462789051234722\n",
      "------------------------------------ epoch 2268 (13602 steps) ------------------------------------\n",
      "Max loss: 0.07999588549137115\n",
      "Min loss: 0.01405730377882719\n",
      "Mean loss: 0.03275084557632605\n",
      "Std loss: 0.0224739128792534\n",
      "Total Loss: 0.19650507345795631\n",
      "------------------------------------ epoch 2269 (13608 steps) ------------------------------------\n",
      "Max loss: 0.057740673422813416\n",
      "Min loss: 0.01901162788271904\n",
      "Mean loss: 0.035222161250809826\n",
      "Std loss: 0.01594379017679686\n",
      "Total Loss: 0.21133296750485897\n",
      "------------------------------------ epoch 2270 (13614 steps) ------------------------------------\n",
      "Max loss: 0.026330534368753433\n",
      "Min loss: 0.012946715578436852\n",
      "Mean loss: 0.018883415032178164\n",
      "Std loss: 0.005248991657461733\n",
      "Total Loss: 0.11330049019306898\n",
      "------------------------------------ epoch 2271 (13620 steps) ------------------------------------\n",
      "Max loss: 0.0486784353852272\n",
      "Min loss: 0.013189621269702911\n",
      "Mean loss: 0.028886358253657818\n",
      "Std loss: 0.013096213546952746\n",
      "Total Loss: 0.1733181495219469\n",
      "------------------------------------ epoch 2272 (13626 steps) ------------------------------------\n",
      "Max loss: 0.04885270446538925\n",
      "Min loss: 0.013244605623185635\n",
      "Mean loss: 0.026151887606829405\n",
      "Std loss: 0.01274941152614814\n",
      "Total Loss: 0.15691132564097643\n",
      "------------------------------------ epoch 2273 (13632 steps) ------------------------------------\n",
      "Max loss: 0.045240484178066254\n",
      "Min loss: 0.015100665390491486\n",
      "Mean loss: 0.02880572061985731\n",
      "Std loss: 0.010309668990064656\n",
      "Total Loss: 0.17283432371914387\n",
      "------------------------------------ epoch 2274 (13638 steps) ------------------------------------\n",
      "Max loss: 0.10764515399932861\n",
      "Min loss: 0.017383752390742302\n",
      "Mean loss: 0.04318053803096215\n",
      "Std loss: 0.030049915420667966\n",
      "Total Loss: 0.2590832281857729\n",
      "------------------------------------ epoch 2275 (13644 steps) ------------------------------------\n",
      "Max loss: 0.06133188307285309\n",
      "Min loss: 0.021224763244390488\n",
      "Mean loss: 0.03343404767413934\n",
      "Std loss: 0.013758100658633955\n",
      "Total Loss: 0.20060428604483604\n",
      "------------------------------------ epoch 2276 (13650 steps) ------------------------------------\n",
      "Max loss: 0.023532461374998093\n",
      "Min loss: 0.011639542877674103\n",
      "Mean loss: 0.018249142294128735\n",
      "Std loss: 0.003986889039520009\n",
      "Total Loss: 0.10949485376477242\n",
      "------------------------------------ epoch 2277 (13656 steps) ------------------------------------\n",
      "Max loss: 0.034370653331279755\n",
      "Min loss: 0.013925976119935513\n",
      "Mean loss: 0.021264140804608662\n",
      "Std loss: 0.0074992353694001744\n",
      "Total Loss: 0.12758484482765198\n",
      "------------------------------------ epoch 2278 (13662 steps) ------------------------------------\n",
      "Max loss: 0.05915728211402893\n",
      "Min loss: 0.02400875650346279\n",
      "Mean loss: 0.033140319089094795\n",
      "Std loss: 0.012426427309867948\n",
      "Total Loss: 0.1988419145345688\n",
      "------------------------------------ epoch 2279 (13668 steps) ------------------------------------\n",
      "Max loss: 0.03242342174053192\n",
      "Min loss: 0.012839394621551037\n",
      "Mean loss: 0.021808085807909567\n",
      "Std loss: 0.005819219999666351\n",
      "Total Loss: 0.1308485148474574\n",
      "------------------------------------ epoch 2280 (13674 steps) ------------------------------------\n",
      "Max loss: 0.03795142471790314\n",
      "Min loss: 0.0107159074395895\n",
      "Mean loss: 0.02040294340501229\n",
      "Std loss: 0.009019258265000534\n",
      "Total Loss: 0.12241766043007374\n",
      "------------------------------------ epoch 2281 (13680 steps) ------------------------------------\n",
      "Max loss: 0.03599698469042778\n",
      "Min loss: 0.009522059001028538\n",
      "Mean loss: 0.020877234172075987\n",
      "Std loss: 0.010808095596522387\n",
      "Total Loss: 0.12526340503245592\n",
      "------------------------------------ epoch 2282 (13686 steps) ------------------------------------\n",
      "Max loss: 0.06842096149921417\n",
      "Min loss: 0.01220027543604374\n",
      "Mean loss: 0.027833770650128525\n",
      "Std loss: 0.019335587826131553\n",
      "Total Loss: 0.16700262390077114\n",
      "------------------------------------ epoch 2283 (13692 steps) ------------------------------------\n",
      "Max loss: 0.033518001437187195\n",
      "Min loss: 0.016767408698797226\n",
      "Mean loss: 0.022366873919963837\n",
      "Std loss: 0.00555660334581274\n",
      "Total Loss: 0.13420124351978302\n",
      "------------------------------------ epoch 2284 (13698 steps) ------------------------------------\n",
      "Max loss: 0.06945788860321045\n",
      "Min loss: 0.018458303064107895\n",
      "Mean loss: 0.03683096511910359\n",
      "Std loss: 0.017780881823929697\n",
      "Total Loss: 0.22098579071462154\n",
      "------------------------------------ epoch 2285 (13704 steps) ------------------------------------\n",
      "Max loss: 0.04311590641736984\n",
      "Min loss: 0.01218925416469574\n",
      "Mean loss: 0.01917917576308052\n",
      "Std loss: 0.010845839390328776\n",
      "Total Loss: 0.1150750545784831\n",
      "------------------------------------ epoch 2286 (13710 steps) ------------------------------------\n",
      "Max loss: 0.05649062246084213\n",
      "Min loss: 0.019952893257141113\n",
      "Mean loss: 0.02766371580461661\n",
      "Std loss: 0.013013434586397497\n",
      "Total Loss: 0.16598229482769966\n",
      "------------------------------------ epoch 2287 (13716 steps) ------------------------------------\n",
      "Max loss: 0.022869523614645004\n",
      "Min loss: 0.01010789256542921\n",
      "Mean loss: 0.017837002407759428\n",
      "Std loss: 0.0042781994579490735\n",
      "Total Loss: 0.10702201444655657\n",
      "------------------------------------ epoch 2288 (13722 steps) ------------------------------------\n",
      "Max loss: 0.05395737290382385\n",
      "Min loss: 0.0134800486266613\n",
      "Mean loss: 0.022482783378412325\n",
      "Std loss: 0.014155058635077884\n",
      "Total Loss: 0.13489670027047396\n",
      "------------------------------------ epoch 2289 (13728 steps) ------------------------------------\n",
      "Max loss: 0.025852210819721222\n",
      "Min loss: 0.013401608914136887\n",
      "Mean loss: 0.01744634409745534\n",
      "Std loss: 0.004028547068511801\n",
      "Total Loss: 0.10467806458473206\n",
      "------------------------------------ epoch 2290 (13734 steps) ------------------------------------\n",
      "Max loss: 0.05124425143003464\n",
      "Min loss: 0.017284635454416275\n",
      "Mean loss: 0.030847049318253994\n",
      "Std loss: 0.010943687790166166\n",
      "Total Loss: 0.18508229590952396\n",
      "------------------------------------ epoch 2291 (13740 steps) ------------------------------------\n",
      "Max loss: 0.04150453582406044\n",
      "Min loss: 0.014418704435229301\n",
      "Mean loss: 0.023895996001859505\n",
      "Std loss: 0.008963380839959225\n",
      "Total Loss: 0.14337597601115704\n",
      "------------------------------------ epoch 2292 (13746 steps) ------------------------------------\n",
      "Max loss: 0.024798355996608734\n",
      "Min loss: 0.012690416537225246\n",
      "Mean loss: 0.018321123750259478\n",
      "Std loss: 0.004487862085063669\n",
      "Total Loss: 0.10992674250155687\n",
      "------------------------------------ epoch 2293 (13752 steps) ------------------------------------\n",
      "Max loss: 0.04402650520205498\n",
      "Min loss: 0.013141738250851631\n",
      "Mean loss: 0.023859909114738304\n",
      "Std loss: 0.009553329340484173\n",
      "Total Loss: 0.14315945468842983\n",
      "------------------------------------ epoch 2294 (13758 steps) ------------------------------------\n",
      "Max loss: 0.046100031584501266\n",
      "Min loss: 0.012514482252299786\n",
      "Mean loss: 0.025963707050929468\n",
      "Std loss: 0.013794475832211179\n",
      "Total Loss: 0.1557822423055768\n",
      "------------------------------------ epoch 2295 (13764 steps) ------------------------------------\n",
      "Max loss: 0.040374379605054855\n",
      "Min loss: 0.009212597273290157\n",
      "Mean loss: 0.021963065024465322\n",
      "Std loss: 0.010470047455578994\n",
      "Total Loss: 0.13177839014679193\n",
      "------------------------------------ epoch 2296 (13770 steps) ------------------------------------\n",
      "Max loss: 0.047111283987760544\n",
      "Min loss: 0.009006213396787643\n",
      "Mean loss: 0.02443420918037494\n",
      "Std loss: 0.011413267021649864\n",
      "Total Loss: 0.14660525508224964\n",
      "------------------------------------ epoch 2297 (13776 steps) ------------------------------------\n",
      "Max loss: 0.04078749939799309\n",
      "Min loss: 0.010187220759689808\n",
      "Mean loss: 0.02078957622870803\n",
      "Std loss: 0.010106798427964239\n",
      "Total Loss: 0.12473745737224817\n",
      "------------------------------------ epoch 2298 (13782 steps) ------------------------------------\n",
      "Max loss: 0.04860407114028931\n",
      "Min loss: 0.01680893823504448\n",
      "Mean loss: 0.02785923331975937\n",
      "Std loss: 0.010494972632588076\n",
      "Total Loss: 0.1671553999185562\n",
      "------------------------------------ epoch 2299 (13788 steps) ------------------------------------\n",
      "Max loss: 0.04179997742176056\n",
      "Min loss: 0.01269357930868864\n",
      "Mean loss: 0.022622693485269945\n",
      "Std loss: 0.009329031241256974\n",
      "Total Loss: 0.13573616091161966\n",
      "------------------------------------ epoch 2300 (13794 steps) ------------------------------------\n",
      "Max loss: 0.023237016052007675\n",
      "Min loss: 0.013077403418719769\n",
      "Mean loss: 0.018508001541097958\n",
      "Std loss: 0.004139433843990639\n",
      "Total Loss: 0.11104800924658775\n",
      "------------------------------------ epoch 2301 (13800 steps) ------------------------------------\n",
      "Max loss: 0.03159879148006439\n",
      "Min loss: 0.011112449690699577\n",
      "Mean loss: 0.020916893457372982\n",
      "Std loss: 0.006524107929684973\n",
      "Total Loss: 0.1255013607442379\n",
      "saved model at ./weights/model_2301.pth\n",
      "------------------------------------ epoch 2302 (13806 steps) ------------------------------------\n",
      "Max loss: 0.030690032988786697\n",
      "Min loss: 0.010291356593370438\n",
      "Mean loss: 0.017308909446001053\n",
      "Std loss: 0.0077489860067543985\n",
      "Total Loss: 0.10385345667600632\n",
      "------------------------------------ epoch 2303 (13812 steps) ------------------------------------\n",
      "Max loss: 0.046287935227155685\n",
      "Min loss: 0.011183816008269787\n",
      "Mean loss: 0.021493572586526472\n",
      "Std loss: 0.0117452844019924\n",
      "Total Loss: 0.12896143551915884\n",
      "------------------------------------ epoch 2304 (13818 steps) ------------------------------------\n",
      "Max loss: 0.0328265056014061\n",
      "Min loss: 0.012715242803096771\n",
      "Mean loss: 0.019093573714296024\n",
      "Std loss: 0.006737030366045661\n",
      "Total Loss: 0.11456144228577614\n",
      "------------------------------------ epoch 2305 (13824 steps) ------------------------------------\n",
      "Max loss: 0.05439060926437378\n",
      "Min loss: 0.013250249437987804\n",
      "Mean loss: 0.027671221178025007\n",
      "Std loss: 0.014693326841989122\n",
      "Total Loss: 0.16602732706815004\n",
      "------------------------------------ epoch 2306 (13830 steps) ------------------------------------\n",
      "Max loss: 0.04460712894797325\n",
      "Min loss: 0.013267144560813904\n",
      "Mean loss: 0.027717428902784984\n",
      "Std loss: 0.011227273805025088\n",
      "Total Loss: 0.1663045734167099\n",
      "------------------------------------ epoch 2307 (13836 steps) ------------------------------------\n",
      "Max loss: 0.025609834119677544\n",
      "Min loss: 0.012591840699315071\n",
      "Mean loss: 0.018864634136358898\n",
      "Std loss: 0.005159417702397513\n",
      "Total Loss: 0.11318780481815338\n",
      "------------------------------------ epoch 2308 (13842 steps) ------------------------------------\n",
      "Max loss: 0.053470365703105927\n",
      "Min loss: 0.010242740623652935\n",
      "Mean loss: 0.0229301027332743\n",
      "Std loss: 0.015000970628684175\n",
      "Total Loss: 0.1375806163996458\n",
      "------------------------------------ epoch 2309 (13848 steps) ------------------------------------\n",
      "Max loss: 0.03892374038696289\n",
      "Min loss: 0.01036207377910614\n",
      "Mean loss: 0.019886539007226627\n",
      "Std loss: 0.009140249868185296\n",
      "Total Loss: 0.11931923404335976\n",
      "------------------------------------ epoch 2310 (13854 steps) ------------------------------------\n",
      "Max loss: 0.030361074954271317\n",
      "Min loss: 0.01151612214744091\n",
      "Mean loss: 0.019916360266506672\n",
      "Std loss: 0.007238374745457923\n",
      "Total Loss: 0.11949816159904003\n",
      "------------------------------------ epoch 2311 (13860 steps) ------------------------------------\n",
      "Max loss: 0.03255791217088699\n",
      "Min loss: 0.011494943872094154\n",
      "Mean loss: 0.020787779862682026\n",
      "Std loss: 0.007480198000509839\n",
      "Total Loss: 0.12472667917609215\n",
      "------------------------------------ epoch 2312 (13866 steps) ------------------------------------\n",
      "Max loss: 0.05565173178911209\n",
      "Min loss: 0.011423379182815552\n",
      "Mean loss: 0.024003269461294014\n",
      "Std loss: 0.015166940705363651\n",
      "Total Loss: 0.1440196167677641\n",
      "------------------------------------ epoch 2313 (13872 steps) ------------------------------------\n",
      "Max loss: 0.02982875518500805\n",
      "Min loss: 0.009115107357501984\n",
      "Mean loss: 0.01674895795683066\n",
      "Std loss: 0.007052919409691544\n",
      "Total Loss: 0.10049374774098396\n",
      "------------------------------------ epoch 2314 (13878 steps) ------------------------------------\n",
      "Max loss: 0.04864680767059326\n",
      "Min loss: 0.01228315755724907\n",
      "Mean loss: 0.0252615949138999\n",
      "Std loss: 0.011302036853581507\n",
      "Total Loss: 0.1515695694833994\n",
      "------------------------------------ epoch 2315 (13884 steps) ------------------------------------\n",
      "Max loss: 0.028184672817587852\n",
      "Min loss: 0.012117823585867882\n",
      "Mean loss: 0.020554003460953634\n",
      "Std loss: 0.006568002905028208\n",
      "Total Loss: 0.1233240207657218\n",
      "------------------------------------ epoch 2316 (13890 steps) ------------------------------------\n",
      "Max loss: 0.047632746398448944\n",
      "Min loss: 0.014079811982810497\n",
      "Mean loss: 0.025286021487166483\n",
      "Std loss: 0.011361918645344304\n",
      "Total Loss: 0.1517161289229989\n",
      "------------------------------------ epoch 2317 (13896 steps) ------------------------------------\n",
      "Max loss: 0.018816139549016953\n",
      "Min loss: 0.010499916039407253\n",
      "Mean loss: 0.015074731471637884\n",
      "Std loss: 0.002922547279192854\n",
      "Total Loss: 0.09044838882982731\n",
      "------------------------------------ epoch 2318 (13902 steps) ------------------------------------\n",
      "Max loss: 0.04352589696645737\n",
      "Min loss: 0.010078375227749348\n",
      "Mean loss: 0.020200084274013836\n",
      "Std loss: 0.011854720029309268\n",
      "Total Loss: 0.12120050564408302\n",
      "------------------------------------ epoch 2319 (13908 steps) ------------------------------------\n",
      "Max loss: 0.026850206777453423\n",
      "Min loss: 0.010176528245210648\n",
      "Mean loss: 0.019040516888101894\n",
      "Std loss: 0.006534491912188555\n",
      "Total Loss: 0.11424310132861137\n",
      "------------------------------------ epoch 2320 (13914 steps) ------------------------------------\n",
      "Max loss: 0.027734501287341118\n",
      "Min loss: 0.009979518130421638\n",
      "Mean loss: 0.017251000739634037\n",
      "Std loss: 0.007222905515130156\n",
      "Total Loss: 0.10350600443780422\n",
      "------------------------------------ epoch 2321 (13920 steps) ------------------------------------\n",
      "Max loss: 0.02258870005607605\n",
      "Min loss: 0.009279727935791016\n",
      "Mean loss: 0.015250691988815865\n",
      "Std loss: 0.004445939519237297\n",
      "Total Loss: 0.09150415193289518\n",
      "------------------------------------ epoch 2322 (13926 steps) ------------------------------------\n",
      "Max loss: 0.030276207253336906\n",
      "Min loss: 0.015231218189001083\n",
      "Mean loss: 0.022047536447644234\n",
      "Std loss: 0.005982205914566568\n",
      "Total Loss: 0.1322852186858654\n",
      "------------------------------------ epoch 2323 (13932 steps) ------------------------------------\n",
      "Max loss: 0.035437844693660736\n",
      "Min loss: 0.011838564649224281\n",
      "Mean loss: 0.01889554566393296\n",
      "Std loss: 0.0077531517545803876\n",
      "Total Loss: 0.11337327398359776\n",
      "------------------------------------ epoch 2324 (13938 steps) ------------------------------------\n",
      "Max loss: 0.029068350791931152\n",
      "Min loss: 0.015573049895465374\n",
      "Mean loss: 0.020580812512586515\n",
      "Std loss: 0.004526561060096177\n",
      "Total Loss: 0.12348487507551908\n",
      "------------------------------------ epoch 2325 (13944 steps) ------------------------------------\n",
      "Max loss: 0.03074270486831665\n",
      "Min loss: 0.010068981908261776\n",
      "Mean loss: 0.019086001130441826\n",
      "Std loss: 0.007555815179381995\n",
      "Total Loss: 0.11451600678265095\n",
      "------------------------------------ epoch 2326 (13950 steps) ------------------------------------\n",
      "Max loss: 0.02923600934445858\n",
      "Min loss: 0.009686345234513283\n",
      "Mean loss: 0.01513588164622585\n",
      "Std loss: 0.006570713636556694\n",
      "Total Loss: 0.0908152898773551\n",
      "------------------------------------ epoch 2327 (13956 steps) ------------------------------------\n",
      "Max loss: 0.03256870061159134\n",
      "Min loss: 0.009593079797923565\n",
      "Mean loss: 0.019369004294276237\n",
      "Std loss: 0.0077012928776888714\n",
      "Total Loss: 0.11621402576565742\n",
      "------------------------------------ epoch 2328 (13962 steps) ------------------------------------\n",
      "Max loss: 0.03945436701178551\n",
      "Min loss: 0.01013243943452835\n",
      "Mean loss: 0.025287628173828125\n",
      "Std loss: 0.008976625405013258\n",
      "Total Loss: 0.15172576904296875\n",
      "------------------------------------ epoch 2329 (13968 steps) ------------------------------------\n",
      "Max loss: 0.027651993557810783\n",
      "Min loss: 0.01119457557797432\n",
      "Mean loss: 0.01809237990528345\n",
      "Std loss: 0.005732040109763474\n",
      "Total Loss: 0.1085542794317007\n",
      "------------------------------------ epoch 2330 (13974 steps) ------------------------------------\n",
      "Max loss: 0.03320189565420151\n",
      "Min loss: 0.010994918644428253\n",
      "Mean loss: 0.021042869581530493\n",
      "Std loss: 0.007895138397935519\n",
      "Total Loss: 0.12625721748918295\n",
      "------------------------------------ epoch 2331 (13980 steps) ------------------------------------\n",
      "Max loss: 0.03334823623299599\n",
      "Min loss: 0.023001790046691895\n",
      "Mean loss: 0.029069822281599045\n",
      "Std loss: 0.0040434086804965455\n",
      "Total Loss: 0.17441893368959427\n",
      "------------------------------------ epoch 2332 (13986 steps) ------------------------------------\n",
      "Max loss: 0.029954463243484497\n",
      "Min loss: 0.013377606868743896\n",
      "Mean loss: 0.018986063698927563\n",
      "Std loss: 0.0066307102463524364\n",
      "Total Loss: 0.11391638219356537\n",
      "------------------------------------ epoch 2333 (13992 steps) ------------------------------------\n",
      "Max loss: 0.043816156685352325\n",
      "Min loss: 0.013695703819394112\n",
      "Mean loss: 0.02321748559673627\n",
      "Std loss: 0.010677315110832482\n",
      "Total Loss: 0.13930491358041763\n",
      "------------------------------------ epoch 2334 (13998 steps) ------------------------------------\n",
      "Max loss: 0.07125210016965866\n",
      "Min loss: 0.013436387293040752\n",
      "Mean loss: 0.032540499387929835\n",
      "Std loss: 0.020630580130622755\n",
      "Total Loss: 0.19524299632757902\n",
      "------------------------------------ epoch 2335 (14004 steps) ------------------------------------\n",
      "Max loss: 0.03654101490974426\n",
      "Min loss: 0.01225576363503933\n",
      "Mean loss: 0.018975903900961082\n",
      "Std loss: 0.00811645863624669\n",
      "Total Loss: 0.11385542340576649\n",
      "------------------------------------ epoch 2336 (14010 steps) ------------------------------------\n",
      "Max loss: 0.09162072837352753\n",
      "Min loss: 0.011078315787017345\n",
      "Mean loss: 0.031985200786342226\n",
      "Std loss: 0.028527637482154995\n",
      "Total Loss: 0.19191120471805334\n",
      "------------------------------------ epoch 2337 (14016 steps) ------------------------------------\n",
      "Max loss: 0.038336943835020065\n",
      "Min loss: 0.016400514170527458\n",
      "Mean loss: 0.02893209798882405\n",
      "Std loss: 0.006988905795843433\n",
      "Total Loss: 0.1735925879329443\n",
      "------------------------------------ epoch 2338 (14022 steps) ------------------------------------\n",
      "Max loss: 0.03300949186086655\n",
      "Min loss: 0.012213239446282387\n",
      "Mean loss: 0.022904360045989353\n",
      "Std loss: 0.008292896840798803\n",
      "Total Loss: 0.13742616027593613\n",
      "------------------------------------ epoch 2339 (14028 steps) ------------------------------------\n",
      "Max loss: 0.036848150193691254\n",
      "Min loss: 0.012412479147315025\n",
      "Mean loss: 0.023279970822234947\n",
      "Std loss: 0.007263269620058661\n",
      "Total Loss: 0.1396798249334097\n",
      "------------------------------------ epoch 2340 (14034 steps) ------------------------------------\n",
      "Max loss: 0.06926793605089188\n",
      "Min loss: 0.013283531181514263\n",
      "Mean loss: 0.03287594843034943\n",
      "Std loss: 0.02221477415891284\n",
      "Total Loss: 0.19725569058209658\n",
      "------------------------------------ epoch 2341 (14040 steps) ------------------------------------\n",
      "Max loss: 0.04402879625558853\n",
      "Min loss: 0.011234721168875694\n",
      "Mean loss: 0.025124536206324894\n",
      "Std loss: 0.0114120034024655\n",
      "Total Loss: 0.15074721723794937\n",
      "------------------------------------ epoch 2342 (14046 steps) ------------------------------------\n",
      "Max loss: 0.040987059473991394\n",
      "Min loss: 0.011128164827823639\n",
      "Mean loss: 0.025508965055147808\n",
      "Std loss: 0.009969654471109372\n",
      "Total Loss: 0.15305379033088684\n",
      "------------------------------------ epoch 2343 (14052 steps) ------------------------------------\n",
      "Max loss: 0.034265391528606415\n",
      "Min loss: 0.013695215806365013\n",
      "Mean loss: 0.021363158399860065\n",
      "Std loss: 0.006619061264884322\n",
      "Total Loss: 0.12817895039916039\n",
      "------------------------------------ epoch 2344 (14058 steps) ------------------------------------\n",
      "Max loss: 0.036998067051172256\n",
      "Min loss: 0.010576635599136353\n",
      "Mean loss: 0.019589478615671396\n",
      "Std loss: 0.008650498770229208\n",
      "Total Loss: 0.11753687169402838\n",
      "------------------------------------ epoch 2345 (14064 steps) ------------------------------------\n",
      "Max loss: 0.031282827258110046\n",
      "Min loss: 0.008916809223592281\n",
      "Mean loss: 0.01827587777127822\n",
      "Std loss: 0.009325935760572128\n",
      "Total Loss: 0.10965526662766933\n",
      "------------------------------------ epoch 2346 (14070 steps) ------------------------------------\n",
      "Max loss: 0.04205424338579178\n",
      "Min loss: 0.010186145082116127\n",
      "Mean loss: 0.019049847808976967\n",
      "Std loss: 0.011659246509162493\n",
      "Total Loss: 0.11429908685386181\n",
      "------------------------------------ epoch 2347 (14076 steps) ------------------------------------\n",
      "Max loss: 0.05316153168678284\n",
      "Min loss: 0.010942801833152771\n",
      "Mean loss: 0.02831265516579151\n",
      "Std loss: 0.01630067752759741\n",
      "Total Loss: 0.16987593099474907\n",
      "------------------------------------ epoch 2348 (14082 steps) ------------------------------------\n",
      "Max loss: 0.03305869176983833\n",
      "Min loss: 0.009387645870447159\n",
      "Mean loss: 0.01968339302887519\n",
      "Std loss: 0.007633751175323009\n",
      "Total Loss: 0.11810035817325115\n",
      "------------------------------------ epoch 2349 (14088 steps) ------------------------------------\n",
      "Max loss: 0.046034395694732666\n",
      "Min loss: 0.010438334196805954\n",
      "Mean loss: 0.02516159911950429\n",
      "Std loss: 0.015299256511183013\n",
      "Total Loss: 0.15096959471702576\n",
      "------------------------------------ epoch 2350 (14094 steps) ------------------------------------\n",
      "Max loss: 0.04789431393146515\n",
      "Min loss: 0.017105791717767715\n",
      "Mean loss: 0.02766790023694436\n",
      "Std loss: 0.012160683255960295\n",
      "Total Loss: 0.16600740142166615\n",
      "------------------------------------ epoch 2351 (14100 steps) ------------------------------------\n",
      "Max loss: 0.043512385338544846\n",
      "Min loss: 0.012549868784844875\n",
      "Mean loss: 0.026717322878539562\n",
      "Std loss: 0.011045408380164472\n",
      "Total Loss: 0.16030393727123737\n",
      "------------------------------------ epoch 2352 (14106 steps) ------------------------------------\n",
      "Max loss: 0.036014147102832794\n",
      "Min loss: 0.013981301337480545\n",
      "Mean loss: 0.023373801882068317\n",
      "Std loss: 0.007125787948443898\n",
      "Total Loss: 0.1402428112924099\n",
      "------------------------------------ epoch 2353 (14112 steps) ------------------------------------\n",
      "Max loss: 0.0238364115357399\n",
      "Min loss: 0.010731250047683716\n",
      "Mean loss: 0.018471638672053814\n",
      "Std loss: 0.004485079184818977\n",
      "Total Loss: 0.11082983203232288\n",
      "------------------------------------ epoch 2354 (14118 steps) ------------------------------------\n",
      "Max loss: 0.02219027280807495\n",
      "Min loss: 0.010946354828774929\n",
      "Mean loss: 0.014695232889304558\n",
      "Std loss: 0.004116991424136509\n",
      "Total Loss: 0.08817139733582735\n",
      "------------------------------------ epoch 2355 (14124 steps) ------------------------------------\n",
      "Max loss: 0.03638094663619995\n",
      "Min loss: 0.01473912876099348\n",
      "Mean loss: 0.02216213025773565\n",
      "Std loss: 0.007092838166916211\n",
      "Total Loss: 0.1329727815464139\n",
      "------------------------------------ epoch 2356 (14130 steps) ------------------------------------\n",
      "Max loss: 0.023193929344415665\n",
      "Min loss: 0.010253614746034145\n",
      "Mean loss: 0.018189577541003626\n",
      "Std loss: 0.004368448697711551\n",
      "Total Loss: 0.10913746524602175\n",
      "------------------------------------ epoch 2357 (14136 steps) ------------------------------------\n",
      "Max loss: 0.051800452172756195\n",
      "Min loss: 0.011895048432052135\n",
      "Mean loss: 0.02478400602315863\n",
      "Std loss: 0.0130262182112073\n",
      "Total Loss: 0.14870403613895178\n",
      "------------------------------------ epoch 2358 (14142 steps) ------------------------------------\n",
      "Max loss: 0.035777222365140915\n",
      "Min loss: 0.0137516213580966\n",
      "Mean loss: 0.01940471027046442\n",
      "Std loss: 0.008199838354281975\n",
      "Total Loss: 0.11642826162278652\n",
      "------------------------------------ epoch 2359 (14148 steps) ------------------------------------\n",
      "Max loss: 0.025092754513025284\n",
      "Min loss: 0.01125338114798069\n",
      "Mean loss: 0.016845541540533304\n",
      "Std loss: 0.004994001562317254\n",
      "Total Loss: 0.10107324924319983\n",
      "------------------------------------ epoch 2360 (14154 steps) ------------------------------------\n",
      "Max loss: 0.03676236793398857\n",
      "Min loss: 0.015486548654735088\n",
      "Mean loss: 0.02563888595129053\n",
      "Std loss: 0.00797058748458677\n",
      "Total Loss: 0.15383331570774317\n",
      "------------------------------------ epoch 2361 (14160 steps) ------------------------------------\n",
      "Max loss: 0.022699754685163498\n",
      "Min loss: 0.00969411339610815\n",
      "Mean loss: 0.014153459419806799\n",
      "Std loss: 0.004617351722680511\n",
      "Total Loss: 0.08492075651884079\n",
      "------------------------------------ epoch 2362 (14166 steps) ------------------------------------\n",
      "Max loss: 0.045035235583782196\n",
      "Min loss: 0.009391429834067822\n",
      "Mean loss: 0.016254709257433813\n",
      "Std loss: 0.012929961006174003\n",
      "Total Loss: 0.09752825554460287\n",
      "------------------------------------ epoch 2363 (14172 steps) ------------------------------------\n",
      "Max loss: 0.025515247136354446\n",
      "Min loss: 0.012089604511857033\n",
      "Mean loss: 0.017497263227899868\n",
      "Std loss: 0.004105152662954612\n",
      "Total Loss: 0.10498357936739922\n",
      "------------------------------------ epoch 2364 (14178 steps) ------------------------------------\n",
      "Max loss: 0.02948123775422573\n",
      "Min loss: 0.011198441497981548\n",
      "Mean loss: 0.021303337533026934\n",
      "Std loss: 0.006381437682533431\n",
      "Total Loss: 0.1278200251981616\n",
      "------------------------------------ epoch 2365 (14184 steps) ------------------------------------\n",
      "Max loss: 0.014015359804034233\n",
      "Min loss: 0.008444946259260178\n",
      "Mean loss: 0.011381402611732483\n",
      "Std loss: 0.0019962805661676163\n",
      "Total Loss: 0.0682884156703949\n",
      "------------------------------------ epoch 2366 (14190 steps) ------------------------------------\n",
      "Max loss: 0.022955266758799553\n",
      "Min loss: 0.01290930062532425\n",
      "Mean loss: 0.018773709734280903\n",
      "Std loss: 0.0033784710257919516\n",
      "Total Loss: 0.11264225840568542\n",
      "------------------------------------ epoch 2367 (14196 steps) ------------------------------------\n",
      "Max loss: 0.024935021996498108\n",
      "Min loss: 0.008595336228609085\n",
      "Mean loss: 0.013109381000200907\n",
      "Std loss: 0.005434431499573083\n",
      "Total Loss: 0.07865628600120544\n",
      "------------------------------------ epoch 2368 (14202 steps) ------------------------------------\n",
      "Max loss: 0.03596606105566025\n",
      "Min loss: 0.009138587862253189\n",
      "Mean loss: 0.016577720021208126\n",
      "Std loss: 0.00936325519323429\n",
      "Total Loss: 0.09946632012724876\n",
      "------------------------------------ epoch 2369 (14208 steps) ------------------------------------\n",
      "Max loss: 0.02261805161833763\n",
      "Min loss: 0.009635493159294128\n",
      "Mean loss: 0.014185473012427488\n",
      "Std loss: 0.0043568731863428065\n",
      "Total Loss: 0.08511283807456493\n",
      "------------------------------------ epoch 2370 (14214 steps) ------------------------------------\n",
      "Max loss: 0.02115023136138916\n",
      "Min loss: 0.010831471532583237\n",
      "Mean loss: 0.01551381079480052\n",
      "Std loss: 0.0037009584330746138\n",
      "Total Loss: 0.09308286476880312\n",
      "------------------------------------ epoch 2371 (14220 steps) ------------------------------------\n",
      "Max loss: 0.02820703387260437\n",
      "Min loss: 0.011157317087054253\n",
      "Mean loss: 0.017658445208022993\n",
      "Std loss: 0.006984168152483353\n",
      "Total Loss: 0.10595067124813795\n",
      "------------------------------------ epoch 2372 (14226 steps) ------------------------------------\n",
      "Max loss: 0.01431853137910366\n",
      "Min loss: 0.007385860197246075\n",
      "Mean loss: 0.010326342967649301\n",
      "Std loss: 0.0028272576079706752\n",
      "Total Loss: 0.061958057805895805\n",
      "------------------------------------ epoch 2373 (14232 steps) ------------------------------------\n",
      "Max loss: 0.021275058388710022\n",
      "Min loss: 0.009317714720964432\n",
      "Mean loss: 0.014171769687285026\n",
      "Std loss: 0.003989031056999069\n",
      "Total Loss: 0.08503061812371016\n",
      "------------------------------------ epoch 2374 (14238 steps) ------------------------------------\n",
      "Max loss: 0.0205395445227623\n",
      "Min loss: 0.008259506896138191\n",
      "Mean loss: 0.012175403069704771\n",
      "Std loss: 0.004351112641957685\n",
      "Total Loss: 0.07305241841822863\n",
      "------------------------------------ epoch 2375 (14244 steps) ------------------------------------\n",
      "Max loss: 0.023685041815042496\n",
      "Min loss: 0.008059408515691757\n",
      "Mean loss: 0.014404636342078447\n",
      "Std loss: 0.0050422227045038345\n",
      "Total Loss: 0.08642781805247068\n",
      "------------------------------------ epoch 2376 (14250 steps) ------------------------------------\n",
      "Max loss: 0.033661358058452606\n",
      "Min loss: 0.011001281440258026\n",
      "Mean loss: 0.020872100566824276\n",
      "Std loss: 0.0070832969971668415\n",
      "Total Loss: 0.12523260340094566\n",
      "------------------------------------ epoch 2377 (14256 steps) ------------------------------------\n",
      "Max loss: 0.033011529594659805\n",
      "Min loss: 0.011647938750684261\n",
      "Mean loss: 0.01887548870096604\n",
      "Std loss: 0.008034354844667151\n",
      "Total Loss: 0.11325293220579624\n",
      "------------------------------------ epoch 2378 (14262 steps) ------------------------------------\n",
      "Max loss: 0.023763736709952354\n",
      "Min loss: 0.009004538878798485\n",
      "Mean loss: 0.014848926415046057\n",
      "Std loss: 0.005749417702378696\n",
      "Total Loss: 0.08909355849027634\n",
      "------------------------------------ epoch 2379 (14268 steps) ------------------------------------\n",
      "Max loss: 0.020266924053430557\n",
      "Min loss: 0.008265779353678226\n",
      "Mean loss: 0.014137572919329008\n",
      "Std loss: 0.003488956132037748\n",
      "Total Loss: 0.08482543751597404\n",
      "------------------------------------ epoch 2380 (14274 steps) ------------------------------------\n",
      "Max loss: 0.025963908061385155\n",
      "Min loss: 0.009439992718398571\n",
      "Mean loss: 0.017977111507207155\n",
      "Std loss: 0.004987446953281681\n",
      "Total Loss: 0.10786266904324293\n",
      "------------------------------------ epoch 2381 (14280 steps) ------------------------------------\n",
      "Max loss: 0.020866399630904198\n",
      "Min loss: 0.008850788697600365\n",
      "Mean loss: 0.01419156277552247\n",
      "Std loss: 0.0039430026902481765\n",
      "Total Loss: 0.08514937665313482\n",
      "------------------------------------ epoch 2382 (14286 steps) ------------------------------------\n",
      "Max loss: 0.03909315913915634\n",
      "Min loss: 0.010967547073960304\n",
      "Mean loss: 0.019455065329869587\n",
      "Std loss: 0.009562374720105416\n",
      "Total Loss: 0.11673039197921753\n",
      "------------------------------------ epoch 2383 (14292 steps) ------------------------------------\n",
      "Max loss: 0.028147539123892784\n",
      "Min loss: 0.00881841778755188\n",
      "Mean loss: 0.013036676061650118\n",
      "Std loss: 0.006811696744295714\n",
      "Total Loss: 0.0782200563699007\n",
      "------------------------------------ epoch 2384 (14298 steps) ------------------------------------\n",
      "Max loss: 0.02534712664783001\n",
      "Min loss: 0.011605119332671165\n",
      "Mean loss: 0.01731023968507846\n",
      "Std loss: 0.005484299978769307\n",
      "Total Loss: 0.10386143811047077\n",
      "------------------------------------ epoch 2385 (14304 steps) ------------------------------------\n",
      "Max loss: 0.08097052574157715\n",
      "Min loss: 0.009808048605918884\n",
      "Mean loss: 0.025405558447043102\n",
      "Std loss: 0.025135173089953385\n",
      "Total Loss: 0.1524333506822586\n",
      "------------------------------------ epoch 2386 (14310 steps) ------------------------------------\n",
      "Max loss: 0.031771935522556305\n",
      "Min loss: 0.008675782009959221\n",
      "Mean loss: 0.016193718649446964\n",
      "Std loss: 0.00785633630429785\n",
      "Total Loss: 0.09716231189668179\n",
      "------------------------------------ epoch 2387 (14316 steps) ------------------------------------\n",
      "Max loss: 0.029755741357803345\n",
      "Min loss: 0.011022682301700115\n",
      "Mean loss: 0.01871573847408096\n",
      "Std loss: 0.007651764241937957\n",
      "Total Loss: 0.11229443084448576\n",
      "------------------------------------ epoch 2388 (14322 steps) ------------------------------------\n",
      "Max loss: 0.022953858599066734\n",
      "Min loss: 0.008871915750205517\n",
      "Mean loss: 0.01473265482733647\n",
      "Std loss: 0.005235458981171041\n",
      "Total Loss: 0.08839592896401882\n",
      "------------------------------------ epoch 2389 (14328 steps) ------------------------------------\n",
      "Max loss: 0.030486464500427246\n",
      "Min loss: 0.01000971719622612\n",
      "Mean loss: 0.01758612769966324\n",
      "Std loss: 0.008969097643049345\n",
      "Total Loss: 0.10551676619797945\n",
      "------------------------------------ epoch 2390 (14334 steps) ------------------------------------\n",
      "Max loss: 0.028941847383975983\n",
      "Min loss: 0.012887954711914062\n",
      "Mean loss: 0.021921958774328232\n",
      "Std loss: 0.005719081259101582\n",
      "Total Loss: 0.1315317526459694\n",
      "------------------------------------ epoch 2391 (14340 steps) ------------------------------------\n",
      "Max loss: 0.040309906005859375\n",
      "Min loss: 0.007992862723767757\n",
      "Mean loss: 0.020405577030032873\n",
      "Std loss: 0.009895796545168811\n",
      "Total Loss: 0.12243346218019724\n",
      "------------------------------------ epoch 2392 (14346 steps) ------------------------------------\n",
      "Max loss: 0.018143972381949425\n",
      "Min loss: 0.014641739428043365\n",
      "Mean loss: 0.0159861344533662\n",
      "Std loss: 0.0011101307126149909\n",
      "Total Loss: 0.0959168067201972\n",
      "------------------------------------ epoch 2393 (14352 steps) ------------------------------------\n",
      "Max loss: 0.06768430769443512\n",
      "Min loss: 0.009258299134671688\n",
      "Mean loss: 0.028544983360916376\n",
      "Std loss: 0.022717125600728947\n",
      "Total Loss: 0.17126990016549826\n",
      "------------------------------------ epoch 2394 (14358 steps) ------------------------------------\n",
      "Max loss: 0.056739624589681625\n",
      "Min loss: 0.012221085838973522\n",
      "Mean loss: 0.02696256075675289\n",
      "Std loss: 0.014694830774350116\n",
      "Total Loss: 0.16177536454051733\n",
      "------------------------------------ epoch 2395 (14364 steps) ------------------------------------\n",
      "Max loss: 0.026845768094062805\n",
      "Min loss: 0.010300382040441036\n",
      "Mean loss: 0.016792852897197008\n",
      "Std loss: 0.005692162812262605\n",
      "Total Loss: 0.10075711738318205\n",
      "------------------------------------ epoch 2396 (14370 steps) ------------------------------------\n",
      "Max loss: 0.03292626142501831\n",
      "Min loss: 0.011722946539521217\n",
      "Mean loss: 0.019126038687924545\n",
      "Std loss: 0.007825164167527484\n",
      "Total Loss: 0.11475623212754726\n",
      "------------------------------------ epoch 2397 (14376 steps) ------------------------------------\n",
      "Max loss: 0.0549282506108284\n",
      "Min loss: 0.011660289019346237\n",
      "Mean loss: 0.026802509867896635\n",
      "Std loss: 0.014202777403992421\n",
      "Total Loss: 0.16081505920737982\n",
      "------------------------------------ epoch 2398 (14382 steps) ------------------------------------\n",
      "Max loss: 0.026118461042642593\n",
      "Min loss: 0.012270187959074974\n",
      "Mean loss: 0.01890046304712693\n",
      "Std loss: 0.00524592072398803\n",
      "Total Loss: 0.11340277828276157\n",
      "------------------------------------ epoch 2399 (14388 steps) ------------------------------------\n",
      "Max loss: 0.03762273117899895\n",
      "Min loss: 0.011290431022644043\n",
      "Mean loss: 0.01991700443128745\n",
      "Std loss: 0.008859243578478218\n",
      "Total Loss: 0.11950202658772469\n",
      "------------------------------------ epoch 2400 (14394 steps) ------------------------------------\n",
      "Max loss: 0.0458357036113739\n",
      "Min loss: 0.010666003450751305\n",
      "Mean loss: 0.020384667596469324\n",
      "Std loss: 0.011946946890926178\n",
      "Total Loss: 0.12230800557881594\n",
      "------------------------------------ epoch 2401 (14400 steps) ------------------------------------\n",
      "Max loss: 0.06424394994974136\n",
      "Min loss: 0.010680017992854118\n",
      "Mean loss: 0.030937311239540577\n",
      "Std loss: 0.018512309684531776\n",
      "Total Loss: 0.18562386743724346\n",
      "saved model at ./weights/model_2401.pth\n",
      "------------------------------------ epoch 2402 (14406 steps) ------------------------------------\n",
      "Max loss: 0.06410416960716248\n",
      "Min loss: 0.010720453225076199\n",
      "Mean loss: 0.02834919902185599\n",
      "Std loss: 0.018346824303966874\n",
      "Total Loss: 0.17009519413113594\n",
      "------------------------------------ epoch 2403 (14412 steps) ------------------------------------\n",
      "Max loss: 0.03882833942770958\n",
      "Min loss: 0.011740166693925858\n",
      "Mean loss: 0.022054547754426796\n",
      "Std loss: 0.009406089393183464\n",
      "Total Loss: 0.13232728652656078\n",
      "------------------------------------ epoch 2404 (14418 steps) ------------------------------------\n",
      "Max loss: 0.05388014018535614\n",
      "Min loss: 0.011375273577868938\n",
      "Mean loss: 0.026986093415568273\n",
      "Std loss: 0.015739324477813975\n",
      "Total Loss: 0.16191656049340963\n",
      "------------------------------------ epoch 2405 (14424 steps) ------------------------------------\n",
      "Max loss: 0.030487101525068283\n",
      "Min loss: 0.012413833290338516\n",
      "Mean loss: 0.019834668685992558\n",
      "Std loss: 0.007259373847642935\n",
      "Total Loss: 0.11900801211595535\n",
      "------------------------------------ epoch 2406 (14430 steps) ------------------------------------\n",
      "Max loss: 0.02581128478050232\n",
      "Min loss: 0.010434938594698906\n",
      "Mean loss: 0.01679203224678834\n",
      "Std loss: 0.005370874758467635\n",
      "Total Loss: 0.10075219348073006\n",
      "------------------------------------ epoch 2407 (14436 steps) ------------------------------------\n",
      "Max loss: 0.03353789448738098\n",
      "Min loss: 0.01245201751589775\n",
      "Mean loss: 0.018016513902693987\n",
      "Std loss: 0.007405324196366054\n",
      "Total Loss: 0.10809908341616392\n",
      "------------------------------------ epoch 2408 (14442 steps) ------------------------------------\n",
      "Max loss: 0.03694002330303192\n",
      "Min loss: 0.010834386572241783\n",
      "Mean loss: 0.01766673242673278\n",
      "Std loss: 0.008931736948645927\n",
      "Total Loss: 0.10600039456039667\n",
      "------------------------------------ epoch 2409 (14448 steps) ------------------------------------\n",
      "Max loss: 0.043972499668598175\n",
      "Min loss: 0.014724120497703552\n",
      "Mean loss: 0.02651230835666259\n",
      "Std loss: 0.011133137426625953\n",
      "Total Loss: 0.15907385013997555\n",
      "------------------------------------ epoch 2410 (14454 steps) ------------------------------------\n",
      "Max loss: 0.059977948665618896\n",
      "Min loss: 0.0148616386577487\n",
      "Mean loss: 0.025498012856890757\n",
      "Std loss: 0.015580021752635818\n",
      "Total Loss: 0.15298807714134455\n",
      "------------------------------------ epoch 2411 (14460 steps) ------------------------------------\n",
      "Max loss: 0.05768049880862236\n",
      "Min loss: 0.016481265425682068\n",
      "Mean loss: 0.033537473219136395\n",
      "Std loss: 0.013579894409927818\n",
      "Total Loss: 0.20122483931481838\n",
      "------------------------------------ epoch 2412 (14466 steps) ------------------------------------\n",
      "Max loss: 0.03501565754413605\n",
      "Min loss: 0.009634331800043583\n",
      "Mean loss: 0.020012729335576296\n",
      "Std loss: 0.008763142618057268\n",
      "Total Loss: 0.12007637601345778\n",
      "------------------------------------ epoch 2413 (14472 steps) ------------------------------------\n",
      "Max loss: 0.028982039541006088\n",
      "Min loss: 0.012427136301994324\n",
      "Mean loss: 0.018303955129037302\n",
      "Std loss: 0.005840575626813989\n",
      "Total Loss: 0.1098237307742238\n",
      "------------------------------------ epoch 2414 (14478 steps) ------------------------------------\n",
      "Max loss: 0.04638969525694847\n",
      "Min loss: 0.013784262351691723\n",
      "Mean loss: 0.02488334182028969\n",
      "Std loss: 0.010308131582638274\n",
      "Total Loss: 0.14930005092173815\n",
      "------------------------------------ epoch 2415 (14484 steps) ------------------------------------\n",
      "Max loss: 0.037252478301525116\n",
      "Min loss: 0.017750587314367294\n",
      "Mean loss: 0.028610019634167354\n",
      "Std loss: 0.006690251299506821\n",
      "Total Loss: 0.17166011780500412\n",
      "------------------------------------ epoch 2416 (14490 steps) ------------------------------------\n",
      "Max loss: 0.03811700642108917\n",
      "Min loss: 0.01197256799787283\n",
      "Mean loss: 0.019931999656061333\n",
      "Std loss: 0.009510773315806813\n",
      "Total Loss: 0.11959199793636799\n",
      "------------------------------------ epoch 2417 (14496 steps) ------------------------------------\n",
      "Max loss: 0.04636864364147186\n",
      "Min loss: 0.010121779516339302\n",
      "Mean loss: 0.023222817728916805\n",
      "Std loss: 0.012845784352893226\n",
      "Total Loss: 0.13933690637350082\n",
      "------------------------------------ epoch 2418 (14502 steps) ------------------------------------\n",
      "Max loss: 0.018489781767129898\n",
      "Min loss: 0.012538212351500988\n",
      "Mean loss: 0.016288631750891607\n",
      "Std loss: 0.0018758687890348521\n",
      "Total Loss: 0.09773179050534964\n",
      "------------------------------------ epoch 2419 (14508 steps) ------------------------------------\n",
      "Max loss: 0.03772452473640442\n",
      "Min loss: 0.009954769164323807\n",
      "Mean loss: 0.020473248790949583\n",
      "Std loss: 0.010529675474761295\n",
      "Total Loss: 0.1228394927456975\n",
      "------------------------------------ epoch 2420 (14514 steps) ------------------------------------\n",
      "Max loss: 0.028038110584020615\n",
      "Min loss: 0.009351125918328762\n",
      "Mean loss: 0.014857562724500895\n",
      "Std loss: 0.006064528687056789\n",
      "Total Loss: 0.08914537634700537\n",
      "------------------------------------ epoch 2421 (14520 steps) ------------------------------------\n",
      "Max loss: 0.028615638613700867\n",
      "Min loss: 0.01359548605978489\n",
      "Mean loss: 0.01937731479605039\n",
      "Std loss: 0.004944681099811946\n",
      "Total Loss: 0.11626388877630234\n",
      "------------------------------------ epoch 2422 (14526 steps) ------------------------------------\n",
      "Max loss: 0.04632357507944107\n",
      "Min loss: 0.0102305319160223\n",
      "Mean loss: 0.022181790322065353\n",
      "Std loss: 0.01194393498059643\n",
      "Total Loss: 0.13309074193239212\n",
      "------------------------------------ epoch 2423 (14532 steps) ------------------------------------\n",
      "Max loss: 0.022491108626127243\n",
      "Min loss: 0.011049296706914902\n",
      "Mean loss: 0.017216153908520937\n",
      "Std loss: 0.004311761559004304\n",
      "Total Loss: 0.10329692345112562\n",
      "------------------------------------ epoch 2424 (14538 steps) ------------------------------------\n",
      "Max loss: 0.07904277741909027\n",
      "Min loss: 0.007986320182681084\n",
      "Mean loss: 0.02962367640187343\n",
      "Std loss: 0.023163332298387625\n",
      "Total Loss: 0.17774205841124058\n",
      "------------------------------------ epoch 2425 (14544 steps) ------------------------------------\n",
      "Max loss: 0.03962915390729904\n",
      "Min loss: 0.01804148033261299\n",
      "Mean loss: 0.026502768509089947\n",
      "Std loss: 0.008819277917275905\n",
      "Total Loss: 0.15901661105453968\n",
      "------------------------------------ epoch 2426 (14550 steps) ------------------------------------\n",
      "Max loss: 0.025230418890714645\n",
      "Min loss: 0.012014791369438171\n",
      "Mean loss: 0.01766743961100777\n",
      "Std loss: 0.004950437087266049\n",
      "Total Loss: 0.10600463766604662\n",
      "------------------------------------ epoch 2427 (14556 steps) ------------------------------------\n",
      "Max loss: 0.047076813876628876\n",
      "Min loss: 0.01185525581240654\n",
      "Mean loss: 0.020043050559858482\n",
      "Std loss: 0.012177500937696015\n",
      "Total Loss: 0.12025830335915089\n",
      "------------------------------------ epoch 2428 (14562 steps) ------------------------------------\n",
      "Max loss: 0.04065246507525444\n",
      "Min loss: 0.010433538816869259\n",
      "Mean loss: 0.023191330954432487\n",
      "Std loss: 0.010966036819963639\n",
      "Total Loss: 0.13914798572659492\n",
      "------------------------------------ epoch 2429 (14568 steps) ------------------------------------\n",
      "Max loss: 0.06026186794042587\n",
      "Min loss: 0.013152167201042175\n",
      "Mean loss: 0.032143633191784225\n",
      "Std loss: 0.01832783744401569\n",
      "Total Loss: 0.19286179915070534\n",
      "------------------------------------ epoch 2430 (14574 steps) ------------------------------------\n",
      "Max loss: 0.03548523038625717\n",
      "Min loss: 0.013940135948359966\n",
      "Mean loss: 0.020253985188901424\n",
      "Std loss: 0.007512356428363805\n",
      "Total Loss: 0.12152391113340855\n",
      "------------------------------------ epoch 2431 (14580 steps) ------------------------------------\n",
      "Max loss: 0.029918422922492027\n",
      "Min loss: 0.013283586129546165\n",
      "Mean loss: 0.020964741085966427\n",
      "Std loss: 0.005534675483224805\n",
      "Total Loss: 0.12578844651579857\n",
      "------------------------------------ epoch 2432 (14586 steps) ------------------------------------\n",
      "Max loss: 0.06361648440361023\n",
      "Min loss: 0.012726467102766037\n",
      "Mean loss: 0.027066742070019245\n",
      "Std loss: 0.016930165339569907\n",
      "Total Loss: 0.16240045242011547\n",
      "------------------------------------ epoch 2433 (14592 steps) ------------------------------------\n",
      "Max loss: 0.024546056985855103\n",
      "Min loss: 0.010725523345172405\n",
      "Mean loss: 0.016375470130393904\n",
      "Std loss: 0.0045889974459320405\n",
      "Total Loss: 0.09825282078236341\n",
      "------------------------------------ epoch 2434 (14598 steps) ------------------------------------\n",
      "Max loss: 0.03756598383188248\n",
      "Min loss: 0.011761631816625595\n",
      "Mean loss: 0.019889014617850382\n",
      "Std loss: 0.008451759660761053\n",
      "Total Loss: 0.1193340877071023\n",
      "------------------------------------ epoch 2435 (14604 steps) ------------------------------------\n",
      "Max loss: 0.038251135498285294\n",
      "Min loss: 0.012636633589863777\n",
      "Mean loss: 0.022725255228579044\n",
      "Std loss: 0.010121405715092567\n",
      "Total Loss: 0.13635153137147427\n",
      "------------------------------------ epoch 2436 (14610 steps) ------------------------------------\n",
      "Max loss: 0.030992990359663963\n",
      "Min loss: 0.00984946545213461\n",
      "Mean loss: 0.018216148329277832\n",
      "Std loss: 0.006907299549972971\n",
      "Total Loss: 0.109296889975667\n",
      "------------------------------------ epoch 2437 (14616 steps) ------------------------------------\n",
      "Max loss: 0.03448295593261719\n",
      "Min loss: 0.00862179696559906\n",
      "Mean loss: 0.021529121634860832\n",
      "Std loss: 0.009972697264320176\n",
      "Total Loss: 0.129174729809165\n",
      "------------------------------------ epoch 2438 (14622 steps) ------------------------------------\n",
      "Max loss: 0.03578052669763565\n",
      "Min loss: 0.011475554667413235\n",
      "Mean loss: 0.019455059276272852\n",
      "Std loss: 0.008313256509386307\n",
      "Total Loss: 0.11673035565763712\n",
      "------------------------------------ epoch 2439 (14628 steps) ------------------------------------\n",
      "Max loss: 0.033601850271224976\n",
      "Min loss: 0.007548728492110968\n",
      "Mean loss: 0.018006135631973546\n",
      "Std loss: 0.010691334361484556\n",
      "Total Loss: 0.10803681379184127\n",
      "------------------------------------ epoch 2440 (14634 steps) ------------------------------------\n",
      "Max loss: 0.02377365343272686\n",
      "Min loss: 0.008958416990935802\n",
      "Mean loss: 0.017246322085460026\n",
      "Std loss: 0.004981630846342812\n",
      "Total Loss: 0.10347793251276016\n",
      "------------------------------------ epoch 2441 (14640 steps) ------------------------------------\n",
      "Max loss: 0.021257823333144188\n",
      "Min loss: 0.011016717180609703\n",
      "Mean loss: 0.015516635030508041\n",
      "Std loss: 0.004212026674906778\n",
      "Total Loss: 0.09309981018304825\n",
      "------------------------------------ epoch 2442 (14646 steps) ------------------------------------\n",
      "Max loss: 0.030047640204429626\n",
      "Min loss: 0.014738015830516815\n",
      "Mean loss: 0.022454918672641117\n",
      "Std loss: 0.005421199886308826\n",
      "Total Loss: 0.1347295120358467\n",
      "------------------------------------ epoch 2443 (14652 steps) ------------------------------------\n",
      "Max loss: 0.022114938125014305\n",
      "Min loss: 0.009535002522170544\n",
      "Mean loss: 0.015429954820623001\n",
      "Std loss: 0.004479408573804927\n",
      "Total Loss: 0.092579728923738\n",
      "------------------------------------ epoch 2444 (14658 steps) ------------------------------------\n",
      "Max loss: 0.02022385410964489\n",
      "Min loss: 0.010772938840091228\n",
      "Mean loss: 0.01333642192184925\n",
      "Std loss: 0.003164669114842875\n",
      "Total Loss: 0.0800185315310955\n",
      "------------------------------------ epoch 2445 (14664 steps) ------------------------------------\n",
      "Max loss: 0.06590375304222107\n",
      "Min loss: 0.01005309447646141\n",
      "Mean loss: 0.022399194538593292\n",
      "Std loss: 0.019909940699687552\n",
      "Total Loss: 0.13439516723155975\n",
      "------------------------------------ epoch 2446 (14670 steps) ------------------------------------\n",
      "Max loss: 0.03351038694381714\n",
      "Min loss: 0.012987012974917889\n",
      "Mean loss: 0.02081713266670704\n",
      "Std loss: 0.007695147084352754\n",
      "Total Loss: 0.12490279600024223\n",
      "------------------------------------ epoch 2447 (14676 steps) ------------------------------------\n",
      "Max loss: 0.03737138211727142\n",
      "Min loss: 0.012710370123386383\n",
      "Mean loss: 0.02275296114385128\n",
      "Std loss: 0.00872886173680473\n",
      "Total Loss: 0.13651776686310768\n",
      "------------------------------------ epoch 2448 (14682 steps) ------------------------------------\n",
      "Max loss: 0.020823346450924873\n",
      "Min loss: 0.012237492017447948\n",
      "Mean loss: 0.015365878430505594\n",
      "Std loss: 0.003313156619145659\n",
      "Total Loss: 0.09219527058303356\n",
      "------------------------------------ epoch 2449 (14688 steps) ------------------------------------\n",
      "Max loss: 0.05153520032763481\n",
      "Min loss: 0.01056681014597416\n",
      "Mean loss: 0.02605449951564272\n",
      "Std loss: 0.015159953204541186\n",
      "Total Loss: 0.15632699709385633\n",
      "------------------------------------ epoch 2450 (14694 steps) ------------------------------------\n",
      "Max loss: 0.020406197756528854\n",
      "Min loss: 0.01211477629840374\n",
      "Mean loss: 0.01516612914080421\n",
      "Std loss: 0.0027072595740334202\n",
      "Total Loss: 0.09099677484482527\n",
      "------------------------------------ epoch 2451 (14700 steps) ------------------------------------\n",
      "Max loss: 0.04217226430773735\n",
      "Min loss: 0.01169314980506897\n",
      "Mean loss: 0.022562276882429916\n",
      "Std loss: 0.009595274044741463\n",
      "Total Loss: 0.1353736612945795\n",
      "------------------------------------ epoch 2452 (14706 steps) ------------------------------------\n",
      "Max loss: 0.04207318648695946\n",
      "Min loss: 0.01055549830198288\n",
      "Mean loss: 0.02523802872747183\n",
      "Std loss: 0.010531699706505063\n",
      "Total Loss: 0.15142817236483097\n",
      "------------------------------------ epoch 2453 (14712 steps) ------------------------------------\n",
      "Max loss: 0.03332355618476868\n",
      "Min loss: 0.015928279608488083\n",
      "Mean loss: 0.021783491286138695\n",
      "Std loss: 0.006219822832138206\n",
      "Total Loss: 0.13070094771683216\n",
      "------------------------------------ epoch 2454 (14718 steps) ------------------------------------\n",
      "Max loss: 0.057378243654966354\n",
      "Min loss: 0.014189310371875763\n",
      "Mean loss: 0.034082852924863495\n",
      "Std loss: 0.014477411943950165\n",
      "Total Loss: 0.20449711754918098\n",
      "------------------------------------ epoch 2455 (14724 steps) ------------------------------------\n",
      "Max loss: 0.032069895416498184\n",
      "Min loss: 0.014057445339858532\n",
      "Mean loss: 0.0216536905306081\n",
      "Std loss: 0.005608155492890962\n",
      "Total Loss: 0.1299221431836486\n",
      "------------------------------------ epoch 2456 (14730 steps) ------------------------------------\n",
      "Max loss: 0.04078974202275276\n",
      "Min loss: 0.01245381124317646\n",
      "Mean loss: 0.028318585827946663\n",
      "Std loss: 0.009487647393373435\n",
      "Total Loss: 0.16991151496767998\n",
      "------------------------------------ epoch 2457 (14736 steps) ------------------------------------\n",
      "Max loss: 0.028281448408961296\n",
      "Min loss: 0.015195618383586407\n",
      "Mean loss: 0.02028941595926881\n",
      "Std loss: 0.004371311128547602\n",
      "Total Loss: 0.12173649575561285\n",
      "------------------------------------ epoch 2458 (14742 steps) ------------------------------------\n",
      "Max loss: 0.041096072643995285\n",
      "Min loss: 0.013128290884196758\n",
      "Mean loss: 0.02375371629993121\n",
      "Std loss: 0.01158313279078143\n",
      "Total Loss: 0.14252229779958725\n",
      "------------------------------------ epoch 2459 (14748 steps) ------------------------------------\n",
      "Max loss: 0.019363146275281906\n",
      "Min loss: 0.011621588841080666\n",
      "Mean loss: 0.014388533929983774\n",
      "Std loss: 0.0024300018051634504\n",
      "Total Loss: 0.08633120357990265\n",
      "------------------------------------ epoch 2460 (14754 steps) ------------------------------------\n",
      "Max loss: 0.03437083959579468\n",
      "Min loss: 0.012468516826629639\n",
      "Mean loss: 0.019960487261414528\n",
      "Std loss: 0.007817923788872772\n",
      "Total Loss: 0.11976292356848717\n",
      "------------------------------------ epoch 2461 (14760 steps) ------------------------------------\n",
      "Max loss: 0.028799589723348618\n",
      "Min loss: 0.012591779232025146\n",
      "Mean loss: 0.016698431534071762\n",
      "Std loss: 0.005515612416604222\n",
      "Total Loss: 0.10019058920443058\n",
      "------------------------------------ epoch 2462 (14766 steps) ------------------------------------\n",
      "Max loss: 0.025695398449897766\n",
      "Min loss: 0.012763972394168377\n",
      "Mean loss: 0.016547308458636206\n",
      "Std loss: 0.004581563045948406\n",
      "Total Loss: 0.09928385075181723\n",
      "------------------------------------ epoch 2463 (14772 steps) ------------------------------------\n",
      "Max loss: 0.07428119331598282\n",
      "Min loss: 0.013728770427405834\n",
      "Mean loss: 0.025171057786792517\n",
      "Std loss: 0.022025638519897712\n",
      "Total Loss: 0.1510263467207551\n",
      "------------------------------------ epoch 2464 (14778 steps) ------------------------------------\n",
      "Max loss: 0.021220525726675987\n",
      "Min loss: 0.010908139869570732\n",
      "Mean loss: 0.013998340349644423\n",
      "Std loss: 0.0033525825002990494\n",
      "Total Loss: 0.08399004209786654\n",
      "------------------------------------ epoch 2465 (14784 steps) ------------------------------------\n",
      "Max loss: 0.036125946789979935\n",
      "Min loss: 0.01731644943356514\n",
      "Mean loss: 0.029126081926127274\n",
      "Std loss: 0.006559807955481335\n",
      "Total Loss: 0.17475649155676365\n",
      "------------------------------------ epoch 2466 (14790 steps) ------------------------------------\n",
      "Max loss: 0.036010559648275375\n",
      "Min loss: 0.010471578687429428\n",
      "Mean loss: 0.023242260484645765\n",
      "Std loss: 0.009435614644068658\n",
      "Total Loss: 0.13945356290787458\n",
      "------------------------------------ epoch 2467 (14796 steps) ------------------------------------\n",
      "Max loss: 0.031041963025927544\n",
      "Min loss: 0.013927144929766655\n",
      "Mean loss: 0.022515497480829556\n",
      "Std loss: 0.006394067185429736\n",
      "Total Loss: 0.13509298488497734\n",
      "------------------------------------ epoch 2468 (14802 steps) ------------------------------------\n",
      "Max loss: 0.04796697944402695\n",
      "Min loss: 0.010484350845217705\n",
      "Mean loss: 0.024482164221505325\n",
      "Std loss: 0.011655414415607654\n",
      "Total Loss: 0.14689298532903194\n",
      "------------------------------------ epoch 2469 (14808 steps) ------------------------------------\n",
      "Max loss: 0.03221263736486435\n",
      "Min loss: 0.010793900117278099\n",
      "Mean loss: 0.02153499952207009\n",
      "Std loss: 0.007007734238362749\n",
      "Total Loss: 0.12920999713242054\n",
      "------------------------------------ epoch 2470 (14814 steps) ------------------------------------\n",
      "Max loss: 0.031013747677206993\n",
      "Min loss: 0.013496831059455872\n",
      "Mean loss: 0.02038086795558532\n",
      "Std loss: 0.007443953374079398\n",
      "Total Loss: 0.12228520773351192\n",
      "------------------------------------ epoch 2471 (14820 steps) ------------------------------------\n",
      "Max loss: 0.02658727392554283\n",
      "Min loss: 0.012611761689186096\n",
      "Mean loss: 0.017913917545229197\n",
      "Std loss: 0.005300741874939619\n",
      "Total Loss: 0.10748350527137518\n",
      "------------------------------------ epoch 2472 (14826 steps) ------------------------------------\n",
      "Max loss: 0.0305081307888031\n",
      "Min loss: 0.010625678114593029\n",
      "Mean loss: 0.019060970439265173\n",
      "Std loss: 0.0074655544723624175\n",
      "Total Loss: 0.11436582263559103\n",
      "------------------------------------ epoch 2473 (14832 steps) ------------------------------------\n",
      "Max loss: 0.04788708686828613\n",
      "Min loss: 0.014445490203797817\n",
      "Mean loss: 0.022418742223332327\n",
      "Std loss: 0.011513527616257565\n",
      "Total Loss: 0.13451245333999395\n",
      "------------------------------------ epoch 2474 (14838 steps) ------------------------------------\n",
      "Max loss: 0.03976351022720337\n",
      "Min loss: 0.010223942808806896\n",
      "Mean loss: 0.019605614710599184\n",
      "Std loss: 0.010454065716928109\n",
      "Total Loss: 0.1176336882635951\n",
      "------------------------------------ epoch 2475 (14844 steps) ------------------------------------\n",
      "Max loss: 0.046555690467357635\n",
      "Min loss: 0.01090256404131651\n",
      "Mean loss: 0.02592240832746029\n",
      "Std loss: 0.013828400911708902\n",
      "Total Loss: 0.15553444996476173\n",
      "------------------------------------ epoch 2476 (14850 steps) ------------------------------------\n",
      "Max loss: 0.02549302577972412\n",
      "Min loss: 0.00943752657622099\n",
      "Mean loss: 0.015056701997915903\n",
      "Std loss: 0.0052866240106542355\n",
      "Total Loss: 0.09034021198749542\n",
      "------------------------------------ epoch 2477 (14856 steps) ------------------------------------\n",
      "Max loss: 0.024219756945967674\n",
      "Min loss: 0.008974564261734486\n",
      "Mean loss: 0.013927906441191832\n",
      "Std loss: 0.005260275782672135\n",
      "Total Loss: 0.083567438647151\n",
      "------------------------------------ epoch 2478 (14862 steps) ------------------------------------\n",
      "Max loss: 0.04636962339282036\n",
      "Min loss: 0.00897820945829153\n",
      "Mean loss: 0.02176359559719761\n",
      "Std loss: 0.014993030751652356\n",
      "Total Loss: 0.13058157358318567\n",
      "------------------------------------ epoch 2479 (14868 steps) ------------------------------------\n",
      "Max loss: 0.02349858544766903\n",
      "Min loss: 0.009059705771505833\n",
      "Mean loss: 0.014851959577451149\n",
      "Std loss: 0.005027890780624407\n",
      "Total Loss: 0.0891117574647069\n",
      "------------------------------------ epoch 2480 (14874 steps) ------------------------------------\n",
      "Max loss: 0.07469361275434494\n",
      "Min loss: 0.010456561110913754\n",
      "Mean loss: 0.02709318355967601\n",
      "Std loss: 0.022207614967844055\n",
      "Total Loss: 0.16255910135805607\n",
      "------------------------------------ epoch 2481 (14880 steps) ------------------------------------\n",
      "Max loss: 0.02584896981716156\n",
      "Min loss: 0.011032196693122387\n",
      "Mean loss: 0.01862578575188915\n",
      "Std loss: 0.00486144108825293\n",
      "Total Loss: 0.1117547145113349\n",
      "------------------------------------ epoch 2482 (14886 steps) ------------------------------------\n",
      "Max loss: 0.02280648611485958\n",
      "Min loss: 0.012978486716747284\n",
      "Mean loss: 0.016853883862495422\n",
      "Std loss: 0.003238802215055661\n",
      "Total Loss: 0.10112330317497253\n",
      "------------------------------------ epoch 2483 (14892 steps) ------------------------------------\n",
      "Max loss: 0.025715496391057968\n",
      "Min loss: 0.010916746221482754\n",
      "Mean loss: 0.01854461245238781\n",
      "Std loss: 0.0063953499899222594\n",
      "Total Loss: 0.11126767471432686\n",
      "------------------------------------ epoch 2484 (14898 steps) ------------------------------------\n",
      "Max loss: 0.037274450063705444\n",
      "Min loss: 0.008436501957476139\n",
      "Mean loss: 0.021132216478387516\n",
      "Std loss: 0.009934731281898637\n",
      "Total Loss: 0.1267932988703251\n",
      "------------------------------------ epoch 2485 (14904 steps) ------------------------------------\n",
      "Max loss: 0.033176131546497345\n",
      "Min loss: 0.011955596506595612\n",
      "Mean loss: 0.018879410345107317\n",
      "Std loss: 0.006753578641302601\n",
      "Total Loss: 0.1132764620706439\n",
      "------------------------------------ epoch 2486 (14910 steps) ------------------------------------\n",
      "Max loss: 0.029958032071590424\n",
      "Min loss: 0.017178885638713837\n",
      "Mean loss: 0.02163856414457162\n",
      "Std loss: 0.004474433456102242\n",
      "Total Loss: 0.12983138486742973\n",
      "------------------------------------ epoch 2487 (14916 steps) ------------------------------------\n",
      "Max loss: 0.04201504588127136\n",
      "Min loss: 0.009015767835080624\n",
      "Mean loss: 0.01896565578257044\n",
      "Std loss: 0.011283247997238443\n",
      "Total Loss: 0.11379393469542265\n",
      "------------------------------------ epoch 2488 (14922 steps) ------------------------------------\n",
      "Max loss: 0.030762460082769394\n",
      "Min loss: 0.00899803638458252\n",
      "Mean loss: 0.01866645614306132\n",
      "Std loss: 0.008602833910063806\n",
      "Total Loss: 0.11199873685836792\n",
      "------------------------------------ epoch 2489 (14928 steps) ------------------------------------\n",
      "Max loss: 0.017442923039197922\n",
      "Min loss: 0.01244429498910904\n",
      "Mean loss: 0.01551586901769042\n",
      "Std loss: 0.0017073722051008185\n",
      "Total Loss: 0.09309521410614252\n",
      "------------------------------------ epoch 2490 (14934 steps) ------------------------------------\n",
      "Max loss: 0.03620614483952522\n",
      "Min loss: 0.009274832904338837\n",
      "Mean loss: 0.019341819143543642\n",
      "Std loss: 0.01055276192309777\n",
      "Total Loss: 0.11605091486126184\n",
      "------------------------------------ epoch 2491 (14940 steps) ------------------------------------\n",
      "Max loss: 0.02113460749387741\n",
      "Min loss: 0.008658339269459248\n",
      "Mean loss: 0.01521917013451457\n",
      "Std loss: 0.004833782755100433\n",
      "Total Loss: 0.09131502080708742\n",
      "------------------------------------ epoch 2492 (14946 steps) ------------------------------------\n",
      "Max loss: 0.030589483678340912\n",
      "Min loss: 0.00840774830430746\n",
      "Mean loss: 0.015541538906594118\n",
      "Std loss: 0.007670568844177113\n",
      "Total Loss: 0.0932492334395647\n",
      "------------------------------------ epoch 2493 (14952 steps) ------------------------------------\n",
      "Max loss: 0.02868655137717724\n",
      "Min loss: 0.011168505996465683\n",
      "Mean loss: 0.01880565673733751\n",
      "Std loss: 0.006702994408765008\n",
      "Total Loss: 0.11283394042402506\n",
      "------------------------------------ epoch 2494 (14958 steps) ------------------------------------\n",
      "Max loss: 0.023071222007274628\n",
      "Min loss: 0.011152582243084908\n",
      "Mean loss: 0.016475085479517777\n",
      "Std loss: 0.004218124730324154\n",
      "Total Loss: 0.09885051287710667\n",
      "------------------------------------ epoch 2495 (14964 steps) ------------------------------------\n",
      "Max loss: 0.0530761294066906\n",
      "Min loss: 0.01186930201947689\n",
      "Mean loss: 0.027850400035579998\n",
      "Std loss: 0.014005132835669804\n",
      "Total Loss: 0.16710240021348\n",
      "------------------------------------ epoch 2496 (14970 steps) ------------------------------------\n",
      "Max loss: 0.03117467276751995\n",
      "Min loss: 0.010528302751481533\n",
      "Mean loss: 0.018755119293928146\n",
      "Std loss: 0.007099969082400552\n",
      "Total Loss: 0.11253071576356888\n",
      "------------------------------------ epoch 2497 (14976 steps) ------------------------------------\n",
      "Max loss: 0.027382194995880127\n",
      "Min loss: 0.010943882167339325\n",
      "Mean loss: 0.016849284525960684\n",
      "Std loss: 0.005497953158756736\n",
      "Total Loss: 0.1010957071557641\n",
      "------------------------------------ epoch 2498 (14982 steps) ------------------------------------\n",
      "Max loss: 0.019784098491072655\n",
      "Min loss: 0.009536730125546455\n",
      "Mean loss: 0.012699056727190813\n",
      "Std loss: 0.003360311059404839\n",
      "Total Loss: 0.07619434036314487\n",
      "------------------------------------ epoch 2499 (14988 steps) ------------------------------------\n",
      "Max loss: 0.021398676559329033\n",
      "Min loss: 0.008552759885787964\n",
      "Mean loss: 0.0135506020548443\n",
      "Std loss: 0.0045846395215279594\n",
      "Total Loss: 0.0813036123290658\n",
      "------------------------------------ epoch 2500 (14994 steps) ------------------------------------\n",
      "Max loss: 0.028906382620334625\n",
      "Min loss: 0.01219223253428936\n",
      "Mean loss: 0.02070102809617917\n",
      "Std loss: 0.00674320899224889\n",
      "Total Loss: 0.124206168577075\n",
      "------------------------------------ epoch 2501 (15000 steps) ------------------------------------\n",
      "Max loss: 0.028467100113630295\n",
      "Min loss: 0.009881727397441864\n",
      "Mean loss: 0.018680210535724957\n",
      "Std loss: 0.005788859981691441\n",
      "Total Loss: 0.11208126321434975\n",
      "saved model at ./weights/model_2501.pth\n",
      "------------------------------------ epoch 2502 (15006 steps) ------------------------------------\n",
      "Max loss: 0.033115219324827194\n",
      "Min loss: 0.010169529356062412\n",
      "Mean loss: 0.019001971775044996\n",
      "Std loss: 0.00748676240012876\n",
      "Total Loss: 0.11401183065026999\n",
      "------------------------------------ epoch 2503 (15012 steps) ------------------------------------\n",
      "Max loss: 0.04072127863764763\n",
      "Min loss: 0.010369137860834599\n",
      "Mean loss: 0.02599502634257078\n",
      "Std loss: 0.012409828723637847\n",
      "Total Loss: 0.1559701580554247\n",
      "------------------------------------ epoch 2504 (15018 steps) ------------------------------------\n",
      "Max loss: 0.04381250590085983\n",
      "Min loss: 0.013214325532317162\n",
      "Mean loss: 0.027503705583512783\n",
      "Std loss: 0.010945679514668887\n",
      "Total Loss: 0.1650222335010767\n",
      "------------------------------------ epoch 2505 (15024 steps) ------------------------------------\n",
      "Max loss: 0.04051828011870384\n",
      "Min loss: 0.013018177822232246\n",
      "Mean loss: 0.0227500240628918\n",
      "Std loss: 0.008475035082457042\n",
      "Total Loss: 0.1365001443773508\n",
      "------------------------------------ epoch 2506 (15030 steps) ------------------------------------\n",
      "Max loss: 0.039816614240407944\n",
      "Min loss: 0.009979769587516785\n",
      "Mean loss: 0.018854239334662754\n",
      "Std loss: 0.010139739087943482\n",
      "Total Loss: 0.11312543600797653\n",
      "------------------------------------ epoch 2507 (15036 steps) ------------------------------------\n",
      "Max loss: 0.03017534129321575\n",
      "Min loss: 0.012438254430890083\n",
      "Mean loss: 0.01937755321462949\n",
      "Std loss: 0.0055227369255848695\n",
      "Total Loss: 0.11626531928777695\n",
      "------------------------------------ epoch 2508 (15042 steps) ------------------------------------\n",
      "Max loss: 0.027152804657816887\n",
      "Min loss: 0.010634200647473335\n",
      "Mean loss: 0.018670028541237116\n",
      "Std loss: 0.005715416040165107\n",
      "Total Loss: 0.1120201712474227\n",
      "------------------------------------ epoch 2509 (15048 steps) ------------------------------------\n",
      "Max loss: 0.03234115242958069\n",
      "Min loss: 0.011508852243423462\n",
      "Mean loss: 0.018677334611614544\n",
      "Std loss: 0.007096607551905258\n",
      "Total Loss: 0.11206400766968727\n",
      "------------------------------------ epoch 2510 (15054 steps) ------------------------------------\n",
      "Max loss: 0.036383211612701416\n",
      "Min loss: 0.01329793967306614\n",
      "Mean loss: 0.019423882321765024\n",
      "Std loss: 0.008084479425247294\n",
      "Total Loss: 0.11654329393059015\n",
      "------------------------------------ epoch 2511 (15060 steps) ------------------------------------\n",
      "Max loss: 0.030957473441958427\n",
      "Min loss: 0.014354581944644451\n",
      "Mean loss: 0.01946139894425869\n",
      "Std loss: 0.005511007475916988\n",
      "Total Loss: 0.11676839366555214\n",
      "------------------------------------ epoch 2512 (15066 steps) ------------------------------------\n",
      "Max loss: 0.028175417333841324\n",
      "Min loss: 0.009383190423250198\n",
      "Mean loss: 0.017859380692243576\n",
      "Std loss: 0.00609063147669075\n",
      "Total Loss: 0.10715628415346146\n",
      "------------------------------------ epoch 2513 (15072 steps) ------------------------------------\n",
      "Max loss: 0.06572785973548889\n",
      "Min loss: 0.009592149406671524\n",
      "Mean loss: 0.022963098871211212\n",
      "Std loss: 0.02025884868347432\n",
      "Total Loss: 0.13777859322726727\n",
      "------------------------------------ epoch 2514 (15078 steps) ------------------------------------\n",
      "Max loss: 0.07300671190023422\n",
      "Min loss: 0.009302714839577675\n",
      "Mean loss: 0.030569777513543766\n",
      "Std loss: 0.02232630331658544\n",
      "Total Loss: 0.1834186650812626\n",
      "------------------------------------ epoch 2515 (15084 steps) ------------------------------------\n",
      "Max loss: 0.029352612793445587\n",
      "Min loss: 0.009613703936338425\n",
      "Mean loss: 0.01627433920900027\n",
      "Std loss: 0.006678406151613691\n",
      "Total Loss: 0.09764603525400162\n",
      "------------------------------------ epoch 2516 (15090 steps) ------------------------------------\n",
      "Max loss: 0.02241397462785244\n",
      "Min loss: 0.009096252731978893\n",
      "Mean loss: 0.015090054521958033\n",
      "Std loss: 0.005106739243709484\n",
      "Total Loss: 0.0905403271317482\n",
      "------------------------------------ epoch 2517 (15096 steps) ------------------------------------\n",
      "Max loss: 0.0336642824113369\n",
      "Min loss: 0.010310214944183826\n",
      "Mean loss: 0.019492365885525942\n",
      "Std loss: 0.007080507494189178\n",
      "Total Loss: 0.11695419531315565\n",
      "------------------------------------ epoch 2518 (15102 steps) ------------------------------------\n",
      "Max loss: 0.022706327959895134\n",
      "Min loss: 0.009874246083199978\n",
      "Mean loss: 0.015846732538193464\n",
      "Std loss: 0.005212597012129509\n",
      "Total Loss: 0.09508039522916079\n",
      "------------------------------------ epoch 2519 (15108 steps) ------------------------------------\n",
      "Max loss: 0.028828276321291924\n",
      "Min loss: 0.01346741896122694\n",
      "Mean loss: 0.020667742006480694\n",
      "Std loss: 0.006120757964629587\n",
      "Total Loss: 0.12400645203888416\n",
      "------------------------------------ epoch 2520 (15114 steps) ------------------------------------\n",
      "Max loss: 0.034781672060489655\n",
      "Min loss: 0.011685719713568687\n",
      "Mean loss: 0.016639290067056816\n",
      "Std loss: 0.008212634612511914\n",
      "Total Loss: 0.09983574040234089\n",
      "------------------------------------ epoch 2521 (15120 steps) ------------------------------------\n",
      "Max loss: 0.018443871289491653\n",
      "Min loss: 0.009454017505049706\n",
      "Mean loss: 0.014243274927139282\n",
      "Std loss: 0.0032817476592764488\n",
      "Total Loss: 0.0854596495628357\n",
      "------------------------------------ epoch 2522 (15126 steps) ------------------------------------\n",
      "Max loss: 0.030561327934265137\n",
      "Min loss: 0.009005802683532238\n",
      "Mean loss: 0.016589487126717966\n",
      "Std loss: 0.006848439655809367\n",
      "Total Loss: 0.09953692276030779\n",
      "------------------------------------ epoch 2523 (15132 steps) ------------------------------------\n",
      "Max loss: 0.02661939524114132\n",
      "Min loss: 0.007368092890828848\n",
      "Mean loss: 0.017773106461390853\n",
      "Std loss: 0.006397915311059217\n",
      "Total Loss: 0.10663863876834512\n",
      "------------------------------------ epoch 2524 (15138 steps) ------------------------------------\n",
      "Max loss: 0.04232240840792656\n",
      "Min loss: 0.010177469812333584\n",
      "Mean loss: 0.01911920340110858\n",
      "Std loss: 0.010639498972256179\n",
      "Total Loss: 0.1147152204066515\n",
      "------------------------------------ epoch 2525 (15144 steps) ------------------------------------\n",
      "Max loss: 0.028934290632605553\n",
      "Min loss: 0.00951006542891264\n",
      "Mean loss: 0.01670243125408888\n",
      "Std loss: 0.006450937734635029\n",
      "Total Loss: 0.10021458752453327\n",
      "------------------------------------ epoch 2526 (15150 steps) ------------------------------------\n",
      "Max loss: 0.047429751604795456\n",
      "Min loss: 0.007512617856264114\n",
      "Mean loss: 0.023146021955957014\n",
      "Std loss: 0.013350926286468636\n",
      "Total Loss: 0.1388761317357421\n",
      "------------------------------------ epoch 2527 (15156 steps) ------------------------------------\n",
      "Max loss: 0.029515452682971954\n",
      "Min loss: 0.008945528417825699\n",
      "Mean loss: 0.016935704896847408\n",
      "Std loss: 0.006385329154239063\n",
      "Total Loss: 0.10161422938108444\n",
      "------------------------------------ epoch 2528 (15162 steps) ------------------------------------\n",
      "Max loss: 0.04893175885081291\n",
      "Min loss: 0.011116357520222664\n",
      "Mean loss: 0.020505931073178846\n",
      "Std loss: 0.012841707951154333\n",
      "Total Loss: 0.12303558643907309\n",
      "------------------------------------ epoch 2529 (15168 steps) ------------------------------------\n",
      "Max loss: 0.021022941917181015\n",
      "Min loss: 0.008112858049571514\n",
      "Mean loss: 0.01128999392191569\n",
      "Std loss: 0.0043939936084209565\n",
      "Total Loss: 0.06773996353149414\n",
      "------------------------------------ epoch 2530 (15174 steps) ------------------------------------\n",
      "Max loss: 0.04464760422706604\n",
      "Min loss: 0.013115203008055687\n",
      "Mean loss: 0.030489507752160232\n",
      "Std loss: 0.011768879456865533\n",
      "Total Loss: 0.1829370465129614\n",
      "------------------------------------ epoch 2531 (15180 steps) ------------------------------------\n",
      "Max loss: 0.04112032428383827\n",
      "Min loss: 0.010862458497285843\n",
      "Mean loss: 0.022607179669042427\n",
      "Std loss: 0.010363249513587422\n",
      "Total Loss: 0.13564307801425457\n",
      "------------------------------------ epoch 2532 (15186 steps) ------------------------------------\n",
      "Max loss: 0.041483115404844284\n",
      "Min loss: 0.013283852487802505\n",
      "Mean loss: 0.023827050657322008\n",
      "Std loss: 0.011080097825530908\n",
      "Total Loss: 0.14296230394393206\n",
      "------------------------------------ epoch 2533 (15192 steps) ------------------------------------\n",
      "Max loss: 0.014764998108148575\n",
      "Min loss: 0.011274704709649086\n",
      "Mean loss: 0.013361185789108276\n",
      "Std loss: 0.0014292038897186078\n",
      "Total Loss: 0.08016711473464966\n",
      "------------------------------------ epoch 2534 (15198 steps) ------------------------------------\n",
      "Max loss: 0.03162257373332977\n",
      "Min loss: 0.009734800085425377\n",
      "Mean loss: 0.018102266515294712\n",
      "Std loss: 0.007915679058157963\n",
      "Total Loss: 0.10861359909176826\n",
      "------------------------------------ epoch 2535 (15204 steps) ------------------------------------\n",
      "Max loss: 0.03523453697562218\n",
      "Min loss: 0.00924140028655529\n",
      "Mean loss: 0.019120313227176666\n",
      "Std loss: 0.008327777648021877\n",
      "Total Loss: 0.11472187936306\n",
      "------------------------------------ epoch 2536 (15210 steps) ------------------------------------\n",
      "Max loss: 0.015973258763551712\n",
      "Min loss: 0.012604798190295696\n",
      "Mean loss: 0.014210403431206942\n",
      "Std loss: 0.0011313019449392316\n",
      "Total Loss: 0.08526242058724165\n",
      "------------------------------------ epoch 2537 (15216 steps) ------------------------------------\n",
      "Max loss: 0.024531636387109756\n",
      "Min loss: 0.013453936204314232\n",
      "Mean loss: 0.02075074954579274\n",
      "Std loss: 0.004316851688781798\n",
      "Total Loss: 0.12450449727475643\n",
      "------------------------------------ epoch 2538 (15222 steps) ------------------------------------\n",
      "Max loss: 0.026605088263750076\n",
      "Min loss: 0.0083788326010108\n",
      "Mean loss: 0.015537430377056202\n",
      "Std loss: 0.0071742796298826066\n",
      "Total Loss: 0.09322458226233721\n",
      "------------------------------------ epoch 2539 (15228 steps) ------------------------------------\n",
      "Max loss: 0.02768402174115181\n",
      "Min loss: 0.010075177066028118\n",
      "Mean loss: 0.01780270303909977\n",
      "Std loss: 0.005910443098817698\n",
      "Total Loss: 0.10681621823459864\n",
      "------------------------------------ epoch 2540 (15234 steps) ------------------------------------\n",
      "Max loss: 0.032736971974372864\n",
      "Min loss: 0.008761624805629253\n",
      "Mean loss: 0.017940260004252195\n",
      "Std loss: 0.009005242305880092\n",
      "Total Loss: 0.10764156002551317\n",
      "------------------------------------ epoch 2541 (15240 steps) ------------------------------------\n",
      "Max loss: 0.020626509562134743\n",
      "Min loss: 0.00961708277463913\n",
      "Mean loss: 0.014898769867916902\n",
      "Std loss: 0.004330470894577508\n",
      "Total Loss: 0.08939261920750141\n",
      "------------------------------------ epoch 2542 (15246 steps) ------------------------------------\n",
      "Max loss: 0.04250934347510338\n",
      "Min loss: 0.010490170679986477\n",
      "Mean loss: 0.02073015986631314\n",
      "Std loss: 0.010921235771168224\n",
      "Total Loss: 0.12438095919787884\n",
      "------------------------------------ epoch 2543 (15252 steps) ------------------------------------\n",
      "Max loss: 0.0495726577937603\n",
      "Min loss: 0.00812038779258728\n",
      "Mean loss: 0.017650595400482416\n",
      "Std loss: 0.014371211962881601\n",
      "Total Loss: 0.1059035724028945\n",
      "------------------------------------ epoch 2544 (15258 steps) ------------------------------------\n",
      "Max loss: 0.02049482986330986\n",
      "Min loss: 0.010479985736310482\n",
      "Mean loss: 0.014926708303391933\n",
      "Std loss: 0.0031226968651901516\n",
      "Total Loss: 0.0895602498203516\n",
      "------------------------------------ epoch 2545 (15264 steps) ------------------------------------\n",
      "Max loss: 0.027795150876045227\n",
      "Min loss: 0.009237537160515785\n",
      "Mean loss: 0.015126127284020185\n",
      "Std loss: 0.006050783198184082\n",
      "Total Loss: 0.09075676370412111\n",
      "------------------------------------ epoch 2546 (15270 steps) ------------------------------------\n",
      "Max loss: 0.031804442405700684\n",
      "Min loss: 0.013359997421503067\n",
      "Mean loss: 0.019791082789500553\n",
      "Std loss: 0.005889418140462045\n",
      "Total Loss: 0.11874649673700333\n",
      "------------------------------------ epoch 2547 (15276 steps) ------------------------------------\n",
      "Max loss: 0.021155130118131638\n",
      "Min loss: 0.00934823602437973\n",
      "Mean loss: 0.016292795383681852\n",
      "Std loss: 0.004943337305971597\n",
      "Total Loss: 0.09775677230209112\n",
      "------------------------------------ epoch 2548 (15282 steps) ------------------------------------\n",
      "Max loss: 0.028129447251558304\n",
      "Min loss: 0.00902978703379631\n",
      "Mean loss: 0.01668544967348377\n",
      "Std loss: 0.006074702573551848\n",
      "Total Loss: 0.10011269804090261\n",
      "------------------------------------ epoch 2549 (15288 steps) ------------------------------------\n",
      "Max loss: 0.028388377279043198\n",
      "Min loss: 0.00890690740197897\n",
      "Mean loss: 0.013970007809499899\n",
      "Std loss: 0.006849508686758677\n",
      "Total Loss: 0.0838200468569994\n",
      "------------------------------------ epoch 2550 (15294 steps) ------------------------------------\n",
      "Max loss: 0.028144806623458862\n",
      "Min loss: 0.008333547040820122\n",
      "Mean loss: 0.01740567913899819\n",
      "Std loss: 0.0064423693732278626\n",
      "Total Loss: 0.10443407483398914\n",
      "------------------------------------ epoch 2551 (15300 steps) ------------------------------------\n",
      "Max loss: 0.02124384045600891\n",
      "Min loss: 0.009304412640631199\n",
      "Mean loss: 0.015366680144021908\n",
      "Std loss: 0.0038779035216787473\n",
      "Total Loss: 0.09220008086413145\n",
      "------------------------------------ epoch 2552 (15306 steps) ------------------------------------\n",
      "Max loss: 0.05963272973895073\n",
      "Min loss: 0.009538854472339153\n",
      "Mean loss: 0.025939467828720808\n",
      "Std loss: 0.020860192563942465\n",
      "Total Loss: 0.15563680697232485\n",
      "------------------------------------ epoch 2553 (15312 steps) ------------------------------------\n",
      "Max loss: 0.049611225724220276\n",
      "Min loss: 0.018522508442401886\n",
      "Mean loss: 0.025462682358920574\n",
      "Std loss: 0.010951000552426836\n",
      "Total Loss: 0.15277609415352345\n",
      "------------------------------------ epoch 2554 (15318 steps) ------------------------------------\n",
      "Max loss: 0.046269163489341736\n",
      "Min loss: 0.016284652054309845\n",
      "Mean loss: 0.0264847365518411\n",
      "Std loss: 0.009801072310692455\n",
      "Total Loss: 0.1589084193110466\n",
      "------------------------------------ epoch 2555 (15324 steps) ------------------------------------\n",
      "Max loss: 0.0442831926047802\n",
      "Min loss: 0.015229734592139721\n",
      "Mean loss: 0.02427786759411295\n",
      "Std loss: 0.009514102740433626\n",
      "Total Loss: 0.14566720556467772\n",
      "------------------------------------ epoch 2556 (15330 steps) ------------------------------------\n",
      "Max loss: 0.032959792762994766\n",
      "Min loss: 0.015304545871913433\n",
      "Mean loss: 0.02351300868516167\n",
      "Std loss: 0.005137482428759577\n",
      "Total Loss: 0.14107805211097002\n",
      "------------------------------------ epoch 2557 (15336 steps) ------------------------------------\n",
      "Max loss: 0.02526484802365303\n",
      "Min loss: 0.01124130841344595\n",
      "Mean loss: 0.015592487063258886\n",
      "Std loss: 0.004551652491588518\n",
      "Total Loss: 0.09355492237955332\n",
      "------------------------------------ epoch 2558 (15342 steps) ------------------------------------\n",
      "Max loss: 0.03326744586229324\n",
      "Min loss: 0.01139691099524498\n",
      "Mean loss: 0.0208908428127567\n",
      "Std loss: 0.008056008635437862\n",
      "Total Loss: 0.12534505687654018\n",
      "------------------------------------ epoch 2559 (15348 steps) ------------------------------------\n",
      "Max loss: 0.08252920210361481\n",
      "Min loss: 0.012521639466285706\n",
      "Mean loss: 0.031750066205859184\n",
      "Std loss: 0.023227734276476255\n",
      "Total Loss: 0.1905003972351551\n",
      "------------------------------------ epoch 2560 (15354 steps) ------------------------------------\n",
      "Max loss: 0.08212803304195404\n",
      "Min loss: 0.013791469857096672\n",
      "Mean loss: 0.037851474868754544\n",
      "Std loss: 0.024114863126412724\n",
      "Total Loss: 0.22710884921252728\n",
      "------------------------------------ epoch 2561 (15360 steps) ------------------------------------\n",
      "Max loss: 0.03343652933835983\n",
      "Min loss: 0.012956826016306877\n",
      "Mean loss: 0.02213979170968135\n",
      "Std loss: 0.008336993991415869\n",
      "Total Loss: 0.1328387502580881\n",
      "------------------------------------ epoch 2562 (15366 steps) ------------------------------------\n",
      "Max loss: 0.06012088805437088\n",
      "Min loss: 0.01073550432920456\n",
      "Mean loss: 0.025904968846589327\n",
      "Std loss: 0.018407685415757078\n",
      "Total Loss: 0.15542981307953596\n",
      "------------------------------------ epoch 2563 (15372 steps) ------------------------------------\n",
      "Max loss: 0.030226659029722214\n",
      "Min loss: 0.01193314790725708\n",
      "Mean loss: 0.01678882384051879\n",
      "Std loss: 0.006182070579551231\n",
      "Total Loss: 0.10073294304311275\n",
      "------------------------------------ epoch 2564 (15378 steps) ------------------------------------\n",
      "Max loss: 0.03455604985356331\n",
      "Min loss: 0.0104925986379385\n",
      "Mean loss: 0.019491990096867085\n",
      "Std loss: 0.008489485286188364\n",
      "Total Loss: 0.11695194058120251\n",
      "------------------------------------ epoch 2565 (15384 steps) ------------------------------------\n",
      "Max loss: 0.044124215841293335\n",
      "Min loss: 0.011220223270356655\n",
      "Mean loss: 0.022042124221722286\n",
      "Std loss: 0.013640121643567682\n",
      "Total Loss: 0.1322527453303337\n",
      "------------------------------------ epoch 2566 (15390 steps) ------------------------------------\n",
      "Max loss: 0.034548863768577576\n",
      "Min loss: 0.011412429623305798\n",
      "Mean loss: 0.01859880021462838\n",
      "Std loss: 0.00743980963079975\n",
      "Total Loss: 0.11159280128777027\n",
      "------------------------------------ epoch 2567 (15396 steps) ------------------------------------\n",
      "Max loss: 0.03876881301403046\n",
      "Min loss: 0.014746482484042645\n",
      "Mean loss: 0.024296130519360304\n",
      "Std loss: 0.008314298093925209\n",
      "Total Loss: 0.14577678311616182\n",
      "------------------------------------ epoch 2568 (15402 steps) ------------------------------------\n",
      "Max loss: 0.028281673789024353\n",
      "Min loss: 0.009489547461271286\n",
      "Mean loss: 0.016034767186890047\n",
      "Std loss: 0.006306768299881745\n",
      "Total Loss: 0.09620860312134027\n",
      "------------------------------------ epoch 2569 (15408 steps) ------------------------------------\n",
      "Max loss: 0.04275381565093994\n",
      "Min loss: 0.00914578977972269\n",
      "Mean loss: 0.021770384007443983\n",
      "Std loss: 0.012865720987549758\n",
      "Total Loss: 0.1306223040446639\n",
      "------------------------------------ epoch 2570 (15414 steps) ------------------------------------\n",
      "Max loss: 0.02583683654665947\n",
      "Min loss: 0.012071486562490463\n",
      "Mean loss: 0.017684770592798788\n",
      "Std loss: 0.004273582633515738\n",
      "Total Loss: 0.10610862355679274\n",
      "------------------------------------ epoch 2571 (15420 steps) ------------------------------------\n",
      "Max loss: 0.05118144303560257\n",
      "Min loss: 0.01363325398415327\n",
      "Mean loss: 0.022323940725376207\n",
      "Std loss: 0.013269759811472822\n",
      "Total Loss: 0.13394364435225725\n",
      "------------------------------------ epoch 2572 (15426 steps) ------------------------------------\n",
      "Max loss: 0.028966089710593224\n",
      "Min loss: 0.012334081344306469\n",
      "Mean loss: 0.017019738753636677\n",
      "Std loss: 0.005805153726373359\n",
      "Total Loss: 0.10211843252182007\n",
      "------------------------------------ epoch 2573 (15432 steps) ------------------------------------\n",
      "Max loss: 0.048808567225933075\n",
      "Min loss: 0.010914234444499016\n",
      "Mean loss: 0.026334851359327633\n",
      "Std loss: 0.014740656810261953\n",
      "Total Loss: 0.1580091081559658\n",
      "------------------------------------ epoch 2574 (15438 steps) ------------------------------------\n",
      "Max loss: 0.026353567838668823\n",
      "Min loss: 0.011828633025288582\n",
      "Mean loss: 0.01971514554073413\n",
      "Std loss: 0.004972138316886348\n",
      "Total Loss: 0.11829087324440479\n",
      "------------------------------------ epoch 2575 (15444 steps) ------------------------------------\n",
      "Max loss: 0.04213207960128784\n",
      "Min loss: 0.01602918654680252\n",
      "Mean loss: 0.02324420182655255\n",
      "Std loss: 0.008598508328676554\n",
      "Total Loss: 0.1394652109593153\n",
      "------------------------------------ epoch 2576 (15450 steps) ------------------------------------\n",
      "Max loss: 0.024591226130723953\n",
      "Min loss: 0.01271004043519497\n",
      "Mean loss: 0.01832513635357221\n",
      "Std loss: 0.0053486807060500845\n",
      "Total Loss: 0.10995081812143326\n",
      "------------------------------------ epoch 2577 (15456 steps) ------------------------------------\n",
      "Max loss: 0.02804173156619072\n",
      "Min loss: 0.009644984267652035\n",
      "Mean loss: 0.01791593510036667\n",
      "Std loss: 0.005967666131645426\n",
      "Total Loss: 0.10749561060220003\n",
      "------------------------------------ epoch 2578 (15462 steps) ------------------------------------\n",
      "Max loss: 0.030302174389362335\n",
      "Min loss: 0.01014243345707655\n",
      "Mean loss: 0.017256049904972315\n",
      "Std loss: 0.006906758029510818\n",
      "Total Loss: 0.10353629942983389\n",
      "------------------------------------ epoch 2579 (15468 steps) ------------------------------------\n",
      "Max loss: 0.030781319364905357\n",
      "Min loss: 0.00903107225894928\n",
      "Mean loss: 0.017430861480534077\n",
      "Std loss: 0.007499160793989622\n",
      "Total Loss: 0.10458516888320446\n",
      "------------------------------------ epoch 2580 (15474 steps) ------------------------------------\n",
      "Max loss: 0.029490267857909203\n",
      "Min loss: 0.008812230080366135\n",
      "Mean loss: 0.019234477852781613\n",
      "Std loss: 0.007798987433471144\n",
      "Total Loss: 0.11540686711668968\n",
      "------------------------------------ epoch 2581 (15480 steps) ------------------------------------\n",
      "Max loss: 0.03303840011358261\n",
      "Min loss: 0.010010365396738052\n",
      "Mean loss: 0.017162224277853966\n",
      "Std loss: 0.008324865934707056\n",
      "Total Loss: 0.1029733456671238\n",
      "------------------------------------ epoch 2582 (15486 steps) ------------------------------------\n",
      "Max loss: 0.07591605186462402\n",
      "Min loss: 0.008960546925663948\n",
      "Mean loss: 0.02072646205003063\n",
      "Std loss: 0.024685422970515868\n",
      "Total Loss: 0.12435877230018377\n",
      "------------------------------------ epoch 2583 (15492 steps) ------------------------------------\n",
      "Max loss: 0.04009519889950752\n",
      "Min loss: 0.00953410193324089\n",
      "Mean loss: 0.028377979372938473\n",
      "Std loss: 0.01065590182265587\n",
      "Total Loss: 0.17026787623763084\n",
      "------------------------------------ epoch 2584 (15498 steps) ------------------------------------\n",
      "Max loss: 0.05056733638048172\n",
      "Min loss: 0.010837125591933727\n",
      "Mean loss: 0.02316323636720578\n",
      "Std loss: 0.014207002082765425\n",
      "Total Loss: 0.13897941820323467\n",
      "------------------------------------ epoch 2585 (15504 steps) ------------------------------------\n",
      "Max loss: 0.030551841482520103\n",
      "Min loss: 0.008251704275608063\n",
      "Mean loss: 0.01879621483385563\n",
      "Std loss: 0.008008835841247416\n",
      "Total Loss: 0.11277728900313377\n",
      "------------------------------------ epoch 2586 (15510 steps) ------------------------------------\n",
      "Max loss: 0.03418225795030594\n",
      "Min loss: 0.008542660623788834\n",
      "Mean loss: 0.023022210225462914\n",
      "Std loss: 0.00898904849127104\n",
      "Total Loss: 0.13813326135277748\n",
      "------------------------------------ epoch 2587 (15516 steps) ------------------------------------\n",
      "Max loss: 0.04999880492687225\n",
      "Min loss: 0.008461608551442623\n",
      "Mean loss: 0.017668167129158974\n",
      "Std loss: 0.014601365935692168\n",
      "Total Loss: 0.10600900277495384\n",
      "------------------------------------ epoch 2588 (15522 steps) ------------------------------------\n",
      "Max loss: 0.05907697230577469\n",
      "Min loss: 0.012355159036815166\n",
      "Mean loss: 0.023803294791529577\n",
      "Std loss: 0.016590028066517503\n",
      "Total Loss: 0.14281976874917746\n",
      "------------------------------------ epoch 2589 (15528 steps) ------------------------------------\n",
      "Max loss: 0.038164135068655014\n",
      "Min loss: 0.014345469884574413\n",
      "Mean loss: 0.02138244779780507\n",
      "Std loss: 0.00774888483484073\n",
      "Total Loss: 0.12829468678683043\n",
      "------------------------------------ epoch 2590 (15534 steps) ------------------------------------\n",
      "Max loss: 0.029517538845539093\n",
      "Min loss: 0.01127653382718563\n",
      "Mean loss: 0.015432343042145172\n",
      "Std loss: 0.006421383642998974\n",
      "Total Loss: 0.09259405825287104\n",
      "------------------------------------ epoch 2591 (15540 steps) ------------------------------------\n",
      "Max loss: 0.02827737294137478\n",
      "Min loss: 0.009237339720129967\n",
      "Mean loss: 0.017302308852473896\n",
      "Std loss: 0.0071630657984456895\n",
      "Total Loss: 0.10381385311484337\n",
      "------------------------------------ epoch 2592 (15546 steps) ------------------------------------\n",
      "Max loss: 0.025881042703986168\n",
      "Min loss: 0.008177954703569412\n",
      "Mean loss: 0.016705559566617012\n",
      "Std loss: 0.0056626723932095635\n",
      "Total Loss: 0.10023335739970207\n",
      "------------------------------------ epoch 2593 (15552 steps) ------------------------------------\n",
      "Max loss: 0.05806741863489151\n",
      "Min loss: 0.009164255112409592\n",
      "Mean loss: 0.024142945806185406\n",
      "Std loss: 0.01673414701529256\n",
      "Total Loss: 0.14485767483711243\n",
      "------------------------------------ epoch 2594 (15558 steps) ------------------------------------\n",
      "Max loss: 0.037494756281375885\n",
      "Min loss: 0.009764958173036575\n",
      "Mean loss: 0.02414709609001875\n",
      "Std loss: 0.012086074596738286\n",
      "Total Loss: 0.1448825765401125\n",
      "------------------------------------ epoch 2595 (15564 steps) ------------------------------------\n",
      "Max loss: 0.035807471722364426\n",
      "Min loss: 0.012478109449148178\n",
      "Mean loss: 0.01904527594645818\n",
      "Std loss: 0.008281283698196874\n",
      "Total Loss: 0.11427165567874908\n",
      "------------------------------------ epoch 2596 (15570 steps) ------------------------------------\n",
      "Max loss: 0.02252875082194805\n",
      "Min loss: 0.009923586621880531\n",
      "Mean loss: 0.016845637001097202\n",
      "Std loss: 0.005101053345670553\n",
      "Total Loss: 0.10107382200658321\n",
      "------------------------------------ epoch 2597 (15576 steps) ------------------------------------\n",
      "Max loss: 0.04713597521185875\n",
      "Min loss: 0.010061978362500668\n",
      "Mean loss: 0.01824843417853117\n",
      "Std loss: 0.013018539820586202\n",
      "Total Loss: 0.10949060507118702\n",
      "------------------------------------ epoch 2598 (15582 steps) ------------------------------------\n",
      "Max loss: 0.06076984852552414\n",
      "Min loss: 0.009635582566261292\n",
      "Mean loss: 0.025290715197722118\n",
      "Std loss: 0.017238188482870405\n",
      "Total Loss: 0.1517442911863327\n",
      "------------------------------------ epoch 2599 (15588 steps) ------------------------------------\n",
      "Max loss: 0.0510115846991539\n",
      "Min loss: 0.014345677569508553\n",
      "Mean loss: 0.029760995879769325\n",
      "Std loss: 0.012337646840479041\n",
      "Total Loss: 0.17856597527861595\n",
      "------------------------------------ epoch 2600 (15594 steps) ------------------------------------\n",
      "Max loss: 0.032553404569625854\n",
      "Min loss: 0.013614965602755547\n",
      "Mean loss: 0.01999273942783475\n",
      "Std loss: 0.006754339785646279\n",
      "Total Loss: 0.1199564365670085\n",
      "------------------------------------ epoch 2601 (15600 steps) ------------------------------------\n",
      "Max loss: 0.0384172648191452\n",
      "Min loss: 0.008852889761328697\n",
      "Mean loss: 0.01832053919012348\n",
      "Std loss: 0.009783317127623065\n",
      "Total Loss: 0.10992323514074087\n",
      "saved model at ./weights/model_2601.pth\n",
      "------------------------------------ epoch 2602 (15606 steps) ------------------------------------\n",
      "Max loss: 0.028045672923326492\n",
      "Min loss: 0.010575748980045319\n",
      "Mean loss: 0.016080969168494146\n",
      "Std loss: 0.0061268646686089435\n",
      "Total Loss: 0.09648581501096487\n",
      "------------------------------------ epoch 2603 (15612 steps) ------------------------------------\n",
      "Max loss: 0.02685355767607689\n",
      "Min loss: 0.009540921077132225\n",
      "Mean loss: 0.015731678654750187\n",
      "Std loss: 0.006405304426274426\n",
      "Total Loss: 0.09439007192850113\n",
      "------------------------------------ epoch 2604 (15618 steps) ------------------------------------\n",
      "Max loss: 0.020126283168792725\n",
      "Min loss: 0.008555608801543713\n",
      "Mean loss: 0.012318808740625778\n",
      "Std loss: 0.0037677377588427696\n",
      "Total Loss: 0.07391285244375467\n",
      "------------------------------------ epoch 2605 (15624 steps) ------------------------------------\n",
      "Max loss: 0.022239714860916138\n",
      "Min loss: 0.007199852727353573\n",
      "Mean loss: 0.015354170929640532\n",
      "Std loss: 0.00476855697510964\n",
      "Total Loss: 0.09212502557784319\n",
      "------------------------------------ epoch 2606 (15630 steps) ------------------------------------\n",
      "Max loss: 0.040722835808992386\n",
      "Min loss: 0.011570541188120842\n",
      "Mean loss: 0.021223523809264105\n",
      "Std loss: 0.010114467784416001\n",
      "Total Loss: 0.12734114285558462\n",
      "------------------------------------ epoch 2607 (15636 steps) ------------------------------------\n",
      "Max loss: 0.02222215197980404\n",
      "Min loss: 0.008376285433769226\n",
      "Mean loss: 0.012881307397037745\n",
      "Std loss: 0.00441713821640724\n",
      "Total Loss: 0.07728784438222647\n",
      "------------------------------------ epoch 2608 (15642 steps) ------------------------------------\n",
      "Max loss: 0.027415355667471886\n",
      "Min loss: 0.008829431608319283\n",
      "Mean loss: 0.015285651509960493\n",
      "Std loss: 0.006299156569138322\n",
      "Total Loss: 0.09171390905976295\n",
      "------------------------------------ epoch 2609 (15648 steps) ------------------------------------\n",
      "Max loss: 0.037217073142528534\n",
      "Min loss: 0.011716111563146114\n",
      "Mean loss: 0.01924235327169299\n",
      "Std loss: 0.008636911283715194\n",
      "Total Loss: 0.11545411963015795\n",
      "------------------------------------ epoch 2610 (15654 steps) ------------------------------------\n",
      "Max loss: 0.0239351075142622\n",
      "Min loss: 0.008271854370832443\n",
      "Mean loss: 0.01442936280121406\n",
      "Std loss: 0.005837767453867595\n",
      "Total Loss: 0.08657617680728436\n",
      "------------------------------------ epoch 2611 (15660 steps) ------------------------------------\n",
      "Max loss: 0.01893075555562973\n",
      "Min loss: 0.008475679904222488\n",
      "Mean loss: 0.012452323300143084\n",
      "Std loss: 0.0032471047708875853\n",
      "Total Loss: 0.0747139398008585\n",
      "------------------------------------ epoch 2612 (15666 steps) ------------------------------------\n",
      "Max loss: 0.020461812615394592\n",
      "Min loss: 0.007570724934339523\n",
      "Mean loss: 0.012884721004714569\n",
      "Std loss: 0.004383752547450978\n",
      "Total Loss: 0.07730832602828741\n",
      "------------------------------------ epoch 2613 (15672 steps) ------------------------------------\n",
      "Max loss: 0.049914758652448654\n",
      "Min loss: 0.012323621660470963\n",
      "Mean loss: 0.02775604526201884\n",
      "Std loss: 0.014607322949915524\n",
      "Total Loss: 0.16653627157211304\n",
      "------------------------------------ epoch 2614 (15678 steps) ------------------------------------\n",
      "Max loss: 0.060956209897994995\n",
      "Min loss: 0.011920559220016003\n",
      "Mean loss: 0.02593004470691085\n",
      "Std loss: 0.016671565413516998\n",
      "Total Loss: 0.1555802682414651\n",
      "------------------------------------ epoch 2615 (15684 steps) ------------------------------------\n",
      "Max loss: 0.030091028660535812\n",
      "Min loss: 0.011292196810245514\n",
      "Mean loss: 0.018956632042924564\n",
      "Std loss: 0.007082599798937543\n",
      "Total Loss: 0.11373979225754738\n",
      "------------------------------------ epoch 2616 (15690 steps) ------------------------------------\n",
      "Max loss: 0.055108048021793365\n",
      "Min loss: 0.009852397255599499\n",
      "Mean loss: 0.021917482527593773\n",
      "Std loss: 0.015342978247394124\n",
      "Total Loss: 0.13150489516556263\n",
      "------------------------------------ epoch 2617 (15696 steps) ------------------------------------\n",
      "Max loss: 0.047976016998291016\n",
      "Min loss: 0.011020936071872711\n",
      "Mean loss: 0.02078272495418787\n",
      "Std loss: 0.012376899531745911\n",
      "Total Loss: 0.12469634972512722\n",
      "------------------------------------ epoch 2618 (15702 steps) ------------------------------------\n",
      "Max loss: 0.0360846146941185\n",
      "Min loss: 0.01031692884862423\n",
      "Mean loss: 0.018018672242760658\n",
      "Std loss: 0.008556670892354548\n",
      "Total Loss: 0.10811203345656395\n",
      "------------------------------------ epoch 2619 (15708 steps) ------------------------------------\n",
      "Max loss: 0.028248025104403496\n",
      "Min loss: 0.009697402827441692\n",
      "Mean loss: 0.014569856071223816\n",
      "Std loss: 0.0063450800316280686\n",
      "Total Loss: 0.08741913642734289\n",
      "------------------------------------ epoch 2620 (15714 steps) ------------------------------------\n",
      "Max loss: 0.03461544215679169\n",
      "Min loss: 0.011417175643146038\n",
      "Mean loss: 0.021520031460871298\n",
      "Std loss: 0.007471074511768361\n",
      "Total Loss: 0.1291201887652278\n",
      "------------------------------------ epoch 2621 (15720 steps) ------------------------------------\n",
      "Max loss: 0.03341317176818848\n",
      "Min loss: 0.00960835162550211\n",
      "Mean loss: 0.018033086167027552\n",
      "Std loss: 0.00809454793980272\n",
      "Total Loss: 0.10819851700216532\n",
      "------------------------------------ epoch 2622 (15726 steps) ------------------------------------\n",
      "Max loss: 0.0315665639936924\n",
      "Min loss: 0.008348540402948856\n",
      "Mean loss: 0.01634044013917446\n",
      "Std loss: 0.008164672692499785\n",
      "Total Loss: 0.09804264083504677\n",
      "------------------------------------ epoch 2623 (15732 steps) ------------------------------------\n",
      "Max loss: 0.031503863632678986\n",
      "Min loss: 0.016851600259542465\n",
      "Mean loss: 0.02491644738862912\n",
      "Std loss: 0.004642418116085137\n",
      "Total Loss: 0.1494986843317747\n",
      "------------------------------------ epoch 2624 (15738 steps) ------------------------------------\n",
      "Max loss: 0.01639179140329361\n",
      "Min loss: 0.01161212008446455\n",
      "Mean loss: 0.014333431608974934\n",
      "Std loss: 0.0018714873484167245\n",
      "Total Loss: 0.0860005896538496\n",
      "------------------------------------ epoch 2625 (15744 steps) ------------------------------------\n",
      "Max loss: 0.025106824934482574\n",
      "Min loss: 0.0078077856451272964\n",
      "Mean loss: 0.013974763608227173\n",
      "Std loss: 0.00556587922498719\n",
      "Total Loss: 0.08384858164936304\n",
      "------------------------------------ epoch 2626 (15750 steps) ------------------------------------\n",
      "Max loss: 0.036382611840963364\n",
      "Min loss: 0.009456628002226353\n",
      "Mean loss: 0.02007752051576972\n",
      "Std loss: 0.008805915818863143\n",
      "Total Loss: 0.12046512309461832\n",
      "------------------------------------ epoch 2627 (15756 steps) ------------------------------------\n",
      "Max loss: 0.04773169383406639\n",
      "Min loss: 0.010571996681392193\n",
      "Mean loss: 0.026339808013290167\n",
      "Std loss: 0.012764414675544913\n",
      "Total Loss: 0.158038848079741\n",
      "------------------------------------ epoch 2628 (15762 steps) ------------------------------------\n",
      "Max loss: 0.032378967851400375\n",
      "Min loss: 0.008379672653973103\n",
      "Mean loss: 0.017047901327411335\n",
      "Std loss: 0.0074511495631628104\n",
      "Total Loss: 0.102287407964468\n",
      "------------------------------------ epoch 2629 (15768 steps) ------------------------------------\n",
      "Max loss: 0.02079516276717186\n",
      "Min loss: 0.008542822673916817\n",
      "Mean loss: 0.014002146509786447\n",
      "Std loss: 0.004703088869631589\n",
      "Total Loss: 0.08401287905871868\n",
      "------------------------------------ epoch 2630 (15774 steps) ------------------------------------\n",
      "Max loss: 0.030646836385130882\n",
      "Min loss: 0.009064629673957825\n",
      "Mean loss: 0.015554080562045177\n",
      "Std loss: 0.007236132556742538\n",
      "Total Loss: 0.09332448337227106\n",
      "------------------------------------ epoch 2631 (15780 steps) ------------------------------------\n",
      "Max loss: 0.03555959463119507\n",
      "Min loss: 0.010513163171708584\n",
      "Mean loss: 0.023198731398830812\n",
      "Std loss: 0.00876125518144071\n",
      "Total Loss: 0.13919238839298487\n",
      "------------------------------------ epoch 2632 (15786 steps) ------------------------------------\n",
      "Max loss: 0.018488291651010513\n",
      "Min loss: 0.008325794711709023\n",
      "Mean loss: 0.013743229831258455\n",
      "Std loss: 0.003506199435033519\n",
      "Total Loss: 0.08245937898755074\n",
      "------------------------------------ epoch 2633 (15792 steps) ------------------------------------\n",
      "Max loss: 0.03236045688390732\n",
      "Min loss: 0.009220510721206665\n",
      "Mean loss: 0.015774602070450783\n",
      "Std loss: 0.007704353541234245\n",
      "Total Loss: 0.0946476124227047\n",
      "------------------------------------ epoch 2634 (15798 steps) ------------------------------------\n",
      "Max loss: 0.017985407263040543\n",
      "Min loss: 0.0073169199749827385\n",
      "Mean loss: 0.011410018894821405\n",
      "Std loss: 0.0034553506557219466\n",
      "Total Loss: 0.06846011336892843\n",
      "------------------------------------ epoch 2635 (15804 steps) ------------------------------------\n",
      "Max loss: 0.0170675590634346\n",
      "Min loss: 0.00940386950969696\n",
      "Mean loss: 0.012500826269388199\n",
      "Std loss: 0.0026588479004813837\n",
      "Total Loss: 0.0750049576163292\n",
      "------------------------------------ epoch 2636 (15810 steps) ------------------------------------\n",
      "Max loss: 0.02264450304210186\n",
      "Min loss: 0.009063426405191422\n",
      "Mean loss: 0.014912243156383434\n",
      "Std loss: 0.004501305729418571\n",
      "Total Loss: 0.08947345893830061\n",
      "------------------------------------ epoch 2637 (15816 steps) ------------------------------------\n",
      "Max loss: 0.029472291469573975\n",
      "Min loss: 0.008862893097102642\n",
      "Mean loss: 0.017961479878673952\n",
      "Std loss: 0.007634868248596665\n",
      "Total Loss: 0.1077688792720437\n",
      "------------------------------------ epoch 2638 (15822 steps) ------------------------------------\n",
      "Max loss: 0.0178881473839283\n",
      "Min loss: 0.010166961699724197\n",
      "Mean loss: 0.013279670539001623\n",
      "Std loss: 0.002875626837402499\n",
      "Total Loss: 0.07967802323400974\n",
      "------------------------------------ epoch 2639 (15828 steps) ------------------------------------\n",
      "Max loss: 0.026080770418047905\n",
      "Min loss: 0.009801225736737251\n",
      "Mean loss: 0.016747720850010712\n",
      "Std loss: 0.0054859266792578135\n",
      "Total Loss: 0.10048632510006428\n",
      "------------------------------------ epoch 2640 (15834 steps) ------------------------------------\n",
      "Max loss: 0.011815264821052551\n",
      "Min loss: 0.0077070435509085655\n",
      "Mean loss: 0.010564369149506092\n",
      "Std loss: 0.0013344913391547914\n",
      "Total Loss: 0.06338621489703655\n",
      "------------------------------------ epoch 2641 (15840 steps) ------------------------------------\n",
      "Max loss: 0.03329302370548248\n",
      "Min loss: 0.008863747119903564\n",
      "Mean loss: 0.017108090532322724\n",
      "Std loss: 0.00942937456699247\n",
      "Total Loss: 0.10264854319393635\n",
      "------------------------------------ epoch 2642 (15846 steps) ------------------------------------\n",
      "Max loss: 0.04313476383686066\n",
      "Min loss: 0.009388381615281105\n",
      "Mean loss: 0.015602638789763054\n",
      "Std loss: 0.012329571138819686\n",
      "Total Loss: 0.09361583273857832\n",
      "------------------------------------ epoch 2643 (15852 steps) ------------------------------------\n",
      "Max loss: 0.01894206553697586\n",
      "Min loss: 0.007061218377202749\n",
      "Mean loss: 0.011099347146227956\n",
      "Std loss: 0.00386942265729685\n",
      "Total Loss: 0.06659608287736773\n",
      "------------------------------------ epoch 2644 (15858 steps) ------------------------------------\n",
      "Max loss: 0.07246746122837067\n",
      "Min loss: 0.00721431989222765\n",
      "Mean loss: 0.025158598863830168\n",
      "Std loss: 0.02342976240406708\n",
      "Total Loss: 0.15095159318298101\n",
      "------------------------------------ epoch 2645 (15864 steps) ------------------------------------\n",
      "Max loss: 0.04591143876314163\n",
      "Min loss: 0.012673540972173214\n",
      "Mean loss: 0.02184074077134331\n",
      "Std loss: 0.0122536328302557\n",
      "Total Loss: 0.13104444462805986\n",
      "------------------------------------ epoch 2646 (15870 steps) ------------------------------------\n",
      "Max loss: 0.04432255029678345\n",
      "Min loss: 0.012126822024583817\n",
      "Mean loss: 0.02291373411814372\n",
      "Std loss: 0.011072529117651574\n",
      "Total Loss: 0.1374824047088623\n",
      "------------------------------------ epoch 2647 (15876 steps) ------------------------------------\n",
      "Max loss: 0.021162336692214012\n",
      "Min loss: 0.009220212697982788\n",
      "Mean loss: 0.013198247179389\n",
      "Std loss: 0.004286788952519117\n",
      "Total Loss: 0.079189483076334\n",
      "------------------------------------ epoch 2648 (15882 steps) ------------------------------------\n",
      "Max loss: 0.036241814494132996\n",
      "Min loss: 0.008882849477231503\n",
      "Mean loss: 0.015908955441166956\n",
      "Std loss: 0.009347996939459667\n",
      "Total Loss: 0.09545373264700174\n",
      "------------------------------------ epoch 2649 (15888 steps) ------------------------------------\n",
      "Max loss: 0.028044050559401512\n",
      "Min loss: 0.009602535516023636\n",
      "Mean loss: 0.01742825408776601\n",
      "Std loss: 0.006036839745446104\n",
      "Total Loss: 0.10456952452659607\n",
      "------------------------------------ epoch 2650 (15894 steps) ------------------------------------\n",
      "Max loss: 0.028822358697652817\n",
      "Min loss: 0.010541649535298347\n",
      "Mean loss: 0.018618410763641197\n",
      "Std loss: 0.007334680429042043\n",
      "Total Loss: 0.11171046458184719\n",
      "------------------------------------ epoch 2651 (15900 steps) ------------------------------------\n",
      "Max loss: 0.03708381950855255\n",
      "Min loss: 0.008952965959906578\n",
      "Mean loss: 0.01680299298216899\n",
      "Std loss: 0.009452981145917395\n",
      "Total Loss: 0.10081795789301395\n",
      "------------------------------------ epoch 2652 (15906 steps) ------------------------------------\n",
      "Max loss: 0.03140499070286751\n",
      "Min loss: 0.008328866213560104\n",
      "Mean loss: 0.018892730741451185\n",
      "Std loss: 0.008228816969452442\n",
      "Total Loss: 0.1133563844487071\n",
      "------------------------------------ epoch 2653 (15912 steps) ------------------------------------\n",
      "Max loss: 0.06686586141586304\n",
      "Min loss: 0.00785921048372984\n",
      "Mean loss: 0.023717093591888744\n",
      "Std loss: 0.02073318773144067\n",
      "Total Loss: 0.14230256155133247\n",
      "------------------------------------ epoch 2654 (15918 steps) ------------------------------------\n",
      "Max loss: 0.023862406611442566\n",
      "Min loss: 0.009017311036586761\n",
      "Mean loss: 0.013539546635001898\n",
      "Std loss: 0.005100130521641752\n",
      "Total Loss: 0.08123727981001139\n",
      "------------------------------------ epoch 2655 (15924 steps) ------------------------------------\n",
      "Max loss: 0.018281977623701096\n",
      "Min loss: 0.009811338968575\n",
      "Mean loss: 0.012936655742426714\n",
      "Std loss: 0.0031152476322839373\n",
      "Total Loss: 0.07761993445456028\n",
      "------------------------------------ epoch 2656 (15930 steps) ------------------------------------\n",
      "Max loss: 0.0568106584250927\n",
      "Min loss: 0.00979651790112257\n",
      "Mean loss: 0.023426038213074207\n",
      "Std loss: 0.017020647530837537\n",
      "Total Loss: 0.14055622927844524\n",
      "------------------------------------ epoch 2657 (15936 steps) ------------------------------------\n",
      "Max loss: 0.041514601558446884\n",
      "Min loss: 0.009814303368330002\n",
      "Mean loss: 0.023817090472827356\n",
      "Std loss: 0.011857124452809767\n",
      "Total Loss: 0.14290254283696413\n",
      "------------------------------------ epoch 2658 (15942 steps) ------------------------------------\n",
      "Max loss: 0.027438487857580185\n",
      "Min loss: 0.010132785886526108\n",
      "Mean loss: 0.017535670505215723\n",
      "Std loss: 0.005808679541280667\n",
      "Total Loss: 0.10521402303129435\n",
      "------------------------------------ epoch 2659 (15948 steps) ------------------------------------\n",
      "Max loss: 0.04142814874649048\n",
      "Min loss: 0.008949060924351215\n",
      "Mean loss: 0.017231037995467584\n",
      "Std loss: 0.01128350968459839\n",
      "Total Loss: 0.1033862279728055\n",
      "------------------------------------ epoch 2660 (15954 steps) ------------------------------------\n",
      "Max loss: 0.05672267824411392\n",
      "Min loss: 0.014459188096225262\n",
      "Mean loss: 0.02960749700044592\n",
      "Std loss: 0.014988459369767051\n",
      "Total Loss: 0.17764498200267553\n",
      "------------------------------------ epoch 2661 (15960 steps) ------------------------------------\n",
      "Max loss: 0.031214376911520958\n",
      "Min loss: 0.010402931831777096\n",
      "Mean loss: 0.017235818939904373\n",
      "Std loss: 0.007239347026623845\n",
      "Total Loss: 0.10341491363942623\n",
      "------------------------------------ epoch 2662 (15966 steps) ------------------------------------\n",
      "Max loss: 0.04508751630783081\n",
      "Min loss: 0.012844203040003777\n",
      "Mean loss: 0.020077505459388096\n",
      "Std loss: 0.011300280229542069\n",
      "Total Loss: 0.12046503275632858\n",
      "------------------------------------ epoch 2663 (15972 steps) ------------------------------------\n",
      "Max loss: 0.05988726019859314\n",
      "Min loss: 0.013831617310643196\n",
      "Mean loss: 0.026992051862180233\n",
      "Std loss: 0.01540521395080335\n",
      "Total Loss: 0.1619523111730814\n",
      "------------------------------------ epoch 2664 (15978 steps) ------------------------------------\n",
      "Max loss: 0.035938262939453125\n",
      "Min loss: 0.010500170290470123\n",
      "Mean loss: 0.02121285830313961\n",
      "Std loss: 0.009320029345225838\n",
      "Total Loss: 0.12727714981883764\n",
      "------------------------------------ epoch 2665 (15984 steps) ------------------------------------\n",
      "Max loss: 0.049271490424871445\n",
      "Min loss: 0.011194879189133644\n",
      "Mean loss: 0.024388987571001053\n",
      "Std loss: 0.012252363356870905\n",
      "Total Loss: 0.14633392542600632\n",
      "------------------------------------ epoch 2666 (15990 steps) ------------------------------------\n",
      "Max loss: 0.054373957216739655\n",
      "Min loss: 0.015065527521073818\n",
      "Mean loss: 0.025451228953897953\n",
      "Std loss: 0.013619050543266373\n",
      "Total Loss: 0.15270737372338772\n",
      "------------------------------------ epoch 2667 (15996 steps) ------------------------------------\n",
      "Max loss: 0.04266033321619034\n",
      "Min loss: 0.012840251438319683\n",
      "Mean loss: 0.02454450431590279\n",
      "Std loss: 0.011333295235456023\n",
      "Total Loss: 0.14726702589541674\n",
      "------------------------------------ epoch 2668 (16002 steps) ------------------------------------\n",
      "Max loss: 0.02669675648212433\n",
      "Min loss: 0.009808210656046867\n",
      "Mean loss: 0.01711270088950793\n",
      "Std loss: 0.006392960314945545\n",
      "Total Loss: 0.10267620533704758\n",
      "------------------------------------ epoch 2669 (16008 steps) ------------------------------------\n",
      "Max loss: 0.018364980816841125\n",
      "Min loss: 0.010586800053715706\n",
      "Mean loss: 0.014142950531095266\n",
      "Std loss: 0.0024875947531683277\n",
      "Total Loss: 0.0848577031865716\n",
      "------------------------------------ epoch 2670 (16014 steps) ------------------------------------\n",
      "Max loss: 0.04507526010274887\n",
      "Min loss: 0.012801272794604301\n",
      "Mean loss: 0.023712483545144398\n",
      "Std loss: 0.011499724550530598\n",
      "Total Loss: 0.1422749012708664\n",
      "------------------------------------ epoch 2671 (16020 steps) ------------------------------------\n",
      "Max loss: 0.025213807821273804\n",
      "Min loss: 0.009506303817033768\n",
      "Mean loss: 0.014356746027866999\n",
      "Std loss: 0.005760519000461789\n",
      "Total Loss: 0.086140476167202\n",
      "------------------------------------ epoch 2672 (16026 steps) ------------------------------------\n",
      "Max loss: 0.02810874581336975\n",
      "Min loss: 0.011883529834449291\n",
      "Mean loss: 0.01846676304315527\n",
      "Std loss: 0.006272666658005585\n",
      "Total Loss: 0.11080057825893164\n",
      "------------------------------------ epoch 2673 (16032 steps) ------------------------------------\n",
      "Max loss: 0.02537330612540245\n",
      "Min loss: 0.01299925148487091\n",
      "Mean loss: 0.017769129791607458\n",
      "Std loss: 0.004719150364117995\n",
      "Total Loss: 0.10661477874964476\n",
      "------------------------------------ epoch 2674 (16038 steps) ------------------------------------\n",
      "Max loss: 0.036563776433467865\n",
      "Min loss: 0.008569756522774696\n",
      "Mean loss: 0.01809221599251032\n",
      "Std loss: 0.009192342028858548\n",
      "Total Loss: 0.10855329595506191\n",
      "------------------------------------ epoch 2675 (16044 steps) ------------------------------------\n",
      "Max loss: 0.02246328443288803\n",
      "Min loss: 0.010728868655860424\n",
      "Mean loss: 0.01649777606750528\n",
      "Std loss: 0.003895354023604139\n",
      "Total Loss: 0.09898665640503168\n",
      "------------------------------------ epoch 2676 (16050 steps) ------------------------------------\n",
      "Max loss: 0.027338145300745964\n",
      "Min loss: 0.01027059368789196\n",
      "Mean loss: 0.01621114027996858\n",
      "Std loss: 0.005374946594929189\n",
      "Total Loss: 0.09726684167981148\n",
      "------------------------------------ epoch 2677 (16056 steps) ------------------------------------\n",
      "Max loss: 0.023003853857517242\n",
      "Min loss: 0.011394098401069641\n",
      "Mean loss: 0.01500645869721969\n",
      "Std loss: 0.003847915723216597\n",
      "Total Loss: 0.09003875218331814\n",
      "------------------------------------ epoch 2678 (16062 steps) ------------------------------------\n",
      "Max loss: 0.03615472838282585\n",
      "Min loss: 0.013801926746964455\n",
      "Mean loss: 0.023480411308507126\n",
      "Std loss: 0.008064605610901176\n",
      "Total Loss: 0.14088246785104275\n",
      "------------------------------------ epoch 2679 (16068 steps) ------------------------------------\n",
      "Max loss: 0.02972905896604061\n",
      "Min loss: 0.009137576445937157\n",
      "Mean loss: 0.014703450724482536\n",
      "Std loss: 0.00684898203922971\n",
      "Total Loss: 0.08822070434689522\n",
      "------------------------------------ epoch 2680 (16074 steps) ------------------------------------\n",
      "Max loss: 0.024061772972345352\n",
      "Min loss: 0.008978726342320442\n",
      "Mean loss: 0.014512032270431519\n",
      "Std loss: 0.005403764231882195\n",
      "Total Loss: 0.08707219362258911\n",
      "------------------------------------ epoch 2681 (16080 steps) ------------------------------------\n",
      "Max loss: 0.023860154673457146\n",
      "Min loss: 0.009388068690896034\n",
      "Mean loss: 0.014615474579234919\n",
      "Std loss: 0.004702163666291096\n",
      "Total Loss: 0.08769284747540951\n",
      "------------------------------------ epoch 2682 (16086 steps) ------------------------------------\n",
      "Max loss: 0.01961458846926689\n",
      "Min loss: 0.008914165198802948\n",
      "Mean loss: 0.013453343727936348\n",
      "Std loss: 0.0033294065659090224\n",
      "Total Loss: 0.08072006236761808\n",
      "------------------------------------ epoch 2683 (16092 steps) ------------------------------------\n",
      "Max loss: 0.03680385649204254\n",
      "Min loss: 0.008994437754154205\n",
      "Mean loss: 0.018377799075096846\n",
      "Std loss: 0.009521385498601482\n",
      "Total Loss: 0.11026679445058107\n",
      "------------------------------------ epoch 2684 (16098 steps) ------------------------------------\n",
      "Max loss: 0.02182350680232048\n",
      "Min loss: 0.009492089971899986\n",
      "Mean loss: 0.014850508576879898\n",
      "Std loss: 0.0037530946268252196\n",
      "Total Loss: 0.08910305146127939\n",
      "------------------------------------ epoch 2685 (16104 steps) ------------------------------------\n",
      "Max loss: 0.037371840327978134\n",
      "Min loss: 0.008589553646743298\n",
      "Mean loss: 0.020665439621855814\n",
      "Std loss: 0.01023018926047135\n",
      "Total Loss: 0.12399263773113489\n",
      "------------------------------------ epoch 2686 (16110 steps) ------------------------------------\n",
      "Max loss: 0.022908907383680344\n",
      "Min loss: 0.009832240641117096\n",
      "Mean loss: 0.014920575699458519\n",
      "Std loss: 0.0043479063267829925\n",
      "Total Loss: 0.08952345419675112\n",
      "------------------------------------ epoch 2687 (16116 steps) ------------------------------------\n",
      "Max loss: 0.03244316205382347\n",
      "Min loss: 0.008764822967350483\n",
      "Mean loss: 0.018352829075107973\n",
      "Std loss: 0.008771493960980717\n",
      "Total Loss: 0.11011697445064783\n",
      "------------------------------------ epoch 2688 (16122 steps) ------------------------------------\n",
      "Max loss: 0.026120563969016075\n",
      "Min loss: 0.007596671115607023\n",
      "Mean loss: 0.013935998314991593\n",
      "Std loss: 0.006565171764397323\n",
      "Total Loss: 0.08361598988994956\n",
      "------------------------------------ epoch 2689 (16128 steps) ------------------------------------\n",
      "Max loss: 0.02406330406665802\n",
      "Min loss: 0.008455978706479073\n",
      "Mean loss: 0.014345779083669186\n",
      "Std loss: 0.006293655680964489\n",
      "Total Loss: 0.08607467450201511\n",
      "------------------------------------ epoch 2690 (16134 steps) ------------------------------------\n",
      "Max loss: 0.029844699427485466\n",
      "Min loss: 0.010096369311213493\n",
      "Mean loss: 0.016635751041273277\n",
      "Std loss: 0.0068285277892656815\n",
      "Total Loss: 0.09981450624763966\n",
      "------------------------------------ epoch 2691 (16140 steps) ------------------------------------\n",
      "Max loss: 0.03816348314285278\n",
      "Min loss: 0.008320992812514305\n",
      "Mean loss: 0.01668183769409855\n",
      "Std loss: 0.010113146451231732\n",
      "Total Loss: 0.10009102616459131\n",
      "------------------------------------ epoch 2692 (16146 steps) ------------------------------------\n",
      "Max loss: 0.015235702507197857\n",
      "Min loss: 0.008045673370361328\n",
      "Mean loss: 0.012824593111872673\n",
      "Std loss: 0.002274524055497826\n",
      "Total Loss: 0.07694755867123604\n",
      "------------------------------------ epoch 2693 (16152 steps) ------------------------------------\n",
      "Max loss: 0.02102699503302574\n",
      "Min loss: 0.008337593637406826\n",
      "Mean loss: 0.01568066468462348\n",
      "Std loss: 0.004262646088213054\n",
      "Total Loss: 0.09408398810774088\n",
      "------------------------------------ epoch 2694 (16158 steps) ------------------------------------\n",
      "Max loss: 0.0353868305683136\n",
      "Min loss: 0.010563230141997337\n",
      "Mean loss: 0.01973199596007665\n",
      "Std loss: 0.008096633171621775\n",
      "Total Loss: 0.1183919757604599\n",
      "------------------------------------ epoch 2695 (16164 steps) ------------------------------------\n",
      "Max loss: 0.03466091677546501\n",
      "Min loss: 0.009705958887934685\n",
      "Mean loss: 0.02205211343243718\n",
      "Std loss: 0.009889223276652067\n",
      "Total Loss: 0.1323126805946231\n",
      "------------------------------------ epoch 2696 (16170 steps) ------------------------------------\n",
      "Max loss: 0.022542763501405716\n",
      "Min loss: 0.009046531282365322\n",
      "Mean loss: 0.01282918406650424\n",
      "Std loss: 0.004763360413112904\n",
      "Total Loss: 0.07697510439902544\n",
      "------------------------------------ epoch 2697 (16176 steps) ------------------------------------\n",
      "Max loss: 0.018917780369520187\n",
      "Min loss: 0.006556616630405188\n",
      "Mean loss: 0.012921939836815\n",
      "Std loss: 0.004236595840170254\n",
      "Total Loss: 0.07753163902089\n",
      "------------------------------------ epoch 2698 (16182 steps) ------------------------------------\n",
      "Max loss: 0.022592872381210327\n",
      "Min loss: 0.008286057971417904\n",
      "Mean loss: 0.01582650902370612\n",
      "Std loss: 0.004627817140107312\n",
      "Total Loss: 0.09495905414223671\n",
      "------------------------------------ epoch 2699 (16188 steps) ------------------------------------\n",
      "Max loss: 0.031963933259248734\n",
      "Min loss: 0.01580483466386795\n",
      "Mean loss: 0.0217360428844889\n",
      "Std loss: 0.0049259180003826866\n",
      "Total Loss: 0.1304162573069334\n",
      "------------------------------------ epoch 2700 (16194 steps) ------------------------------------\n",
      "Max loss: 0.03771185129880905\n",
      "Min loss: 0.00997145101428032\n",
      "Mean loss: 0.01593503402546048\n",
      "Std loss: 0.009794942324572857\n",
      "Total Loss: 0.09561020415276289\n",
      "------------------------------------ epoch 2701 (16200 steps) ------------------------------------\n",
      "Max loss: 0.02914305403828621\n",
      "Min loss: 0.008727985434234142\n",
      "Mean loss: 0.015845535478244226\n",
      "Std loss: 0.006771022788060165\n",
      "Total Loss: 0.09507321286946535\n",
      "saved model at ./weights/model_2701.pth\n",
      "------------------------------------ epoch 2702 (16206 steps) ------------------------------------\n",
      "Max loss: 0.04425382241606712\n",
      "Min loss: 0.00983419083058834\n",
      "Mean loss: 0.022452617219338816\n",
      "Std loss: 0.013797647477601357\n",
      "Total Loss: 0.1347157033160329\n",
      "------------------------------------ epoch 2703 (16212 steps) ------------------------------------\n",
      "Max loss: 0.03636915609240532\n",
      "Min loss: 0.008272596634924412\n",
      "Mean loss: 0.02146910720815261\n",
      "Std loss: 0.010524116425875363\n",
      "Total Loss: 0.12881464324891567\n",
      "------------------------------------ epoch 2704 (16218 steps) ------------------------------------\n",
      "Max loss: 0.03237995132803917\n",
      "Min loss: 0.013004484586417675\n",
      "Mean loss: 0.0181138898866872\n",
      "Std loss: 0.006638697796748323\n",
      "Total Loss: 0.1086833393201232\n",
      "------------------------------------ epoch 2705 (16224 steps) ------------------------------------\n",
      "Max loss: 0.02262595295906067\n",
      "Min loss: 0.010207428596913815\n",
      "Mean loss: 0.01511040066058437\n",
      "Std loss: 0.004233596356543582\n",
      "Total Loss: 0.09066240396350622\n",
      "------------------------------------ epoch 2706 (16230 steps) ------------------------------------\n",
      "Max loss: 0.04920557886362076\n",
      "Min loss: 0.009235519915819168\n",
      "Mean loss: 0.0223404448479414\n",
      "Std loss: 0.014836801299277823\n",
      "Total Loss: 0.1340426690876484\n",
      "------------------------------------ epoch 2707 (16236 steps) ------------------------------------\n",
      "Max loss: 0.026826590299606323\n",
      "Min loss: 0.010468504391610622\n",
      "Mean loss: 0.016536985679219168\n",
      "Std loss: 0.005052193098984801\n",
      "Total Loss: 0.099221914075315\n",
      "------------------------------------ epoch 2708 (16242 steps) ------------------------------------\n",
      "Max loss: 0.02491619996726513\n",
      "Min loss: 0.009053248912096024\n",
      "Mean loss: 0.016425801751514275\n",
      "Std loss: 0.0062537859981011265\n",
      "Total Loss: 0.09855481050908566\n",
      "------------------------------------ epoch 2709 (16248 steps) ------------------------------------\n",
      "Max loss: 0.051461994647979736\n",
      "Min loss: 0.00951685942709446\n",
      "Mean loss: 0.020315266990413267\n",
      "Std loss: 0.014225725667088017\n",
      "Total Loss: 0.12189160194247961\n",
      "------------------------------------ epoch 2710 (16254 steps) ------------------------------------\n",
      "Max loss: 0.015782173722982407\n",
      "Min loss: 0.010733522474765778\n",
      "Mean loss: 0.012894686156262955\n",
      "Std loss: 0.0017199569179795961\n",
      "Total Loss: 0.07736811693757772\n",
      "------------------------------------ epoch 2711 (16260 steps) ------------------------------------\n",
      "Max loss: 0.02108440175652504\n",
      "Min loss: 0.010644673369824886\n",
      "Mean loss: 0.0151416826993227\n",
      "Std loss: 0.004244307613928929\n",
      "Total Loss: 0.0908500961959362\n",
      "------------------------------------ epoch 2712 (16266 steps) ------------------------------------\n",
      "Max loss: 0.03745397925376892\n",
      "Min loss: 0.008962081745266914\n",
      "Mean loss: 0.022212110925465822\n",
      "Std loss: 0.010235838155617882\n",
      "Total Loss: 0.13327266555279493\n",
      "------------------------------------ epoch 2713 (16272 steps) ------------------------------------\n",
      "Max loss: 0.05207691714167595\n",
      "Min loss: 0.013205964118242264\n",
      "Mean loss: 0.022824084075788658\n",
      "Std loss: 0.013414945113990676\n",
      "Total Loss: 0.13694450445473194\n",
      "------------------------------------ epoch 2714 (16278 steps) ------------------------------------\n",
      "Max loss: 0.031539104878902435\n",
      "Min loss: 0.010380884632468224\n",
      "Mean loss: 0.019904300415267546\n",
      "Std loss: 0.0069368109622474005\n",
      "Total Loss: 0.11942580249160528\n",
      "------------------------------------ epoch 2715 (16284 steps) ------------------------------------\n",
      "Max loss: 0.032256804406642914\n",
      "Min loss: 0.007856621406972408\n",
      "Mean loss: 0.015854198019951582\n",
      "Std loss: 0.00820571758033108\n",
      "Total Loss: 0.09512518811970949\n",
      "------------------------------------ epoch 2716 (16290 steps) ------------------------------------\n",
      "Max loss: 0.02752181701362133\n",
      "Min loss: 0.009448738768696785\n",
      "Mean loss: 0.014443021267652512\n",
      "Std loss: 0.005954197874277744\n",
      "Total Loss: 0.08665812760591507\n",
      "------------------------------------ epoch 2717 (16296 steps) ------------------------------------\n",
      "Max loss: 0.026081055402755737\n",
      "Min loss: 0.008753201924264431\n",
      "Mean loss: 0.014027521014213562\n",
      "Std loss: 0.006555578612318773\n",
      "Total Loss: 0.08416512608528137\n",
      "------------------------------------ epoch 2718 (16302 steps) ------------------------------------\n",
      "Max loss: 0.043595414608716965\n",
      "Min loss: 0.009866902604699135\n",
      "Mean loss: 0.020619453707089026\n",
      "Std loss: 0.011462704938975747\n",
      "Total Loss: 0.12371672224253416\n",
      "------------------------------------ epoch 2719 (16308 steps) ------------------------------------\n",
      "Max loss: 0.030539516359567642\n",
      "Min loss: 0.011110811494290829\n",
      "Mean loss: 0.018377546841899555\n",
      "Std loss: 0.007888707348362261\n",
      "Total Loss: 0.11026528105139732\n",
      "------------------------------------ epoch 2720 (16314 steps) ------------------------------------\n",
      "Max loss: 0.03532000631093979\n",
      "Min loss: 0.010587859898805618\n",
      "Mean loss: 0.018065335229039192\n",
      "Std loss: 0.008972724716057544\n",
      "Total Loss: 0.10839201137423515\n",
      "------------------------------------ epoch 2721 (16320 steps) ------------------------------------\n",
      "Max loss: 0.022490566596388817\n",
      "Min loss: 0.01203533262014389\n",
      "Mean loss: 0.016118410974740982\n",
      "Std loss: 0.004111330534487024\n",
      "Total Loss: 0.09671046584844589\n",
      "------------------------------------ epoch 2722 (16326 steps) ------------------------------------\n",
      "Max loss: 0.028778504580259323\n",
      "Min loss: 0.008173773996531963\n",
      "Mean loss: 0.012493392607818047\n",
      "Std loss: 0.0073727993544315425\n",
      "Total Loss: 0.07496035564690828\n",
      "------------------------------------ epoch 2723 (16332 steps) ------------------------------------\n",
      "Max loss: 0.02497907541692257\n",
      "Min loss: 0.011867424473166466\n",
      "Mean loss: 0.018312766837577026\n",
      "Std loss: 0.004345235596370448\n",
      "Total Loss: 0.10987660102546215\n",
      "------------------------------------ epoch 2724 (16338 steps) ------------------------------------\n",
      "Max loss: 0.02934417687356472\n",
      "Min loss: 0.00819067656993866\n",
      "Mean loss: 0.013942880555987358\n",
      "Std loss: 0.007218181806740348\n",
      "Total Loss: 0.08365728333592415\n",
      "------------------------------------ epoch 2725 (16344 steps) ------------------------------------\n",
      "Max loss: 0.02531951293349266\n",
      "Min loss: 0.01140211708843708\n",
      "Mean loss: 0.018022564860681694\n",
      "Std loss: 0.005273455704106617\n",
      "Total Loss: 0.10813538916409016\n",
      "------------------------------------ epoch 2726 (16350 steps) ------------------------------------\n",
      "Max loss: 0.044442884624004364\n",
      "Min loss: 0.011014234274625778\n",
      "Mean loss: 0.020441525770972174\n",
      "Std loss: 0.011749292089607125\n",
      "Total Loss: 0.12264915462583303\n",
      "------------------------------------ epoch 2727 (16356 steps) ------------------------------------\n",
      "Max loss: 0.02340206317603588\n",
      "Min loss: 0.009811855852603912\n",
      "Mean loss: 0.014138193024943272\n",
      "Std loss: 0.004446575344801078\n",
      "Total Loss: 0.08482915814965963\n",
      "------------------------------------ epoch 2728 (16362 steps) ------------------------------------\n",
      "Max loss: 0.028069015592336655\n",
      "Min loss: 0.008280347101390362\n",
      "Mean loss: 0.012565501344700655\n",
      "Std loss: 0.00698130319841009\n",
      "Total Loss: 0.07539300806820393\n",
      "------------------------------------ epoch 2729 (16368 steps) ------------------------------------\n",
      "Max loss: 0.015527857467532158\n",
      "Min loss: 0.009700354188680649\n",
      "Mean loss: 0.013282090735932192\n",
      "Std loss: 0.002011530631388272\n",
      "Total Loss: 0.07969254441559315\n",
      "------------------------------------ epoch 2730 (16374 steps) ------------------------------------\n",
      "Max loss: 0.01992075704038143\n",
      "Min loss: 0.008580570109188557\n",
      "Mean loss: 0.01455493705968062\n",
      "Std loss: 0.004112921734895174\n",
      "Total Loss: 0.08732962235808372\n",
      "------------------------------------ epoch 2731 (16380 steps) ------------------------------------\n",
      "Max loss: 0.02077590301632881\n",
      "Min loss: 0.010313156992197037\n",
      "Mean loss: 0.014911664028962454\n",
      "Std loss: 0.003206477903786422\n",
      "Total Loss: 0.08946998417377472\n",
      "------------------------------------ epoch 2732 (16386 steps) ------------------------------------\n",
      "Max loss: 0.016756461933255196\n",
      "Min loss: 0.009627936407923698\n",
      "Mean loss: 0.01267825641358892\n",
      "Std loss: 0.002351158115882514\n",
      "Total Loss: 0.07606953848153353\n",
      "------------------------------------ epoch 2733 (16392 steps) ------------------------------------\n",
      "Max loss: 0.027831315994262695\n",
      "Min loss: 0.008334426209330559\n",
      "Mean loss: 0.017500830348581076\n",
      "Std loss: 0.0069563598108844885\n",
      "Total Loss: 0.10500498209148645\n",
      "------------------------------------ epoch 2734 (16398 steps) ------------------------------------\n",
      "Max loss: 0.03606470674276352\n",
      "Min loss: 0.009041802957654\n",
      "Mean loss: 0.023619931501646835\n",
      "Std loss: 0.011094725704445507\n",
      "Total Loss: 0.14171958900988102\n",
      "------------------------------------ epoch 2735 (16404 steps) ------------------------------------\n",
      "Max loss: 0.016980446875095367\n",
      "Min loss: 0.009098305366933346\n",
      "Mean loss: 0.01305214020734032\n",
      "Std loss: 0.0030429829122612627\n",
      "Total Loss: 0.07831284124404192\n",
      "------------------------------------ epoch 2736 (16410 steps) ------------------------------------\n",
      "Max loss: 0.05251767486333847\n",
      "Min loss: 0.010128581896424294\n",
      "Mean loss: 0.023392166942358017\n",
      "Std loss: 0.014088548618964154\n",
      "Total Loss: 0.1403530016541481\n",
      "------------------------------------ epoch 2737 (16416 steps) ------------------------------------\n",
      "Max loss: 0.033303529024124146\n",
      "Min loss: 0.00779561884701252\n",
      "Mean loss: 0.01864430479084452\n",
      "Std loss: 0.010429781675106069\n",
      "Total Loss: 0.11186582874506712\n",
      "------------------------------------ epoch 2738 (16422 steps) ------------------------------------\n",
      "Max loss: 0.05140290781855583\n",
      "Min loss: 0.010913258418440819\n",
      "Mean loss: 0.029197178625812132\n",
      "Std loss: 0.01538262953380798\n",
      "Total Loss: 0.1751830717548728\n",
      "------------------------------------ epoch 2739 (16428 steps) ------------------------------------\n",
      "Max loss: 0.04374769330024719\n",
      "Min loss: 0.012631441466510296\n",
      "Mean loss: 0.02621325400347511\n",
      "Std loss: 0.012738047034668716\n",
      "Total Loss: 0.15727952402085066\n",
      "------------------------------------ epoch 2740 (16434 steps) ------------------------------------\n",
      "Max loss: 0.058326687663793564\n",
      "Min loss: 0.013333683833479881\n",
      "Mean loss: 0.02906405956794818\n",
      "Std loss: 0.01554597599585574\n",
      "Total Loss: 0.1743843574076891\n",
      "------------------------------------ epoch 2741 (16440 steps) ------------------------------------\n",
      "Max loss: 0.027034929022192955\n",
      "Min loss: 0.011812057346105576\n",
      "Mean loss: 0.017600616129736107\n",
      "Std loss: 0.006120886476933101\n",
      "Total Loss: 0.10560369677841663\n",
      "------------------------------------ epoch 2742 (16446 steps) ------------------------------------\n",
      "Max loss: 0.023279573768377304\n",
      "Min loss: 0.012013239786028862\n",
      "Mean loss: 0.01548436051234603\n",
      "Std loss: 0.004160491584446952\n",
      "Total Loss: 0.09290616307407618\n",
      "------------------------------------ epoch 2743 (16452 steps) ------------------------------------\n",
      "Max loss: 0.04378405213356018\n",
      "Min loss: 0.009844007901847363\n",
      "Mean loss: 0.02351224810505907\n",
      "Std loss: 0.012060829863557535\n",
      "Total Loss: 0.1410734886303544\n",
      "------------------------------------ epoch 2744 (16458 steps) ------------------------------------\n",
      "Max loss: 0.025260526686906815\n",
      "Min loss: 0.009936356917023659\n",
      "Mean loss: 0.017879987756411236\n",
      "Std loss: 0.005442533612397338\n",
      "Total Loss: 0.10727992653846741\n",
      "------------------------------------ epoch 2745 (16464 steps) ------------------------------------\n",
      "Max loss: 0.05402137339115143\n",
      "Min loss: 0.011129291728138924\n",
      "Mean loss: 0.032662671990692616\n",
      "Std loss: 0.01663886305151901\n",
      "Total Loss: 0.1959760319441557\n",
      "------------------------------------ epoch 2746 (16470 steps) ------------------------------------\n",
      "Max loss: 0.02506706863641739\n",
      "Min loss: 0.009752614423632622\n",
      "Mean loss: 0.017650729821374018\n",
      "Std loss: 0.005738373285882347\n",
      "Total Loss: 0.10590437892824411\n",
      "------------------------------------ epoch 2747 (16476 steps) ------------------------------------\n",
      "Max loss: 0.02572248876094818\n",
      "Min loss: 0.012280203402042389\n",
      "Mean loss: 0.017791915064056713\n",
      "Std loss: 0.00539201892574339\n",
      "Total Loss: 0.10675149038434029\n",
      "------------------------------------ epoch 2748 (16482 steps) ------------------------------------\n",
      "Max loss: 0.03648783639073372\n",
      "Min loss: 0.012024017982184887\n",
      "Mean loss: 0.022330935889234144\n",
      "Std loss: 0.008011769489489982\n",
      "Total Loss: 0.13398561533540487\n",
      "------------------------------------ epoch 2749 (16488 steps) ------------------------------------\n",
      "Max loss: 0.02693457528948784\n",
      "Min loss: 0.00962275080382824\n",
      "Mean loss: 0.01515162456780672\n",
      "Std loss: 0.0057224009420059905\n",
      "Total Loss: 0.09090974740684032\n",
      "------------------------------------ epoch 2750 (16494 steps) ------------------------------------\n",
      "Max loss: 0.042107027024030685\n",
      "Min loss: 0.009014995768666267\n",
      "Mean loss: 0.021741350802282493\n",
      "Std loss: 0.009966541587217525\n",
      "Total Loss: 0.13044810481369495\n",
      "------------------------------------ epoch 2751 (16500 steps) ------------------------------------\n",
      "Max loss: 0.03919859975576401\n",
      "Min loss: 0.011103793978691101\n",
      "Mean loss: 0.023362147932251293\n",
      "Std loss: 0.01013612318883235\n",
      "Total Loss: 0.14017288759350777\n",
      "------------------------------------ epoch 2752 (16506 steps) ------------------------------------\n",
      "Max loss: 0.026154711842536926\n",
      "Min loss: 0.010369894094765186\n",
      "Mean loss: 0.015633572358638048\n",
      "Std loss: 0.005105933736268645\n",
      "Total Loss: 0.09380143415182829\n",
      "------------------------------------ epoch 2753 (16512 steps) ------------------------------------\n",
      "Max loss: 0.0145206768065691\n",
      "Min loss: 0.008437016978859901\n",
      "Mean loss: 0.011859209276735783\n",
      "Std loss: 0.002436013570085811\n",
      "Total Loss: 0.0711552556604147\n",
      "------------------------------------ epoch 2754 (16518 steps) ------------------------------------\n",
      "Max loss: 0.01908634603023529\n",
      "Min loss: 0.01284201443195343\n",
      "Mean loss: 0.016348897479474545\n",
      "Std loss: 0.0024524637966285285\n",
      "Total Loss: 0.09809338487684727\n",
      "------------------------------------ epoch 2755 (16524 steps) ------------------------------------\n",
      "Max loss: 0.030440013855695724\n",
      "Min loss: 0.009714392945170403\n",
      "Mean loss: 0.01895685214549303\n",
      "Std loss: 0.007959940434025325\n",
      "Total Loss: 0.11374111287295818\n",
      "------------------------------------ epoch 2756 (16530 steps) ------------------------------------\n",
      "Max loss: 0.03499404713511467\n",
      "Min loss: 0.009605362080037594\n",
      "Mean loss: 0.01781832582006852\n",
      "Std loss: 0.008461639915722519\n",
      "Total Loss: 0.10690995492041111\n",
      "------------------------------------ epoch 2757 (16536 steps) ------------------------------------\n",
      "Max loss: 0.021027138456702232\n",
      "Min loss: 0.008486448787152767\n",
      "Mean loss: 0.012069246110816797\n",
      "Std loss: 0.0042450107488091835\n",
      "Total Loss: 0.07241547666490078\n",
      "------------------------------------ epoch 2758 (16542 steps) ------------------------------------\n",
      "Max loss: 0.025255940854549408\n",
      "Min loss: 0.011447494849562645\n",
      "Mean loss: 0.01683283116047581\n",
      "Std loss: 0.005093351392548976\n",
      "Total Loss: 0.10099698696285486\n",
      "------------------------------------ epoch 2759 (16548 steps) ------------------------------------\n",
      "Max loss: 0.025119168683886528\n",
      "Min loss: 0.010050336830317974\n",
      "Mean loss: 0.017333705133448046\n",
      "Std loss: 0.0044476205704510096\n",
      "Total Loss: 0.10400223080068827\n",
      "------------------------------------ epoch 2760 (16554 steps) ------------------------------------\n",
      "Max loss: 0.018903955817222595\n",
      "Min loss: 0.011384755373001099\n",
      "Mean loss: 0.013793224158386389\n",
      "Std loss: 0.0024498999466033263\n",
      "Total Loss: 0.08275934495031834\n",
      "------------------------------------ epoch 2761 (16560 steps) ------------------------------------\n",
      "Max loss: 0.01593216322362423\n",
      "Min loss: 0.008433680981397629\n",
      "Mean loss: 0.012393066038688024\n",
      "Std loss: 0.002844071953509201\n",
      "Total Loss: 0.07435839623212814\n",
      "------------------------------------ epoch 2762 (16566 steps) ------------------------------------\n",
      "Max loss: 0.028421875089406967\n",
      "Min loss: 0.006933126132935286\n",
      "Mean loss: 0.01784855724933247\n",
      "Std loss: 0.008415535085658246\n",
      "Total Loss: 0.1070913434959948\n",
      "------------------------------------ epoch 2763 (16572 steps) ------------------------------------\n",
      "Max loss: 0.03204286843538284\n",
      "Min loss: 0.011026191525161266\n",
      "Mean loss: 0.016250267780075472\n",
      "Std loss: 0.007238272237411057\n",
      "Total Loss: 0.09750160668045282\n",
      "------------------------------------ epoch 2764 (16578 steps) ------------------------------------\n",
      "Max loss: 0.020194217562675476\n",
      "Min loss: 0.010339349508285522\n",
      "Mean loss: 0.012903180594245592\n",
      "Std loss: 0.0034216197975701604\n",
      "Total Loss: 0.07741908356547356\n",
      "------------------------------------ epoch 2765 (16584 steps) ------------------------------------\n",
      "Max loss: 0.017015349119901657\n",
      "Min loss: 0.006690210662782192\n",
      "Mean loss: 0.011339008808135986\n",
      "Std loss: 0.0031499503358063652\n",
      "Total Loss: 0.06803405284881592\n",
      "------------------------------------ epoch 2766 (16590 steps) ------------------------------------\n",
      "Max loss: 0.07542472332715988\n",
      "Min loss: 0.007632940076291561\n",
      "Mean loss: 0.021407463277379673\n",
      "Std loss: 0.02426678291112583\n",
      "Total Loss: 0.12844477966427803\n",
      "------------------------------------ epoch 2767 (16596 steps) ------------------------------------\n",
      "Max loss: 0.02673003077507019\n",
      "Min loss: 0.010447675362229347\n",
      "Mean loss: 0.01609411494185527\n",
      "Std loss: 0.007385474146228489\n",
      "Total Loss: 0.09656468965113163\n",
      "------------------------------------ epoch 2768 (16602 steps) ------------------------------------\n",
      "Max loss: 0.04071732982993126\n",
      "Min loss: 0.008256507106125355\n",
      "Mean loss: 0.019726867477099102\n",
      "Std loss: 0.010852218351667154\n",
      "Total Loss: 0.1183612048625946\n",
      "------------------------------------ epoch 2769 (16608 steps) ------------------------------------\n",
      "Max loss: 0.01828916184604168\n",
      "Min loss: 0.01072128489613533\n",
      "Mean loss: 0.014040030849476656\n",
      "Std loss: 0.0028464081571759315\n",
      "Total Loss: 0.08424018509685993\n",
      "------------------------------------ epoch 2770 (16614 steps) ------------------------------------\n",
      "Max loss: 0.01318097673356533\n",
      "Min loss: 0.007762063294649124\n",
      "Mean loss: 0.011296623852103949\n",
      "Std loss: 0.0019035867980398693\n",
      "Total Loss: 0.06777974311262369\n",
      "------------------------------------ epoch 2771 (16620 steps) ------------------------------------\n",
      "Max loss: 0.019975198432803154\n",
      "Min loss: 0.008870292454957962\n",
      "Mean loss: 0.012926921093215546\n",
      "Std loss: 0.004505549068305145\n",
      "Total Loss: 0.07756152655929327\n",
      "------------------------------------ epoch 2772 (16626 steps) ------------------------------------\n",
      "Max loss: 0.021849315613508224\n",
      "Min loss: 0.00847672950476408\n",
      "Mean loss: 0.01556417237346371\n",
      "Std loss: 0.004682190704037441\n",
      "Total Loss: 0.09338503424078226\n",
      "------------------------------------ epoch 2773 (16632 steps) ------------------------------------\n",
      "Max loss: 0.03448358178138733\n",
      "Min loss: 0.0086372597143054\n",
      "Mean loss: 0.019153589848428965\n",
      "Std loss: 0.009958859184157605\n",
      "Total Loss: 0.11492153909057379\n",
      "------------------------------------ epoch 2774 (16638 steps) ------------------------------------\n",
      "Max loss: 0.029824933037161827\n",
      "Min loss: 0.009604359976947308\n",
      "Mean loss: 0.014932212264587482\n",
      "Std loss: 0.006801815335254367\n",
      "Total Loss: 0.08959327358752489\n",
      "------------------------------------ epoch 2775 (16644 steps) ------------------------------------\n",
      "Max loss: 0.0338662713766098\n",
      "Min loss: 0.010685409419238567\n",
      "Mean loss: 0.01722838372612993\n",
      "Std loss: 0.00883247097919279\n",
      "Total Loss: 0.10337030235677958\n",
      "------------------------------------ epoch 2776 (16650 steps) ------------------------------------\n",
      "Max loss: 0.05387528985738754\n",
      "Min loss: 0.009545283392071724\n",
      "Mean loss: 0.020719918111960094\n",
      "Std loss: 0.015129121326238124\n",
      "Total Loss: 0.12431950867176056\n",
      "------------------------------------ epoch 2777 (16656 steps) ------------------------------------\n",
      "Max loss: 0.0458853654563427\n",
      "Min loss: 0.01272437535226345\n",
      "Mean loss: 0.0242548743262887\n",
      "Std loss: 0.010794028722629022\n",
      "Total Loss: 0.1455292459577322\n",
      "------------------------------------ epoch 2778 (16662 steps) ------------------------------------\n",
      "Max loss: 0.054946791380643845\n",
      "Min loss: 0.01256919838488102\n",
      "Mean loss: 0.02712016273289919\n",
      "Std loss: 0.015321623393090995\n",
      "Total Loss: 0.16272097639739513\n",
      "------------------------------------ epoch 2779 (16668 steps) ------------------------------------\n",
      "Max loss: 0.027822818607091904\n",
      "Min loss: 0.014126775786280632\n",
      "Mean loss: 0.02024941969042023\n",
      "Std loss: 0.005059202249323302\n",
      "Total Loss: 0.12149651814252138\n",
      "------------------------------------ epoch 2780 (16674 steps) ------------------------------------\n",
      "Max loss: 0.016694650053977966\n",
      "Min loss: 0.010272881016135216\n",
      "Mean loss: 0.012889126781374216\n",
      "Std loss: 0.002235405848962683\n",
      "Total Loss: 0.0773347606882453\n",
      "------------------------------------ epoch 2781 (16680 steps) ------------------------------------\n",
      "Max loss: 0.04229972884058952\n",
      "Min loss: 0.010909870266914368\n",
      "Mean loss: 0.02726068099339803\n",
      "Std loss: 0.01078782345667715\n",
      "Total Loss: 0.16356408596038818\n",
      "------------------------------------ epoch 2782 (16686 steps) ------------------------------------\n",
      "Max loss: 0.031211674213409424\n",
      "Min loss: 0.012948393821716309\n",
      "Mean loss: 0.02022064725557963\n",
      "Std loss: 0.0075876596497897585\n",
      "Total Loss: 0.12132388353347778\n",
      "------------------------------------ epoch 2783 (16692 steps) ------------------------------------\n",
      "Max loss: 0.04205005615949631\n",
      "Min loss: 0.008561212569475174\n",
      "Mean loss: 0.022643415567775566\n",
      "Std loss: 0.01125920818902621\n",
      "Total Loss: 0.1358604934066534\n",
      "------------------------------------ epoch 2784 (16698 steps) ------------------------------------\n",
      "Max loss: 0.05567903444170952\n",
      "Min loss: 0.00901423953473568\n",
      "Mean loss: 0.02203480200842023\n",
      "Std loss: 0.016134900840666114\n",
      "Total Loss: 0.13220881205052137\n",
      "------------------------------------ epoch 2785 (16704 steps) ------------------------------------\n",
      "Max loss: 0.02681395411491394\n",
      "Min loss: 0.017582911998033524\n",
      "Mean loss: 0.021684440473715465\n",
      "Std loss: 0.003119736748360086\n",
      "Total Loss: 0.13010664284229279\n",
      "------------------------------------ epoch 2786 (16710 steps) ------------------------------------\n",
      "Max loss: 0.028885258361697197\n",
      "Min loss: 0.012120175175368786\n",
      "Mean loss: 0.021037050678084295\n",
      "Std loss: 0.005834379362384103\n",
      "Total Loss: 0.12622230406850576\n",
      "------------------------------------ epoch 2787 (16716 steps) ------------------------------------\n",
      "Max loss: 0.03052576631307602\n",
      "Min loss: 0.016038548201322556\n",
      "Mean loss: 0.02091541886329651\n",
      "Std loss: 0.00486877025176677\n",
      "Total Loss: 0.12549251317977905\n",
      "------------------------------------ epoch 2788 (16722 steps) ------------------------------------\n",
      "Max loss: 0.05619226023554802\n",
      "Min loss: 0.01027920376509428\n",
      "Mean loss: 0.02595914574339986\n",
      "Std loss: 0.016377343756478865\n",
      "Total Loss: 0.15575487446039915\n",
      "------------------------------------ epoch 2789 (16728 steps) ------------------------------------\n",
      "Max loss: 0.038972966372966766\n",
      "Min loss: 0.011140118353068829\n",
      "Mean loss: 0.023076552897691727\n",
      "Std loss: 0.009041230332390557\n",
      "Total Loss: 0.13845931738615036\n",
      "------------------------------------ epoch 2790 (16734 steps) ------------------------------------\n",
      "Max loss: 0.04676089435815811\n",
      "Min loss: 0.014821003191173077\n",
      "Mean loss: 0.023508699145168066\n",
      "Std loss: 0.011011243465017414\n",
      "Total Loss: 0.1410521948710084\n",
      "------------------------------------ epoch 2791 (16740 steps) ------------------------------------\n",
      "Max loss: 0.01848439872264862\n",
      "Min loss: 0.01227080263197422\n",
      "Mean loss: 0.015147220188130936\n",
      "Std loss: 0.002541219925203315\n",
      "Total Loss: 0.09088332112878561\n",
      "------------------------------------ epoch 2792 (16746 steps) ------------------------------------\n",
      "Max loss: 0.018902231007814407\n",
      "Min loss: 0.010287120938301086\n",
      "Mean loss: 0.014763301393638054\n",
      "Std loss: 0.0032112752076021347\n",
      "Total Loss: 0.08857980836182833\n",
      "------------------------------------ epoch 2793 (16752 steps) ------------------------------------\n",
      "Max loss: 0.021587498486042023\n",
      "Min loss: 0.008436713367700577\n",
      "Mean loss: 0.01489271316677332\n",
      "Std loss: 0.004918071869519333\n",
      "Total Loss: 0.08935627900063992\n",
      "------------------------------------ epoch 2794 (16758 steps) ------------------------------------\n",
      "Max loss: 0.03638032078742981\n",
      "Min loss: 0.008792772889137268\n",
      "Mean loss: 0.01786612377812465\n",
      "Std loss: 0.009225452188663566\n",
      "Total Loss: 0.1071967426687479\n",
      "------------------------------------ epoch 2795 (16764 steps) ------------------------------------\n",
      "Max loss: 0.01534293219447136\n",
      "Min loss: 0.007498852908611298\n",
      "Mean loss: 0.011338064912706614\n",
      "Std loss: 0.002409726942561943\n",
      "Total Loss: 0.06802838947623968\n",
      "------------------------------------ epoch 2796 (16770 steps) ------------------------------------\n",
      "Max loss: 0.026384159922599792\n",
      "Min loss: 0.009961633011698723\n",
      "Mean loss: 0.01641614083200693\n",
      "Std loss: 0.005845788639277821\n",
      "Total Loss: 0.09849684499204159\n",
      "------------------------------------ epoch 2797 (16776 steps) ------------------------------------\n",
      "Max loss: 0.030960528180003166\n",
      "Min loss: 0.00869791954755783\n",
      "Mean loss: 0.0222860857223471\n",
      "Std loss: 0.006794798147943065\n",
      "Total Loss: 0.1337165143340826\n",
      "------------------------------------ epoch 2798 (16782 steps) ------------------------------------\n",
      "Max loss: 0.04393316060304642\n",
      "Min loss: 0.010458128526806831\n",
      "Mean loss: 0.020616955123841763\n",
      "Std loss: 0.011081328013387228\n",
      "Total Loss: 0.12370173074305058\n",
      "------------------------------------ epoch 2799 (16788 steps) ------------------------------------\n",
      "Max loss: 0.028526483103632927\n",
      "Min loss: 0.007939619943499565\n",
      "Mean loss: 0.01578863974039753\n",
      "Std loss: 0.008661717600596421\n",
      "Total Loss: 0.0947318384423852\n",
      "------------------------------------ epoch 2800 (16794 steps) ------------------------------------\n",
      "Max loss: 0.019189927726984024\n",
      "Min loss: 0.010093709453940392\n",
      "Mean loss: 0.014647677230338255\n",
      "Std loss: 0.002888141330623748\n",
      "Total Loss: 0.08788606338202953\n",
      "------------------------------------ epoch 2801 (16800 steps) ------------------------------------\n",
      "Max loss: 0.04815825819969177\n",
      "Min loss: 0.013918474316596985\n",
      "Mean loss: 0.023379819312443335\n",
      "Std loss: 0.012182677001042247\n",
      "Total Loss: 0.14027891587466002\n",
      "saved model at ./weights/model_2801.pth\n",
      "------------------------------------ epoch 2802 (16806 steps) ------------------------------------\n",
      "Max loss: 0.050875768065452576\n",
      "Min loss: 0.010738348588347435\n",
      "Mean loss: 0.022207987494766712\n",
      "Std loss: 0.013214219688301053\n",
      "Total Loss: 0.13324792496860027\n",
      "------------------------------------ epoch 2803 (16812 steps) ------------------------------------\n",
      "Max loss: 0.03279147669672966\n",
      "Min loss: 0.010079950094223022\n",
      "Mean loss: 0.016696110057334106\n",
      "Std loss: 0.007904233945733762\n",
      "Total Loss: 0.10017666034400463\n",
      "------------------------------------ epoch 2804 (16818 steps) ------------------------------------\n",
      "Max loss: 0.027321919798851013\n",
      "Min loss: 0.011107221245765686\n",
      "Mean loss: 0.019081221893429756\n",
      "Std loss: 0.005086167425715464\n",
      "Total Loss: 0.11448733136057854\n",
      "------------------------------------ epoch 2805 (16824 steps) ------------------------------------\n",
      "Max loss: 0.026040125638246536\n",
      "Min loss: 0.010805526748299599\n",
      "Mean loss: 0.017857305395106476\n",
      "Std loss: 0.005495440171097425\n",
      "Total Loss: 0.10714383237063885\n",
      "------------------------------------ epoch 2806 (16830 steps) ------------------------------------\n",
      "Max loss: 0.01954358071088791\n",
      "Min loss: 0.01008935272693634\n",
      "Mean loss: 0.013733996544033289\n",
      "Std loss: 0.003086644807892497\n",
      "Total Loss: 0.08240397926419973\n",
      "------------------------------------ epoch 2807 (16836 steps) ------------------------------------\n",
      "Max loss: 0.017287012189626694\n",
      "Min loss: 0.008733164519071579\n",
      "Mean loss: 0.013780393172055483\n",
      "Std loss: 0.002860103841638505\n",
      "Total Loss: 0.0826823590323329\n",
      "------------------------------------ epoch 2808 (16842 steps) ------------------------------------\n",
      "Max loss: 0.024007197469472885\n",
      "Min loss: 0.010334632359445095\n",
      "Mean loss: 0.016533597527692716\n",
      "Std loss: 0.004802312984276458\n",
      "Total Loss: 0.09920158516615629\n",
      "------------------------------------ epoch 2809 (16848 steps) ------------------------------------\n",
      "Max loss: 0.03168196603655815\n",
      "Min loss: 0.007647302933037281\n",
      "Mean loss: 0.01682283605138461\n",
      "Std loss: 0.009773093248442395\n",
      "Total Loss: 0.10093701630830765\n",
      "------------------------------------ epoch 2810 (16854 steps) ------------------------------------\n",
      "Max loss: 0.03636925667524338\n",
      "Min loss: 0.008484783582389355\n",
      "Mean loss: 0.014527573095013699\n",
      "Std loss: 0.009895576641910445\n",
      "Total Loss: 0.08716543857008219\n",
      "------------------------------------ epoch 2811 (16860 steps) ------------------------------------\n",
      "Max loss: 0.038414329290390015\n",
      "Min loss: 0.009581160731613636\n",
      "Mean loss: 0.016461564227938652\n",
      "Std loss: 0.009993861391489531\n",
      "Total Loss: 0.09876938536763191\n",
      "------------------------------------ epoch 2812 (16866 steps) ------------------------------------\n",
      "Max loss: 0.049591392278671265\n",
      "Min loss: 0.010737642645835876\n",
      "Mean loss: 0.028224097875257332\n",
      "Std loss: 0.014179813352598979\n",
      "Total Loss: 0.169344587251544\n",
      "------------------------------------ epoch 2813 (16872 steps) ------------------------------------\n",
      "Max loss: 0.03294292837381363\n",
      "Min loss: 0.017857663333415985\n",
      "Mean loss: 0.027067709093292553\n",
      "Std loss: 0.0059022697760809815\n",
      "Total Loss: 0.16240625455975533\n",
      "------------------------------------ epoch 2814 (16878 steps) ------------------------------------\n",
      "Max loss: 0.07389219850301743\n",
      "Min loss: 0.01169298030436039\n",
      "Mean loss: 0.030783423222601414\n",
      "Std loss: 0.021248034433517995\n",
      "Total Loss: 0.18470053933560848\n",
      "------------------------------------ epoch 2815 (16884 steps) ------------------------------------\n",
      "Max loss: 0.02345304749906063\n",
      "Min loss: 0.008406521752476692\n",
      "Mean loss: 0.01774086772153775\n",
      "Std loss: 0.004817511923407544\n",
      "Total Loss: 0.1064452063292265\n",
      "------------------------------------ epoch 2816 (16890 steps) ------------------------------------\n",
      "Max loss: 0.033891256898641586\n",
      "Min loss: 0.008912383578717709\n",
      "Mean loss: 0.01731682134171327\n",
      "Std loss: 0.008238700843526051\n",
      "Total Loss: 0.10390092805027962\n",
      "------------------------------------ epoch 2817 (16896 steps) ------------------------------------\n",
      "Max loss: 0.024722963571548462\n",
      "Min loss: 0.00967262964695692\n",
      "Mean loss: 0.019465064785132807\n",
      "Std loss: 0.0048192962764170835\n",
      "Total Loss: 0.11679038871079683\n",
      "------------------------------------ epoch 2818 (16902 steps) ------------------------------------\n",
      "Max loss: 0.03554336726665497\n",
      "Min loss: 0.01344861090183258\n",
      "Mean loss: 0.019511371850967407\n",
      "Std loss: 0.007914981163740082\n",
      "Total Loss: 0.11706823110580444\n",
      "------------------------------------ epoch 2819 (16908 steps) ------------------------------------\n",
      "Max loss: 0.04215259477496147\n",
      "Min loss: 0.007923434488475323\n",
      "Mean loss: 0.017761489376425743\n",
      "Std loss: 0.012181076632331691\n",
      "Total Loss: 0.10656893625855446\n",
      "------------------------------------ epoch 2820 (16914 steps) ------------------------------------\n",
      "Max loss: 0.051879048347473145\n",
      "Min loss: 0.0074214451014995575\n",
      "Mean loss: 0.01726237777620554\n",
      "Std loss: 0.015674120229041008\n",
      "Total Loss: 0.10357426665723324\n",
      "------------------------------------ epoch 2821 (16920 steps) ------------------------------------\n",
      "Max loss: 0.02556738816201687\n",
      "Min loss: 0.00986492820084095\n",
      "Mean loss: 0.015596199315041304\n",
      "Std loss: 0.0055916709405469875\n",
      "Total Loss: 0.09357719589024782\n",
      "------------------------------------ epoch 2822 (16926 steps) ------------------------------------\n",
      "Max loss: 0.015980353578925133\n",
      "Min loss: 0.007598997093737125\n",
      "Mean loss: 0.01100492523983121\n",
      "Std loss: 0.0028079001155100467\n",
      "Total Loss: 0.06602955143898726\n",
      "------------------------------------ epoch 2823 (16932 steps) ------------------------------------\n",
      "Max loss: 0.07569953799247742\n",
      "Min loss: 0.008336750790476799\n",
      "Mean loss: 0.03433310271551212\n",
      "Std loss: 0.02139955134819516\n",
      "Total Loss: 0.2059986162930727\n",
      "------------------------------------ epoch 2824 (16938 steps) ------------------------------------\n",
      "Max loss: 0.04754357039928436\n",
      "Min loss: 0.016116438433527946\n",
      "Mean loss: 0.027438327049215634\n",
      "Std loss: 0.009965320786000431\n",
      "Total Loss: 0.1646299622952938\n",
      "------------------------------------ epoch 2825 (16944 steps) ------------------------------------\n",
      "Max loss: 0.035609494894742966\n",
      "Min loss: 0.01699836179614067\n",
      "Mean loss: 0.024191657391687233\n",
      "Std loss: 0.006303175125049204\n",
      "Total Loss: 0.1451499443501234\n",
      "------------------------------------ epoch 2826 (16950 steps) ------------------------------------\n",
      "Max loss: 0.026092205196619034\n",
      "Min loss: 0.012651692144572735\n",
      "Mean loss: 0.020022881372521322\n",
      "Std loss: 0.00493410044487758\n",
      "Total Loss: 0.12013728823512793\n",
      "------------------------------------ epoch 2827 (16956 steps) ------------------------------------\n",
      "Max loss: 0.021843453869223595\n",
      "Min loss: 0.011397510766983032\n",
      "Mean loss: 0.01660287380218506\n",
      "Std loss: 0.0032924766863988786\n",
      "Total Loss: 0.09961724281311035\n",
      "------------------------------------ epoch 2828 (16962 steps) ------------------------------------\n",
      "Max loss: 0.04071565717458725\n",
      "Min loss: 0.010299453511834145\n",
      "Mean loss: 0.021019593812525272\n",
      "Std loss: 0.01006365160203276\n",
      "Total Loss: 0.12611756287515163\n",
      "------------------------------------ epoch 2829 (16968 steps) ------------------------------------\n",
      "Max loss: 0.028213029727339745\n",
      "Min loss: 0.009555540978908539\n",
      "Mean loss: 0.015344673302024603\n",
      "Std loss: 0.006448719524309964\n",
      "Total Loss: 0.09206803981214762\n",
      "------------------------------------ epoch 2830 (16974 steps) ------------------------------------\n",
      "Max loss: 0.0268331877887249\n",
      "Min loss: 0.007753661833703518\n",
      "Mean loss: 0.016447845691194136\n",
      "Std loss: 0.005998533061389362\n",
      "Total Loss: 0.09868707414716482\n",
      "------------------------------------ epoch 2831 (16980 steps) ------------------------------------\n",
      "Max loss: 0.04275275394320488\n",
      "Min loss: 0.00953768566250801\n",
      "Mean loss: 0.019680141160885494\n",
      "Std loss: 0.011145119271907337\n",
      "Total Loss: 0.11808084696531296\n",
      "------------------------------------ epoch 2832 (16986 steps) ------------------------------------\n",
      "Max loss: 0.03840308636426926\n",
      "Min loss: 0.008954797871410847\n",
      "Mean loss: 0.01636393244067828\n",
      "Std loss: 0.009997683670606562\n",
      "Total Loss: 0.09818359464406967\n",
      "------------------------------------ epoch 2833 (16992 steps) ------------------------------------\n",
      "Max loss: 0.026250822469592094\n",
      "Min loss: 0.013505074195563793\n",
      "Mean loss: 0.017242219299077988\n",
      "Std loss: 0.004185486491949089\n",
      "Total Loss: 0.10345331579446793\n",
      "------------------------------------ epoch 2834 (16998 steps) ------------------------------------\n",
      "Max loss: 0.021722160279750824\n",
      "Min loss: 0.009558324702084064\n",
      "Mean loss: 0.0147414345604678\n",
      "Std loss: 0.0039278287092034065\n",
      "Total Loss: 0.0884486073628068\n",
      "------------------------------------ epoch 2835 (17004 steps) ------------------------------------\n",
      "Max loss: 0.03126053512096405\n",
      "Min loss: 0.009695848450064659\n",
      "Mean loss: 0.014974483599265417\n",
      "Std loss: 0.0074168349053599005\n",
      "Total Loss: 0.0898469015955925\n",
      "------------------------------------ epoch 2836 (17010 steps) ------------------------------------\n",
      "Max loss: 0.036888547241687775\n",
      "Min loss: 0.008324936032295227\n",
      "Mean loss: 0.019201540543387335\n",
      "Std loss: 0.01132117825088387\n",
      "Total Loss: 0.115209243260324\n",
      "------------------------------------ epoch 2837 (17016 steps) ------------------------------------\n",
      "Max loss: 0.037081725895404816\n",
      "Min loss: 0.01303080003708601\n",
      "Mean loss: 0.022298251123478014\n",
      "Std loss: 0.009633624543855853\n",
      "Total Loss: 0.1337895067408681\n",
      "------------------------------------ epoch 2838 (17022 steps) ------------------------------------\n",
      "Max loss: 0.042371056973934174\n",
      "Min loss: 0.009266715496778488\n",
      "Mean loss: 0.019051037573566038\n",
      "Std loss: 0.011434085961111484\n",
      "Total Loss: 0.11430622544139624\n",
      "------------------------------------ epoch 2839 (17028 steps) ------------------------------------\n",
      "Max loss: 0.03222619742155075\n",
      "Min loss: 0.014484236016869545\n",
      "Mean loss: 0.022391210310161114\n",
      "Std loss: 0.005546792969304867\n",
      "Total Loss: 0.13434726186096668\n",
      "------------------------------------ epoch 2840 (17034 steps) ------------------------------------\n",
      "Max loss: 0.01772746443748474\n",
      "Min loss: 0.011475450359284878\n",
      "Mean loss: 0.014485869711885849\n",
      "Std loss: 0.0026324726693301875\n",
      "Total Loss: 0.0869152182713151\n",
      "------------------------------------ epoch 2841 (17040 steps) ------------------------------------\n",
      "Max loss: 0.02936778962612152\n",
      "Min loss: 0.010037347674369812\n",
      "Mean loss: 0.01962123780200879\n",
      "Std loss: 0.005811443457407189\n",
      "Total Loss: 0.11772742681205273\n",
      "------------------------------------ epoch 2842 (17046 steps) ------------------------------------\n",
      "Max loss: 0.018180884420871735\n",
      "Min loss: 0.010414252057671547\n",
      "Mean loss: 0.014335945714265108\n",
      "Std loss: 0.0029998985854335267\n",
      "Total Loss: 0.08601567428559065\n",
      "------------------------------------ epoch 2843 (17052 steps) ------------------------------------\n",
      "Max loss: 0.024171989411115646\n",
      "Min loss: 0.009236007928848267\n",
      "Mean loss: 0.013094887292633453\n",
      "Std loss: 0.0051607451664829225\n",
      "Total Loss: 0.07856932375580072\n",
      "------------------------------------ epoch 2844 (17058 steps) ------------------------------------\n",
      "Max loss: 0.031041797250509262\n",
      "Min loss: 0.0076822321861982346\n",
      "Mean loss: 0.01799114467576146\n",
      "Std loss: 0.008859376032135442\n",
      "Total Loss: 0.10794686805456877\n",
      "------------------------------------ epoch 2845 (17064 steps) ------------------------------------\n",
      "Max loss: 0.02997136488556862\n",
      "Min loss: 0.013923089019954205\n",
      "Mean loss: 0.01937218088035782\n",
      "Std loss: 0.005184900501498147\n",
      "Total Loss: 0.11623308528214693\n",
      "------------------------------------ epoch 2846 (17070 steps) ------------------------------------\n",
      "Max loss: 0.03041006252169609\n",
      "Min loss: 0.008371463045477867\n",
      "Mean loss: 0.013839728819827238\n",
      "Std loss: 0.007510383360439228\n",
      "Total Loss: 0.08303837291896343\n",
      "------------------------------------ epoch 2847 (17076 steps) ------------------------------------\n",
      "Max loss: 0.030360601842403412\n",
      "Min loss: 0.009393768385052681\n",
      "Mean loss: 0.016136346850544214\n",
      "Std loss: 0.008049445270522974\n",
      "Total Loss: 0.09681808110326529\n",
      "------------------------------------ epoch 2848 (17082 steps) ------------------------------------\n",
      "Max loss: 0.05753492936491966\n",
      "Min loss: 0.011954016983509064\n",
      "Mean loss: 0.02369716390967369\n",
      "Std loss: 0.015802734471275226\n",
      "Total Loss: 0.14218298345804214\n",
      "------------------------------------ epoch 2849 (17088 steps) ------------------------------------\n",
      "Max loss: 0.051118336617946625\n",
      "Min loss: 0.009422603994607925\n",
      "Mean loss: 0.022736168311287958\n",
      "Std loss: 0.014188136015070933\n",
      "Total Loss: 0.13641700986772776\n",
      "------------------------------------ epoch 2850 (17094 steps) ------------------------------------\n",
      "Max loss: 0.02214866690337658\n",
      "Min loss: 0.009457769803702831\n",
      "Mean loss: 0.012119649909436703\n",
      "Std loss: 0.004522841365402475\n",
      "Total Loss: 0.07271789945662022\n",
      "------------------------------------ epoch 2851 (17100 steps) ------------------------------------\n",
      "Max loss: 0.027666356414556503\n",
      "Min loss: 0.013998948037624359\n",
      "Mean loss: 0.02083188621327281\n",
      "Std loss: 0.005400165302444026\n",
      "Total Loss: 0.12499131727963686\n",
      "------------------------------------ epoch 2852 (17106 steps) ------------------------------------\n",
      "Max loss: 0.03253926336765289\n",
      "Min loss: 0.008703378960490227\n",
      "Mean loss: 0.018694099970161915\n",
      "Std loss: 0.007638860535726433\n",
      "Total Loss: 0.11216459982097149\n",
      "------------------------------------ epoch 2853 (17112 steps) ------------------------------------\n",
      "Max loss: 0.022733762860298157\n",
      "Min loss: 0.008112364448606968\n",
      "Mean loss: 0.012610594586779674\n",
      "Std loss: 0.005204158098364727\n",
      "Total Loss: 0.07566356752067804\n",
      "------------------------------------ epoch 2854 (17118 steps) ------------------------------------\n",
      "Max loss: 0.030846774578094482\n",
      "Min loss: 0.009373139590024948\n",
      "Mean loss: 0.018066367289672296\n",
      "Std loss: 0.007756629198404336\n",
      "Total Loss: 0.10839820373803377\n",
      "------------------------------------ epoch 2855 (17124 steps) ------------------------------------\n",
      "Max loss: 0.03712399676442146\n",
      "Min loss: 0.010278713889420033\n",
      "Mean loss: 0.02411018017058571\n",
      "Std loss: 0.009691862849750085\n",
      "Total Loss: 0.14466108102351427\n",
      "------------------------------------ epoch 2856 (17130 steps) ------------------------------------\n",
      "Max loss: 0.022806650027632713\n",
      "Min loss: 0.010859168134629726\n",
      "Mean loss: 0.014876545406877995\n",
      "Std loss: 0.004714265977281886\n",
      "Total Loss: 0.08925927244126797\n",
      "------------------------------------ epoch 2857 (17136 steps) ------------------------------------\n",
      "Max loss: 0.019356433302164078\n",
      "Min loss: 0.00788207072764635\n",
      "Mean loss: 0.013021115213632584\n",
      "Std loss: 0.003528905188882847\n",
      "Total Loss: 0.0781266912817955\n",
      "------------------------------------ epoch 2858 (17142 steps) ------------------------------------\n",
      "Max loss: 0.03003658354282379\n",
      "Min loss: 0.009281240403652191\n",
      "Mean loss: 0.01730376047392686\n",
      "Std loss: 0.008009616416740454\n",
      "Total Loss: 0.10382256284356117\n",
      "------------------------------------ epoch 2859 (17148 steps) ------------------------------------\n",
      "Max loss: 0.030572816729545593\n",
      "Min loss: 0.009265787899494171\n",
      "Mean loss: 0.018362396241476137\n",
      "Std loss: 0.007953912557838243\n",
      "Total Loss: 0.11017437744885683\n",
      "------------------------------------ epoch 2860 (17154 steps) ------------------------------------\n",
      "Max loss: 0.019917801022529602\n",
      "Min loss: 0.0074507081881165504\n",
      "Mean loss: 0.01243055115143458\n",
      "Std loss: 0.004045096037333628\n",
      "Total Loss: 0.07458330690860748\n",
      "------------------------------------ epoch 2861 (17160 steps) ------------------------------------\n",
      "Max loss: 0.01961798593401909\n",
      "Min loss: 0.0076682670041918755\n",
      "Mean loss: 0.012484670151025057\n",
      "Std loss: 0.00437328278496669\n",
      "Total Loss: 0.07490802090615034\n",
      "------------------------------------ epoch 2862 (17166 steps) ------------------------------------\n",
      "Max loss: 0.03867173194885254\n",
      "Min loss: 0.008422314189374447\n",
      "Mean loss: 0.017993283302833635\n",
      "Std loss: 0.011071547828183788\n",
      "Total Loss: 0.10795969981700182\n",
      "------------------------------------ epoch 2863 (17172 steps) ------------------------------------\n",
      "Max loss: 0.02144523710012436\n",
      "Min loss: 0.008517513982951641\n",
      "Mean loss: 0.013139519219597181\n",
      "Std loss: 0.00404206411641857\n",
      "Total Loss: 0.07883711531758308\n",
      "------------------------------------ epoch 2864 (17178 steps) ------------------------------------\n",
      "Max loss: 0.03206184133887291\n",
      "Min loss: 0.009296302683651447\n",
      "Mean loss: 0.017141228076070547\n",
      "Std loss: 0.008077480741429657\n",
      "Total Loss: 0.10284736845642328\n",
      "------------------------------------ epoch 2865 (17184 steps) ------------------------------------\n",
      "Max loss: 0.03801310062408447\n",
      "Min loss: 0.010916756466031075\n",
      "Mean loss: 0.018107968537757795\n",
      "Std loss: 0.009226259186722211\n",
      "Total Loss: 0.10864781122654676\n",
      "------------------------------------ epoch 2866 (17190 steps) ------------------------------------\n",
      "Max loss: 0.03133530169725418\n",
      "Min loss: 0.007957397028803825\n",
      "Mean loss: 0.017365087134142716\n",
      "Std loss: 0.007733926979567146\n",
      "Total Loss: 0.1041905228048563\n",
      "------------------------------------ epoch 2867 (17196 steps) ------------------------------------\n",
      "Max loss: 0.015636103227734566\n",
      "Min loss: 0.00787929818034172\n",
      "Mean loss: 0.011134081054478884\n",
      "Std loss: 0.00252319134582213\n",
      "Total Loss: 0.0668044863268733\n",
      "------------------------------------ epoch 2868 (17202 steps) ------------------------------------\n",
      "Max loss: 0.027781736105680466\n",
      "Min loss: 0.008052101358771324\n",
      "Mean loss: 0.015601411617050568\n",
      "Std loss: 0.00768873509101734\n",
      "Total Loss: 0.09360846970230341\n",
      "------------------------------------ epoch 2869 (17208 steps) ------------------------------------\n",
      "Max loss: 0.023822685703635216\n",
      "Min loss: 0.007181285880506039\n",
      "Mean loss: 0.014399055081109205\n",
      "Std loss: 0.005907907084134979\n",
      "Total Loss: 0.08639433048665524\n",
      "------------------------------------ epoch 2870 (17214 steps) ------------------------------------\n",
      "Max loss: 0.035658348351716995\n",
      "Min loss: 0.012776853516697884\n",
      "Mean loss: 0.018003510156025488\n",
      "Std loss: 0.00801270424959508\n",
      "Total Loss: 0.10802106093615294\n",
      "------------------------------------ epoch 2871 (17220 steps) ------------------------------------\n",
      "Max loss: 0.07028554379940033\n",
      "Min loss: 0.008617676794528961\n",
      "Mean loss: 0.02285791498919328\n",
      "Std loss: 0.021590600035509416\n",
      "Total Loss: 0.13714748993515968\n",
      "------------------------------------ epoch 2872 (17226 steps) ------------------------------------\n",
      "Max loss: 0.019173528999090195\n",
      "Min loss: 0.00855238363146782\n",
      "Mean loss: 0.014381016449381908\n",
      "Std loss: 0.0038502952866063633\n",
      "Total Loss: 0.08628609869629145\n",
      "------------------------------------ epoch 2873 (17232 steps) ------------------------------------\n",
      "Max loss: 0.021429790183901787\n",
      "Min loss: 0.008593486621975899\n",
      "Mean loss: 0.011885930628826221\n",
      "Std loss: 0.004345761878685863\n",
      "Total Loss: 0.07131558377295732\n",
      "------------------------------------ epoch 2874 (17238 steps) ------------------------------------\n",
      "Max loss: 0.045289039611816406\n",
      "Min loss: 0.008477013558149338\n",
      "Mean loss: 0.020125914365053177\n",
      "Std loss: 0.012573631953289043\n",
      "Total Loss: 0.12075548619031906\n",
      "------------------------------------ epoch 2875 (17244 steps) ------------------------------------\n",
      "Max loss: 0.032523415982723236\n",
      "Min loss: 0.013782449066638947\n",
      "Mean loss: 0.022609030827879906\n",
      "Std loss: 0.005765548136706363\n",
      "Total Loss: 0.13565418496727943\n",
      "------------------------------------ epoch 2876 (17250 steps) ------------------------------------\n",
      "Max loss: 0.0461331307888031\n",
      "Min loss: 0.0084705064073205\n",
      "Mean loss: 0.02227147715166211\n",
      "Std loss: 0.012475226310697723\n",
      "Total Loss: 0.13362886290997267\n",
      "------------------------------------ epoch 2877 (17256 steps) ------------------------------------\n",
      "Max loss: 0.023718837648630142\n",
      "Min loss: 0.01219035591930151\n",
      "Mean loss: 0.014744287977615992\n",
      "Std loss: 0.004099940769888794\n",
      "Total Loss: 0.08846572786569595\n",
      "------------------------------------ epoch 2878 (17262 steps) ------------------------------------\n",
      "Max loss: 0.09495809674263\n",
      "Min loss: 0.0094322944059968\n",
      "Mean loss: 0.027431081049144268\n",
      "Std loss: 0.030831139771085366\n",
      "Total Loss: 0.1645864862948656\n",
      "------------------------------------ epoch 2879 (17268 steps) ------------------------------------\n",
      "Max loss: 0.026625139638781548\n",
      "Min loss: 0.009632254019379616\n",
      "Mean loss: 0.016944416022549074\n",
      "Std loss: 0.0066216141406281745\n",
      "Total Loss: 0.10166649613529444\n",
      "------------------------------------ epoch 2880 (17274 steps) ------------------------------------\n",
      "Max loss: 0.01694411039352417\n",
      "Min loss: 0.008198757655918598\n",
      "Mean loss: 0.012551937097062668\n",
      "Std loss: 0.0026803041026067023\n",
      "Total Loss: 0.075311622582376\n",
      "------------------------------------ epoch 2881 (17280 steps) ------------------------------------\n",
      "Max loss: 0.02084003947675228\n",
      "Min loss: 0.008835751563310623\n",
      "Mean loss: 0.012072605857004723\n",
      "Std loss: 0.00408214786840016\n",
      "Total Loss: 0.07243563514202833\n",
      "------------------------------------ epoch 2882 (17286 steps) ------------------------------------\n",
      "Max loss: 0.02544374018907547\n",
      "Min loss: 0.009257499128580093\n",
      "Mean loss: 0.015468529270341\n",
      "Std loss: 0.005362396794191859\n",
      "Total Loss: 0.092811175622046\n",
      "------------------------------------ epoch 2883 (17292 steps) ------------------------------------\n",
      "Max loss: 0.028868407011032104\n",
      "Min loss: 0.01018587313592434\n",
      "Mean loss: 0.016629482464243967\n",
      "Std loss: 0.00642372048162486\n",
      "Total Loss: 0.09977689478546381\n",
      "------------------------------------ epoch 2884 (17298 steps) ------------------------------------\n",
      "Max loss: 0.022033562883734703\n",
      "Min loss: 0.008886621333658695\n",
      "Mean loss: 0.014999888992557922\n",
      "Std loss: 0.005103134179335114\n",
      "Total Loss: 0.08999933395534754\n",
      "------------------------------------ epoch 2885 (17304 steps) ------------------------------------\n",
      "Max loss: 0.03749758005142212\n",
      "Min loss: 0.008369943127036095\n",
      "Mean loss: 0.016183741701145966\n",
      "Std loss: 0.010010515897408985\n",
      "Total Loss: 0.0971024502068758\n",
      "------------------------------------ epoch 2886 (17310 steps) ------------------------------------\n",
      "Max loss: 0.03129894658923149\n",
      "Min loss: 0.009079228155314922\n",
      "Mean loss: 0.020013671989242237\n",
      "Std loss: 0.007761299962989657\n",
      "Total Loss: 0.12008203193545341\n",
      "------------------------------------ epoch 2887 (17316 steps) ------------------------------------\n",
      "Max loss: 0.023109987378120422\n",
      "Min loss: 0.00826913584023714\n",
      "Mean loss: 0.013361820175002018\n",
      "Std loss: 0.004890725940448474\n",
      "Total Loss: 0.08017092105001211\n",
      "------------------------------------ epoch 2888 (17322 steps) ------------------------------------\n",
      "Max loss: 0.04075241833925247\n",
      "Min loss: 0.008764857426285744\n",
      "Mean loss: 0.023752157265941303\n",
      "Std loss: 0.011407147976646928\n",
      "Total Loss: 0.1425129435956478\n",
      "------------------------------------ epoch 2889 (17328 steps) ------------------------------------\n",
      "Max loss: 0.05108019337058067\n",
      "Min loss: 0.007912958040833473\n",
      "Mean loss: 0.02727854282905658\n",
      "Std loss: 0.013602825292718316\n",
      "Total Loss: 0.16367125697433949\n",
      "------------------------------------ epoch 2890 (17334 steps) ------------------------------------\n",
      "Max loss: 0.027162348851561546\n",
      "Min loss: 0.008331860415637493\n",
      "Mean loss: 0.015381628802667061\n",
      "Std loss: 0.006385742595817379\n",
      "Total Loss: 0.09228977281600237\n",
      "------------------------------------ epoch 2891 (17340 steps) ------------------------------------\n",
      "Max loss: 0.0393730066716671\n",
      "Min loss: 0.008274901658296585\n",
      "Mean loss: 0.01737303597231706\n",
      "Std loss: 0.010370070626159073\n",
      "Total Loss: 0.10423821583390236\n",
      "------------------------------------ epoch 2892 (17346 steps) ------------------------------------\n",
      "Max loss: 0.036909379065036774\n",
      "Min loss: 0.0070710694417357445\n",
      "Mean loss: 0.017359879178305466\n",
      "Std loss: 0.009684050683893266\n",
      "Total Loss: 0.1041592750698328\n",
      "------------------------------------ epoch 2893 (17352 steps) ------------------------------------\n",
      "Max loss: 0.03746325150132179\n",
      "Min loss: 0.015643175691366196\n",
      "Mean loss: 0.022162719008823235\n",
      "Std loss: 0.007606257003386044\n",
      "Total Loss: 0.13297631405293941\n",
      "------------------------------------ epoch 2894 (17358 steps) ------------------------------------\n",
      "Max loss: 0.0219394750893116\n",
      "Min loss: 0.009316032752394676\n",
      "Mean loss: 0.015839479863643646\n",
      "Std loss: 0.004793915486712952\n",
      "Total Loss: 0.09503687918186188\n",
      "------------------------------------ epoch 2895 (17364 steps) ------------------------------------\n",
      "Max loss: 0.03297494351863861\n",
      "Min loss: 0.008691178634762764\n",
      "Mean loss: 0.017959611645589273\n",
      "Std loss: 0.010682454266984411\n",
      "Total Loss: 0.10775766987353563\n",
      "------------------------------------ epoch 2896 (17370 steps) ------------------------------------\n",
      "Max loss: 0.013471144251525402\n",
      "Min loss: 0.009420400485396385\n",
      "Mean loss: 0.011551028738419214\n",
      "Std loss: 0.0015789983526925471\n",
      "Total Loss: 0.06930617243051529\n",
      "------------------------------------ epoch 2897 (17376 steps) ------------------------------------\n",
      "Max loss: 0.028495587408542633\n",
      "Min loss: 0.012120786122977734\n",
      "Mean loss: 0.01653452745328347\n",
      "Std loss: 0.005567135165042141\n",
      "Total Loss: 0.09920716471970081\n",
      "------------------------------------ epoch 2898 (17382 steps) ------------------------------------\n",
      "Max loss: 0.03312323987483978\n",
      "Min loss: 0.0078808031976223\n",
      "Mean loss: 0.01812710256005327\n",
      "Std loss: 0.009291693611868445\n",
      "Total Loss: 0.10876261536031961\n",
      "------------------------------------ epoch 2899 (17388 steps) ------------------------------------\n",
      "Max loss: 0.013723201118409634\n",
      "Min loss: 0.008710901252925396\n",
      "Mean loss: 0.011351014487445354\n",
      "Std loss: 0.0016283537061216379\n",
      "Total Loss: 0.06810608692467213\n",
      "------------------------------------ epoch 2900 (17394 steps) ------------------------------------\n",
      "Max loss: 0.060148075222969055\n",
      "Min loss: 0.013232789002358913\n",
      "Mean loss: 0.024625080171972513\n",
      "Std loss: 0.01613517208878828\n",
      "Total Loss: 0.14775048103183508\n",
      "------------------------------------ epoch 2901 (17400 steps) ------------------------------------\n",
      "Max loss: 0.04616611450910568\n",
      "Min loss: 0.011036048643290997\n",
      "Mean loss: 0.02200251988445719\n",
      "Std loss: 0.01140617941078453\n",
      "Total Loss: 0.13201511930674314\n",
      "saved model at ./weights/model_2901.pth\n",
      "------------------------------------ epoch 2902 (17406 steps) ------------------------------------\n",
      "Max loss: 0.0707525908946991\n",
      "Min loss: 0.008556704968214035\n",
      "Mean loss: 0.02805444101492564\n",
      "Std loss: 0.021171755581402503\n",
      "Total Loss: 0.16832664608955383\n",
      "------------------------------------ epoch 2903 (17412 steps) ------------------------------------\n",
      "Max loss: 0.023645108565688133\n",
      "Min loss: 0.009061198681592941\n",
      "Mean loss: 0.014111382731546959\n",
      "Std loss: 0.004659856778205759\n",
      "Total Loss: 0.08466829638928175\n",
      "------------------------------------ epoch 2904 (17418 steps) ------------------------------------\n",
      "Max loss: 0.033176835626363754\n",
      "Min loss: 0.012611350044608116\n",
      "Mean loss: 0.020140796744575102\n",
      "Std loss: 0.007187059936914333\n",
      "Total Loss: 0.12084478046745062\n",
      "------------------------------------ epoch 2905 (17424 steps) ------------------------------------\n",
      "Max loss: 0.016208825632929802\n",
      "Min loss: 0.01063518039882183\n",
      "Mean loss: 0.012772854883223772\n",
      "Std loss: 0.0023032258070762347\n",
      "Total Loss: 0.07663712929934263\n",
      "------------------------------------ epoch 2906 (17430 steps) ------------------------------------\n",
      "Max loss: 0.028137153014540672\n",
      "Min loss: 0.009318938478827477\n",
      "Mean loss: 0.015160200651735067\n",
      "Std loss: 0.0062722479274153605\n",
      "Total Loss: 0.0909612039104104\n",
      "------------------------------------ epoch 2907 (17436 steps) ------------------------------------\n",
      "Max loss: 0.022268448024988174\n",
      "Min loss: 0.008226528763771057\n",
      "Mean loss: 0.013198362197726965\n",
      "Std loss: 0.005197552382261091\n",
      "Total Loss: 0.07919017318636179\n",
      "------------------------------------ epoch 2908 (17442 steps) ------------------------------------\n",
      "Max loss: 0.013710906729102135\n",
      "Min loss: 0.008569441735744476\n",
      "Mean loss: 0.009915777326871952\n",
      "Std loss: 0.0017717713135761125\n",
      "Total Loss: 0.05949466396123171\n",
      "------------------------------------ epoch 2909 (17448 steps) ------------------------------------\n",
      "Max loss: 0.031388357281684875\n",
      "Min loss: 0.011217750608921051\n",
      "Mean loss: 0.02194613218307495\n",
      "Std loss: 0.007882639039726693\n",
      "Total Loss: 0.1316767930984497\n",
      "------------------------------------ epoch 2910 (17454 steps) ------------------------------------\n",
      "Max loss: 0.03520685061812401\n",
      "Min loss: 0.011508161202073097\n",
      "Mean loss: 0.017371487182875473\n",
      "Std loss: 0.008070466247284734\n",
      "Total Loss: 0.10422892309725285\n",
      "------------------------------------ epoch 2911 (17460 steps) ------------------------------------\n",
      "Max loss: 0.02203904092311859\n",
      "Min loss: 0.009661773219704628\n",
      "Mean loss: 0.015022005264957746\n",
      "Std loss: 0.004730317308660203\n",
      "Total Loss: 0.09013203158974648\n",
      "------------------------------------ epoch 2912 (17466 steps) ------------------------------------\n",
      "Max loss: 0.022990789264440536\n",
      "Min loss: 0.008557755500078201\n",
      "Mean loss: 0.014086787433673939\n",
      "Std loss: 0.004615560475425271\n",
      "Total Loss: 0.08452072460204363\n",
      "------------------------------------ epoch 2913 (17472 steps) ------------------------------------\n",
      "Max loss: 0.040501806885004044\n",
      "Min loss: 0.009217699989676476\n",
      "Mean loss: 0.0228740059149762\n",
      "Std loss: 0.010887489699427733\n",
      "Total Loss: 0.1372440354898572\n",
      "------------------------------------ epoch 2914 (17478 steps) ------------------------------------\n",
      "Max loss: 0.060331713408231735\n",
      "Min loss: 0.011133560910820961\n",
      "Mean loss: 0.03071328407774369\n",
      "Std loss: 0.01922437194388604\n",
      "Total Loss: 0.18427970446646214\n",
      "------------------------------------ epoch 2915 (17484 steps) ------------------------------------\n",
      "Max loss: 0.062251269817352295\n",
      "Min loss: 0.010072200559079647\n",
      "Mean loss: 0.023672670902063448\n",
      "Std loss: 0.018479802156645377\n",
      "Total Loss: 0.1420360254123807\n",
      "------------------------------------ epoch 2916 (17490 steps) ------------------------------------\n",
      "Max loss: 0.03363442048430443\n",
      "Min loss: 0.00960838794708252\n",
      "Mean loss: 0.02154974639415741\n",
      "Std loss: 0.0091034863069711\n",
      "Total Loss: 0.12929847836494446\n",
      "------------------------------------ epoch 2917 (17496 steps) ------------------------------------\n",
      "Max loss: 0.021451309323310852\n",
      "Min loss: 0.012318545952439308\n",
      "Mean loss: 0.0161192601857086\n",
      "Std loss: 0.0032659856890168665\n",
      "Total Loss: 0.09671556111425161\n",
      "------------------------------------ epoch 2918 (17502 steps) ------------------------------------\n",
      "Max loss: 0.023918505758047104\n",
      "Min loss: 0.008250052109360695\n",
      "Mean loss: 0.014600981492549181\n",
      "Std loss: 0.004874244326697701\n",
      "Total Loss: 0.08760588895529509\n",
      "------------------------------------ epoch 2919 (17508 steps) ------------------------------------\n",
      "Max loss: 0.019205449149012566\n",
      "Min loss: 0.008269410580396652\n",
      "Mean loss: 0.013125330830613771\n",
      "Std loss: 0.003907181965320634\n",
      "Total Loss: 0.07875198498368263\n",
      "------------------------------------ epoch 2920 (17514 steps) ------------------------------------\n",
      "Max loss: 0.033229827880859375\n",
      "Min loss: 0.008331283926963806\n",
      "Mean loss: 0.019131862092763186\n",
      "Std loss: 0.008784859843086237\n",
      "Total Loss: 0.11479117255657911\n",
      "------------------------------------ epoch 2921 (17520 steps) ------------------------------------\n",
      "Max loss: 0.03631434589624405\n",
      "Min loss: 0.008442379534244537\n",
      "Mean loss: 0.015492610322932402\n",
      "Std loss: 0.009716527939996639\n",
      "Total Loss: 0.09295566193759441\n",
      "------------------------------------ epoch 2922 (17526 steps) ------------------------------------\n",
      "Max loss: 0.04043819010257721\n",
      "Min loss: 0.009859595447778702\n",
      "Mean loss: 0.020507407064239185\n",
      "Std loss: 0.010626135743269205\n",
      "Total Loss: 0.1230444423854351\n",
      "------------------------------------ epoch 2923 (17532 steps) ------------------------------------\n",
      "Max loss: 0.021544199436903\n",
      "Min loss: 0.010425135493278503\n",
      "Mean loss: 0.015711529025187094\n",
      "Std loss: 0.003567434211307215\n",
      "Total Loss: 0.09426917415112257\n",
      "------------------------------------ epoch 2924 (17538 steps) ------------------------------------\n",
      "Max loss: 0.030345043167471886\n",
      "Min loss: 0.009484623558819294\n",
      "Mean loss: 0.01513972661147515\n",
      "Std loss: 0.0077632344849395294\n",
      "Total Loss: 0.0908383596688509\n",
      "------------------------------------ epoch 2925 (17544 steps) ------------------------------------\n",
      "Max loss: 0.028931712731719017\n",
      "Min loss: 0.0076841614209115505\n",
      "Mean loss: 0.01655512450573345\n",
      "Std loss: 0.008258461360997725\n",
      "Total Loss: 0.0993307470344007\n",
      "------------------------------------ epoch 2926 (17550 steps) ------------------------------------\n",
      "Max loss: 0.023779820650815964\n",
      "Min loss: 0.009787026792764664\n",
      "Mean loss: 0.014895434025675058\n",
      "Std loss: 0.004789264498056224\n",
      "Total Loss: 0.08937260415405035\n",
      "------------------------------------ epoch 2927 (17556 steps) ------------------------------------\n",
      "Max loss: 0.03903666138648987\n",
      "Min loss: 0.009759396314620972\n",
      "Mean loss: 0.01798838097602129\n",
      "Std loss: 0.010071161073645208\n",
      "Total Loss: 0.10793028585612774\n",
      "------------------------------------ epoch 2928 (17562 steps) ------------------------------------\n",
      "Max loss: 0.02353449910879135\n",
      "Min loss: 0.00808412954211235\n",
      "Mean loss: 0.01394563727080822\n",
      "Std loss: 0.005239846681981322\n",
      "Total Loss: 0.08367382362484932\n",
      "------------------------------------ epoch 2929 (17568 steps) ------------------------------------\n",
      "Max loss: 0.018929533660411835\n",
      "Min loss: 0.010644279420375824\n",
      "Mean loss: 0.015242461115121841\n",
      "Std loss: 0.0033801070641584904\n",
      "Total Loss: 0.09145476669073105\n",
      "------------------------------------ epoch 2930 (17574 steps) ------------------------------------\n",
      "Max loss: 0.04278898239135742\n",
      "Min loss: 0.008752039633691311\n",
      "Mean loss: 0.01810483494773507\n",
      "Std loss: 0.011553775057002311\n",
      "Total Loss: 0.10862900968641043\n",
      "------------------------------------ epoch 2931 (17580 steps) ------------------------------------\n",
      "Max loss: 0.07080646604299545\n",
      "Min loss: 0.009617633186280727\n",
      "Mean loss: 0.027049485128372908\n",
      "Std loss: 0.020964102959926647\n",
      "Total Loss: 0.16229691077023745\n",
      "------------------------------------ epoch 2932 (17586 steps) ------------------------------------\n",
      "Max loss: 0.025784064084291458\n",
      "Min loss: 0.007093561813235283\n",
      "Mean loss: 0.015769360897441704\n",
      "Std loss: 0.006592227911854543\n",
      "Total Loss: 0.09461616538465023\n",
      "------------------------------------ epoch 2933 (17592 steps) ------------------------------------\n",
      "Max loss: 0.03399837017059326\n",
      "Min loss: 0.009744915179908276\n",
      "Mean loss: 0.014760046266019344\n",
      "Std loss: 0.00876489210935596\n",
      "Total Loss: 0.08856027759611607\n",
      "------------------------------------ epoch 2934 (17598 steps) ------------------------------------\n",
      "Max loss: 0.028218861669301987\n",
      "Min loss: 0.009529815055429935\n",
      "Mean loss: 0.016965788478652637\n",
      "Std loss: 0.006799919836656531\n",
      "Total Loss: 0.10179473087191582\n",
      "------------------------------------ epoch 2935 (17604 steps) ------------------------------------\n",
      "Max loss: 0.03700259327888489\n",
      "Min loss: 0.008498050272464752\n",
      "Mean loss: 0.01499118066082398\n",
      "Std loss: 0.009984963860436676\n",
      "Total Loss: 0.08994708396494389\n",
      "------------------------------------ epoch 2936 (17610 steps) ------------------------------------\n",
      "Max loss: 0.024043602868914604\n",
      "Min loss: 0.008340956643223763\n",
      "Mean loss: 0.014627488485227028\n",
      "Std loss: 0.005299578231580943\n",
      "Total Loss: 0.08776493091136217\n",
      "------------------------------------ epoch 2937 (17616 steps) ------------------------------------\n",
      "Max loss: 0.02188236080110073\n",
      "Min loss: 0.008377019315958023\n",
      "Mean loss: 0.015352702544381222\n",
      "Std loss: 0.0044319126623679735\n",
      "Total Loss: 0.09211621526628733\n",
      "------------------------------------ epoch 2938 (17622 steps) ------------------------------------\n",
      "Max loss: 0.03460686653852463\n",
      "Min loss: 0.01023158710449934\n",
      "Mean loss: 0.015845028217881918\n",
      "Std loss: 0.008490167457416594\n",
      "Total Loss: 0.09507016930729151\n",
      "------------------------------------ epoch 2939 (17628 steps) ------------------------------------\n",
      "Max loss: 0.025228548794984818\n",
      "Min loss: 0.006982211954891682\n",
      "Mean loss: 0.01282019525145491\n",
      "Std loss: 0.005905776302601963\n",
      "Total Loss: 0.07692117150872946\n",
      "------------------------------------ epoch 2940 (17634 steps) ------------------------------------\n",
      "Max loss: 0.06388309597969055\n",
      "Min loss: 0.009307671338319778\n",
      "Mean loss: 0.02455503089974324\n",
      "Std loss: 0.018671750492776563\n",
      "Total Loss: 0.14733018539845943\n",
      "------------------------------------ epoch 2941 (17640 steps) ------------------------------------\n",
      "Max loss: 0.04294922947883606\n",
      "Min loss: 0.008348498493432999\n",
      "Mean loss: 0.017737969135244686\n",
      "Std loss: 0.011984908073943173\n",
      "Total Loss: 0.10642781481146812\n",
      "------------------------------------ epoch 2942 (17646 steps) ------------------------------------\n",
      "Max loss: 0.028912261128425598\n",
      "Min loss: 0.014136772602796555\n",
      "Mean loss: 0.018448847346007824\n",
      "Std loss: 0.0051889441511078545\n",
      "Total Loss: 0.11069308407604694\n",
      "------------------------------------ epoch 2943 (17652 steps) ------------------------------------\n",
      "Max loss: 0.03294099494814873\n",
      "Min loss: 0.007949188351631165\n",
      "Mean loss: 0.01723021517197291\n",
      "Std loss: 0.00813501232043848\n",
      "Total Loss: 0.10338129103183746\n",
      "------------------------------------ epoch 2944 (17658 steps) ------------------------------------\n",
      "Max loss: 0.022775037214159966\n",
      "Min loss: 0.009637530893087387\n",
      "Mean loss: 0.01390037980551521\n",
      "Std loss: 0.004394427200522614\n",
      "Total Loss: 0.08340227883309126\n",
      "------------------------------------ epoch 2945 (17664 steps) ------------------------------------\n",
      "Max loss: 0.04694812744855881\n",
      "Min loss: 0.008341566659510136\n",
      "Mean loss: 0.0179717973805964\n",
      "Std loss: 0.013837424615614411\n",
      "Total Loss: 0.1078307842835784\n",
      "------------------------------------ epoch 2946 (17670 steps) ------------------------------------\n",
      "Max loss: 0.049805134534835815\n",
      "Min loss: 0.010816333815455437\n",
      "Mean loss: 0.030880074948072433\n",
      "Std loss: 0.014328902300604462\n",
      "Total Loss: 0.1852804496884346\n",
      "------------------------------------ epoch 2947 (17676 steps) ------------------------------------\n",
      "Max loss: 0.028206076472997665\n",
      "Min loss: 0.00997471809387207\n",
      "Mean loss: 0.019551322950671118\n",
      "Std loss: 0.0063744962295080605\n",
      "Total Loss: 0.1173079377040267\n",
      "------------------------------------ epoch 2948 (17682 steps) ------------------------------------\n",
      "Max loss: 0.021382208913564682\n",
      "Min loss: 0.010108213871717453\n",
      "Mean loss: 0.014612191046277681\n",
      "Std loss: 0.004241948434070046\n",
      "Total Loss: 0.08767314627766609\n",
      "------------------------------------ epoch 2949 (17688 steps) ------------------------------------\n",
      "Max loss: 0.043580006808042526\n",
      "Min loss: 0.008242842741310596\n",
      "Mean loss: 0.020682264119386673\n",
      "Std loss: 0.01435884024404467\n",
      "Total Loss: 0.12409358471632004\n",
      "------------------------------------ epoch 2950 (17694 steps) ------------------------------------\n",
      "Max loss: 0.02541651576757431\n",
      "Min loss: 0.009066002443432808\n",
      "Mean loss: 0.014250048901885748\n",
      "Std loss: 0.005492752493057906\n",
      "Total Loss: 0.08550029341131449\n",
      "------------------------------------ epoch 2951 (17700 steps) ------------------------------------\n",
      "Max loss: 0.05043667554855347\n",
      "Min loss: 0.0082098338752985\n",
      "Mean loss: 0.0251520414215823\n",
      "Std loss: 0.013483104671830943\n",
      "Total Loss: 0.1509122485294938\n",
      "------------------------------------ epoch 2952 (17706 steps) ------------------------------------\n",
      "Max loss: 0.019444309175014496\n",
      "Min loss: 0.009986921213567257\n",
      "Mean loss: 0.01377093760917584\n",
      "Std loss: 0.0031581821009808012\n",
      "Total Loss: 0.08262562565505505\n",
      "------------------------------------ epoch 2953 (17712 steps) ------------------------------------\n",
      "Max loss: 0.018343856558203697\n",
      "Min loss: 0.007158495485782623\n",
      "Mean loss: 0.011996857821941376\n",
      "Std loss: 0.004159100346302204\n",
      "Total Loss: 0.07198114693164825\n",
      "------------------------------------ epoch 2954 (17718 steps) ------------------------------------\n",
      "Max loss: 0.03964003548026085\n",
      "Min loss: 0.008977415040135384\n",
      "Mean loss: 0.019266120468576748\n",
      "Std loss: 0.010330113365268548\n",
      "Total Loss: 0.1155967228114605\n",
      "------------------------------------ epoch 2955 (17724 steps) ------------------------------------\n",
      "Max loss: 0.032294001430273056\n",
      "Min loss: 0.009540246799588203\n",
      "Mean loss: 0.019919381632159155\n",
      "Std loss: 0.00807977568138289\n",
      "Total Loss: 0.11951628979295492\n",
      "------------------------------------ epoch 2956 (17730 steps) ------------------------------------\n",
      "Max loss: 0.05777687579393387\n",
      "Min loss: 0.009572435170412064\n",
      "Mean loss: 0.024283422001947958\n",
      "Std loss: 0.01615046406835073\n",
      "Total Loss: 0.14570053201168776\n",
      "------------------------------------ epoch 2957 (17736 steps) ------------------------------------\n",
      "Max loss: 0.022965919226408005\n",
      "Min loss: 0.007607683073729277\n",
      "Mean loss: 0.01539730552273492\n",
      "Std loss: 0.005076103393510259\n",
      "Total Loss: 0.09238383313640952\n",
      "------------------------------------ epoch 2958 (17742 steps) ------------------------------------\n",
      "Max loss: 0.024090271443128586\n",
      "Min loss: 0.007606030907481909\n",
      "Mean loss: 0.013574724628900489\n",
      "Std loss: 0.005427329563726605\n",
      "Total Loss: 0.08144834777340293\n",
      "------------------------------------ epoch 2959 (17748 steps) ------------------------------------\n",
      "Max loss: 0.019515113905072212\n",
      "Min loss: 0.008668394759297371\n",
      "Mean loss: 0.013704471134891113\n",
      "Std loss: 0.0039089181077512965\n",
      "Total Loss: 0.08222682680934668\n",
      "------------------------------------ epoch 2960 (17754 steps) ------------------------------------\n",
      "Max loss: 0.02126048132777214\n",
      "Min loss: 0.008234374225139618\n",
      "Mean loss: 0.015599728096276522\n",
      "Std loss: 0.004255488847910514\n",
      "Total Loss: 0.09359836857765913\n",
      "------------------------------------ epoch 2961 (17760 steps) ------------------------------------\n",
      "Max loss: 0.022446012124419212\n",
      "Min loss: 0.007010638248175383\n",
      "Mean loss: 0.01390450408992668\n",
      "Std loss: 0.00481311898438914\n",
      "Total Loss: 0.08342702453956008\n",
      "------------------------------------ epoch 2962 (17766 steps) ------------------------------------\n",
      "Max loss: 0.035889849066734314\n",
      "Min loss: 0.00770168099552393\n",
      "Mean loss: 0.01820560482641061\n",
      "Std loss: 0.010433025640885095\n",
      "Total Loss: 0.10923362895846367\n",
      "------------------------------------ epoch 2963 (17772 steps) ------------------------------------\n",
      "Max loss: 0.019135544076561928\n",
      "Min loss: 0.010295497253537178\n",
      "Mean loss: 0.015117427334189415\n",
      "Std loss: 0.0031779675284541137\n",
      "Total Loss: 0.09070456400513649\n",
      "------------------------------------ epoch 2964 (17778 steps) ------------------------------------\n",
      "Max loss: 0.030770733952522278\n",
      "Min loss: 0.008748646825551987\n",
      "Mean loss: 0.016518835599223774\n",
      "Std loss: 0.007706420469103599\n",
      "Total Loss: 0.09911301359534264\n",
      "------------------------------------ epoch 2965 (17784 steps) ------------------------------------\n",
      "Max loss: 0.05652133747935295\n",
      "Min loss: 0.009591620415449142\n",
      "Mean loss: 0.021394848978767794\n",
      "Std loss: 0.015892849607045508\n",
      "Total Loss: 0.12836909387260675\n",
      "------------------------------------ epoch 2966 (17790 steps) ------------------------------------\n",
      "Max loss: 0.029352009296417236\n",
      "Min loss: 0.008436808362603188\n",
      "Mean loss: 0.01616432781641682\n",
      "Std loss: 0.007388179082388097\n",
      "Total Loss: 0.09698596689850092\n",
      "------------------------------------ epoch 2967 (17796 steps) ------------------------------------\n",
      "Max loss: 0.04173963889479637\n",
      "Min loss: 0.011989115737378597\n",
      "Mean loss: 0.020818719795594614\n",
      "Std loss: 0.009836867035501773\n",
      "Total Loss: 0.12491231877356768\n",
      "------------------------------------ epoch 2968 (17802 steps) ------------------------------------\n",
      "Max loss: 0.019105492159724236\n",
      "Min loss: 0.011689694598317146\n",
      "Mean loss: 0.014239697872350613\n",
      "Std loss: 0.0026161758798921085\n",
      "Total Loss: 0.08543818723410368\n",
      "------------------------------------ epoch 2969 (17808 steps) ------------------------------------\n",
      "Max loss: 0.015985721722245216\n",
      "Min loss: 0.009411520324647427\n",
      "Mean loss: 0.012230980054785809\n",
      "Std loss: 0.002125567789487001\n",
      "Total Loss: 0.07338588032871485\n",
      "------------------------------------ epoch 2970 (17814 steps) ------------------------------------\n",
      "Max loss: 0.01975029893219471\n",
      "Min loss: 0.008167234249413013\n",
      "Mean loss: 0.012674971328427395\n",
      "Std loss: 0.0042308347520112624\n",
      "Total Loss: 0.07604982797056437\n",
      "------------------------------------ epoch 2971 (17820 steps) ------------------------------------\n",
      "Max loss: 0.020404985174536705\n",
      "Min loss: 0.006584543734788895\n",
      "Mean loss: 0.01341936131939292\n",
      "Std loss: 0.005317074957393794\n",
      "Total Loss: 0.08051616791635752\n",
      "------------------------------------ epoch 2972 (17826 steps) ------------------------------------\n",
      "Max loss: 0.028758395463228226\n",
      "Min loss: 0.008919614367187023\n",
      "Mean loss: 0.018965203315019608\n",
      "Std loss: 0.00875523787086941\n",
      "Total Loss: 0.11379121989011765\n",
      "------------------------------------ epoch 2973 (17832 steps) ------------------------------------\n",
      "Max loss: 0.022370873019099236\n",
      "Min loss: 0.007848614826798439\n",
      "Mean loss: 0.01626927638426423\n",
      "Std loss: 0.0049264615219758254\n",
      "Total Loss: 0.09761565830558538\n",
      "------------------------------------ epoch 2974 (17838 steps) ------------------------------------\n",
      "Max loss: 0.012284456752240658\n",
      "Min loss: 0.0074659897945821285\n",
      "Mean loss: 0.00951759897482892\n",
      "Std loss: 0.0015534135646092936\n",
      "Total Loss: 0.05710559384897351\n",
      "------------------------------------ epoch 2975 (17844 steps) ------------------------------------\n",
      "Max loss: 0.025138413533568382\n",
      "Min loss: 0.011467250995337963\n",
      "Mean loss: 0.016416100785136223\n",
      "Std loss: 0.005128226705870717\n",
      "Total Loss: 0.09849660471081734\n",
      "------------------------------------ epoch 2976 (17850 steps) ------------------------------------\n",
      "Max loss: 0.032743774354457855\n",
      "Min loss: 0.008378986269235611\n",
      "Mean loss: 0.01715849619358778\n",
      "Std loss: 0.00805309090610253\n",
      "Total Loss: 0.10295097716152668\n",
      "------------------------------------ epoch 2977 (17856 steps) ------------------------------------\n",
      "Max loss: 0.040555812418460846\n",
      "Min loss: 0.009945276193320751\n",
      "Mean loss: 0.020417723183830578\n",
      "Std loss: 0.011905021753891493\n",
      "Total Loss: 0.12250633910298347\n",
      "------------------------------------ epoch 2978 (17862 steps) ------------------------------------\n",
      "Max loss: 0.036562830209732056\n",
      "Min loss: 0.010669702664017677\n",
      "Mean loss: 0.017537736644347508\n",
      "Std loss: 0.009073442644680423\n",
      "Total Loss: 0.10522641986608505\n",
      "------------------------------------ epoch 2979 (17868 steps) ------------------------------------\n",
      "Max loss: 0.017318395897746086\n",
      "Min loss: 0.01071577426046133\n",
      "Mean loss: 0.013602161159118017\n",
      "Std loss: 0.002591603212283741\n",
      "Total Loss: 0.0816129669547081\n",
      "------------------------------------ epoch 2980 (17874 steps) ------------------------------------\n",
      "Max loss: 0.033177077770233154\n",
      "Min loss: 0.009095552377402782\n",
      "Mean loss: 0.015550091241796812\n",
      "Std loss: 0.00857831151086866\n",
      "Total Loss: 0.09330054745078087\n",
      "------------------------------------ epoch 2981 (17880 steps) ------------------------------------\n",
      "Max loss: 0.020925842225551605\n",
      "Min loss: 0.01150493323802948\n",
      "Mean loss: 0.016074708973368008\n",
      "Std loss: 0.00423952141324123\n",
      "Total Loss: 0.09644825384020805\n",
      "------------------------------------ epoch 2982 (17886 steps) ------------------------------------\n",
      "Max loss: 0.04408986493945122\n",
      "Min loss: 0.011123361997306347\n",
      "Mean loss: 0.02251335900897781\n",
      "Std loss: 0.011130246438859445\n",
      "Total Loss: 0.13508015405386686\n",
      "------------------------------------ epoch 2983 (17892 steps) ------------------------------------\n",
      "Max loss: 0.02918686531484127\n",
      "Min loss: 0.01037212647497654\n",
      "Mean loss: 0.016062291494260233\n",
      "Std loss: 0.00660704101289818\n",
      "Total Loss: 0.09637374896556139\n",
      "------------------------------------ epoch 2984 (17898 steps) ------------------------------------\n",
      "Max loss: 0.022348951548337936\n",
      "Min loss: 0.010390788316726685\n",
      "Mean loss: 0.01757972004512946\n",
      "Std loss: 0.003885623459293046\n",
      "Total Loss: 0.10547832027077675\n",
      "------------------------------------ epoch 2985 (17904 steps) ------------------------------------\n",
      "Max loss: 0.027290478348731995\n",
      "Min loss: 0.00879564881324768\n",
      "Mean loss: 0.016768810028831165\n",
      "Std loss: 0.006629809193846385\n",
      "Total Loss: 0.10061286017298698\n",
      "------------------------------------ epoch 2986 (17910 steps) ------------------------------------\n",
      "Max loss: 0.048878248780965805\n",
      "Min loss: 0.009028103202581406\n",
      "Mean loss: 0.01936724518115322\n",
      "Std loss: 0.013662147563616684\n",
      "Total Loss: 0.11620347108691931\n",
      "------------------------------------ epoch 2987 (17916 steps) ------------------------------------\n",
      "Max loss: 0.038736745715141296\n",
      "Min loss: 0.00973738543689251\n",
      "Mean loss: 0.02157081275557478\n",
      "Std loss: 0.0112526345824586\n",
      "Total Loss: 0.1294248765334487\n",
      "------------------------------------ epoch 2988 (17922 steps) ------------------------------------\n",
      "Max loss: 0.017430081963539124\n",
      "Min loss: 0.010530306026339531\n",
      "Mean loss: 0.013667212954411903\n",
      "Std loss: 0.002205649151181732\n",
      "Total Loss: 0.08200327772647142\n",
      "------------------------------------ epoch 2989 (17928 steps) ------------------------------------\n",
      "Max loss: 0.025409629568457603\n",
      "Min loss: 0.011204421520233154\n",
      "Mean loss: 0.017039732231448095\n",
      "Std loss: 0.004815167830331472\n",
      "Total Loss: 0.10223839338868856\n",
      "------------------------------------ epoch 2990 (17934 steps) ------------------------------------\n",
      "Max loss: 0.03125312179327011\n",
      "Min loss: 0.009400543756783009\n",
      "Mean loss: 0.023418818600475788\n",
      "Std loss: 0.008260723424434879\n",
      "Total Loss: 0.14051291160285473\n",
      "------------------------------------ epoch 2991 (17940 steps) ------------------------------------\n",
      "Max loss: 0.029264170676469803\n",
      "Min loss: 0.010165820829570293\n",
      "Mean loss: 0.017964635820438463\n",
      "Std loss: 0.00677879268323877\n",
      "Total Loss: 0.10778781492263079\n",
      "------------------------------------ epoch 2992 (17946 steps) ------------------------------------\n",
      "Max loss: 0.028531525284051895\n",
      "Min loss: 0.009914025664329529\n",
      "Mean loss: 0.01607754702369372\n",
      "Std loss: 0.006381776903809365\n",
      "Total Loss: 0.09646528214216232\n",
      "------------------------------------ epoch 2993 (17952 steps) ------------------------------------\n",
      "Max loss: 0.02802952192723751\n",
      "Min loss: 0.008984302170574665\n",
      "Mean loss: 0.015845132526010275\n",
      "Std loss: 0.006262206195117207\n",
      "Total Loss: 0.09507079515606165\n",
      "------------------------------------ epoch 2994 (17958 steps) ------------------------------------\n",
      "Max loss: 0.025745663791894913\n",
      "Min loss: 0.009798383340239525\n",
      "Mean loss: 0.014102916388461987\n",
      "Std loss: 0.005672795590705144\n",
      "Total Loss: 0.08461749833077192\n",
      "------------------------------------ epoch 2995 (17964 steps) ------------------------------------\n",
      "Max loss: 0.016300959512591362\n",
      "Min loss: 0.009449111297726631\n",
      "Mean loss: 0.011614225339144468\n",
      "Std loss: 0.00237274545199982\n",
      "Total Loss: 0.06968535203486681\n",
      "------------------------------------ epoch 2996 (17970 steps) ------------------------------------\n",
      "Max loss: 0.03159484267234802\n",
      "Min loss: 0.007404301315546036\n",
      "Mean loss: 0.014407117540637652\n",
      "Std loss: 0.00795840147752956\n",
      "Total Loss: 0.08644270524382591\n",
      "------------------------------------ epoch 2997 (17976 steps) ------------------------------------\n",
      "Max loss: 0.018138527870178223\n",
      "Min loss: 0.009765657596290112\n",
      "Mean loss: 0.014556939092775186\n",
      "Std loss: 0.0026839047201381414\n",
      "Total Loss: 0.08734163455665112\n",
      "------------------------------------ epoch 2998 (17982 steps) ------------------------------------\n",
      "Max loss: 0.025081224739551544\n",
      "Min loss: 0.010427195578813553\n",
      "Mean loss: 0.015566031448543072\n",
      "Std loss: 0.0053425834142248\n",
      "Total Loss: 0.09339618869125843\n",
      "------------------------------------ epoch 2999 (17988 steps) ------------------------------------\n",
      "Max loss: 0.017759909853339195\n",
      "Min loss: 0.00972067005932331\n",
      "Mean loss: 0.013131635263562202\n",
      "Std loss: 0.0027923385220953406\n",
      "Total Loss: 0.07878981158137321\n",
      "------------------------------------ epoch 3000 (17994 steps) ------------------------------------\n",
      "Max loss: 0.0221073217689991\n",
      "Min loss: 0.007078113965690136\n",
      "Mean loss: 0.011974990367889404\n",
      "Std loss: 0.005033263456439047\n",
      "Total Loss: 0.07184994220733643\n",
      "------------------------------------ epoch 3001 (18000 steps) ------------------------------------\n",
      "Max loss: 0.018870150670409203\n",
      "Min loss: 0.007668091915547848\n",
      "Mean loss: 0.01207151481260856\n",
      "Std loss: 0.0041399706696131746\n",
      "Total Loss: 0.07242908887565136\n",
      "saved model at ./weights/model_3001.pth\n",
      "------------------------------------ epoch 3002 (18006 steps) ------------------------------------\n",
      "Max loss: 0.026710232719779015\n",
      "Min loss: 0.007899656891822815\n",
      "Mean loss: 0.015089369844645262\n",
      "Std loss: 0.0068886746314305355\n",
      "Total Loss: 0.09053621906787157\n",
      "------------------------------------ epoch 3003 (18012 steps) ------------------------------------\n",
      "Max loss: 0.028344398364424706\n",
      "Min loss: 0.007908213883638382\n",
      "Mean loss: 0.014749410562217236\n",
      "Std loss: 0.008393194992389465\n",
      "Total Loss: 0.08849646337330341\n",
      "------------------------------------ epoch 3004 (18018 steps) ------------------------------------\n",
      "Max loss: 0.01121161412447691\n",
      "Min loss: 0.006495501846075058\n",
      "Mean loss: 0.009098011689881483\n",
      "Std loss: 0.0018314059730245502\n",
      "Total Loss: 0.0545880701392889\n",
      "------------------------------------ epoch 3005 (18024 steps) ------------------------------------\n",
      "Max loss: 0.015173378400504589\n",
      "Min loss: 0.005760297179222107\n",
      "Mean loss: 0.00986915019651254\n",
      "Std loss: 0.003175156407896087\n",
      "Total Loss: 0.05921490117907524\n",
      "------------------------------------ epoch 3006 (18030 steps) ------------------------------------\n",
      "Max loss: 0.031306054443120956\n",
      "Min loss: 0.006358413957059383\n",
      "Mean loss: 0.01730372927462061\n",
      "Std loss: 0.010564865305234974\n",
      "Total Loss: 0.10382237564772367\n",
      "------------------------------------ epoch 3007 (18036 steps) ------------------------------------\n",
      "Max loss: 0.020766928791999817\n",
      "Min loss: 0.007662943564355373\n",
      "Mean loss: 0.014145310192058483\n",
      "Std loss: 0.0038652384894629624\n",
      "Total Loss: 0.0848718611523509\n",
      "------------------------------------ epoch 3008 (18042 steps) ------------------------------------\n",
      "Max loss: 0.0449858233332634\n",
      "Min loss: 0.008049845695495605\n",
      "Mean loss: 0.020361367457856733\n",
      "Std loss: 0.01220902779288478\n",
      "Total Loss: 0.12216820474714041\n",
      "------------------------------------ epoch 3009 (18048 steps) ------------------------------------\n",
      "Max loss: 0.020296715199947357\n",
      "Min loss: 0.008958965539932251\n",
      "Mean loss: 0.01552315599595507\n",
      "Std loss: 0.0043608661719216684\n",
      "Total Loss: 0.09313893597573042\n",
      "------------------------------------ epoch 3010 (18054 steps) ------------------------------------\n",
      "Max loss: 0.01335226185619831\n",
      "Min loss: 0.006760680582374334\n",
      "Mean loss: 0.009443932678550482\n",
      "Std loss: 0.0027755565065188757\n",
      "Total Loss: 0.05666359607130289\n",
      "------------------------------------ epoch 3011 (18060 steps) ------------------------------------\n",
      "Max loss: 0.027499115094542503\n",
      "Min loss: 0.00738252978771925\n",
      "Mean loss: 0.014856633575012287\n",
      "Std loss: 0.0073452520675761504\n",
      "Total Loss: 0.08913980145007372\n",
      "------------------------------------ epoch 3012 (18066 steps) ------------------------------------\n",
      "Max loss: 0.023219268769025803\n",
      "Min loss: 0.008092861622571945\n",
      "Mean loss: 0.013500391505658627\n",
      "Std loss: 0.004933657820443816\n",
      "Total Loss: 0.08100234903395176\n",
      "------------------------------------ epoch 3013 (18072 steps) ------------------------------------\n",
      "Max loss: 0.020216893404722214\n",
      "Min loss: 0.007415708154439926\n",
      "Mean loss: 0.012911693503459295\n",
      "Std loss: 0.005203462794942587\n",
      "Total Loss: 0.07747016102075577\n",
      "------------------------------------ epoch 3014 (18078 steps) ------------------------------------\n",
      "Max loss: 0.013575753197073936\n",
      "Min loss: 0.007372505031526089\n",
      "Mean loss: 0.011091825862725576\n",
      "Std loss: 0.0020642864755357664\n",
      "Total Loss: 0.06655095517635345\n",
      "------------------------------------ epoch 3015 (18084 steps) ------------------------------------\n",
      "Max loss: 0.01431802473962307\n",
      "Min loss: 0.006051364354789257\n",
      "Mean loss: 0.010071376338601112\n",
      "Std loss: 0.0026400007724388134\n",
      "Total Loss: 0.060428258031606674\n",
      "------------------------------------ epoch 3016 (18090 steps) ------------------------------------\n",
      "Max loss: 0.027508839964866638\n",
      "Min loss: 0.006999625824391842\n",
      "Mean loss: 0.017946910268316667\n",
      "Std loss: 0.0062584352447755025\n",
      "Total Loss: 0.1076814616099\n",
      "------------------------------------ epoch 3017 (18096 steps) ------------------------------------\n",
      "Max loss: 0.019697606563568115\n",
      "Min loss: 0.008323715068399906\n",
      "Mean loss: 0.013081859642018875\n",
      "Std loss: 0.004781589452001097\n",
      "Total Loss: 0.07849115785211325\n",
      "------------------------------------ epoch 3018 (18102 steps) ------------------------------------\n",
      "Max loss: 0.01816495507955551\n",
      "Min loss: 0.009259839542210102\n",
      "Mean loss: 0.013726125316073498\n",
      "Std loss: 0.0031404343222736298\n",
      "Total Loss: 0.08235675189644098\n",
      "------------------------------------ epoch 3019 (18108 steps) ------------------------------------\n",
      "Max loss: 0.024036787450313568\n",
      "Min loss: 0.00764802098274231\n",
      "Mean loss: 0.013680946081876755\n",
      "Std loss: 0.005662410386072039\n",
      "Total Loss: 0.08208567649126053\n",
      "------------------------------------ epoch 3020 (18114 steps) ------------------------------------\n",
      "Max loss: 0.013852924108505249\n",
      "Min loss: 0.007773447781801224\n",
      "Mean loss: 0.009920728082458178\n",
      "Std loss: 0.0023685180676114347\n",
      "Total Loss: 0.05952436849474907\n",
      "------------------------------------ epoch 3021 (18120 steps) ------------------------------------\n",
      "Max loss: 0.0220155268907547\n",
      "Min loss: 0.008129432797431946\n",
      "Mean loss: 0.013950484804809093\n",
      "Std loss: 0.00433761382398293\n",
      "Total Loss: 0.08370290882885456\n",
      "------------------------------------ epoch 3022 (18126 steps) ------------------------------------\n",
      "Max loss: 0.04018692672252655\n",
      "Min loss: 0.007523627020418644\n",
      "Mean loss: 0.0160314926567177\n",
      "Std loss: 0.011127517057744528\n",
      "Total Loss: 0.09618895594030619\n",
      "------------------------------------ epoch 3023 (18132 steps) ------------------------------------\n",
      "Max loss: 0.016254108399152756\n",
      "Min loss: 0.007073934189975262\n",
      "Mean loss: 0.013138474275668463\n",
      "Std loss: 0.0031779043399695\n",
      "Total Loss: 0.07883084565401077\n",
      "------------------------------------ epoch 3024 (18138 steps) ------------------------------------\n",
      "Max loss: 0.06343548744916916\n",
      "Min loss: 0.010531486943364143\n",
      "Mean loss: 0.025170995543400448\n",
      "Std loss: 0.017664973878190354\n",
      "Total Loss: 0.15102597326040268\n",
      "------------------------------------ epoch 3025 (18144 steps) ------------------------------------\n",
      "Max loss: 0.0247548408806324\n",
      "Min loss: 0.009723424911499023\n",
      "Mean loss: 0.015338292345404625\n",
      "Std loss: 0.004972249990045706\n",
      "Total Loss: 0.09202975407242775\n",
      "------------------------------------ epoch 3026 (18150 steps) ------------------------------------\n",
      "Max loss: 0.025304600596427917\n",
      "Min loss: 0.008090308867394924\n",
      "Mean loss: 0.013036271091550589\n",
      "Std loss: 0.00565282865159647\n",
      "Total Loss: 0.07821762654930353\n",
      "------------------------------------ epoch 3027 (18156 steps) ------------------------------------\n",
      "Max loss: 0.059445470571517944\n",
      "Min loss: 0.007952667772769928\n",
      "Mean loss: 0.023787379420051973\n",
      "Std loss: 0.017949050301449655\n",
      "Total Loss: 0.14272427652031183\n",
      "------------------------------------ epoch 3028 (18162 steps) ------------------------------------\n",
      "Max loss: 0.023724958300590515\n",
      "Min loss: 0.010973911732435226\n",
      "Mean loss: 0.01755827944725752\n",
      "Std loss: 0.00485826990799959\n",
      "Total Loss: 0.10534967668354511\n",
      "------------------------------------ epoch 3029 (18168 steps) ------------------------------------\n",
      "Max loss: 0.039045803248882294\n",
      "Min loss: 0.01637320965528488\n",
      "Mean loss: 0.0220042842750748\n",
      "Std loss: 0.007753212268815314\n",
      "Total Loss: 0.1320257056504488\n",
      "------------------------------------ epoch 3030 (18174 steps) ------------------------------------\n",
      "Max loss: 0.022981783375144005\n",
      "Min loss: 0.01246790774166584\n",
      "Mean loss: 0.018431744227806728\n",
      "Std loss: 0.003203707323676392\n",
      "Total Loss: 0.11059046536684036\n",
      "------------------------------------ epoch 3031 (18180 steps) ------------------------------------\n",
      "Max loss: 0.053731974214315414\n",
      "Min loss: 0.01152623351663351\n",
      "Mean loss: 0.030183165178944666\n",
      "Std loss: 0.017001281802504372\n",
      "Total Loss: 0.181098991073668\n",
      "------------------------------------ epoch 3032 (18186 steps) ------------------------------------\n",
      "Max loss: 0.049254536628723145\n",
      "Min loss: 0.012102614156901836\n",
      "Mean loss: 0.021974859448770683\n",
      "Std loss: 0.012902367305778664\n",
      "Total Loss: 0.1318491566926241\n",
      "------------------------------------ epoch 3033 (18192 steps) ------------------------------------\n",
      "Max loss: 0.03208025544881821\n",
      "Min loss: 0.013364061713218689\n",
      "Mean loss: 0.02422118652611971\n",
      "Std loss: 0.008073219034391434\n",
      "Total Loss: 0.14532711915671825\n",
      "------------------------------------ epoch 3034 (18198 steps) ------------------------------------\n",
      "Max loss: 0.037636056542396545\n",
      "Min loss: 0.010639792308211327\n",
      "Mean loss: 0.01988281433780988\n",
      "Std loss: 0.009253536058920942\n",
      "Total Loss: 0.11929688602685928\n",
      "------------------------------------ epoch 3035 (18204 steps) ------------------------------------\n",
      "Max loss: 0.02530386671423912\n",
      "Min loss: 0.009401338174939156\n",
      "Mean loss: 0.017985219446321327\n",
      "Std loss: 0.005480255115836359\n",
      "Total Loss: 0.10791131667792797\n",
      "------------------------------------ epoch 3036 (18210 steps) ------------------------------------\n",
      "Max loss: 0.01994827575981617\n",
      "Min loss: 0.009110724553465843\n",
      "Mean loss: 0.015022373913476864\n",
      "Std loss: 0.004020589260806144\n",
      "Total Loss: 0.09013424348086119\n",
      "------------------------------------ epoch 3037 (18216 steps) ------------------------------------\n",
      "Max loss: 0.02159677818417549\n",
      "Min loss: 0.010077644139528275\n",
      "Mean loss: 0.014557964634150267\n",
      "Std loss: 0.004346542235548649\n",
      "Total Loss: 0.0873477878049016\n",
      "------------------------------------ epoch 3038 (18222 steps) ------------------------------------\n",
      "Max loss: 0.03216094151139259\n",
      "Min loss: 0.008403853513300419\n",
      "Mean loss: 0.017026121107240517\n",
      "Std loss: 0.009023477140003755\n",
      "Total Loss: 0.10215672664344311\n",
      "------------------------------------ epoch 3039 (18228 steps) ------------------------------------\n",
      "Max loss: 0.025452788919210434\n",
      "Min loss: 0.00875888578593731\n",
      "Mean loss: 0.01637496721620361\n",
      "Std loss: 0.006525144927466064\n",
      "Total Loss: 0.09824980329722166\n",
      "------------------------------------ epoch 3040 (18234 steps) ------------------------------------\n",
      "Max loss: 0.020051341503858566\n",
      "Min loss: 0.010080292820930481\n",
      "Mean loss: 0.014824365731328726\n",
      "Std loss: 0.0036740188778013206\n",
      "Total Loss: 0.08894619438797235\n",
      "------------------------------------ epoch 3041 (18240 steps) ------------------------------------\n",
      "Max loss: 0.04359345883131027\n",
      "Min loss: 0.007857014425098896\n",
      "Mean loss: 0.01821949038033684\n",
      "Std loss: 0.011848452492081957\n",
      "Total Loss: 0.10931694228202105\n",
      "------------------------------------ epoch 3042 (18246 steps) ------------------------------------\n",
      "Max loss: 0.016508987173438072\n",
      "Min loss: 0.007948299869894981\n",
      "Mean loss: 0.012229722924530506\n",
      "Std loss: 0.002729099215138345\n",
      "Total Loss: 0.07337833754718304\n",
      "------------------------------------ epoch 3043 (18252 steps) ------------------------------------\n",
      "Max loss: 0.029504787176847458\n",
      "Min loss: 0.008030158467590809\n",
      "Mean loss: 0.01618128999446829\n",
      "Std loss: 0.006888802833856736\n",
      "Total Loss: 0.09708773996680975\n",
      "------------------------------------ epoch 3044 (18258 steps) ------------------------------------\n",
      "Max loss: 0.01806839369237423\n",
      "Min loss: 0.008444712497293949\n",
      "Mean loss: 0.013051004614681005\n",
      "Std loss: 0.003018030845819847\n",
      "Total Loss: 0.07830602768808603\n",
      "------------------------------------ epoch 3045 (18264 steps) ------------------------------------\n",
      "Max loss: 0.018632860854268074\n",
      "Min loss: 0.00946127250790596\n",
      "Mean loss: 0.012461347505450249\n",
      "Std loss: 0.0030689605395148663\n",
      "Total Loss: 0.07476808503270149\n",
      "------------------------------------ epoch 3046 (18270 steps) ------------------------------------\n",
      "Max loss: 0.031909532845020294\n",
      "Min loss: 0.007090381346642971\n",
      "Mean loss: 0.013939203694462776\n",
      "Std loss: 0.008537192662843442\n",
      "Total Loss: 0.08363522216677666\n",
      "------------------------------------ epoch 3047 (18276 steps) ------------------------------------\n",
      "Max loss: 0.019076872617006302\n",
      "Min loss: 0.0074811168015003204\n",
      "Mean loss: 0.011838302792360386\n",
      "Std loss: 0.004057963204277049\n",
      "Total Loss: 0.07102981675416231\n",
      "------------------------------------ epoch 3048 (18282 steps) ------------------------------------\n",
      "Max loss: 0.04412209615111351\n",
      "Min loss: 0.01020580343902111\n",
      "Mean loss: 0.020672837427506845\n",
      "Std loss: 0.011901695861866218\n",
      "Total Loss: 0.12403702456504107\n",
      "------------------------------------ epoch 3049 (18288 steps) ------------------------------------\n",
      "Max loss: 0.012430757284164429\n",
      "Min loss: 0.0072026606649160385\n",
      "Mean loss: 0.008940985193476081\n",
      "Std loss: 0.0019009183725759164\n",
      "Total Loss: 0.053645911160856485\n",
      "------------------------------------ epoch 3050 (18294 steps) ------------------------------------\n",
      "Max loss: 0.02914116345345974\n",
      "Min loss: 0.010991024784743786\n",
      "Mean loss: 0.01904461331044634\n",
      "Std loss: 0.006338564149626304\n",
      "Total Loss: 0.11426767986267805\n",
      "------------------------------------ epoch 3051 (18300 steps) ------------------------------------\n",
      "Max loss: 0.03340272232890129\n",
      "Min loss: 0.00864369422197342\n",
      "Mean loss: 0.01627708940456311\n",
      "Std loss: 0.00813404900796834\n",
      "Total Loss: 0.09766253642737865\n",
      "------------------------------------ epoch 3052 (18306 steps) ------------------------------------\n",
      "Max loss: 0.028884656727313995\n",
      "Min loss: 0.007800232619047165\n",
      "Mean loss: 0.015107664900521437\n",
      "Std loss: 0.0068308855067368696\n",
      "Total Loss: 0.09064598940312862\n",
      "------------------------------------ epoch 3053 (18312 steps) ------------------------------------\n",
      "Max loss: 0.03252897411584854\n",
      "Min loss: 0.007920579984784126\n",
      "Mean loss: 0.018583130712310474\n",
      "Std loss: 0.009216450924853356\n",
      "Total Loss: 0.11149878427386284\n",
      "------------------------------------ epoch 3054 (18318 steps) ------------------------------------\n",
      "Max loss: 0.026530511677265167\n",
      "Min loss: 0.008886381983757019\n",
      "Mean loss: 0.016064061628033716\n",
      "Std loss: 0.006457990735860583\n",
      "Total Loss: 0.0963843697682023\n",
      "------------------------------------ epoch 3055 (18324 steps) ------------------------------------\n",
      "Max loss: 0.021235918626189232\n",
      "Min loss: 0.009361477568745613\n",
      "Mean loss: 0.014487995766103268\n",
      "Std loss: 0.004089172778495954\n",
      "Total Loss: 0.0869279745966196\n",
      "------------------------------------ epoch 3056 (18330 steps) ------------------------------------\n",
      "Max loss: 0.022687366232275963\n",
      "Min loss: 0.010199344716966152\n",
      "Mean loss: 0.014091018432130417\n",
      "Std loss: 0.004218626458356913\n",
      "Total Loss: 0.0845461105927825\n",
      "------------------------------------ epoch 3057 (18336 steps) ------------------------------------\n",
      "Max loss: 0.04136617109179497\n",
      "Min loss: 0.009093092754483223\n",
      "Mean loss: 0.021448766967902582\n",
      "Std loss: 0.012662045600808388\n",
      "Total Loss: 0.12869260180741549\n",
      "------------------------------------ epoch 3058 (18342 steps) ------------------------------------\n",
      "Max loss: 0.017882268875837326\n",
      "Min loss: 0.0076540494337677956\n",
      "Mean loss: 0.013220377111186584\n",
      "Std loss: 0.003739565539534681\n",
      "Total Loss: 0.0793222626671195\n",
      "------------------------------------ epoch 3059 (18348 steps) ------------------------------------\n",
      "Max loss: 0.04626957327127457\n",
      "Min loss: 0.009318312630057335\n",
      "Mean loss: 0.017597702331840992\n",
      "Std loss: 0.012960539696300507\n",
      "Total Loss: 0.10558621399104595\n",
      "------------------------------------ epoch 3060 (18354 steps) ------------------------------------\n",
      "Max loss: 0.026325121521949768\n",
      "Min loss: 0.00863664411008358\n",
      "Mean loss: 0.018028102815151215\n",
      "Std loss: 0.006393348460942265\n",
      "Total Loss: 0.10816861689090729\n",
      "------------------------------------ epoch 3061 (18360 steps) ------------------------------------\n",
      "Max loss: 0.05168825387954712\n",
      "Min loss: 0.0072479248046875\n",
      "Mean loss: 0.02126686243961255\n",
      "Std loss: 0.014482272063788566\n",
      "Total Loss: 0.12760117463767529\n",
      "------------------------------------ epoch 3062 (18366 steps) ------------------------------------\n",
      "Max loss: 0.03514845296740532\n",
      "Min loss: 0.00817714910954237\n",
      "Mean loss: 0.016529321980973084\n",
      "Std loss: 0.009855615176655569\n",
      "Total Loss: 0.09917593188583851\n",
      "------------------------------------ epoch 3063 (18372 steps) ------------------------------------\n",
      "Max loss: 0.029535263776779175\n",
      "Min loss: 0.013991052284836769\n",
      "Mean loss: 0.02108927567799886\n",
      "Std loss: 0.005476492605725796\n",
      "Total Loss: 0.12653565406799316\n",
      "------------------------------------ epoch 3064 (18378 steps) ------------------------------------\n",
      "Max loss: 0.04995451122522354\n",
      "Min loss: 0.01300249993801117\n",
      "Mean loss: 0.02978140829751889\n",
      "Std loss: 0.01520850220348944\n",
      "Total Loss: 0.17868844978511333\n",
      "------------------------------------ epoch 3065 (18384 steps) ------------------------------------\n",
      "Max loss: 0.03539912402629852\n",
      "Min loss: 0.010954304598271847\n",
      "Mean loss: 0.019607797109832365\n",
      "Std loss: 0.008737138385454528\n",
      "Total Loss: 0.1176467826589942\n",
      "------------------------------------ epoch 3066 (18390 steps) ------------------------------------\n",
      "Max loss: 0.050275079905986786\n",
      "Min loss: 0.01072395034134388\n",
      "Mean loss: 0.022353753137091797\n",
      "Std loss: 0.015301395527303868\n",
      "Total Loss: 0.13412251882255077\n",
      "------------------------------------ epoch 3067 (18396 steps) ------------------------------------\n",
      "Max loss: 0.03976256400346756\n",
      "Min loss: 0.01048546563833952\n",
      "Mean loss: 0.02006361090267698\n",
      "Std loss: 0.010571856432403255\n",
      "Total Loss: 0.12038166541606188\n",
      "------------------------------------ epoch 3068 (18402 steps) ------------------------------------\n",
      "Max loss: 0.030816838145256042\n",
      "Min loss: 0.011092403903603554\n",
      "Mean loss: 0.01735543350999554\n",
      "Std loss: 0.0074145120035975535\n",
      "Total Loss: 0.10413260105997324\n",
      "------------------------------------ epoch 3069 (18408 steps) ------------------------------------\n",
      "Max loss: 0.061787813901901245\n",
      "Min loss: 0.01106328796595335\n",
      "Mean loss: 0.021225910168141127\n",
      "Std loss: 0.018337407893682902\n",
      "Total Loss: 0.12735546100884676\n",
      "------------------------------------ epoch 3070 (18414 steps) ------------------------------------\n",
      "Max loss: 0.056695662438869476\n",
      "Min loss: 0.010822511278092861\n",
      "Mean loss: 0.020720940083265305\n",
      "Std loss: 0.01626422368712168\n",
      "Total Loss: 0.12432564049959183\n",
      "------------------------------------ epoch 3071 (18420 steps) ------------------------------------\n",
      "Max loss: 0.016040317714214325\n",
      "Min loss: 0.010198216885328293\n",
      "Mean loss: 0.01267984447379907\n",
      "Std loss: 0.0020295738611038386\n",
      "Total Loss: 0.07607906684279442\n",
      "------------------------------------ epoch 3072 (18426 steps) ------------------------------------\n",
      "Max loss: 0.02643459104001522\n",
      "Min loss: 0.007919478230178356\n",
      "Mean loss: 0.015232048152635494\n",
      "Std loss: 0.005926450762769262\n",
      "Total Loss: 0.09139228891581297\n",
      "------------------------------------ epoch 3073 (18432 steps) ------------------------------------\n",
      "Max loss: 0.024771003052592278\n",
      "Min loss: 0.008858315646648407\n",
      "Mean loss: 0.01308057690039277\n",
      "Std loss: 0.005340497589235401\n",
      "Total Loss: 0.07848346140235662\n",
      "------------------------------------ epoch 3074 (18438 steps) ------------------------------------\n",
      "Max loss: 0.013194653205573559\n",
      "Min loss: 0.009398055262863636\n",
      "Mean loss: 0.011422205716371536\n",
      "Std loss: 0.0013226099832937524\n",
      "Total Loss: 0.06853323429822922\n",
      "------------------------------------ epoch 3075 (18444 steps) ------------------------------------\n",
      "Max loss: 0.029632320627570152\n",
      "Min loss: 0.008639611303806305\n",
      "Mean loss: 0.015852791722863913\n",
      "Std loss: 0.008137369744241169\n",
      "Total Loss: 0.09511675033718348\n",
      "------------------------------------ epoch 3076 (18450 steps) ------------------------------------\n",
      "Max loss: 0.04765639826655388\n",
      "Min loss: 0.009858308359980583\n",
      "Mean loss: 0.02138203615322709\n",
      "Std loss: 0.012457443571200683\n",
      "Total Loss: 0.12829221691936255\n",
      "------------------------------------ epoch 3077 (18456 steps) ------------------------------------\n",
      "Max loss: 0.03202536329627037\n",
      "Min loss: 0.009488992393016815\n",
      "Mean loss: 0.019925781525671482\n",
      "Std loss: 0.009365811881414063\n",
      "Total Loss: 0.11955468915402889\n",
      "------------------------------------ epoch 3078 (18462 steps) ------------------------------------\n",
      "Max loss: 0.015598349273204803\n",
      "Min loss: 0.009143311530351639\n",
      "Mean loss: 0.01196230590964357\n",
      "Std loss: 0.002229420734285138\n",
      "Total Loss: 0.07177383545786142\n",
      "------------------------------------ epoch 3079 (18468 steps) ------------------------------------\n",
      "Max loss: 0.0350615531206131\n",
      "Min loss: 0.007955959998071194\n",
      "Mean loss: 0.018774027780940134\n",
      "Std loss: 0.008278943044792448\n",
      "Total Loss: 0.11264416668564081\n",
      "------------------------------------ epoch 3080 (18474 steps) ------------------------------------\n",
      "Max loss: 0.019109319895505905\n",
      "Min loss: 0.00937599316239357\n",
      "Mean loss: 0.013831542494396368\n",
      "Std loss: 0.003889271938311824\n",
      "Total Loss: 0.08298925496637821\n",
      "------------------------------------ epoch 3081 (18480 steps) ------------------------------------\n",
      "Max loss: 0.033142589032649994\n",
      "Min loss: 0.009741978719830513\n",
      "Mean loss: 0.01902321632951498\n",
      "Std loss: 0.009955506914211992\n",
      "Total Loss: 0.11413929797708988\n",
      "------------------------------------ epoch 3082 (18486 steps) ------------------------------------\n",
      "Max loss: 0.027826320379972458\n",
      "Min loss: 0.008818177506327629\n",
      "Mean loss: 0.01291630913813909\n",
      "Std loss: 0.006736544577399149\n",
      "Total Loss: 0.07749785482883453\n",
      "------------------------------------ epoch 3083 (18492 steps) ------------------------------------\n",
      "Max loss: 0.035379260778427124\n",
      "Min loss: 0.007408290170133114\n",
      "Mean loss: 0.018453380869080622\n",
      "Std loss: 0.01149271414194885\n",
      "Total Loss: 0.11072028521448374\n",
      "------------------------------------ epoch 3084 (18498 steps) ------------------------------------\n",
      "Max loss: 0.02779969945549965\n",
      "Min loss: 0.009095221757888794\n",
      "Mean loss: 0.016655991940448683\n",
      "Std loss: 0.006007032779340108\n",
      "Total Loss: 0.09993595164269209\n",
      "------------------------------------ epoch 3085 (18504 steps) ------------------------------------\n",
      "Max loss: 0.04429129511117935\n",
      "Min loss: 0.009407849982380867\n",
      "Mean loss: 0.021063738968223333\n",
      "Std loss: 0.011158559824739275\n",
      "Total Loss: 0.12638243380934\n",
      "------------------------------------ epoch 3086 (18510 steps) ------------------------------------\n",
      "Max loss: 0.015915384516119957\n",
      "Min loss: 0.008922737091779709\n",
      "Mean loss: 0.012285000955065092\n",
      "Std loss: 0.002462960281071215\n",
      "Total Loss: 0.07371000573039055\n",
      "------------------------------------ epoch 3087 (18516 steps) ------------------------------------\n",
      "Max loss: 0.025200258940458298\n",
      "Min loss: 0.012124409899115562\n",
      "Mean loss: 0.017849362920969725\n",
      "Std loss: 0.004656112839673928\n",
      "Total Loss: 0.10709617752581835\n",
      "------------------------------------ epoch 3088 (18522 steps) ------------------------------------\n",
      "Max loss: 0.032711900770664215\n",
      "Min loss: 0.007453802973031998\n",
      "Mean loss: 0.013298481237143278\n",
      "Std loss: 0.008827347239318661\n",
      "Total Loss: 0.07979088742285967\n",
      "------------------------------------ epoch 3089 (18528 steps) ------------------------------------\n",
      "Max loss: 0.0251584704965353\n",
      "Min loss: 0.008779764175415039\n",
      "Mean loss: 0.013721778833617767\n",
      "Std loss: 0.005743069389900702\n",
      "Total Loss: 0.0823306730017066\n",
      "------------------------------------ epoch 3090 (18534 steps) ------------------------------------\n",
      "Max loss: 0.01850484311580658\n",
      "Min loss: 0.007802167441695929\n",
      "Mean loss: 0.012612322733427087\n",
      "Std loss: 0.003434119171427773\n",
      "Total Loss: 0.07567393640056252\n",
      "------------------------------------ epoch 3091 (18540 steps) ------------------------------------\n",
      "Max loss: 0.027760639786720276\n",
      "Min loss: 0.007795065641403198\n",
      "Mean loss: 0.013713172171264887\n",
      "Std loss: 0.006764296938595211\n",
      "Total Loss: 0.08227903302758932\n",
      "------------------------------------ epoch 3092 (18546 steps) ------------------------------------\n",
      "Max loss: 0.07006748020648956\n",
      "Min loss: 0.007269405759871006\n",
      "Mean loss: 0.024163497456659872\n",
      "Std loss: 0.020957151634964723\n",
      "Total Loss: 0.14498098473995924\n",
      "------------------------------------ epoch 3093 (18552 steps) ------------------------------------\n",
      "Max loss: 0.01806696504354477\n",
      "Min loss: 0.008554559201002121\n",
      "Mean loss: 0.012335075996816158\n",
      "Std loss: 0.0032298723454281955\n",
      "Total Loss: 0.07401045598089695\n",
      "------------------------------------ epoch 3094 (18558 steps) ------------------------------------\n",
      "Max loss: 0.01577753573656082\n",
      "Min loss: 0.006978591438382864\n",
      "Mean loss: 0.011492807495718202\n",
      "Std loss: 0.0033661590351210385\n",
      "Total Loss: 0.0689568449743092\n",
      "------------------------------------ epoch 3095 (18564 steps) ------------------------------------\n",
      "Max loss: 0.034974537789821625\n",
      "Min loss: 0.011980053037405014\n",
      "Mean loss: 0.017696235484133165\n",
      "Std loss: 0.00783098172889338\n",
      "Total Loss: 0.10617741290479898\n",
      "------------------------------------ epoch 3096 (18570 steps) ------------------------------------\n",
      "Max loss: 0.026568621397018433\n",
      "Min loss: 0.008106295019388199\n",
      "Mean loss: 0.015933107895155747\n",
      "Std loss: 0.0075934669806190245\n",
      "Total Loss: 0.09559864737093449\n",
      "------------------------------------ epoch 3097 (18576 steps) ------------------------------------\n",
      "Max loss: 0.02126406878232956\n",
      "Min loss: 0.007081096060574055\n",
      "Mean loss: 0.012877859019984802\n",
      "Std loss: 0.005408265143622287\n",
      "Total Loss: 0.07726715411990881\n",
      "------------------------------------ epoch 3098 (18582 steps) ------------------------------------\n",
      "Max loss: 0.0425497442483902\n",
      "Min loss: 0.006389106623828411\n",
      "Mean loss: 0.0163023939045767\n",
      "Std loss: 0.012409707642332637\n",
      "Total Loss: 0.0978143634274602\n",
      "------------------------------------ epoch 3099 (18588 steps) ------------------------------------\n",
      "Max loss: 0.02576947771012783\n",
      "Min loss: 0.008635776117444038\n",
      "Mean loss: 0.016358307252327602\n",
      "Std loss: 0.005163568376937329\n",
      "Total Loss: 0.0981498435139656\n",
      "------------------------------------ epoch 3100 (18594 steps) ------------------------------------\n",
      "Max loss: 0.01464314665645361\n",
      "Min loss: 0.008053556084632874\n",
      "Mean loss: 0.012011972721666098\n",
      "Std loss: 0.002358431830289082\n",
      "Total Loss: 0.07207183632999659\n",
      "------------------------------------ epoch 3101 (18600 steps) ------------------------------------\n",
      "Max loss: 0.036682453006505966\n",
      "Min loss: 0.007335122674703598\n",
      "Mean loss: 0.0150141641497612\n",
      "Std loss: 0.010138375154484994\n",
      "Total Loss: 0.0900849848985672\n",
      "saved model at ./weights/model_3101.pth\n",
      "------------------------------------ epoch 3102 (18606 steps) ------------------------------------\n",
      "Max loss: 0.01666809618473053\n",
      "Min loss: 0.008815161883831024\n",
      "Mean loss: 0.012211247502515713\n",
      "Std loss: 0.0027562405981796097\n",
      "Total Loss: 0.07326748501509428\n",
      "------------------------------------ epoch 3103 (18612 steps) ------------------------------------\n",
      "Max loss: 0.027543647214770317\n",
      "Min loss: 0.010500139556825161\n",
      "Mean loss: 0.016969985794276\n",
      "Std loss: 0.006479577176971534\n",
      "Total Loss: 0.101819914765656\n",
      "------------------------------------ epoch 3104 (18618 steps) ------------------------------------\n",
      "Max loss: 0.02337069623172283\n",
      "Min loss: 0.007004127837717533\n",
      "Mean loss: 0.011652259388938546\n",
      "Std loss: 0.005513316554866988\n",
      "Total Loss: 0.06991355633363128\n",
      "------------------------------------ epoch 3105 (18624 steps) ------------------------------------\n",
      "Max loss: 0.0497225783765316\n",
      "Min loss: 0.006897508166730404\n",
      "Mean loss: 0.01855033713703354\n",
      "Std loss: 0.015032635669948334\n",
      "Total Loss: 0.11130202282220125\n",
      "------------------------------------ epoch 3106 (18630 steps) ------------------------------------\n",
      "Max loss: 0.026500120759010315\n",
      "Min loss: 0.008812127634882927\n",
      "Mean loss: 0.01565017830580473\n",
      "Std loss: 0.0063491248485260535\n",
      "Total Loss: 0.09390106983482838\n",
      "------------------------------------ epoch 3107 (18636 steps) ------------------------------------\n",
      "Max loss: 0.03329131379723549\n",
      "Min loss: 0.007908198982477188\n",
      "Mean loss: 0.018040860537439585\n",
      "Std loss: 0.01045457939670842\n",
      "Total Loss: 0.10824516322463751\n",
      "------------------------------------ epoch 3108 (18642 steps) ------------------------------------\n",
      "Max loss: 0.021033955737948418\n",
      "Min loss: 0.006870177574455738\n",
      "Mean loss: 0.012919332676877579\n",
      "Std loss: 0.004192662899982334\n",
      "Total Loss: 0.07751599606126547\n",
      "------------------------------------ epoch 3109 (18648 steps) ------------------------------------\n",
      "Max loss: 0.01696411333978176\n",
      "Min loss: 0.007906853221356869\n",
      "Mean loss: 0.012593949058403572\n",
      "Std loss: 0.0031917121883649193\n",
      "Total Loss: 0.07556369435042143\n",
      "------------------------------------ epoch 3110 (18654 steps) ------------------------------------\n",
      "Max loss: 0.051432691514492035\n",
      "Min loss: 0.00961846113204956\n",
      "Mean loss: 0.019625824720909197\n",
      "Std loss: 0.014638054919054552\n",
      "Total Loss: 0.11775494832545519\n",
      "------------------------------------ epoch 3111 (18660 steps) ------------------------------------\n",
      "Max loss: 0.035246387124061584\n",
      "Min loss: 0.007045470178127289\n",
      "Mean loss: 0.016774852760136127\n",
      "Std loss: 0.012386332866543824\n",
      "Total Loss: 0.10064911656081676\n",
      "------------------------------------ epoch 3112 (18666 steps) ------------------------------------\n",
      "Max loss: 0.02659459412097931\n",
      "Min loss: 0.00781480222940445\n",
      "Mean loss: 0.01731977704912424\n",
      "Std loss: 0.007329815838989888\n",
      "Total Loss: 0.10391866229474545\n",
      "------------------------------------ epoch 3113 (18672 steps) ------------------------------------\n",
      "Max loss: 0.017851920798420906\n",
      "Min loss: 0.008043735288083553\n",
      "Mean loss: 0.013525054013977448\n",
      "Std loss: 0.0033256800683001803\n",
      "Total Loss: 0.08115032408386469\n",
      "------------------------------------ epoch 3114 (18678 steps) ------------------------------------\n",
      "Max loss: 0.012316098436713219\n",
      "Min loss: 0.008327609859406948\n",
      "Mean loss: 0.009788433089852333\n",
      "Std loss: 0.001590512603965956\n",
      "Total Loss: 0.058730598539114\n",
      "------------------------------------ epoch 3115 (18684 steps) ------------------------------------\n",
      "Max loss: 0.02518174797296524\n",
      "Min loss: 0.009784962981939316\n",
      "Mean loss: 0.016250718695422012\n",
      "Std loss: 0.005893518591082795\n",
      "Total Loss: 0.09750431217253208\n",
      "------------------------------------ epoch 3116 (18690 steps) ------------------------------------\n",
      "Max loss: 0.038799386471509933\n",
      "Min loss: 0.012500844895839691\n",
      "Mean loss: 0.020421278662979603\n",
      "Std loss: 0.008940684849384964\n",
      "Total Loss: 0.12252767197787762\n",
      "------------------------------------ epoch 3117 (18696 steps) ------------------------------------\n",
      "Max loss: 0.04315432906150818\n",
      "Min loss: 0.014341111294925213\n",
      "Mean loss: 0.023412878159433603\n",
      "Std loss: 0.009875098102987388\n",
      "Total Loss: 0.14047726895660162\n",
      "------------------------------------ epoch 3118 (18702 steps) ------------------------------------\n",
      "Max loss: 0.0201578252017498\n",
      "Min loss: 0.009744685143232346\n",
      "Mean loss: 0.013874291442334652\n",
      "Std loss: 0.003817789639737498\n",
      "Total Loss: 0.08324574865400791\n",
      "------------------------------------ epoch 3119 (18708 steps) ------------------------------------\n",
      "Max loss: 0.011178497225046158\n",
      "Min loss: 0.008300617337226868\n",
      "Mean loss: 0.00974930248533686\n",
      "Std loss: 0.0011038488034230145\n",
      "Total Loss: 0.05849581491202116\n",
      "------------------------------------ epoch 3120 (18714 steps) ------------------------------------\n",
      "Max loss: 0.026902396231889725\n",
      "Min loss: 0.007659664377570152\n",
      "Mean loss: 0.015508726394424835\n",
      "Std loss: 0.008046649466783094\n",
      "Total Loss: 0.09305235836654902\n",
      "------------------------------------ epoch 3121 (18720 steps) ------------------------------------\n",
      "Max loss: 0.025071170181035995\n",
      "Min loss: 0.008172387257218361\n",
      "Mean loss: 0.013779265340417624\n",
      "Std loss: 0.00597116348965525\n",
      "Total Loss: 0.08267559204250574\n",
      "------------------------------------ epoch 3122 (18726 steps) ------------------------------------\n",
      "Max loss: 0.06108829379081726\n",
      "Min loss: 0.011256697587668896\n",
      "Mean loss: 0.026523970688382786\n",
      "Std loss: 0.018382445062681392\n",
      "Total Loss: 0.1591438241302967\n",
      "------------------------------------ epoch 3123 (18732 steps) ------------------------------------\n",
      "Max loss: 0.019230980426073074\n",
      "Min loss: 0.012736432254314423\n",
      "Mean loss: 0.015452116572608551\n",
      "Std loss: 0.0024463170210675513\n",
      "Total Loss: 0.0927126994356513\n",
      "------------------------------------ epoch 3124 (18738 steps) ------------------------------------\n",
      "Max loss: 0.016432713717222214\n",
      "Min loss: 0.009267430752515793\n",
      "Mean loss: 0.013998331812520822\n",
      "Std loss: 0.0024389432481783523\n",
      "Total Loss: 0.08398999087512493\n",
      "------------------------------------ epoch 3125 (18744 steps) ------------------------------------\n",
      "Max loss: 0.025375131517648697\n",
      "Min loss: 0.007721642963588238\n",
      "Mean loss: 0.016652538751562435\n",
      "Std loss: 0.007041118807867421\n",
      "Total Loss: 0.09991523250937462\n",
      "------------------------------------ epoch 3126 (18750 steps) ------------------------------------\n",
      "Max loss: 0.045005910098552704\n",
      "Min loss: 0.008540531620383263\n",
      "Mean loss: 0.017008831879744928\n",
      "Std loss: 0.012605496857071515\n",
      "Total Loss: 0.10205299127846956\n",
      "------------------------------------ epoch 3127 (18756 steps) ------------------------------------\n",
      "Max loss: 0.03492448478937149\n",
      "Min loss: 0.008525719866156578\n",
      "Mean loss: 0.022399531211704016\n",
      "Std loss: 0.009772785760111088\n",
      "Total Loss: 0.1343971872702241\n",
      "------------------------------------ epoch 3128 (18762 steps) ------------------------------------\n",
      "Max loss: 0.017008919268846512\n",
      "Min loss: 0.01063680648803711\n",
      "Mean loss: 0.013988918624818325\n",
      "Std loss: 0.0024640045467887238\n",
      "Total Loss: 0.08393351174890995\n",
      "------------------------------------ epoch 3129 (18768 steps) ------------------------------------\n",
      "Max loss: 0.03170908987522125\n",
      "Min loss: 0.015356814488768578\n",
      "Mean loss: 0.024673876042167347\n",
      "Std loss: 0.005276682125312121\n",
      "Total Loss: 0.14804325625300407\n",
      "------------------------------------ epoch 3130 (18774 steps) ------------------------------------\n",
      "Max loss: 0.060002487152814865\n",
      "Min loss: 0.010093513876199722\n",
      "Mean loss: 0.021929931206007797\n",
      "Std loss: 0.01738615230570788\n",
      "Total Loss: 0.1315795872360468\n",
      "------------------------------------ epoch 3131 (18780 steps) ------------------------------------\n",
      "Max loss: 0.07029636949300766\n",
      "Min loss: 0.01273079589009285\n",
      "Mean loss: 0.034115757482747235\n",
      "Std loss: 0.01965387722569891\n",
      "Total Loss: 0.20469454489648342\n",
      "------------------------------------ epoch 3132 (18786 steps) ------------------------------------\n",
      "Max loss: 0.03888455033302307\n",
      "Min loss: 0.025385230779647827\n",
      "Mean loss: 0.03167489015807708\n",
      "Std loss: 0.005560702356558571\n",
      "Total Loss: 0.1900493409484625\n",
      "------------------------------------ epoch 3133 (18792 steps) ------------------------------------\n",
      "Max loss: 0.043213099241256714\n",
      "Min loss: 0.013717353343963623\n",
      "Mean loss: 0.026558177235225838\n",
      "Std loss: 0.010091356992754062\n",
      "Total Loss: 0.15934906341135502\n",
      "------------------------------------ epoch 3134 (18798 steps) ------------------------------------\n",
      "Max loss: 0.022618010640144348\n",
      "Min loss: 0.012422388419508934\n",
      "Mean loss: 0.01654527708888054\n",
      "Std loss: 0.003052461067085477\n",
      "Total Loss: 0.09927166253328323\n",
      "------------------------------------ epoch 3135 (18804 steps) ------------------------------------\n",
      "Max loss: 0.029933594167232513\n",
      "Min loss: 0.018250128254294395\n",
      "Mean loss: 0.02414493386944135\n",
      "Std loss: 0.004304614983308997\n",
      "Total Loss: 0.1448696032166481\n",
      "------------------------------------ epoch 3136 (18810 steps) ------------------------------------\n",
      "Max loss: 0.049547262489795685\n",
      "Min loss: 0.012350822798907757\n",
      "Mean loss: 0.023067462723702192\n",
      "Std loss: 0.012849407186613124\n",
      "Total Loss: 0.13840477634221315\n",
      "------------------------------------ epoch 3137 (18816 steps) ------------------------------------\n",
      "Max loss: 0.02250102162361145\n",
      "Min loss: 0.013606968335807323\n",
      "Mean loss: 0.01636743840451042\n",
      "Std loss: 0.0031517545497066687\n",
      "Total Loss: 0.09820463042706251\n",
      "------------------------------------ epoch 3138 (18822 steps) ------------------------------------\n",
      "Max loss: 0.027069222182035446\n",
      "Min loss: 0.012423026375472546\n",
      "Mean loss: 0.020135701323548954\n",
      "Std loss: 0.005256803366096077\n",
      "Total Loss: 0.12081420794129372\n",
      "------------------------------------ epoch 3139 (18828 steps) ------------------------------------\n",
      "Max loss: 0.020156044512987137\n",
      "Min loss: 0.009187672287225723\n",
      "Mean loss: 0.013310501351952553\n",
      "Std loss: 0.003382709947016729\n",
      "Total Loss: 0.07986300811171532\n",
      "------------------------------------ epoch 3140 (18834 steps) ------------------------------------\n",
      "Max loss: 0.01632048189640045\n",
      "Min loss: 0.011761758476495743\n",
      "Mean loss: 0.013648697175085545\n",
      "Std loss: 0.0015028625703093967\n",
      "Total Loss: 0.08189218305051327\n",
      "------------------------------------ epoch 3141 (18840 steps) ------------------------------------\n",
      "Max loss: 0.030180517584085464\n",
      "Min loss: 0.00984291359782219\n",
      "Mean loss: 0.016233945731073618\n",
      "Std loss: 0.0080099655940769\n",
      "Total Loss: 0.09740367438644171\n",
      "------------------------------------ epoch 3142 (18846 steps) ------------------------------------\n",
      "Max loss: 0.02172776311635971\n",
      "Min loss: 0.008029243908822536\n",
      "Mean loss: 0.012315863743424416\n",
      "Std loss: 0.004431519905680772\n",
      "Total Loss: 0.0738951824605465\n",
      "------------------------------------ epoch 3143 (18852 steps) ------------------------------------\n",
      "Max loss: 0.02027205005288124\n",
      "Min loss: 0.008539500646293163\n",
      "Mean loss: 0.013517841075857481\n",
      "Std loss: 0.003916902107640057\n",
      "Total Loss: 0.08110704645514488\n",
      "------------------------------------ epoch 3144 (18858 steps) ------------------------------------\n",
      "Max loss: 0.03034411370754242\n",
      "Min loss: 0.010380290448665619\n",
      "Mean loss: 0.016968705070515473\n",
      "Std loss: 0.006766312621436929\n",
      "Total Loss: 0.10181223042309284\n",
      "------------------------------------ epoch 3145 (18864 steps) ------------------------------------\n",
      "Max loss: 0.02081611007452011\n",
      "Min loss: 0.00811548437923193\n",
      "Mean loss: 0.013186518102884293\n",
      "Std loss: 0.005096786494119753\n",
      "Total Loss: 0.07911910861730576\n",
      "------------------------------------ epoch 3146 (18870 steps) ------------------------------------\n",
      "Max loss: 0.05059703439474106\n",
      "Min loss: 0.01083498727530241\n",
      "Mean loss: 0.021578247503687937\n",
      "Std loss: 0.013252812947290726\n",
      "Total Loss: 0.12946948502212763\n",
      "------------------------------------ epoch 3147 (18876 steps) ------------------------------------\n",
      "Max loss: 0.025930538773536682\n",
      "Min loss: 0.007626925595104694\n",
      "Mean loss: 0.015476293706645569\n",
      "Std loss: 0.0061537709633959535\n",
      "Total Loss: 0.09285776223987341\n",
      "------------------------------------ epoch 3148 (18882 steps) ------------------------------------\n",
      "Max loss: 0.022942587733268738\n",
      "Min loss: 0.008961809799075127\n",
      "Mean loss: 0.01318365785603722\n",
      "Std loss: 0.004650029025041838\n",
      "Total Loss: 0.07910194713622332\n",
      "------------------------------------ epoch 3149 (18888 steps) ------------------------------------\n",
      "Max loss: 0.04284943267703056\n",
      "Min loss: 0.007474774494767189\n",
      "Mean loss: 0.019931531821688015\n",
      "Std loss: 0.011240424531258343\n",
      "Total Loss: 0.1195891909301281\n",
      "------------------------------------ epoch 3150 (18894 steps) ------------------------------------\n",
      "Max loss: 0.02930140122771263\n",
      "Min loss: 0.012513477355241776\n",
      "Mean loss: 0.01882636562610666\n",
      "Std loss: 0.007097969608390694\n",
      "Total Loss: 0.11295819375663996\n",
      "------------------------------------ epoch 3151 (18900 steps) ------------------------------------\n",
      "Max loss: 0.030205588787794113\n",
      "Min loss: 0.008548040874302387\n",
      "Mean loss: 0.013434140011668205\n",
      "Std loss: 0.0076786146876137735\n",
      "Total Loss: 0.08060484007000923\n",
      "------------------------------------ epoch 3152 (18906 steps) ------------------------------------\n",
      "Max loss: 0.03478965163230896\n",
      "Min loss: 0.006997283548116684\n",
      "Mean loss: 0.015012189435462156\n",
      "Std loss: 0.009251668946860414\n",
      "Total Loss: 0.09007313661277294\n",
      "------------------------------------ epoch 3153 (18912 steps) ------------------------------------\n",
      "Max loss: 0.029346290975809097\n",
      "Min loss: 0.009141355752944946\n",
      "Mean loss: 0.017163951881229877\n",
      "Std loss: 0.007198162246163957\n",
      "Total Loss: 0.10298371128737926\n",
      "------------------------------------ epoch 3154 (18918 steps) ------------------------------------\n",
      "Max loss: 0.029057277366518974\n",
      "Min loss: 0.006894763559103012\n",
      "Mean loss: 0.016110059339553118\n",
      "Std loss: 0.008155215937288516\n",
      "Total Loss: 0.0966603560373187\n",
      "------------------------------------ epoch 3155 (18924 steps) ------------------------------------\n",
      "Max loss: 0.05198705196380615\n",
      "Min loss: 0.008158894255757332\n",
      "Mean loss: 0.024739425939818222\n",
      "Std loss: 0.015031771126614812\n",
      "Total Loss: 0.14843655563890934\n",
      "------------------------------------ epoch 3156 (18930 steps) ------------------------------------\n",
      "Max loss: 0.02215430699288845\n",
      "Min loss: 0.009900690987706184\n",
      "Mean loss: 0.017133763059973717\n",
      "Std loss: 0.004393791905808275\n",
      "Total Loss: 0.1028025783598423\n",
      "------------------------------------ epoch 3157 (18936 steps) ------------------------------------\n",
      "Max loss: 0.03003201261162758\n",
      "Min loss: 0.008271659724414349\n",
      "Mean loss: 0.01653282344341278\n",
      "Std loss: 0.007136423001789325\n",
      "Total Loss: 0.09919694066047668\n",
      "------------------------------------ epoch 3158 (18942 steps) ------------------------------------\n",
      "Max loss: 0.023236950859427452\n",
      "Min loss: 0.010863518342375755\n",
      "Mean loss: 0.017036661505699158\n",
      "Std loss: 0.003773346501624147\n",
      "Total Loss: 0.10221996903419495\n",
      "------------------------------------ epoch 3159 (18948 steps) ------------------------------------\n",
      "Max loss: 0.015715796500444412\n",
      "Min loss: 0.006734700873494148\n",
      "Mean loss: 0.010802576706434289\n",
      "Std loss: 0.0035326825994402133\n",
      "Total Loss: 0.06481546023860574\n",
      "------------------------------------ epoch 3160 (18954 steps) ------------------------------------\n",
      "Max loss: 0.02226201817393303\n",
      "Min loss: 0.009565136395394802\n",
      "Mean loss: 0.015007234333703915\n",
      "Std loss: 0.004388320123748329\n",
      "Total Loss: 0.09004340600222349\n",
      "------------------------------------ epoch 3161 (18960 steps) ------------------------------------\n",
      "Max loss: 0.025198306888341904\n",
      "Min loss: 0.00845695286989212\n",
      "Mean loss: 0.016137384188671906\n",
      "Std loss: 0.005217729601729866\n",
      "Total Loss: 0.09682430513203144\n",
      "------------------------------------ epoch 3162 (18966 steps) ------------------------------------\n",
      "Max loss: 0.018356723710894585\n",
      "Min loss: 0.007505665998905897\n",
      "Mean loss: 0.011654451411838332\n",
      "Std loss: 0.003638232009766825\n",
      "Total Loss: 0.06992670847103\n",
      "------------------------------------ epoch 3163 (18972 steps) ------------------------------------\n",
      "Max loss: 0.014899800531566143\n",
      "Min loss: 0.00756737170740962\n",
      "Mean loss: 0.010690337046980858\n",
      "Std loss: 0.0026217811701111904\n",
      "Total Loss: 0.06414202228188515\n",
      "------------------------------------ epoch 3164 (18978 steps) ------------------------------------\n",
      "Max loss: 0.027773160487413406\n",
      "Min loss: 0.006457418203353882\n",
      "Mean loss: 0.014694742237528166\n",
      "Std loss: 0.007278117561708968\n",
      "Total Loss: 0.08816845342516899\n",
      "------------------------------------ epoch 3165 (18984 steps) ------------------------------------\n",
      "Max loss: 0.024396784603595734\n",
      "Min loss: 0.007038336247205734\n",
      "Mean loss: 0.014663662140568098\n",
      "Std loss: 0.005936683040832068\n",
      "Total Loss: 0.08798197284340858\n",
      "------------------------------------ epoch 3166 (18990 steps) ------------------------------------\n",
      "Max loss: 0.055886950343847275\n",
      "Min loss: 0.008448227308690548\n",
      "Mean loss: 0.025515037588775158\n",
      "Std loss: 0.01750598751899419\n",
      "Total Loss: 0.15309022553265095\n",
      "------------------------------------ epoch 3167 (18996 steps) ------------------------------------\n",
      "Max loss: 0.05297030508518219\n",
      "Min loss: 0.008194120600819588\n",
      "Mean loss: 0.02081302786245942\n",
      "Std loss: 0.015094797943765195\n",
      "Total Loss: 0.12487816717475653\n",
      "------------------------------------ epoch 3168 (19002 steps) ------------------------------------\n",
      "Max loss: 0.019637588411569595\n",
      "Min loss: 0.013870814815163612\n",
      "Mean loss: 0.015554614830762148\n",
      "Std loss: 0.0019697490369219007\n",
      "Total Loss: 0.09332768898457289\n",
      "------------------------------------ epoch 3169 (19008 steps) ------------------------------------\n",
      "Max loss: 0.037111274898052216\n",
      "Min loss: 0.009164116345345974\n",
      "Mean loss: 0.020305498813589413\n",
      "Std loss: 0.010136299009722934\n",
      "Total Loss: 0.12183299288153648\n",
      "------------------------------------ epoch 3170 (19014 steps) ------------------------------------\n",
      "Max loss: 0.04344587028026581\n",
      "Min loss: 0.007606059312820435\n",
      "Mean loss: 0.022365017017970484\n",
      "Std loss: 0.01225610312498842\n",
      "Total Loss: 0.1341901021078229\n",
      "------------------------------------ epoch 3171 (19020 steps) ------------------------------------\n",
      "Max loss: 0.024158142507076263\n",
      "Min loss: 0.011685962788760662\n",
      "Mean loss: 0.015962448591987293\n",
      "Std loss: 0.004440565351437923\n",
      "Total Loss: 0.09577469155192375\n",
      "------------------------------------ epoch 3172 (19026 steps) ------------------------------------\n",
      "Max loss: 0.04077240824699402\n",
      "Min loss: 0.010941272601485252\n",
      "Mean loss: 0.016201738578577835\n",
      "Std loss: 0.010991466129180937\n",
      "Total Loss: 0.09721043147146702\n",
      "------------------------------------ epoch 3173 (19032 steps) ------------------------------------\n",
      "Max loss: 0.019163956865668297\n",
      "Min loss: 0.009682206436991692\n",
      "Mean loss: 0.0136927569595476\n",
      "Std loss: 0.003485494971326016\n",
      "Total Loss: 0.0821565417572856\n",
      "------------------------------------ epoch 3174 (19038 steps) ------------------------------------\n",
      "Max loss: 0.035338032990694046\n",
      "Min loss: 0.010774752125144005\n",
      "Mean loss: 0.023104952027400334\n",
      "Std loss: 0.00912334616577818\n",
      "Total Loss: 0.138629712164402\n",
      "------------------------------------ epoch 3175 (19044 steps) ------------------------------------\n",
      "Max loss: 0.02561628445982933\n",
      "Min loss: 0.008196204900741577\n",
      "Mean loss: 0.018052353834112484\n",
      "Std loss: 0.007026790049981092\n",
      "Total Loss: 0.10831412300467491\n",
      "------------------------------------ epoch 3176 (19050 steps) ------------------------------------\n",
      "Max loss: 0.03618689626455307\n",
      "Min loss: 0.009054020047187805\n",
      "Mean loss: 0.018942374736070633\n",
      "Std loss: 0.010526045941886527\n",
      "Total Loss: 0.1136542484164238\n",
      "------------------------------------ epoch 3177 (19056 steps) ------------------------------------\n",
      "Max loss: 0.0324062816798687\n",
      "Min loss: 0.008979979902505875\n",
      "Mean loss: 0.019322894979268312\n",
      "Std loss: 0.008426876630060032\n",
      "Total Loss: 0.11593736987560987\n",
      "------------------------------------ epoch 3178 (19062 steps) ------------------------------------\n",
      "Max loss: 0.03925606608390808\n",
      "Min loss: 0.009163322858512402\n",
      "Mean loss: 0.017943215866883595\n",
      "Std loss: 0.010075026376807872\n",
      "Total Loss: 0.10765929520130157\n",
      "------------------------------------ epoch 3179 (19068 steps) ------------------------------------\n",
      "Max loss: 0.037860266864299774\n",
      "Min loss: 0.00984619464725256\n",
      "Mean loss: 0.019749800364176433\n",
      "Std loss: 0.00946412916120894\n",
      "Total Loss: 0.1184988021850586\n",
      "------------------------------------ epoch 3180 (19074 steps) ------------------------------------\n",
      "Max loss: 0.036183759570121765\n",
      "Min loss: 0.007834425196051598\n",
      "Mean loss: 0.01998279041921099\n",
      "Std loss: 0.011005249343892732\n",
      "Total Loss: 0.11989674251526594\n",
      "------------------------------------ epoch 3181 (19080 steps) ------------------------------------\n",
      "Max loss: 0.01975964941084385\n",
      "Min loss: 0.008194510824978352\n",
      "Mean loss: 0.014038593353082737\n",
      "Std loss: 0.004897347125204737\n",
      "Total Loss: 0.08423156011849642\n",
      "------------------------------------ epoch 3182 (19086 steps) ------------------------------------\n",
      "Max loss: 0.0411234088242054\n",
      "Min loss: 0.009109273552894592\n",
      "Mean loss: 0.02169198263436556\n",
      "Std loss: 0.010397545984818032\n",
      "Total Loss: 0.13015189580619335\n",
      "------------------------------------ epoch 3183 (19092 steps) ------------------------------------\n",
      "Max loss: 0.02529183030128479\n",
      "Min loss: 0.00822489894926548\n",
      "Mean loss: 0.018099607123682897\n",
      "Std loss: 0.005679435515067909\n",
      "Total Loss: 0.10859764274209738\n",
      "------------------------------------ epoch 3184 (19098 steps) ------------------------------------\n",
      "Max loss: 0.032492924481630325\n",
      "Min loss: 0.009914979338645935\n",
      "Mean loss: 0.01852075258890788\n",
      "Std loss: 0.008425744563012363\n",
      "Total Loss: 0.11112451553344727\n",
      "------------------------------------ epoch 3185 (19104 steps) ------------------------------------\n",
      "Max loss: 0.03860573470592499\n",
      "Min loss: 0.008100718259811401\n",
      "Mean loss: 0.015100368609031042\n",
      "Std loss: 0.010599641433601833\n",
      "Total Loss: 0.09060221165418625\n",
      "------------------------------------ epoch 3186 (19110 steps) ------------------------------------\n",
      "Max loss: 0.023925794288516045\n",
      "Min loss: 0.006809777580201626\n",
      "Mean loss: 0.012099986430257559\n",
      "Std loss: 0.005833782920373918\n",
      "Total Loss: 0.07259991858154535\n",
      "------------------------------------ epoch 3187 (19116 steps) ------------------------------------\n",
      "Max loss: 0.03380163758993149\n",
      "Min loss: 0.007405032869428396\n",
      "Mean loss: 0.014556397373477617\n",
      "Std loss: 0.00919407356714998\n",
      "Total Loss: 0.08733838424086571\n",
      "------------------------------------ epoch 3188 (19122 steps) ------------------------------------\n",
      "Max loss: 0.0738692432641983\n",
      "Min loss: 0.007457919418811798\n",
      "Mean loss: 0.02145757448549072\n",
      "Std loss: 0.02363922190563898\n",
      "Total Loss: 0.12874544691294432\n",
      "------------------------------------ epoch 3189 (19128 steps) ------------------------------------\n",
      "Max loss: 0.017280196771025658\n",
      "Min loss: 0.00735686719417572\n",
      "Mean loss: 0.01209804384658734\n",
      "Std loss: 0.0037114972296987945\n",
      "Total Loss: 0.07258826307952404\n",
      "------------------------------------ epoch 3190 (19134 steps) ------------------------------------\n",
      "Max loss: 0.028220808133482933\n",
      "Min loss: 0.007683116942644119\n",
      "Mean loss: 0.014225698386629423\n",
      "Std loss: 0.0076165814142430096\n",
      "Total Loss: 0.08535419031977654\n",
      "------------------------------------ epoch 3191 (19140 steps) ------------------------------------\n",
      "Max loss: 0.028392096981406212\n",
      "Min loss: 0.006983472034335136\n",
      "Mean loss: 0.015202268026769161\n",
      "Std loss: 0.0068641376546571525\n",
      "Total Loss: 0.09121360816061497\n",
      "------------------------------------ epoch 3192 (19146 steps) ------------------------------------\n",
      "Max loss: 0.02953752502799034\n",
      "Min loss: 0.01033666729927063\n",
      "Mean loss: 0.02063793797666828\n",
      "Std loss: 0.007295562090039867\n",
      "Total Loss: 0.12382762786000967\n",
      "------------------------------------ epoch 3193 (19152 steps) ------------------------------------\n",
      "Max loss: 0.04024682566523552\n",
      "Min loss: 0.009649969637393951\n",
      "Mean loss: 0.016836858664949734\n",
      "Std loss: 0.01110241930189687\n",
      "Total Loss: 0.10102115198969841\n",
      "------------------------------------ epoch 3194 (19158 steps) ------------------------------------\n",
      "Max loss: 0.05706527829170227\n",
      "Min loss: 0.008950728923082352\n",
      "Mean loss: 0.020484584694107372\n",
      "Std loss: 0.017363216961943203\n",
      "Total Loss: 0.12290750816464424\n",
      "------------------------------------ epoch 3195 (19164 steps) ------------------------------------\n",
      "Max loss: 0.044193070381879807\n",
      "Min loss: 0.009783913381397724\n",
      "Mean loss: 0.01959273995210727\n",
      "Std loss: 0.012647505130942547\n",
      "Total Loss: 0.11755643971264362\n",
      "------------------------------------ epoch 3196 (19170 steps) ------------------------------------\n",
      "Max loss: 0.02417941391468048\n",
      "Min loss: 0.012052811682224274\n",
      "Mean loss: 0.020033488360544045\n",
      "Std loss: 0.004463752422120937\n",
      "Total Loss: 0.12020093016326427\n",
      "------------------------------------ epoch 3197 (19176 steps) ------------------------------------\n",
      "Max loss: 0.031185965985059738\n",
      "Min loss: 0.009134864434599876\n",
      "Mean loss: 0.017335094045847654\n",
      "Std loss: 0.006717448818292339\n",
      "Total Loss: 0.10401056427508593\n",
      "------------------------------------ epoch 3198 (19182 steps) ------------------------------------\n",
      "Max loss: 0.027058793231844902\n",
      "Min loss: 0.010573742911219597\n",
      "Mean loss: 0.017334964592009783\n",
      "Std loss: 0.005714665183384051\n",
      "Total Loss: 0.1040097875520587\n",
      "------------------------------------ epoch 3199 (19188 steps) ------------------------------------\n",
      "Max loss: 0.035367533564567566\n",
      "Min loss: 0.00944806169718504\n",
      "Mean loss: 0.019771046781291563\n",
      "Std loss: 0.009014060228974758\n",
      "Total Loss: 0.11862628068774939\n",
      "------------------------------------ epoch 3200 (19194 steps) ------------------------------------\n",
      "Max loss: 0.018990807235240936\n",
      "Min loss: 0.008036540821194649\n",
      "Mean loss: 0.010790905449539423\n",
      "Std loss: 0.0037627404293970475\n",
      "Total Loss: 0.06474543269723654\n",
      "------------------------------------ epoch 3201 (19200 steps) ------------------------------------\n",
      "Max loss: 0.01855337992310524\n",
      "Min loss: 0.008459046483039856\n",
      "Mean loss: 0.012673615167538324\n",
      "Std loss: 0.0037864658243787074\n",
      "Total Loss: 0.07604169100522995\n",
      "saved model at ./weights/model_3201.pth\n",
      "------------------------------------ epoch 3202 (19206 steps) ------------------------------------\n",
      "Max loss: 0.024590853601694107\n",
      "Min loss: 0.008015064522624016\n",
      "Mean loss: 0.013416163623332977\n",
      "Std loss: 0.005511728389380282\n",
      "Total Loss: 0.08049698173999786\n",
      "------------------------------------ epoch 3203 (19212 steps) ------------------------------------\n",
      "Max loss: 0.046677641570568085\n",
      "Min loss: 0.0071619777008891106\n",
      "Mean loss: 0.022019141043225925\n",
      "Std loss: 0.012940316254637678\n",
      "Total Loss: 0.13211484625935555\n",
      "------------------------------------ epoch 3204 (19218 steps) ------------------------------------\n",
      "Max loss: 0.03870069980621338\n",
      "Min loss: 0.008176028728485107\n",
      "Mean loss: 0.02009001544987162\n",
      "Std loss: 0.010052922857048686\n",
      "Total Loss: 0.12054009269922972\n",
      "------------------------------------ epoch 3205 (19224 steps) ------------------------------------\n",
      "Max loss: 0.02950873225927353\n",
      "Min loss: 0.009983864612877369\n",
      "Mean loss: 0.017326783544073503\n",
      "Std loss: 0.007022504439361481\n",
      "Total Loss: 0.10396070126444101\n",
      "------------------------------------ epoch 3206 (19230 steps) ------------------------------------\n",
      "Max loss: 0.029447048902511597\n",
      "Min loss: 0.011295570060610771\n",
      "Mean loss: 0.01930634956806898\n",
      "Std loss: 0.00599010502323465\n",
      "Total Loss: 0.11583809740841389\n",
      "------------------------------------ epoch 3207 (19236 steps) ------------------------------------\n",
      "Max loss: 0.04179401323199272\n",
      "Min loss: 0.01218391489237547\n",
      "Mean loss: 0.019121636326114338\n",
      "Std loss: 0.01033323276304279\n",
      "Total Loss: 0.11472981795668602\n",
      "------------------------------------ epoch 3208 (19242 steps) ------------------------------------\n",
      "Max loss: 0.05446988344192505\n",
      "Min loss: 0.00979666132479906\n",
      "Mean loss: 0.027515150761852663\n",
      "Std loss: 0.01420504928144492\n",
      "Total Loss: 0.16509090457111597\n",
      "------------------------------------ epoch 3209 (19248 steps) ------------------------------------\n",
      "Max loss: 0.02819991670548916\n",
      "Min loss: 0.010241537354886532\n",
      "Mean loss: 0.016429441204915445\n",
      "Std loss: 0.0058182984717507335\n",
      "Total Loss: 0.09857664722949266\n",
      "------------------------------------ epoch 3210 (19254 steps) ------------------------------------\n",
      "Max loss: 0.045559175312519073\n",
      "Min loss: 0.00961870513856411\n",
      "Mean loss: 0.025773536724348862\n",
      "Std loss: 0.01261172424782379\n",
      "Total Loss: 0.15464122034609318\n",
      "------------------------------------ epoch 3211 (19260 steps) ------------------------------------\n",
      "Max loss: 0.03977837786078453\n",
      "Min loss: 0.012363987974822521\n",
      "Mean loss: 0.01920666266232729\n",
      "Std loss: 0.009398048272758445\n",
      "Total Loss: 0.11523997597396374\n",
      "------------------------------------ epoch 3212 (19266 steps) ------------------------------------\n",
      "Max loss: 0.023346111178398132\n",
      "Min loss: 0.00805505272001028\n",
      "Mean loss: 0.015040070128937563\n",
      "Std loss: 0.005212636073201487\n",
      "Total Loss: 0.09024042077362537\n",
      "------------------------------------ epoch 3213 (19272 steps) ------------------------------------\n",
      "Max loss: 0.020216841250658035\n",
      "Min loss: 0.00945768877863884\n",
      "Mean loss: 0.013784616254270077\n",
      "Std loss: 0.003969265751968119\n",
      "Total Loss: 0.08270769752562046\n",
      "------------------------------------ epoch 3214 (19278 steps) ------------------------------------\n",
      "Max loss: 0.0477462112903595\n",
      "Min loss: 0.01017076801508665\n",
      "Mean loss: 0.02135396810869376\n",
      "Std loss: 0.014546192487476732\n",
      "Total Loss: 0.12812380865216255\n",
      "------------------------------------ epoch 3215 (19284 steps) ------------------------------------\n",
      "Max loss: 0.021865755319595337\n",
      "Min loss: 0.00901912059634924\n",
      "Mean loss: 0.013244765965888897\n",
      "Std loss: 0.004653329597123073\n",
      "Total Loss: 0.07946859579533339\n",
      "------------------------------------ epoch 3216 (19290 steps) ------------------------------------\n",
      "Max loss: 0.019132336601614952\n",
      "Min loss: 0.011065611615777016\n",
      "Mean loss: 0.014055048115551472\n",
      "Std loss: 0.002654816772279384\n",
      "Total Loss: 0.08433028869330883\n",
      "------------------------------------ epoch 3217 (19296 steps) ------------------------------------\n",
      "Max loss: 0.02831156551837921\n",
      "Min loss: 0.00729350745677948\n",
      "Mean loss: 0.01492968900129199\n",
      "Std loss: 0.007643523509969294\n",
      "Total Loss: 0.08957813400775194\n",
      "------------------------------------ epoch 3218 (19302 steps) ------------------------------------\n",
      "Max loss: 0.024249475449323654\n",
      "Min loss: 0.011944791302084923\n",
      "Mean loss: 0.014987383969128132\n",
      "Std loss: 0.004255001672035972\n",
      "Total Loss: 0.08992430381476879\n",
      "------------------------------------ epoch 3219 (19308 steps) ------------------------------------\n",
      "Max loss: 0.023939426988363266\n",
      "Min loss: 0.0089461300522089\n",
      "Mean loss: 0.01687338063493371\n",
      "Std loss: 0.0052116713827322705\n",
      "Total Loss: 0.10124028380960226\n",
      "------------------------------------ epoch 3220 (19314 steps) ------------------------------------\n",
      "Max loss: 0.01716696470975876\n",
      "Min loss: 0.0080118328332901\n",
      "Mean loss: 0.010882684340079626\n",
      "Std loss: 0.003249271361910835\n",
      "Total Loss: 0.06529610604047775\n",
      "------------------------------------ epoch 3221 (19320 steps) ------------------------------------\n",
      "Max loss: 0.01767977699637413\n",
      "Min loss: 0.008834682404994965\n",
      "Mean loss: 0.012387627580513557\n",
      "Std loss: 0.0029370722330227493\n",
      "Total Loss: 0.07432576548308134\n",
      "------------------------------------ epoch 3222 (19326 steps) ------------------------------------\n",
      "Max loss: 0.025033295154571533\n",
      "Min loss: 0.0066077737137675285\n",
      "Mean loss: 0.0133248302154243\n",
      "Std loss: 0.006073615841781679\n",
      "Total Loss: 0.0799489812925458\n",
      "------------------------------------ epoch 3223 (19332 steps) ------------------------------------\n",
      "Max loss: 0.03384809195995331\n",
      "Min loss: 0.006608007475733757\n",
      "Mean loss: 0.020413284345219534\n",
      "Std loss: 0.010030377545298336\n",
      "Total Loss: 0.1224797060713172\n",
      "------------------------------------ epoch 3224 (19338 steps) ------------------------------------\n",
      "Max loss: 0.033560171723365784\n",
      "Min loss: 0.0135898906737566\n",
      "Mean loss: 0.02049978543072939\n",
      "Std loss: 0.00706719075331579\n",
      "Total Loss: 0.12299871258437634\n",
      "------------------------------------ epoch 3225 (19344 steps) ------------------------------------\n",
      "Max loss: 0.048286229372024536\n",
      "Min loss: 0.0105354068800807\n",
      "Mean loss: 0.02269804586345951\n",
      "Std loss: 0.012148805979744687\n",
      "Total Loss: 0.13618827518075705\n",
      "------------------------------------ epoch 3226 (19350 steps) ------------------------------------\n",
      "Max loss: 0.06156201288104057\n",
      "Min loss: 0.010605509392917156\n",
      "Mean loss: 0.033582699950784445\n",
      "Std loss: 0.017738096477877045\n",
      "Total Loss: 0.20149619970470667\n",
      "------------------------------------ epoch 3227 (19356 steps) ------------------------------------\n",
      "Max loss: 0.05463681370019913\n",
      "Min loss: 0.01158156618475914\n",
      "Mean loss: 0.02678882423788309\n",
      "Std loss: 0.014733071068967022\n",
      "Total Loss: 0.16073294542729855\n",
      "------------------------------------ epoch 3228 (19362 steps) ------------------------------------\n",
      "Max loss: 0.05059367045760155\n",
      "Min loss: 0.009404690936207771\n",
      "Mean loss: 0.0229768514012297\n",
      "Std loss: 0.012978194172878635\n",
      "Total Loss: 0.1378611084073782\n",
      "------------------------------------ epoch 3229 (19368 steps) ------------------------------------\n",
      "Max loss: 0.03817468881607056\n",
      "Min loss: 0.012859450653195381\n",
      "Mean loss: 0.022584634677817423\n",
      "Std loss: 0.009946887266120067\n",
      "Total Loss: 0.13550780806690454\n",
      "------------------------------------ epoch 3230 (19374 steps) ------------------------------------\n",
      "Max loss: 0.04274440556764603\n",
      "Min loss: 0.01225939393043518\n",
      "Mean loss: 0.02141691992680232\n",
      "Std loss: 0.009916049731867478\n",
      "Total Loss: 0.1285015195608139\n",
      "------------------------------------ epoch 3231 (19380 steps) ------------------------------------\n",
      "Max loss: 0.03077760711312294\n",
      "Min loss: 0.008848940953612328\n",
      "Mean loss: 0.019207448388139408\n",
      "Std loss: 0.007077359354904908\n",
      "Total Loss: 0.11524469032883644\n",
      "------------------------------------ epoch 3232 (19386 steps) ------------------------------------\n",
      "Max loss: 0.02565821073949337\n",
      "Min loss: 0.009405599907040596\n",
      "Mean loss: 0.014872484685232243\n",
      "Std loss: 0.0059951508774006585\n",
      "Total Loss: 0.08923490811139345\n",
      "------------------------------------ epoch 3233 (19392 steps) ------------------------------------\n",
      "Max loss: 0.01639055460691452\n",
      "Min loss: 0.00849594920873642\n",
      "Mean loss: 0.012389370085050663\n",
      "Std loss: 0.002677663276730733\n",
      "Total Loss: 0.07433622051030397\n",
      "------------------------------------ epoch 3234 (19398 steps) ------------------------------------\n",
      "Max loss: 0.02004016935825348\n",
      "Min loss: 0.00914260558784008\n",
      "Mean loss: 0.011873877296845118\n",
      "Std loss: 0.003724621430486304\n",
      "Total Loss: 0.07124326378107071\n",
      "------------------------------------ epoch 3235 (19404 steps) ------------------------------------\n",
      "Max loss: 0.04305486008524895\n",
      "Min loss: 0.00669112429022789\n",
      "Mean loss: 0.018599275810023148\n",
      "Std loss: 0.012931541895797352\n",
      "Total Loss: 0.1115956548601389\n",
      "------------------------------------ epoch 3236 (19410 steps) ------------------------------------\n",
      "Max loss: 0.017052549868822098\n",
      "Min loss: 0.011736433021724224\n",
      "Mean loss: 0.013477244724829992\n",
      "Std loss: 0.001686212366062839\n",
      "Total Loss: 0.08086346834897995\n",
      "------------------------------------ epoch 3237 (19416 steps) ------------------------------------\n",
      "Max loss: 0.022161340340971947\n",
      "Min loss: 0.007486983668059111\n",
      "Mean loss: 0.013616137284164628\n",
      "Std loss: 0.004655869964781851\n",
      "Total Loss: 0.08169682370498776\n",
      "------------------------------------ epoch 3238 (19422 steps) ------------------------------------\n",
      "Max loss: 0.02062479965388775\n",
      "Min loss: 0.008299107663333416\n",
      "Mean loss: 0.01311690096432964\n",
      "Std loss: 0.004089243748617127\n",
      "Total Loss: 0.07870140578597784\n",
      "------------------------------------ epoch 3239 (19428 steps) ------------------------------------\n",
      "Max loss: 0.02556210570037365\n",
      "Min loss: 0.008414828218519688\n",
      "Mean loss: 0.01490895946820577\n",
      "Std loss: 0.005463389228523904\n",
      "Total Loss: 0.08945375680923462\n",
      "------------------------------------ epoch 3240 (19434 steps) ------------------------------------\n",
      "Max loss: 0.027000820264220238\n",
      "Min loss: 0.006715116556733847\n",
      "Mean loss: 0.017662595103805263\n",
      "Std loss: 0.006953082496578364\n",
      "Total Loss: 0.10597557062283158\n",
      "------------------------------------ epoch 3241 (19440 steps) ------------------------------------\n",
      "Max loss: 0.034785374999046326\n",
      "Min loss: 0.008218945935368538\n",
      "Mean loss: 0.016229096489648025\n",
      "Std loss: 0.009177011750306998\n",
      "Total Loss: 0.09737457893788815\n",
      "------------------------------------ epoch 3242 (19446 steps) ------------------------------------\n",
      "Max loss: 0.07736829668283463\n",
      "Min loss: 0.009121537208557129\n",
      "Mean loss: 0.025447887213279802\n",
      "Std loss: 0.023667742955994924\n",
      "Total Loss: 0.15268732327967882\n",
      "------------------------------------ epoch 3243 (19452 steps) ------------------------------------\n",
      "Max loss: 0.019090520218014717\n",
      "Min loss: 0.006805130746215582\n",
      "Mean loss: 0.012277982585753003\n",
      "Std loss: 0.00455617788943276\n",
      "Total Loss: 0.07366789551451802\n",
      "------------------------------------ epoch 3244 (19458 steps) ------------------------------------\n",
      "Max loss: 0.045610349625349045\n",
      "Min loss: 0.009723622351884842\n",
      "Mean loss: 0.018836316497375567\n",
      "Std loss: 0.012384897840319493\n",
      "Total Loss: 0.1130178989842534\n",
      "------------------------------------ epoch 3245 (19464 steps) ------------------------------------\n",
      "Max loss: 0.040917254984378815\n",
      "Min loss: 0.008715672418475151\n",
      "Mean loss: 0.021850387876232464\n",
      "Std loss: 0.010042406937045901\n",
      "Total Loss: 0.1311023272573948\n",
      "------------------------------------ epoch 3246 (19470 steps) ------------------------------------\n",
      "Max loss: 0.01979326456785202\n",
      "Min loss: 0.00808960385620594\n",
      "Mean loss: 0.012724571395665407\n",
      "Std loss: 0.004646751897050385\n",
      "Total Loss: 0.07634742837399244\n",
      "------------------------------------ epoch 3247 (19476 steps) ------------------------------------\n",
      "Max loss: 0.030348246917128563\n",
      "Min loss: 0.010459217242896557\n",
      "Mean loss: 0.016018759614477556\n",
      "Std loss: 0.006607117527463982\n",
      "Total Loss: 0.09611255768686533\n",
      "------------------------------------ epoch 3248 (19482 steps) ------------------------------------\n",
      "Max loss: 0.01882445439696312\n",
      "Min loss: 0.007077736314386129\n",
      "Mean loss: 0.010885843619083365\n",
      "Std loss: 0.0041350529821718605\n",
      "Total Loss: 0.06531506171450019\n",
      "------------------------------------ epoch 3249 (19488 steps) ------------------------------------\n",
      "Max loss: 0.03354080021381378\n",
      "Min loss: 0.0090139489620924\n",
      "Mean loss: 0.019007603637874126\n",
      "Std loss: 0.008575929122385527\n",
      "Total Loss: 0.11404562182724476\n",
      "------------------------------------ epoch 3250 (19494 steps) ------------------------------------\n",
      "Max loss: 0.023276478052139282\n",
      "Min loss: 0.00690834503620863\n",
      "Mean loss: 0.012928628828376532\n",
      "Std loss: 0.006247595176032005\n",
      "Total Loss: 0.07757177297025919\n",
      "------------------------------------ epoch 3251 (19500 steps) ------------------------------------\n",
      "Max loss: 0.029649965465068817\n",
      "Min loss: 0.012151996605098248\n",
      "Mean loss: 0.01790392616142829\n",
      "Std loss: 0.0057923610286767435\n",
      "Total Loss: 0.10742355696856976\n",
      "------------------------------------ epoch 3252 (19506 steps) ------------------------------------\n",
      "Max loss: 0.05153457075357437\n",
      "Min loss: 0.007274426054209471\n",
      "Mean loss: 0.019636280757064622\n",
      "Std loss: 0.016249385752547703\n",
      "Total Loss: 0.11781768454238772\n",
      "------------------------------------ epoch 3253 (19512 steps) ------------------------------------\n",
      "Max loss: 0.05838903412222862\n",
      "Min loss: 0.007685238029807806\n",
      "Mean loss: 0.018112310441210866\n",
      "Std loss: 0.018131466406561957\n",
      "Total Loss: 0.1086738626472652\n",
      "------------------------------------ epoch 3254 (19518 steps) ------------------------------------\n",
      "Max loss: 0.06742820143699646\n",
      "Min loss: 0.009387405589222908\n",
      "Mean loss: 0.024220401731630165\n",
      "Std loss: 0.019746756173963896\n",
      "Total Loss: 0.145322410389781\n",
      "------------------------------------ epoch 3255 (19524 steps) ------------------------------------\n",
      "Max loss: 0.03390063717961311\n",
      "Min loss: 0.007775735110044479\n",
      "Mean loss: 0.017633140397568543\n",
      "Std loss: 0.009587957865857758\n",
      "Total Loss: 0.10579884238541126\n",
      "------------------------------------ epoch 3256 (19530 steps) ------------------------------------\n",
      "Max loss: 0.027952492237091064\n",
      "Min loss: 0.007369647733867168\n",
      "Mean loss: 0.015478699002414942\n",
      "Std loss: 0.007274010583690393\n",
      "Total Loss: 0.09287219401448965\n",
      "------------------------------------ epoch 3257 (19536 steps) ------------------------------------\n",
      "Max loss: 0.02296609804034233\n",
      "Min loss: 0.008584380149841309\n",
      "Mean loss: 0.015611045838644108\n",
      "Std loss: 0.005896579706458938\n",
      "Total Loss: 0.09366627503186464\n",
      "------------------------------------ epoch 3258 (19542 steps) ------------------------------------\n",
      "Max loss: 0.03313829004764557\n",
      "Min loss: 0.007424945943057537\n",
      "Mean loss: 0.017331682611256838\n",
      "Std loss: 0.009025420319087997\n",
      "Total Loss: 0.10399009566754103\n",
      "------------------------------------ epoch 3259 (19548 steps) ------------------------------------\n",
      "Max loss: 0.0267767533659935\n",
      "Min loss: 0.0071074627339839935\n",
      "Mean loss: 0.015640163483719032\n",
      "Std loss: 0.006362367935734329\n",
      "Total Loss: 0.09384098090231419\n",
      "------------------------------------ epoch 3260 (19554 steps) ------------------------------------\n",
      "Max loss: 0.031927384436130524\n",
      "Min loss: 0.006814768072217703\n",
      "Mean loss: 0.018110396262879174\n",
      "Std loss: 0.0073886911019951625\n",
      "Total Loss: 0.10866237757727504\n",
      "------------------------------------ epoch 3261 (19560 steps) ------------------------------------\n",
      "Max loss: 0.01106492429971695\n",
      "Min loss: 0.008912356570363045\n",
      "Mean loss: 0.010521389544010162\n",
      "Std loss: 0.0007632576436989878\n",
      "Total Loss: 0.06312833726406097\n",
      "------------------------------------ epoch 3262 (19566 steps) ------------------------------------\n",
      "Max loss: 0.017192261293530464\n",
      "Min loss: 0.007759064435958862\n",
      "Mean loss: 0.011172176183511814\n",
      "Std loss: 0.002884430776566916\n",
      "Total Loss: 0.06703305710107088\n",
      "------------------------------------ epoch 3263 (19572 steps) ------------------------------------\n",
      "Max loss: 0.02118600904941559\n",
      "Min loss: 0.00871045421808958\n",
      "Mean loss: 0.013895014300942421\n",
      "Std loss: 0.004240725106180631\n",
      "Total Loss: 0.08337008580565453\n",
      "------------------------------------ epoch 3264 (19578 steps) ------------------------------------\n",
      "Max loss: 0.03255775570869446\n",
      "Min loss: 0.006378879304975271\n",
      "Mean loss: 0.01340856752358377\n",
      "Std loss: 0.008998388514675848\n",
      "Total Loss: 0.08045140514150262\n",
      "------------------------------------ epoch 3265 (19584 steps) ------------------------------------\n",
      "Max loss: 0.013824932277202606\n",
      "Min loss: 0.007561638951301575\n",
      "Mean loss: 0.010224913402150074\n",
      "Std loss: 0.0020584379444863717\n",
      "Total Loss: 0.06134948041290045\n",
      "------------------------------------ epoch 3266 (19590 steps) ------------------------------------\n",
      "Max loss: 0.02198871038854122\n",
      "Min loss: 0.008833914995193481\n",
      "Mean loss: 0.015612356830388308\n",
      "Std loss: 0.005004885595675936\n",
      "Total Loss: 0.09367414098232985\n",
      "------------------------------------ epoch 3267 (19596 steps) ------------------------------------\n",
      "Max loss: 0.04627957567572594\n",
      "Min loss: 0.008960469625890255\n",
      "Mean loss: 0.01638552825897932\n",
      "Std loss: 0.013392162224640134\n",
      "Total Loss: 0.09831316955387592\n",
      "------------------------------------ epoch 3268 (19602 steps) ------------------------------------\n",
      "Max loss: 0.030725538730621338\n",
      "Min loss: 0.007153464015573263\n",
      "Mean loss: 0.013350852377091845\n",
      "Std loss: 0.008162716470390356\n",
      "Total Loss: 0.08010511426255107\n",
      "------------------------------------ epoch 3269 (19608 steps) ------------------------------------\n",
      "Max loss: 0.04668256640434265\n",
      "Min loss: 0.007404656149446964\n",
      "Mean loss: 0.016189002587149542\n",
      "Std loss: 0.013851347549722741\n",
      "Total Loss: 0.09713401552289724\n",
      "------------------------------------ epoch 3270 (19614 steps) ------------------------------------\n",
      "Max loss: 0.01964469626545906\n",
      "Min loss: 0.0074967010878026485\n",
      "Mean loss: 0.013570290757343173\n",
      "Std loss: 0.004249989336948715\n",
      "Total Loss: 0.08142174454405904\n",
      "------------------------------------ epoch 3271 (19620 steps) ------------------------------------\n",
      "Max loss: 0.01531133707612753\n",
      "Min loss: 0.00900318007916212\n",
      "Mean loss: 0.011576393308738867\n",
      "Std loss: 0.002555267751662386\n",
      "Total Loss: 0.0694583598524332\n",
      "------------------------------------ epoch 3272 (19626 steps) ------------------------------------\n",
      "Max loss: 0.01962561532855034\n",
      "Min loss: 0.007093021180480719\n",
      "Mean loss: 0.013186847403024634\n",
      "Std loss: 0.004327985071663413\n",
      "Total Loss: 0.0791210844181478\n",
      "------------------------------------ epoch 3273 (19632 steps) ------------------------------------\n",
      "Max loss: 0.05451812595129013\n",
      "Min loss: 0.006164844147861004\n",
      "Mean loss: 0.019912205015619595\n",
      "Std loss: 0.016657904364347418\n",
      "Total Loss: 0.11947323009371758\n",
      "------------------------------------ epoch 3274 (19638 steps) ------------------------------------\n",
      "Max loss: 0.04380505532026291\n",
      "Min loss: 0.006965562701225281\n",
      "Mean loss: 0.015541696610550085\n",
      "Std loss: 0.012884399101581307\n",
      "Total Loss: 0.09325017966330051\n",
      "------------------------------------ epoch 3275 (19644 steps) ------------------------------------\n",
      "Max loss: 0.02184518240392208\n",
      "Min loss: 0.009702377021312714\n",
      "Mean loss: 0.017215620105465252\n",
      "Std loss: 0.0041273993111105075\n",
      "Total Loss: 0.10329372063279152\n",
      "------------------------------------ epoch 3276 (19650 steps) ------------------------------------\n",
      "Max loss: 0.028028681874275208\n",
      "Min loss: 0.01228329073637724\n",
      "Mean loss: 0.01982158391426007\n",
      "Std loss: 0.006144111305216799\n",
      "Total Loss: 0.11892950348556042\n",
      "------------------------------------ epoch 3277 (19656 steps) ------------------------------------\n",
      "Max loss: 0.05650682747364044\n",
      "Min loss: 0.007714185863733292\n",
      "Mean loss: 0.022167997434735298\n",
      "Std loss: 0.0165533696159034\n",
      "Total Loss: 0.1330079846084118\n",
      "------------------------------------ epoch 3278 (19662 steps) ------------------------------------\n",
      "Max loss: 0.022214872762560844\n",
      "Min loss: 0.008230064064264297\n",
      "Mean loss: 0.013053782129039368\n",
      "Std loss: 0.005015417558028823\n",
      "Total Loss: 0.0783226927742362\n",
      "------------------------------------ epoch 3279 (19668 steps) ------------------------------------\n",
      "Max loss: 0.024328097701072693\n",
      "Min loss: 0.007993746548891068\n",
      "Mean loss: 0.012713387298087278\n",
      "Std loss: 0.005671063436874631\n",
      "Total Loss: 0.07628032378852367\n",
      "------------------------------------ epoch 3280 (19674 steps) ------------------------------------\n",
      "Max loss: 0.012766431085765362\n",
      "Min loss: 0.0068485327064991\n",
      "Mean loss: 0.010316168113301197\n",
      "Std loss: 0.002013754682944521\n",
      "Total Loss: 0.061897008679807186\n",
      "------------------------------------ epoch 3281 (19680 steps) ------------------------------------\n",
      "Max loss: 0.014563044533133507\n",
      "Min loss: 0.006507655140012503\n",
      "Mean loss: 0.011210482955599824\n",
      "Std loss: 0.0026763965291845414\n",
      "Total Loss: 0.06726289773359895\n",
      "------------------------------------ epoch 3282 (19686 steps) ------------------------------------\n",
      "Max loss: 0.03954778239130974\n",
      "Min loss: 0.006609340663999319\n",
      "Mean loss: 0.01829975680448115\n",
      "Std loss: 0.011275307630041495\n",
      "Total Loss: 0.10979854082688689\n",
      "------------------------------------ epoch 3283 (19692 steps) ------------------------------------\n",
      "Max loss: 0.02896079421043396\n",
      "Min loss: 0.007161644287407398\n",
      "Mean loss: 0.014580855301270882\n",
      "Std loss: 0.007584972302163232\n",
      "Total Loss: 0.0874851318076253\n",
      "------------------------------------ epoch 3284 (19698 steps) ------------------------------------\n",
      "Max loss: 0.039813168346881866\n",
      "Min loss: 0.014728959649801254\n",
      "Mean loss: 0.02340661035850644\n",
      "Std loss: 0.009008670914290182\n",
      "Total Loss: 0.14043966215103865\n",
      "------------------------------------ epoch 3285 (19704 steps) ------------------------------------\n",
      "Max loss: 0.033564284443855286\n",
      "Min loss: 0.011615997180342674\n",
      "Mean loss: 0.016927693349619705\n",
      "Std loss: 0.007630371276611382\n",
      "Total Loss: 0.10156616009771824\n",
      "------------------------------------ epoch 3286 (19710 steps) ------------------------------------\n",
      "Max loss: 0.05568603426218033\n",
      "Min loss: 0.011071134358644485\n",
      "Mean loss: 0.02278994070366025\n",
      "Std loss: 0.015058656989540706\n",
      "Total Loss: 0.1367396442219615\n",
      "------------------------------------ epoch 3287 (19716 steps) ------------------------------------\n",
      "Max loss: 0.0385541096329689\n",
      "Min loss: 0.007181310560554266\n",
      "Mean loss: 0.0154081917523096\n",
      "Std loss: 0.010724124097027235\n",
      "Total Loss: 0.0924491505138576\n",
      "------------------------------------ epoch 3288 (19722 steps) ------------------------------------\n",
      "Max loss: 0.035356681793928146\n",
      "Min loss: 0.009609697386622429\n",
      "Mean loss: 0.01936761460577448\n",
      "Std loss: 0.008763119050770232\n",
      "Total Loss: 0.11620568763464689\n",
      "------------------------------------ epoch 3289 (19728 steps) ------------------------------------\n",
      "Max loss: 0.022714026272296906\n",
      "Min loss: 0.008524258621037006\n",
      "Mean loss: 0.012640807932863632\n",
      "Std loss: 0.004779808726856613\n",
      "Total Loss: 0.0758448475971818\n",
      "------------------------------------ epoch 3290 (19734 steps) ------------------------------------\n",
      "Max loss: 0.016003839671611786\n",
      "Min loss: 0.008959203027188778\n",
      "Mean loss: 0.012537411414086819\n",
      "Std loss: 0.0026864467084429145\n",
      "Total Loss: 0.07522446848452091\n",
      "------------------------------------ epoch 3291 (19740 steps) ------------------------------------\n",
      "Max loss: 0.01509544812142849\n",
      "Min loss: 0.009344736114144325\n",
      "Mean loss: 0.011318067243943611\n",
      "Std loss: 0.002108725348159881\n",
      "Total Loss: 0.06790840346366167\n",
      "------------------------------------ epoch 3292 (19746 steps) ------------------------------------\n",
      "Max loss: 0.025881584733724594\n",
      "Min loss: 0.009263619780540466\n",
      "Mean loss: 0.015439857573558887\n",
      "Std loss: 0.005618829187064407\n",
      "Total Loss: 0.09263914544135332\n",
      "------------------------------------ epoch 3293 (19752 steps) ------------------------------------\n",
      "Max loss: 0.05704249441623688\n",
      "Min loss: 0.008916356600821018\n",
      "Mean loss: 0.01961387073000272\n",
      "Std loss: 0.016836148677305676\n",
      "Total Loss: 0.11768322438001633\n",
      "------------------------------------ epoch 3294 (19758 steps) ------------------------------------\n",
      "Max loss: 0.021775037050247192\n",
      "Min loss: 0.006682786159217358\n",
      "Mean loss: 0.011392617442955574\n",
      "Std loss: 0.005171732923820391\n",
      "Total Loss: 0.06835570465773344\n",
      "------------------------------------ epoch 3295 (19764 steps) ------------------------------------\n",
      "Max loss: 0.029342815279960632\n",
      "Min loss: 0.007860172539949417\n",
      "Mean loss: 0.015007738644878069\n",
      "Std loss: 0.006917940779886381\n",
      "Total Loss: 0.09004643186926842\n",
      "------------------------------------ epoch 3296 (19770 steps) ------------------------------------\n",
      "Max loss: 0.03662034124135971\n",
      "Min loss: 0.00928969494998455\n",
      "Mean loss: 0.018885584082454443\n",
      "Std loss: 0.009131218808621454\n",
      "Total Loss: 0.11331350449472666\n",
      "------------------------------------ epoch 3297 (19776 steps) ------------------------------------\n",
      "Max loss: 0.018401876091957092\n",
      "Min loss: 0.006989871617406607\n",
      "Mean loss: 0.009786355076357722\n",
      "Std loss: 0.003993482791658232\n",
      "Total Loss: 0.058718130458146334\n",
      "------------------------------------ epoch 3298 (19782 steps) ------------------------------------\n",
      "Max loss: 0.0397864431142807\n",
      "Min loss: 0.010591363534331322\n",
      "Mean loss: 0.022667283192276955\n",
      "Std loss: 0.010612152679394736\n",
      "Total Loss: 0.13600369915366173\n",
      "------------------------------------ epoch 3299 (19788 steps) ------------------------------------\n",
      "Max loss: 0.023329418152570724\n",
      "Min loss: 0.007361759897321463\n",
      "Mean loss: 0.015879831354444224\n",
      "Std loss: 0.00609428502943749\n",
      "Total Loss: 0.09527898812666535\n",
      "------------------------------------ epoch 3300 (19794 steps) ------------------------------------\n",
      "Max loss: 0.021056849509477615\n",
      "Min loss: 0.007832021452486515\n",
      "Mean loss: 0.01279500670110186\n",
      "Std loss: 0.0044891485678127604\n",
      "Total Loss: 0.07677004020661116\n",
      "------------------------------------ epoch 3301 (19800 steps) ------------------------------------\n",
      "Max loss: 0.03193507716059685\n",
      "Min loss: 0.00968528725206852\n",
      "Mean loss: 0.016290588304400444\n",
      "Std loss: 0.00781253582195869\n",
      "Total Loss: 0.09774352982640266\n",
      "saved model at ./weights/model_3301.pth\n",
      "------------------------------------ epoch 3302 (19806 steps) ------------------------------------\n",
      "Max loss: 0.03047776035964489\n",
      "Min loss: 0.007975980639457703\n",
      "Mean loss: 0.015461521688848734\n",
      "Std loss: 0.007210624353792389\n",
      "Total Loss: 0.0927691301330924\n",
      "------------------------------------ epoch 3303 (19812 steps) ------------------------------------\n",
      "Max loss: 0.017055699601769447\n",
      "Min loss: 0.008207524195313454\n",
      "Mean loss: 0.01102897198870778\n",
      "Std loss: 0.0031465097206689156\n",
      "Total Loss: 0.06617383193224669\n",
      "------------------------------------ epoch 3304 (19818 steps) ------------------------------------\n",
      "Max loss: 0.02435402385890484\n",
      "Min loss: 0.008163394406437874\n",
      "Mean loss: 0.014087393879890442\n",
      "Std loss: 0.005173581095416577\n",
      "Total Loss: 0.08452436327934265\n",
      "------------------------------------ epoch 3305 (19824 steps) ------------------------------------\n",
      "Max loss: 0.03438985347747803\n",
      "Min loss: 0.009741554036736488\n",
      "Mean loss: 0.01922217570245266\n",
      "Std loss: 0.008566188781190476\n",
      "Total Loss: 0.11533305421471596\n",
      "------------------------------------ epoch 3306 (19830 steps) ------------------------------------\n",
      "Max loss: 0.017810914665460587\n",
      "Min loss: 0.009111437946557999\n",
      "Mean loss: 0.012318874088426432\n",
      "Std loss: 0.002684860193517275\n",
      "Total Loss: 0.07391324453055859\n",
      "------------------------------------ epoch 3307 (19836 steps) ------------------------------------\n",
      "Max loss: 0.03347865864634514\n",
      "Min loss: 0.008409380912780762\n",
      "Mean loss: 0.015430555368463198\n",
      "Std loss: 0.008661764066755794\n",
      "Total Loss: 0.09258333221077919\n",
      "------------------------------------ epoch 3308 (19842 steps) ------------------------------------\n",
      "Max loss: 0.015894664451479912\n",
      "Min loss: 0.0065824128687381744\n",
      "Mean loss: 0.011255962463716665\n",
      "Std loss: 0.003519842164490995\n",
      "Total Loss: 0.0675357747823\n",
      "------------------------------------ epoch 3309 (19848 steps) ------------------------------------\n",
      "Max loss: 0.018851548433303833\n",
      "Min loss: 0.00850764662027359\n",
      "Mean loss: 0.012366366727898518\n",
      "Std loss: 0.003377790369219126\n",
      "Total Loss: 0.07419820036739111\n",
      "------------------------------------ epoch 3310 (19854 steps) ------------------------------------\n",
      "Max loss: 0.011680588126182556\n",
      "Min loss: 0.007479195948690176\n",
      "Mean loss: 0.00888292328454554\n",
      "Std loss: 0.001356331397206499\n",
      "Total Loss: 0.053297539707273245\n",
      "------------------------------------ epoch 3311 (19860 steps) ------------------------------------\n",
      "Max loss: 0.021443281322717667\n",
      "Min loss: 0.007034053560346365\n",
      "Mean loss: 0.014138441300019622\n",
      "Std loss: 0.005415358598269737\n",
      "Total Loss: 0.08483064780011773\n",
      "------------------------------------ epoch 3312 (19866 steps) ------------------------------------\n",
      "Max loss: 0.04099467769265175\n",
      "Min loss: 0.007341126911342144\n",
      "Mean loss: 0.019172671406219404\n",
      "Std loss: 0.013848090416548416\n",
      "Total Loss: 0.11503602843731642\n",
      "------------------------------------ epoch 3313 (19872 steps) ------------------------------------\n",
      "Max loss: 0.028047632426023483\n",
      "Min loss: 0.006932571064680815\n",
      "Mean loss: 0.013481837309276065\n",
      "Std loss: 0.0075756106140951645\n",
      "Total Loss: 0.08089102385565639\n",
      "------------------------------------ epoch 3314 (19878 steps) ------------------------------------\n",
      "Max loss: 0.0389137901365757\n",
      "Min loss: 0.00814227294176817\n",
      "Mean loss: 0.02016035607084632\n",
      "Std loss: 0.010951575683490943\n",
      "Total Loss: 0.12096213642507792\n",
      "------------------------------------ epoch 3315 (19884 steps) ------------------------------------\n",
      "Max loss: 0.01879846677184105\n",
      "Min loss: 0.010138713754713535\n",
      "Mean loss: 0.013932440585146347\n",
      "Std loss: 0.0032200968861301233\n",
      "Total Loss: 0.08359464351087809\n",
      "------------------------------------ epoch 3316 (19890 steps) ------------------------------------\n",
      "Max loss: 0.04212913662195206\n",
      "Min loss: 0.00770620396360755\n",
      "Mean loss: 0.01971798863572379\n",
      "Std loss: 0.011532934381674936\n",
      "Total Loss: 0.11830793181434274\n",
      "------------------------------------ epoch 3317 (19896 steps) ------------------------------------\n",
      "Max loss: 0.04864078760147095\n",
      "Min loss: 0.008766880258917809\n",
      "Mean loss: 0.01912531473984321\n",
      "Std loss: 0.013668627718584247\n",
      "Total Loss: 0.11475188843905926\n",
      "------------------------------------ epoch 3318 (19902 steps) ------------------------------------\n",
      "Max loss: 0.027078745886683464\n",
      "Min loss: 0.014726045541465282\n",
      "Mean loss: 0.02167181872452299\n",
      "Std loss: 0.005023987749675241\n",
      "Total Loss: 0.13003091234713793\n",
      "------------------------------------ epoch 3319 (19908 steps) ------------------------------------\n",
      "Max loss: 0.04928989335894585\n",
      "Min loss: 0.01290740817785263\n",
      "Mean loss: 0.022472221093873184\n",
      "Std loss: 0.012418173187957682\n",
      "Total Loss: 0.1348333265632391\n",
      "------------------------------------ epoch 3320 (19914 steps) ------------------------------------\n",
      "Max loss: 0.020898014307022095\n",
      "Min loss: 0.009211656637489796\n",
      "Mean loss: 0.015498386385540167\n",
      "Std loss: 0.0035580315376625635\n",
      "Total Loss: 0.092990318313241\n",
      "------------------------------------ epoch 3321 (19920 steps) ------------------------------------\n",
      "Max loss: 0.018011702224612236\n",
      "Min loss: 0.008121415972709656\n",
      "Mean loss: 0.012149448351313671\n",
      "Std loss: 0.0038903804926742674\n",
      "Total Loss: 0.07289669010788202\n",
      "------------------------------------ epoch 3322 (19926 steps) ------------------------------------\n",
      "Max loss: 0.024879980832338333\n",
      "Min loss: 0.00924706645309925\n",
      "Mean loss: 0.01460221145922939\n",
      "Std loss: 0.0056951747598972275\n",
      "Total Loss: 0.08761326875537634\n",
      "------------------------------------ epoch 3323 (19932 steps) ------------------------------------\n",
      "Max loss: 0.03465433791279793\n",
      "Min loss: 0.007456154562532902\n",
      "Mean loss: 0.012988626646498838\n",
      "Std loss: 0.00971797217161989\n",
      "Total Loss: 0.07793175987899303\n",
      "------------------------------------ epoch 3324 (19938 steps) ------------------------------------\n",
      "Max loss: 0.014765320345759392\n",
      "Min loss: 0.008567547425627708\n",
      "Mean loss: 0.012261473418523869\n",
      "Std loss: 0.001958207856977272\n",
      "Total Loss: 0.07356884051114321\n",
      "------------------------------------ epoch 3325 (19944 steps) ------------------------------------\n",
      "Max loss: 0.019594717770814896\n",
      "Min loss: 0.007649943232536316\n",
      "Mean loss: 0.011611743364483118\n",
      "Std loss: 0.003971733816452218\n",
      "Total Loss: 0.06967046018689871\n",
      "------------------------------------ epoch 3326 (19950 steps) ------------------------------------\n",
      "Max loss: 0.04222028702497482\n",
      "Min loss: 0.010554352775216103\n",
      "Mean loss: 0.018951169680804014\n",
      "Std loss: 0.010845870499791549\n",
      "Total Loss: 0.11370701808482409\n",
      "------------------------------------ epoch 3327 (19956 steps) ------------------------------------\n",
      "Max loss: 0.01434013806283474\n",
      "Min loss: 0.00816342979669571\n",
      "Mean loss: 0.011229177471250296\n",
      "Std loss: 0.0022576990957893484\n",
      "Total Loss: 0.06737506482750177\n",
      "------------------------------------ epoch 3328 (19962 steps) ------------------------------------\n",
      "Max loss: 0.046294234693050385\n",
      "Min loss: 0.009098056703805923\n",
      "Mean loss: 0.01994388612608115\n",
      "Std loss: 0.012451354408214203\n",
      "Total Loss: 0.11966331675648689\n",
      "------------------------------------ epoch 3329 (19968 steps) ------------------------------------\n",
      "Max loss: 0.046140968799591064\n",
      "Min loss: 0.009862195700407028\n",
      "Mean loss: 0.022820250907291968\n",
      "Std loss: 0.013559704124054078\n",
      "Total Loss: 0.1369215054437518\n",
      "------------------------------------ epoch 3330 (19974 steps) ------------------------------------\n",
      "Max loss: 0.04887183755636215\n",
      "Min loss: 0.010439334437251091\n",
      "Mean loss: 0.018993752853324015\n",
      "Std loss: 0.013636942263682752\n",
      "Total Loss: 0.1139625171199441\n",
      "------------------------------------ epoch 3331 (19980 steps) ------------------------------------\n",
      "Max loss: 0.04054886847734451\n",
      "Min loss: 0.009105137549340725\n",
      "Mean loss: 0.019669593156625826\n",
      "Std loss: 0.011156053569641814\n",
      "Total Loss: 0.11801755893975496\n",
      "------------------------------------ epoch 3332 (19986 steps) ------------------------------------\n",
      "Max loss: 0.0241161547601223\n",
      "Min loss: 0.007467781659215689\n",
      "Mean loss: 0.015474402733768025\n",
      "Std loss: 0.006384489220787653\n",
      "Total Loss: 0.09284641640260816\n",
      "------------------------------------ epoch 3333 (19992 steps) ------------------------------------\n",
      "Max loss: 0.026746630668640137\n",
      "Min loss: 0.006550678983330727\n",
      "Mean loss: 0.01390532637014985\n",
      "Std loss: 0.006178427200930166\n",
      "Total Loss: 0.0834319582208991\n",
      "------------------------------------ epoch 3334 (19998 steps) ------------------------------------\n",
      "Max loss: 0.013631644658744335\n",
      "Min loss: 0.0074567487463355064\n",
      "Mean loss: 0.009794633525113264\n",
      "Std loss: 0.002340843066449012\n",
      "Total Loss: 0.05876780115067959\n",
      "------------------------------------ epoch 3335 (20004 steps) ------------------------------------\n",
      "Max loss: 0.03674817085266113\n",
      "Min loss: 0.01039890106767416\n",
      "Mean loss: 0.016483225238819916\n",
      "Std loss: 0.009161777767471754\n",
      "Total Loss: 0.0988993514329195\n",
      "------------------------------------ epoch 3336 (20010 steps) ------------------------------------\n",
      "Max loss: 0.06223134696483612\n",
      "Min loss: 0.009590961039066315\n",
      "Mean loss: 0.019774541879693668\n",
      "Std loss: 0.01903636689058384\n",
      "Total Loss: 0.118647251278162\n",
      "------------------------------------ epoch 3337 (20016 steps) ------------------------------------\n",
      "Max loss: 0.07387087494134903\n",
      "Min loss: 0.01183992251753807\n",
      "Mean loss: 0.025301333516836166\n",
      "Std loss: 0.021899455405832154\n",
      "Total Loss: 0.151808001101017\n",
      "------------------------------------ epoch 3338 (20022 steps) ------------------------------------\n",
      "Max loss: 0.047263287007808685\n",
      "Min loss: 0.009737379848957062\n",
      "Mean loss: 0.02079716045409441\n",
      "Std loss: 0.014229686792386545\n",
      "Total Loss: 0.12478296272456646\n",
      "------------------------------------ epoch 3339 (20028 steps) ------------------------------------\n",
      "Max loss: 0.05241679400205612\n",
      "Min loss: 0.015960050746798515\n",
      "Mean loss: 0.029733533039689064\n",
      "Std loss: 0.013750334411435989\n",
      "Total Loss: 0.17840119823813438\n",
      "------------------------------------ epoch 3340 (20034 steps) ------------------------------------\n",
      "Max loss: 0.07087019830942154\n",
      "Min loss: 0.009702522307634354\n",
      "Mean loss: 0.026307944984485705\n",
      "Std loss: 0.02114689406110889\n",
      "Total Loss: 0.15784766990691423\n",
      "------------------------------------ epoch 3341 (20040 steps) ------------------------------------\n",
      "Max loss: 0.044220417737960815\n",
      "Min loss: 0.009106687270104885\n",
      "Mean loss: 0.020021863436947267\n",
      "Std loss: 0.01164077980216748\n",
      "Total Loss: 0.1201311806216836\n",
      "------------------------------------ epoch 3342 (20046 steps) ------------------------------------\n",
      "Max loss: 0.031542692333459854\n",
      "Min loss: 0.009118547663092613\n",
      "Mean loss: 0.014781349804252386\n",
      "Std loss: 0.007633451067631916\n",
      "Total Loss: 0.08868809882551432\n",
      "------------------------------------ epoch 3343 (20052 steps) ------------------------------------\n",
      "Max loss: 0.04034093767404556\n",
      "Min loss: 0.011759269051253796\n",
      "Mean loss: 0.01885725138708949\n",
      "Std loss: 0.009883251480203647\n",
      "Total Loss: 0.11314350832253695\n",
      "------------------------------------ epoch 3344 (20058 steps) ------------------------------------\n",
      "Max loss: 0.03680414333939552\n",
      "Min loss: 0.010637197643518448\n",
      "Mean loss: 0.022076206592222054\n",
      "Std loss: 0.009462644928726237\n",
      "Total Loss: 0.13245723955333233\n",
      "------------------------------------ epoch 3345 (20064 steps) ------------------------------------\n",
      "Max loss: 0.05540093779563904\n",
      "Min loss: 0.008822891861200333\n",
      "Mean loss: 0.018867780764897663\n",
      "Std loss: 0.016458651829639834\n",
      "Total Loss: 0.11320668458938599\n",
      "------------------------------------ epoch 3346 (20070 steps) ------------------------------------\n",
      "Max loss: 0.05132205784320831\n",
      "Min loss: 0.014239284209907055\n",
      "Mean loss: 0.024111382197588682\n",
      "Std loss: 0.012666917382167135\n",
      "Total Loss: 0.1446682931855321\n",
      "------------------------------------ epoch 3347 (20076 steps) ------------------------------------\n",
      "Max loss: 0.047608282417058945\n",
      "Min loss: 0.008735788986086845\n",
      "Mean loss: 0.023582090623676777\n",
      "Std loss: 0.01212278865139987\n",
      "Total Loss: 0.14149254374206066\n",
      "------------------------------------ epoch 3348 (20082 steps) ------------------------------------\n",
      "Max loss: 0.01753203384578228\n",
      "Min loss: 0.006806324236094952\n",
      "Mean loss: 0.012720015831291676\n",
      "Std loss: 0.004041049509958941\n",
      "Total Loss: 0.07632009498775005\n",
      "------------------------------------ epoch 3349 (20088 steps) ------------------------------------\n",
      "Max loss: 0.015216785483062267\n",
      "Min loss: 0.009075285866856575\n",
      "Mean loss: 0.011488842312246561\n",
      "Std loss: 0.002444195428078459\n",
      "Total Loss: 0.06893305387347937\n",
      "------------------------------------ epoch 3350 (20094 steps) ------------------------------------\n",
      "Max loss: 0.020306482911109924\n",
      "Min loss: 0.006692337803542614\n",
      "Mean loss: 0.01362678175792098\n",
      "Std loss: 0.0048266575431740325\n",
      "Total Loss: 0.08176069054752588\n",
      "------------------------------------ epoch 3351 (20100 steps) ------------------------------------\n",
      "Max loss: 0.01827019639313221\n",
      "Min loss: 0.007177080027759075\n",
      "Mean loss: 0.011242314241826534\n",
      "Std loss: 0.004127367779442279\n",
      "Total Loss: 0.0674538854509592\n",
      "------------------------------------ epoch 3352 (20106 steps) ------------------------------------\n",
      "Max loss: 0.01500498317182064\n",
      "Min loss: 0.006370933260768652\n",
      "Mean loss: 0.010680172204350432\n",
      "Std loss: 0.0029969991472402675\n",
      "Total Loss: 0.06408103322610259\n",
      "------------------------------------ epoch 3353 (20112 steps) ------------------------------------\n",
      "Max loss: 0.015365161001682281\n",
      "Min loss: 0.006906417664140463\n",
      "Mean loss: 0.011302577564492822\n",
      "Std loss: 0.0031220595362980723\n",
      "Total Loss: 0.06781546538695693\n",
      "------------------------------------ epoch 3354 (20118 steps) ------------------------------------\n",
      "Max loss: 0.04635259136557579\n",
      "Min loss: 0.008135220035910606\n",
      "Mean loss: 0.01916957087814808\n",
      "Std loss: 0.012995240918124826\n",
      "Total Loss: 0.11501742526888847\n",
      "------------------------------------ epoch 3355 (20124 steps) ------------------------------------\n",
      "Max loss: 0.048151083290576935\n",
      "Min loss: 0.0067115165293216705\n",
      "Mean loss: 0.0187254265571634\n",
      "Std loss: 0.014106115715410905\n",
      "Total Loss: 0.11235255934298038\n",
      "------------------------------------ epoch 3356 (20130 steps) ------------------------------------\n",
      "Max loss: 0.02312137745320797\n",
      "Min loss: 0.00840116385370493\n",
      "Mean loss: 0.013310625993957123\n",
      "Std loss: 0.0049166957253507036\n",
      "Total Loss: 0.07986375596374273\n",
      "------------------------------------ epoch 3357 (20136 steps) ------------------------------------\n",
      "Max loss: 0.02035905048251152\n",
      "Min loss: 0.006268637254834175\n",
      "Mean loss: 0.0138175825898846\n",
      "Std loss: 0.005061338047194523\n",
      "Total Loss: 0.0829054955393076\n",
      "------------------------------------ epoch 3358 (20142 steps) ------------------------------------\n",
      "Max loss: 0.021120121702551842\n",
      "Min loss: 0.007937180809676647\n",
      "Mean loss: 0.014285418981065353\n",
      "Std loss: 0.006191209550844939\n",
      "Total Loss: 0.08571251388639212\n",
      "------------------------------------ epoch 3359 (20148 steps) ------------------------------------\n",
      "Max loss: 0.030826767906546593\n",
      "Min loss: 0.009943626821041107\n",
      "Mean loss: 0.017120536727209885\n",
      "Std loss: 0.007122931788866638\n",
      "Total Loss: 0.10272322036325932\n",
      "------------------------------------ epoch 3360 (20154 steps) ------------------------------------\n",
      "Max loss: 0.019172435626387596\n",
      "Min loss: 0.00624646944925189\n",
      "Mean loss: 0.012423385167494416\n",
      "Std loss: 0.004271541081598198\n",
      "Total Loss: 0.0745403110049665\n",
      "------------------------------------ epoch 3361 (20160 steps) ------------------------------------\n",
      "Max loss: 0.019464721903204918\n",
      "Min loss: 0.007103997748345137\n",
      "Mean loss: 0.011031333589926362\n",
      "Std loss: 0.003936527059243802\n",
      "Total Loss: 0.06618800153955817\n",
      "------------------------------------ epoch 3362 (20166 steps) ------------------------------------\n",
      "Max loss: 0.02142704837024212\n",
      "Min loss: 0.00721219927072525\n",
      "Mean loss: 0.011767067015171051\n",
      "Std loss: 0.00509967862504154\n",
      "Total Loss: 0.0706024020910263\n",
      "------------------------------------ epoch 3363 (20172 steps) ------------------------------------\n",
      "Max loss: 0.02898309752345085\n",
      "Min loss: 0.00793141033500433\n",
      "Mean loss: 0.016000275034457445\n",
      "Std loss: 0.006593861939510813\n",
      "Total Loss: 0.09600165020674467\n",
      "------------------------------------ epoch 3364 (20178 steps) ------------------------------------\n",
      "Max loss: 0.014459408819675446\n",
      "Min loss: 0.00818374939262867\n",
      "Mean loss: 0.01137882083033522\n",
      "Std loss: 0.0024829266891898036\n",
      "Total Loss: 0.06827292498201132\n",
      "------------------------------------ epoch 3365 (20184 steps) ------------------------------------\n",
      "Max loss: 0.04659378528594971\n",
      "Min loss: 0.0067786602303385735\n",
      "Mean loss: 0.01732644873360793\n",
      "Std loss: 0.013791907723547362\n",
      "Total Loss: 0.10395869240164757\n",
      "------------------------------------ epoch 3366 (20190 steps) ------------------------------------\n",
      "Max loss: 0.029080353677272797\n",
      "Min loss: 0.009154848754405975\n",
      "Mean loss: 0.017511589142183464\n",
      "Std loss: 0.007852628429027187\n",
      "Total Loss: 0.10506953485310078\n",
      "------------------------------------ epoch 3367 (20196 steps) ------------------------------------\n",
      "Max loss: 0.01976199261844158\n",
      "Min loss: 0.008879976347088814\n",
      "Mean loss: 0.013400371031214794\n",
      "Std loss: 0.0036041494509605043\n",
      "Total Loss: 0.08040222618728876\n",
      "------------------------------------ epoch 3368 (20202 steps) ------------------------------------\n",
      "Max loss: 0.01613052934408188\n",
      "Min loss: 0.008322430774569511\n",
      "Mean loss: 0.011967409091691176\n",
      "Std loss: 0.0024611739078086053\n",
      "Total Loss: 0.07180445455014706\n",
      "------------------------------------ epoch 3369 (20208 steps) ------------------------------------\n",
      "Max loss: 0.033790625631809235\n",
      "Min loss: 0.0077706738375127316\n",
      "Mean loss: 0.017514994290346902\n",
      "Std loss: 0.00851570208928799\n",
      "Total Loss: 0.1050899657420814\n",
      "------------------------------------ epoch 3370 (20214 steps) ------------------------------------\n",
      "Max loss: 0.05240769684314728\n",
      "Min loss: 0.009159229695796967\n",
      "Mean loss: 0.020353718815992277\n",
      "Std loss: 0.015303873059886354\n",
      "Total Loss: 0.12212231289595366\n",
      "------------------------------------ epoch 3371 (20220 steps) ------------------------------------\n",
      "Max loss: 0.027391066774725914\n",
      "Min loss: 0.0073312572203576565\n",
      "Mean loss: 0.016122815432026982\n",
      "Std loss: 0.007687572848200344\n",
      "Total Loss: 0.0967368925921619\n",
      "------------------------------------ epoch 3372 (20226 steps) ------------------------------------\n",
      "Max loss: 0.061789896339178085\n",
      "Min loss: 0.013977247290313244\n",
      "Mean loss: 0.03112591663375497\n",
      "Std loss: 0.017649219795214506\n",
      "Total Loss: 0.1867554998025298\n",
      "------------------------------------ epoch 3373 (20232 steps) ------------------------------------\n",
      "Max loss: 0.023580631241202354\n",
      "Min loss: 0.014146843925118446\n",
      "Mean loss: 0.018185204205413658\n",
      "Std loss: 0.003294732931902081\n",
      "Total Loss: 0.10911122523248196\n",
      "------------------------------------ epoch 3374 (20238 steps) ------------------------------------\n",
      "Max loss: 0.025411970913410187\n",
      "Min loss: 0.012683774344623089\n",
      "Mean loss: 0.01718082930892706\n",
      "Std loss: 0.005019424476052637\n",
      "Total Loss: 0.10308497585356236\n",
      "------------------------------------ epoch 3375 (20244 steps) ------------------------------------\n",
      "Max loss: 0.5443717837333679\n",
      "Min loss: 0.09776939451694489\n",
      "Mean loss: 0.3604563996195793\n",
      "Std loss: 0.1491354367869551\n",
      "Total Loss: 2.162738397717476\n",
      "------------------------------------ epoch 3376 (20250 steps) ------------------------------------\n",
      "Max loss: 1.3118579387664795\n",
      "Min loss: 0.56337571144104\n",
      "Mean loss: 0.8106249670187632\n",
      "Std loss: 0.2526486839878419\n",
      "Total Loss: 4.863749802112579\n",
      "------------------------------------ epoch 3377 (20256 steps) ------------------------------------\n",
      "Max loss: 0.8514301180839539\n",
      "Min loss: 0.5713276863098145\n",
      "Mean loss: 0.6967355211575826\n",
      "Std loss: 0.09025771116439957\n",
      "Total Loss: 4.180413126945496\n",
      "------------------------------------ epoch 3378 (20262 steps) ------------------------------------\n",
      "Max loss: 0.5458958148956299\n",
      "Min loss: 0.3969920873641968\n",
      "Mean loss: 0.47937317689259845\n",
      "Std loss: 0.04886174557944653\n",
      "Total Loss: 2.876239061355591\n",
      "------------------------------------ epoch 3379 (20268 steps) ------------------------------------\n",
      "Max loss: 0.39449119567871094\n",
      "Min loss: 0.3262430727481842\n",
      "Mean loss: 0.36508922775586444\n",
      "Std loss: 0.02648227079250411\n",
      "Total Loss: 2.1905353665351868\n",
      "------------------------------------ epoch 3380 (20274 steps) ------------------------------------\n",
      "Max loss: 0.35241079330444336\n",
      "Min loss: 0.26858776807785034\n",
      "Mean loss: 0.30354956289132434\n",
      "Std loss: 0.029057514369005024\n",
      "Total Loss: 1.8212973773479462\n",
      "------------------------------------ epoch 3381 (20280 steps) ------------------------------------\n",
      "Max loss: 0.28051862120628357\n",
      "Min loss: 0.19102630019187927\n",
      "Mean loss: 0.2306965465346972\n",
      "Std loss: 0.027539839307149688\n",
      "Total Loss: 1.3841792792081833\n",
      "------------------------------------ epoch 3382 (20286 steps) ------------------------------------\n",
      "Max loss: 0.22398662567138672\n",
      "Min loss: 0.18031424283981323\n",
      "Mean loss: 0.1985874871412913\n",
      "Std loss: 0.016735091083950097\n",
      "Total Loss: 1.1915249228477478\n",
      "------------------------------------ epoch 3383 (20292 steps) ------------------------------------\n",
      "Max loss: 0.20595239102840424\n",
      "Min loss: 0.14463938772678375\n",
      "Mean loss: 0.16995953768491745\n",
      "Std loss: 0.020560735809182492\n",
      "Total Loss: 1.0197572261095047\n",
      "------------------------------------ epoch 3384 (20298 steps) ------------------------------------\n",
      "Max loss: 0.1857166588306427\n",
      "Min loss: 0.11328853666782379\n",
      "Mean loss: 0.1559713731209437\n",
      "Std loss: 0.027433187688563027\n",
      "Total Loss: 0.9358282387256622\n",
      "------------------------------------ epoch 3385 (20304 steps) ------------------------------------\n",
      "Max loss: 0.15081480145454407\n",
      "Min loss: 0.09487470984458923\n",
      "Mean loss: 0.12054528668522835\n",
      "Std loss: 0.018393870440363922\n",
      "Total Loss: 0.7232717201113701\n",
      "------------------------------------ epoch 3386 (20310 steps) ------------------------------------\n",
      "Max loss: 0.17446307837963104\n",
      "Min loss: 0.06850378960371017\n",
      "Mean loss: 0.10305490593115489\n",
      "Std loss: 0.03399241214082469\n",
      "Total Loss: 0.6183294355869293\n",
      "------------------------------------ epoch 3387 (20316 steps) ------------------------------------\n",
      "Max loss: 0.1180652529001236\n",
      "Min loss: 0.07222272455692291\n",
      "Mean loss: 0.0904353881875674\n",
      "Std loss: 0.015940141908294854\n",
      "Total Loss: 0.5426123291254044\n",
      "------------------------------------ epoch 3388 (20322 steps) ------------------------------------\n",
      "Max loss: 0.1042972058057785\n",
      "Min loss: 0.07164043188095093\n",
      "Mean loss: 0.08430973440408707\n",
      "Std loss: 0.012593332267794608\n",
      "Total Loss: 0.5058584064245224\n",
      "------------------------------------ epoch 3389 (20328 steps) ------------------------------------\n",
      "Max loss: 0.10427187383174896\n",
      "Min loss: 0.04652319848537445\n",
      "Mean loss: 0.0665172648926576\n",
      "Std loss: 0.0186440700470063\n",
      "Total Loss: 0.3991035893559456\n",
      "------------------------------------ epoch 3390 (20334 steps) ------------------------------------\n",
      "Max loss: 0.0903228148818016\n",
      "Min loss: 0.04430451616644859\n",
      "Mean loss: 0.060829617703954376\n",
      "Std loss: 0.014683259387483664\n",
      "Total Loss: 0.3649777062237263\n",
      "------------------------------------ epoch 3391 (20340 steps) ------------------------------------\n",
      "Max loss: 0.08143027871847153\n",
      "Min loss: 0.029526960104703903\n",
      "Mean loss: 0.055626168847084045\n",
      "Std loss: 0.017807740090855378\n",
      "Total Loss: 0.3337570130825043\n",
      "------------------------------------ epoch 3392 (20346 steps) ------------------------------------\n",
      "Max loss: 0.07209402322769165\n",
      "Min loss: 0.02998163178563118\n",
      "Mean loss: 0.04635654886563619\n",
      "Std loss: 0.014958998643915767\n",
      "Total Loss: 0.27813929319381714\n",
      "------------------------------------ epoch 3393 (20352 steps) ------------------------------------\n",
      "Max loss: 0.11515428870916367\n",
      "Min loss: 0.036851055920124054\n",
      "Mean loss: 0.06315697853763898\n",
      "Std loss: 0.025032340593482246\n",
      "Total Loss: 0.3789418712258339\n",
      "------------------------------------ epoch 3394 (20358 steps) ------------------------------------\n",
      "Max loss: 0.06706875562667847\n",
      "Min loss: 0.029588919132947922\n",
      "Mean loss: 0.044747327764829\n",
      "Std loss: 0.013168098439801027\n",
      "Total Loss: 0.268483966588974\n",
      "------------------------------------ epoch 3395 (20364 steps) ------------------------------------\n",
      "Max loss: 0.04917890578508377\n",
      "Min loss: 0.023065820336341858\n",
      "Mean loss: 0.03684547978142897\n",
      "Std loss: 0.008921523571300615\n",
      "Total Loss: 0.22107287868857384\n",
      "------------------------------------ epoch 3396 (20370 steps) ------------------------------------\n",
      "Max loss: 0.040330614894628525\n",
      "Min loss: 0.02558048814535141\n",
      "Mean loss: 0.03513485689957937\n",
      "Std loss: 0.005322311978494392\n",
      "Total Loss: 0.2108091413974762\n",
      "------------------------------------ epoch 3397 (20376 steps) ------------------------------------\n",
      "Max loss: 0.0549921914935112\n",
      "Min loss: 0.020811840891838074\n",
      "Mean loss: 0.03065187359849612\n",
      "Std loss: 0.01164442907720246\n",
      "Total Loss: 0.18391124159097672\n",
      "------------------------------------ epoch 3398 (20382 steps) ------------------------------------\n",
      "Max loss: 0.03456073999404907\n",
      "Min loss: 0.018453702330589294\n",
      "Mean loss: 0.02713429493208726\n",
      "Std loss: 0.006478208830935364\n",
      "Total Loss: 0.16280576959252357\n",
      "------------------------------------ epoch 3399 (20388 steps) ------------------------------------\n",
      "Max loss: 0.04222095012664795\n",
      "Min loss: 0.02058504894375801\n",
      "Mean loss: 0.029604561006029446\n",
      "Std loss: 0.0074039349399000745\n",
      "Total Loss: 0.17762736603617668\n",
      "------------------------------------ epoch 3400 (20394 steps) ------------------------------------\n",
      "Max loss: 0.05207474157214165\n",
      "Min loss: 0.020013155415654182\n",
      "Mean loss: 0.0327152411142985\n",
      "Std loss: 0.011427703077000497\n",
      "Total Loss: 0.19629144668579102\n",
      "------------------------------------ epoch 3401 (20400 steps) ------------------------------------\n",
      "Max loss: 0.05103772133588791\n",
      "Min loss: 0.01794271171092987\n",
      "Mean loss: 0.03185207179437081\n",
      "Std loss: 0.01110535294790149\n",
      "Total Loss: 0.19111243076622486\n",
      "saved model at ./weights/model_3401.pth\n",
      "------------------------------------ epoch 3402 (20406 steps) ------------------------------------\n",
      "Max loss: 0.04422185942530632\n",
      "Min loss: 0.01914527639746666\n",
      "Mean loss: 0.028103222139179707\n",
      "Std loss: 0.008167898464830972\n",
      "Total Loss: 0.16861933283507824\n",
      "------------------------------------ epoch 3403 (20412 steps) ------------------------------------\n",
      "Max loss: 0.04677233844995499\n",
      "Min loss: 0.01768176443874836\n",
      "Mean loss: 0.02834380604326725\n",
      "Std loss: 0.010426678125718743\n",
      "Total Loss: 0.1700628362596035\n",
      "------------------------------------ epoch 3404 (20418 steps) ------------------------------------\n",
      "Max loss: 0.041405774652957916\n",
      "Min loss: 0.01755911484360695\n",
      "Mean loss: 0.02471275596568982\n",
      "Std loss: 0.0077922539932209454\n",
      "Total Loss: 0.1482765357941389\n",
      "------------------------------------ epoch 3405 (20424 steps) ------------------------------------\n",
      "Max loss: 0.06167348474264145\n",
      "Min loss: 0.017723867669701576\n",
      "Mean loss: 0.03520565324773391\n",
      "Std loss: 0.014510368375906748\n",
      "Total Loss: 0.21123391948640347\n",
      "------------------------------------ epoch 3406 (20430 steps) ------------------------------------\n",
      "Max loss: 0.051976680755615234\n",
      "Min loss: 0.014978483319282532\n",
      "Mean loss: 0.03527121183772882\n",
      "Std loss: 0.01358607232192204\n",
      "Total Loss: 0.2116272710263729\n",
      "------------------------------------ epoch 3407 (20436 steps) ------------------------------------\n",
      "Max loss: 0.03177585452795029\n",
      "Min loss: 0.017940156161785126\n",
      "Mean loss: 0.025319024299581844\n",
      "Std loss: 0.00485576514807331\n",
      "Total Loss: 0.15191414579749107\n",
      "------------------------------------ epoch 3408 (20442 steps) ------------------------------------\n",
      "Max loss: 0.034833066165447235\n",
      "Min loss: 0.013682322576642036\n",
      "Mean loss: 0.022392985100547474\n",
      "Std loss: 0.008312089681986028\n",
      "Total Loss: 0.13435791060328484\n",
      "------------------------------------ epoch 3409 (20448 steps) ------------------------------------\n",
      "Max loss: 0.03988203406333923\n",
      "Min loss: 0.015374615788459778\n",
      "Mean loss: 0.024031912287076313\n",
      "Std loss: 0.008314472113136627\n",
      "Total Loss: 0.14419147372245789\n",
      "------------------------------------ epoch 3410 (20454 steps) ------------------------------------\n",
      "Max loss: 0.06270310282707214\n",
      "Min loss: 0.017567457631230354\n",
      "Mean loss: 0.03852926039447387\n",
      "Std loss: 0.014083602663713697\n",
      "Total Loss: 0.23117556236684322\n",
      "------------------------------------ epoch 3411 (20460 steps) ------------------------------------\n",
      "Max loss: 0.05969554930925369\n",
      "Min loss: 0.015956254675984383\n",
      "Mean loss: 0.03247284019986788\n",
      "Std loss: 0.013475817391843314\n",
      "Total Loss: 0.1948370411992073\n",
      "------------------------------------ epoch 3412 (20466 steps) ------------------------------------\n",
      "Max loss: 0.03849639743566513\n",
      "Min loss: 0.013809438794851303\n",
      "Mean loss: 0.021028568968176842\n",
      "Std loss: 0.008266758445177668\n",
      "Total Loss: 0.12617141380906105\n",
      "------------------------------------ epoch 3413 (20472 steps) ------------------------------------\n",
      "Max loss: 0.06272286176681519\n",
      "Min loss: 0.015412002801895142\n",
      "Mean loss: 0.02951582831641038\n",
      "Std loss: 0.016144660077486415\n",
      "Total Loss: 0.1770949698984623\n",
      "------------------------------------ epoch 3414 (20478 steps) ------------------------------------\n",
      "Max loss: 0.03114837035536766\n",
      "Min loss: 0.013295091688632965\n",
      "Mean loss: 0.020386558181295793\n",
      "Std loss: 0.00635471041837334\n",
      "Total Loss: 0.12231934908777475\n",
      "------------------------------------ epoch 3415 (20484 steps) ------------------------------------\n",
      "Max loss: 0.0300733745098114\n",
      "Min loss: 0.014642596244812012\n",
      "Mean loss: 0.021949590804676216\n",
      "Std loss: 0.0066904208295068615\n",
      "Total Loss: 0.1316975448280573\n",
      "------------------------------------ epoch 3416 (20490 steps) ------------------------------------\n",
      "Max loss: 0.03081050142645836\n",
      "Min loss: 0.013211634010076523\n",
      "Mean loss: 0.02209323551505804\n",
      "Std loss: 0.0069216134129290174\n",
      "Total Loss: 0.13255941309034824\n",
      "------------------------------------ epoch 3417 (20496 steps) ------------------------------------\n",
      "Max loss: 0.06928958743810654\n",
      "Min loss: 0.016565043479204178\n",
      "Mean loss: 0.03474765426168839\n",
      "Std loss: 0.017099160769510617\n",
      "Total Loss: 0.20848592557013035\n",
      "------------------------------------ epoch 3418 (20502 steps) ------------------------------------\n",
      "Max loss: 0.033857665956020355\n",
      "Min loss: 0.017425227910280228\n",
      "Mean loss: 0.02214620355516672\n",
      "Std loss: 0.0055230307603524155\n",
      "Total Loss: 0.13287722133100033\n",
      "------------------------------------ epoch 3419 (20508 steps) ------------------------------------\n",
      "Max loss: 0.07767602801322937\n",
      "Min loss: 0.014195915311574936\n",
      "Mean loss: 0.031709507728616394\n",
      "Std loss: 0.021619366048627027\n",
      "Total Loss: 0.19025704637169838\n",
      "------------------------------------ epoch 3420 (20514 steps) ------------------------------------\n",
      "Max loss: 0.04588613659143448\n",
      "Min loss: 0.01564885303378105\n",
      "Mean loss: 0.03165673092007637\n",
      "Std loss: 0.012748112424077047\n",
      "Total Loss: 0.18994038552045822\n",
      "------------------------------------ epoch 3421 (20520 steps) ------------------------------------\n",
      "Max loss: 0.035293325781822205\n",
      "Min loss: 0.013564148917794228\n",
      "Mean loss: 0.02181560716902216\n",
      "Std loss: 0.008375289868575832\n",
      "Total Loss: 0.13089364301413298\n",
      "------------------------------------ epoch 3422 (20526 steps) ------------------------------------\n",
      "Max loss: 0.037897657603025436\n",
      "Min loss: 0.011759727261960506\n",
      "Mean loss: 0.021986530628055334\n",
      "Std loss: 0.009576463548263298\n",
      "Total Loss: 0.131919183768332\n",
      "------------------------------------ epoch 3423 (20532 steps) ------------------------------------\n",
      "Max loss: 0.04162183031439781\n",
      "Min loss: 0.014283428899943829\n",
      "Mean loss: 0.02445608361934622\n",
      "Std loss: 0.00925496226776134\n",
      "Total Loss: 0.14673650171607733\n",
      "------------------------------------ epoch 3424 (20538 steps) ------------------------------------\n",
      "Max loss: 0.02786010131239891\n",
      "Min loss: 0.011211486533284187\n",
      "Mean loss: 0.01873900710294644\n",
      "Std loss: 0.0049548200450834036\n",
      "Total Loss: 0.11243404261767864\n",
      "------------------------------------ epoch 3425 (20544 steps) ------------------------------------\n",
      "Max loss: 0.06694084405899048\n",
      "Min loss: 0.01152261346578598\n",
      "Mean loss: 0.02857952192425728\n",
      "Std loss: 0.01979235118817012\n",
      "Total Loss: 0.17147713154554367\n",
      "------------------------------------ epoch 3426 (20550 steps) ------------------------------------\n",
      "Max loss: 0.03102777525782585\n",
      "Min loss: 0.011723373085260391\n",
      "Mean loss: 0.020383588193605345\n",
      "Std loss: 0.006882501617536503\n",
      "Total Loss: 0.12230152916163206\n",
      "------------------------------------ epoch 3427 (20556 steps) ------------------------------------\n",
      "Max loss: 0.060794562101364136\n",
      "Min loss: 0.012106243520975113\n",
      "Mean loss: 0.034757882046202816\n",
      "Std loss: 0.019727460989129638\n",
      "Total Loss: 0.2085472922772169\n",
      "------------------------------------ epoch 3428 (20562 steps) ------------------------------------\n",
      "Max loss: 0.03029048442840576\n",
      "Min loss: 0.017009643837809563\n",
      "Mean loss: 0.02328582356373469\n",
      "Std loss: 0.005147204950095648\n",
      "Total Loss: 0.13971494138240814\n",
      "------------------------------------ epoch 3429 (20568 steps) ------------------------------------\n",
      "Max loss: 0.02927917242050171\n",
      "Min loss: 0.013230341486632824\n",
      "Mean loss: 0.017493389857312042\n",
      "Std loss: 0.005837426368595974\n",
      "Total Loss: 0.10496033914387226\n",
      "------------------------------------ epoch 3430 (20574 steps) ------------------------------------\n",
      "Max loss: 0.05332798510789871\n",
      "Min loss: 0.011423139832913876\n",
      "Mean loss: 0.02449792142336567\n",
      "Std loss: 0.013542472276641269\n",
      "Total Loss: 0.14698752854019403\n",
      "------------------------------------ epoch 3431 (20580 steps) ------------------------------------\n",
      "Max loss: 0.04112403467297554\n",
      "Min loss: 0.012227162718772888\n",
      "Mean loss: 0.02648484365393718\n",
      "Std loss: 0.012609144302853086\n",
      "Total Loss: 0.15890906192362309\n",
      "------------------------------------ epoch 3432 (20586 steps) ------------------------------------\n",
      "Max loss: 0.02495957724750042\n",
      "Min loss: 0.010621385648846626\n",
      "Mean loss: 0.01909783063456416\n",
      "Std loss: 0.00474621126177058\n",
      "Total Loss: 0.11458698380738497\n",
      "------------------------------------ epoch 3433 (20592 steps) ------------------------------------\n",
      "Max loss: 0.029487941414117813\n",
      "Min loss: 0.01469600573182106\n",
      "Mean loss: 0.02042128269871076\n",
      "Std loss: 0.004929750311393589\n",
      "Total Loss: 0.12252769619226456\n",
      "------------------------------------ epoch 3434 (20598 steps) ------------------------------------\n",
      "Max loss: 0.03739595413208008\n",
      "Min loss: 0.012658531777560711\n",
      "Mean loss: 0.02017427406584223\n",
      "Std loss: 0.00863180904341174\n",
      "Total Loss: 0.12104564439505339\n",
      "------------------------------------ epoch 3435 (20604 steps) ------------------------------------\n",
      "Max loss: 0.05024651810526848\n",
      "Min loss: 0.0128487478941679\n",
      "Mean loss: 0.025826914856831234\n",
      "Std loss: 0.012307188181724198\n",
      "Total Loss: 0.1549614891409874\n",
      "------------------------------------ epoch 3436 (20610 steps) ------------------------------------\n",
      "Max loss: 0.04109325259923935\n",
      "Min loss: 0.011441802605986595\n",
      "Mean loss: 0.019519478548318148\n",
      "Std loss: 0.01035988319384493\n",
      "Total Loss: 0.11711687128990889\n",
      "------------------------------------ epoch 3437 (20616 steps) ------------------------------------\n",
      "Max loss: 0.024038081988692284\n",
      "Min loss: 0.010805974714457989\n",
      "Mean loss: 0.018657006168117125\n",
      "Std loss: 0.0044071397518014675\n",
      "Total Loss: 0.11194203700870275\n",
      "------------------------------------ epoch 3438 (20622 steps) ------------------------------------\n",
      "Max loss: 0.10221925377845764\n",
      "Min loss: 0.01174095831811428\n",
      "Mean loss: 0.03765424263353149\n",
      "Std loss: 0.03448557751217734\n",
      "Total Loss: 0.22592545580118895\n",
      "------------------------------------ epoch 3439 (20628 steps) ------------------------------------\n",
      "Max loss: 0.017346613109111786\n",
      "Min loss: 0.011264892295002937\n",
      "Mean loss: 0.013727258890867233\n",
      "Std loss: 0.0019426203459641104\n",
      "Total Loss: 0.0823635533452034\n",
      "------------------------------------ epoch 3440 (20634 steps) ------------------------------------\n",
      "Max loss: 0.02689397893846035\n",
      "Min loss: 0.014466534368693829\n",
      "Mean loss: 0.02050484453017513\n",
      "Std loss: 0.0045211108486550335\n",
      "Total Loss: 0.12302906718105078\n",
      "------------------------------------ epoch 3441 (20640 steps) ------------------------------------\n",
      "Max loss: 0.03191513568162918\n",
      "Min loss: 0.01161644235253334\n",
      "Mean loss: 0.01654759778951605\n",
      "Std loss: 0.0071218330307269255\n",
      "Total Loss: 0.09928558673709631\n",
      "------------------------------------ epoch 3442 (20646 steps) ------------------------------------\n",
      "Max loss: 0.028212394565343857\n",
      "Min loss: 0.009170671924948692\n",
      "Mean loss: 0.015314480755478144\n",
      "Std loss: 0.006192102457168288\n",
      "Total Loss: 0.09188688453286886\n",
      "------------------------------------ epoch 3443 (20652 steps) ------------------------------------\n",
      "Max loss: 0.02547292225062847\n",
      "Min loss: 0.010547983460128307\n",
      "Mean loss: 0.018309757113456726\n",
      "Std loss: 0.0054029068150112545\n",
      "Total Loss: 0.10985854268074036\n",
      "------------------------------------ epoch 3444 (20658 steps) ------------------------------------\n",
      "Max loss: 0.02541325055062771\n",
      "Min loss: 0.011187201365828514\n",
      "Mean loss: 0.01905802885691325\n",
      "Std loss: 0.005475893102004398\n",
      "Total Loss: 0.11434817314147949\n",
      "------------------------------------ epoch 3445 (20664 steps) ------------------------------------\n",
      "Max loss: 0.06479852646589279\n",
      "Min loss: 0.009557975456118584\n",
      "Mean loss: 0.025888788358618815\n",
      "Std loss: 0.02002929118335858\n",
      "Total Loss: 0.1553327301517129\n",
      "------------------------------------ epoch 3446 (20670 steps) ------------------------------------\n",
      "Max loss: 0.029667485505342484\n",
      "Min loss: 0.010197024792432785\n",
      "Mean loss: 0.018115617024401825\n",
      "Std loss: 0.0061862211114957506\n",
      "Total Loss: 0.10869370214641094\n",
      "------------------------------------ epoch 3447 (20676 steps) ------------------------------------\n",
      "Max loss: 0.020795835182070732\n",
      "Min loss: 0.009311044588685036\n",
      "Mean loss: 0.014921325414131084\n",
      "Std loss: 0.004310236129101688\n",
      "Total Loss: 0.08952795248478651\n",
      "------------------------------------ epoch 3448 (20682 steps) ------------------------------------\n",
      "Max loss: 0.0341087244451046\n",
      "Min loss: 0.011445535346865654\n",
      "Mean loss: 0.01907199838509162\n",
      "Std loss: 0.007848147158817997\n",
      "Total Loss: 0.11443199031054974\n",
      "------------------------------------ epoch 3449 (20688 steps) ------------------------------------\n",
      "Max loss: 0.026486298069357872\n",
      "Min loss: 0.010278391651809216\n",
      "Mean loss: 0.01882524959122141\n",
      "Std loss: 0.0053957931530558\n",
      "Total Loss: 0.11295149754732847\n",
      "------------------------------------ epoch 3450 (20694 steps) ------------------------------------\n",
      "Max loss: 0.026380112394690514\n",
      "Min loss: 0.008591966703534126\n",
      "Mean loss: 0.016411433617273968\n",
      "Std loss: 0.00636511686705962\n",
      "Total Loss: 0.0984686017036438\n",
      "------------------------------------ epoch 3451 (20700 steps) ------------------------------------\n",
      "Max loss: 0.03909391537308693\n",
      "Min loss: 0.01200488768517971\n",
      "Mean loss: 0.021779808060576517\n",
      "Std loss: 0.009308957869047315\n",
      "Total Loss: 0.1306788483634591\n",
      "------------------------------------ epoch 3452 (20706 steps) ------------------------------------\n",
      "Max loss: 0.030627302825450897\n",
      "Min loss: 0.012074041180312634\n",
      "Mean loss: 0.01914956436182062\n",
      "Std loss: 0.006299908060510667\n",
      "Total Loss: 0.11489738617092371\n",
      "------------------------------------ epoch 3453 (20712 steps) ------------------------------------\n",
      "Max loss: 0.03383193910121918\n",
      "Min loss: 0.01012162584811449\n",
      "Mean loss: 0.018943337102731068\n",
      "Std loss: 0.007830770296296806\n",
      "Total Loss: 0.11366002261638641\n",
      "------------------------------------ epoch 3454 (20718 steps) ------------------------------------\n",
      "Max loss: 0.02195894718170166\n",
      "Min loss: 0.011306291446089745\n",
      "Mean loss: 0.01630680790791909\n",
      "Std loss: 0.003336277546218431\n",
      "Total Loss: 0.09784084744751453\n",
      "------------------------------------ epoch 3455 (20724 steps) ------------------------------------\n",
      "Max loss: 0.03436871990561485\n",
      "Min loss: 0.011406898498535156\n",
      "Mean loss: 0.020716475322842598\n",
      "Std loss: 0.007486768499207615\n",
      "Total Loss: 0.12429885193705559\n",
      "------------------------------------ epoch 3456 (20730 steps) ------------------------------------\n",
      "Max loss: 0.05785740539431572\n",
      "Min loss: 0.010241884738206863\n",
      "Mean loss: 0.020549805214007694\n",
      "Std loss: 0.016934587426048656\n",
      "Total Loss: 0.12329883128404617\n",
      "------------------------------------ epoch 3457 (20736 steps) ------------------------------------\n",
      "Max loss: 0.016076255589723587\n",
      "Min loss: 0.013100948184728622\n",
      "Mean loss: 0.01445014247049888\n",
      "Std loss: 0.0010525955050895233\n",
      "Total Loss: 0.08670085482299328\n",
      "------------------------------------ epoch 3458 (20742 steps) ------------------------------------\n",
      "Max loss: 0.03797353804111481\n",
      "Min loss: 0.011576233431696892\n",
      "Mean loss: 0.018555392666409414\n",
      "Std loss: 0.009335529534545\n",
      "Total Loss: 0.11133235599845648\n",
      "------------------------------------ epoch 3459 (20748 steps) ------------------------------------\n",
      "Max loss: 0.027929332107305527\n",
      "Min loss: 0.008643409237265587\n",
      "Mean loss: 0.01640795807664593\n",
      "Std loss: 0.007072676116972375\n",
      "Total Loss: 0.09844774845987558\n",
      "------------------------------------ epoch 3460 (20754 steps) ------------------------------------\n",
      "Max loss: 0.02657473459839821\n",
      "Min loss: 0.00952768512070179\n",
      "Mean loss: 0.018541843475153048\n",
      "Std loss: 0.006394878880686402\n",
      "Total Loss: 0.11125106085091829\n",
      "------------------------------------ epoch 3461 (20760 steps) ------------------------------------\n",
      "Max loss: 0.05272778868675232\n",
      "Min loss: 0.016996867954730988\n",
      "Mean loss: 0.028704481820265453\n",
      "Std loss: 0.012480246195351449\n",
      "Total Loss: 0.1722268909215927\n",
      "------------------------------------ epoch 3462 (20766 steps) ------------------------------------\n",
      "Max loss: 0.0774570107460022\n",
      "Min loss: 0.009656351059675217\n",
      "Mean loss: 0.027410552681734163\n",
      "Std loss: 0.02303982480396465\n",
      "Total Loss: 0.164463316090405\n",
      "------------------------------------ epoch 3463 (20772 steps) ------------------------------------\n",
      "Max loss: 0.03719722479581833\n",
      "Min loss: 0.011822925880551338\n",
      "Mean loss: 0.019479847823580105\n",
      "Std loss: 0.00898905479650279\n",
      "Total Loss: 0.11687908694148064\n",
      "------------------------------------ epoch 3464 (20778 steps) ------------------------------------\n",
      "Max loss: 0.033050283789634705\n",
      "Min loss: 0.0102152144536376\n",
      "Mean loss: 0.01738931859532992\n",
      "Std loss: 0.007529042839344624\n",
      "Total Loss: 0.10433591157197952\n",
      "------------------------------------ epoch 3465 (20784 steps) ------------------------------------\n",
      "Max loss: 0.02142786793410778\n",
      "Min loss: 0.0087025947868824\n",
      "Mean loss: 0.01440142591794332\n",
      "Std loss: 0.004097252341067121\n",
      "Total Loss: 0.08640855550765991\n",
      "------------------------------------ epoch 3466 (20790 steps) ------------------------------------\n",
      "Max loss: 0.03647584095597267\n",
      "Min loss: 0.00884102564305067\n",
      "Mean loss: 0.016606561684360106\n",
      "Std loss: 0.009583710382124485\n",
      "Total Loss: 0.09963937010616064\n",
      "------------------------------------ epoch 3467 (20796 steps) ------------------------------------\n",
      "Max loss: 0.025397179648280144\n",
      "Min loss: 0.010947642847895622\n",
      "Mean loss: 0.01644259722282489\n",
      "Std loss: 0.005037543901615113\n",
      "Total Loss: 0.09865558333694935\n",
      "------------------------------------ epoch 3468 (20802 steps) ------------------------------------\n",
      "Max loss: 0.03286012262105942\n",
      "Min loss: 0.00882256031036377\n",
      "Mean loss: 0.01484265768279632\n",
      "Std loss: 0.008261088565551234\n",
      "Total Loss: 0.08905594609677792\n",
      "------------------------------------ epoch 3469 (20808 steps) ------------------------------------\n",
      "Max loss: 0.03067741170525551\n",
      "Min loss: 0.008443834260106087\n",
      "Mean loss: 0.017100255315502483\n",
      "Std loss: 0.008201773948489775\n",
      "Total Loss: 0.10260153189301491\n",
      "------------------------------------ epoch 3470 (20814 steps) ------------------------------------\n",
      "Max loss: 0.04112981632351875\n",
      "Min loss: 0.010207190178334713\n",
      "Mean loss: 0.021629116653154295\n",
      "Std loss: 0.012667927686397195\n",
      "Total Loss: 0.12977469991892576\n",
      "------------------------------------ epoch 3471 (20820 steps) ------------------------------------\n",
      "Max loss: 0.04060117155313492\n",
      "Min loss: 0.009792660363018513\n",
      "Mean loss: 0.01718942013879617\n",
      "Std loss: 0.010763500457826333\n",
      "Total Loss: 0.10313652083277702\n",
      "------------------------------------ epoch 3472 (20826 steps) ------------------------------------\n",
      "Max loss: 0.020535031333565712\n",
      "Min loss: 0.008162552490830421\n",
      "Mean loss: 0.01371437031775713\n",
      "Std loss: 0.00430330910351632\n",
      "Total Loss: 0.08228622190654278\n",
      "------------------------------------ epoch 3473 (20832 steps) ------------------------------------\n",
      "Max loss: 0.032189540565013885\n",
      "Min loss: 0.009958913549780846\n",
      "Mean loss: 0.018406085359553497\n",
      "Std loss: 0.0070097002031205\n",
      "Total Loss: 0.11043651215732098\n",
      "------------------------------------ epoch 3474 (20838 steps) ------------------------------------\n",
      "Max loss: 0.02375102788209915\n",
      "Min loss: 0.009694131091237068\n",
      "Mean loss: 0.015063909348100424\n",
      "Std loss: 0.005434741126895895\n",
      "Total Loss: 0.09038345608860254\n",
      "------------------------------------ epoch 3475 (20844 steps) ------------------------------------\n",
      "Max loss: 0.0500454418361187\n",
      "Min loss: 0.009728167206048965\n",
      "Mean loss: 0.017840994832416374\n",
      "Std loss: 0.014451309906038184\n",
      "Total Loss: 0.10704596899449825\n",
      "------------------------------------ epoch 3476 (20850 steps) ------------------------------------\n",
      "Max loss: 0.03677378222346306\n",
      "Min loss: 0.009891282767057419\n",
      "Mean loss: 0.021336452725032966\n",
      "Std loss: 0.008293933848510418\n",
      "Total Loss: 0.1280187163501978\n",
      "------------------------------------ epoch 3477 (20856 steps) ------------------------------------\n",
      "Max loss: 0.024349823594093323\n",
      "Min loss: 0.012755933217704296\n",
      "Mean loss: 0.017722241580486298\n",
      "Std loss: 0.00467380637872935\n",
      "Total Loss: 0.10633344948291779\n",
      "------------------------------------ epoch 3478 (20862 steps) ------------------------------------\n",
      "Max loss: 0.015558588318526745\n",
      "Min loss: 0.009711112827062607\n",
      "Mean loss: 0.012020723894238472\n",
      "Std loss: 0.0020020148304988566\n",
      "Total Loss: 0.07212434336543083\n",
      "------------------------------------ epoch 3479 (20868 steps) ------------------------------------\n",
      "Max loss: 0.03800106793642044\n",
      "Min loss: 0.007853499613702297\n",
      "Mean loss: 0.018292764201760292\n",
      "Std loss: 0.010471396036745084\n",
      "Total Loss: 0.10975658521056175\n",
      "------------------------------------ epoch 3480 (20874 steps) ------------------------------------\n",
      "Max loss: 0.037904415279626846\n",
      "Min loss: 0.009837696328759193\n",
      "Mean loss: 0.021707541930178802\n",
      "Std loss: 0.009233415081671536\n",
      "Total Loss: 0.1302452515810728\n",
      "------------------------------------ epoch 3481 (20880 steps) ------------------------------------\n",
      "Max loss: 0.02233818732202053\n",
      "Min loss: 0.00940089114010334\n",
      "Mean loss: 0.015076981081316868\n",
      "Std loss: 0.00490568825129464\n",
      "Total Loss: 0.09046188648790121\n",
      "------------------------------------ epoch 3482 (20886 steps) ------------------------------------\n",
      "Max loss: 0.03462573140859604\n",
      "Min loss: 0.00809173658490181\n",
      "Mean loss: 0.016361633005241554\n",
      "Std loss: 0.008974920430879274\n",
      "Total Loss: 0.09816979803144932\n",
      "------------------------------------ epoch 3483 (20892 steps) ------------------------------------\n",
      "Max loss: 0.026868417859077454\n",
      "Min loss: 0.008059648796916008\n",
      "Mean loss: 0.012539843407769998\n",
      "Std loss: 0.006513571164087893\n",
      "Total Loss: 0.07523906044661999\n",
      "------------------------------------ epoch 3484 (20898 steps) ------------------------------------\n",
      "Max loss: 0.02851983532309532\n",
      "Min loss: 0.009490450844168663\n",
      "Mean loss: 0.01739467556277911\n",
      "Std loss: 0.00788780220502198\n",
      "Total Loss: 0.10436805337667465\n",
      "------------------------------------ epoch 3485 (20904 steps) ------------------------------------\n",
      "Max loss: 0.021704331040382385\n",
      "Min loss: 0.00854247435927391\n",
      "Mean loss: 0.0140471908574303\n",
      "Std loss: 0.0047651251518952146\n",
      "Total Loss: 0.0842831451445818\n",
      "------------------------------------ epoch 3486 (20910 steps) ------------------------------------\n",
      "Max loss: 0.02577757090330124\n",
      "Min loss: 0.010268381796777248\n",
      "Mean loss: 0.018007338823129732\n",
      "Std loss: 0.004755733771681803\n",
      "Total Loss: 0.1080440329387784\n",
      "------------------------------------ epoch 3487 (20916 steps) ------------------------------------\n",
      "Max loss: 0.026528503745794296\n",
      "Min loss: 0.009713178500533104\n",
      "Mean loss: 0.01645825415228804\n",
      "Std loss: 0.006932138765543106\n",
      "Total Loss: 0.09874952491372824\n",
      "------------------------------------ epoch 3488 (20922 steps) ------------------------------------\n",
      "Max loss: 0.021463006734848022\n",
      "Min loss: 0.01267818920314312\n",
      "Mean loss: 0.016077415086328983\n",
      "Std loss: 0.0027904043110072415\n",
      "Total Loss: 0.0964644905179739\n",
      "------------------------------------ epoch 3489 (20928 steps) ------------------------------------\n",
      "Max loss: 0.028537025675177574\n",
      "Min loss: 0.008901773951947689\n",
      "Mean loss: 0.017689673850933712\n",
      "Std loss: 0.007057801731259479\n",
      "Total Loss: 0.10613804310560226\n",
      "------------------------------------ epoch 3490 (20934 steps) ------------------------------------\n",
      "Max loss: 0.016236554831266403\n",
      "Min loss: 0.008282105438411236\n",
      "Mean loss: 0.012537619564682245\n",
      "Std loss: 0.0028315160538132096\n",
      "Total Loss: 0.07522571738809347\n",
      "------------------------------------ epoch 3491 (20940 steps) ------------------------------------\n",
      "Max loss: 0.02600223943591118\n",
      "Min loss: 0.008214909583330154\n",
      "Mean loss: 0.01482638002683719\n",
      "Std loss: 0.0067171436190640275\n",
      "Total Loss: 0.08895828016102314\n",
      "------------------------------------ epoch 3492 (20946 steps) ------------------------------------\n",
      "Max loss: 0.04520699381828308\n",
      "Min loss: 0.010046844370663166\n",
      "Mean loss: 0.02016934799030423\n",
      "Std loss: 0.012458124598607068\n",
      "Total Loss: 0.12101608794182539\n",
      "------------------------------------ epoch 3493 (20952 steps) ------------------------------------\n",
      "Max loss: 0.034587763249874115\n",
      "Min loss: 0.00787801668047905\n",
      "Mean loss: 0.01751383611311515\n",
      "Std loss: 0.008650805847285338\n",
      "Total Loss: 0.10508301667869091\n",
      "------------------------------------ epoch 3494 (20958 steps) ------------------------------------\n",
      "Max loss: 0.03692765533924103\n",
      "Min loss: 0.010229571722447872\n",
      "Mean loss: 0.021421990357339382\n",
      "Std loss: 0.009814304909610883\n",
      "Total Loss: 0.1285319421440363\n",
      "------------------------------------ epoch 3495 (20964 steps) ------------------------------------\n",
      "Max loss: 0.037330262362957\n",
      "Min loss: 0.007796973921358585\n",
      "Mean loss: 0.016515464211503666\n",
      "Std loss: 0.010206821932331547\n",
      "Total Loss: 0.09909278526902199\n",
      "------------------------------------ epoch 3496 (20970 steps) ------------------------------------\n",
      "Max loss: 0.04856289550662041\n",
      "Min loss: 0.014339993707835674\n",
      "Mean loss: 0.022706641349941492\n",
      "Std loss: 0.011729773586452795\n",
      "Total Loss: 0.13623984809964895\n",
      "------------------------------------ epoch 3497 (20976 steps) ------------------------------------\n",
      "Max loss: 0.018583334982395172\n",
      "Min loss: 0.00991064216941595\n",
      "Mean loss: 0.014800955386211475\n",
      "Std loss: 0.0032421732223325444\n",
      "Total Loss: 0.08880573231726885\n",
      "------------------------------------ epoch 3498 (20982 steps) ------------------------------------\n",
      "Max loss: 0.0631784051656723\n",
      "Min loss: 0.009490637108683586\n",
      "Mean loss: 0.026075551907221477\n",
      "Std loss: 0.017479142997522252\n",
      "Total Loss: 0.15645331144332886\n",
      "------------------------------------ epoch 3499 (20988 steps) ------------------------------------\n",
      "Max loss: 0.04404597729444504\n",
      "Min loss: 0.008259689435362816\n",
      "Mean loss: 0.023695139214396477\n",
      "Std loss: 0.014978437183233113\n",
      "Total Loss: 0.14217083528637886\n",
      "------------------------------------ epoch 3500 (20994 steps) ------------------------------------\n",
      "Max loss: 0.02581647038459778\n",
      "Min loss: 0.01154560036957264\n",
      "Mean loss: 0.015843191804985206\n",
      "Std loss: 0.004986638300524822\n",
      "Total Loss: 0.09505915082991123\n",
      "------------------------------------ epoch 3501 (21000 steps) ------------------------------------\n",
      "Max loss: 0.03364487737417221\n",
      "Min loss: 0.010776675306260586\n",
      "Mean loss: 0.016395120260616142\n",
      "Std loss: 0.007814410420617162\n",
      "Total Loss: 0.09837072156369686\n",
      "saved model at ./weights/model_3501.pth\n",
      "------------------------------------ epoch 3502 (21006 steps) ------------------------------------\n",
      "Max loss: 0.024899035692214966\n",
      "Min loss: 0.01040731929242611\n",
      "Mean loss: 0.016593868223329384\n",
      "Std loss: 0.004439654951528755\n",
      "Total Loss: 0.09956320933997631\n",
      "------------------------------------ epoch 3503 (21012 steps) ------------------------------------\n",
      "Max loss: 0.052109282463788986\n",
      "Min loss: 0.008137671276926994\n",
      "Mean loss: 0.01977259696771701\n",
      "Std loss: 0.015421160442387224\n",
      "Total Loss: 0.11863558180630207\n",
      "------------------------------------ epoch 3504 (21018 steps) ------------------------------------\n",
      "Max loss: 0.017850596457719803\n",
      "Min loss: 0.008066313341259956\n",
      "Mean loss: 0.013164415024220943\n",
      "Std loss: 0.0031284028393853344\n",
      "Total Loss: 0.07898649014532566\n",
      "------------------------------------ epoch 3505 (21024 steps) ------------------------------------\n",
      "Max loss: 0.025180524215102196\n",
      "Min loss: 0.007889253087341785\n",
      "Mean loss: 0.015995946402351063\n",
      "Std loss: 0.005348623966721806\n",
      "Total Loss: 0.09597567841410637\n",
      "------------------------------------ epoch 3506 (21030 steps) ------------------------------------\n",
      "Max loss: 0.0410771518945694\n",
      "Min loss: 0.008424082770943642\n",
      "Mean loss: 0.018205517902970314\n",
      "Std loss: 0.011182167929691421\n",
      "Total Loss: 0.10923310741782188\n",
      "------------------------------------ epoch 3507 (21036 steps) ------------------------------------\n",
      "Max loss: 0.038314055651426315\n",
      "Min loss: 0.008990459144115448\n",
      "Mean loss: 0.01969037779296438\n",
      "Std loss: 0.009931199021207008\n",
      "Total Loss: 0.11814226675778627\n",
      "------------------------------------ epoch 3508 (21042 steps) ------------------------------------\n",
      "Max loss: 0.019009564071893692\n",
      "Min loss: 0.009731627069413662\n",
      "Mean loss: 0.013325649468849102\n",
      "Std loss: 0.003822159991721867\n",
      "Total Loss: 0.07995389681309462\n",
      "------------------------------------ epoch 3509 (21048 steps) ------------------------------------\n",
      "Max loss: 0.024257205426692963\n",
      "Min loss: 0.00851354654878378\n",
      "Mean loss: 0.0149167290267845\n",
      "Std loss: 0.005429542191561676\n",
      "Total Loss: 0.089500374160707\n",
      "------------------------------------ epoch 3510 (21054 steps) ------------------------------------\n",
      "Max loss: 0.01861412450671196\n",
      "Min loss: 0.010442379862070084\n",
      "Mean loss: 0.013610159667829672\n",
      "Std loss: 0.0030262758796712396\n",
      "Total Loss: 0.08166095800697803\n",
      "------------------------------------ epoch 3511 (21060 steps) ------------------------------------\n",
      "Max loss: 0.026448752731084824\n",
      "Min loss: 0.010096286423504353\n",
      "Mean loss: 0.01641452731564641\n",
      "Std loss: 0.00589739469328965\n",
      "Total Loss: 0.09848716389387846\n",
      "------------------------------------ epoch 3512 (21066 steps) ------------------------------------\n",
      "Max loss: 0.029336903244256973\n",
      "Min loss: 0.01001463457942009\n",
      "Mean loss: 0.017845752493788797\n",
      "Std loss: 0.007231322376488337\n",
      "Total Loss: 0.10707451496273279\n",
      "------------------------------------ epoch 3513 (21072 steps) ------------------------------------\n",
      "Max loss: 0.015998918563127518\n",
      "Min loss: 0.009383240714669228\n",
      "Mean loss: 0.012540618578592936\n",
      "Std loss: 0.002449877474880078\n",
      "Total Loss: 0.07524371147155762\n",
      "------------------------------------ epoch 3514 (21078 steps) ------------------------------------\n",
      "Max loss: 0.02735763043165207\n",
      "Min loss: 0.00949302688241005\n",
      "Mean loss: 0.01440772833302617\n",
      "Std loss: 0.006030410513505186\n",
      "Total Loss: 0.08644636999815702\n",
      "------------------------------------ epoch 3515 (21084 steps) ------------------------------------\n",
      "Max loss: 0.053872108459472656\n",
      "Min loss: 0.008219338953495026\n",
      "Mean loss: 0.020689308643341064\n",
      "Std loss: 0.015397014902720963\n",
      "Total Loss: 0.12413585186004639\n",
      "------------------------------------ epoch 3516 (21090 steps) ------------------------------------\n",
      "Max loss: 0.03388196974992752\n",
      "Min loss: 0.009330793283879757\n",
      "Mean loss: 0.015969103357444208\n",
      "Std loss: 0.009067814572601696\n",
      "Total Loss: 0.09581462014466524\n",
      "------------------------------------ epoch 3517 (21096 steps) ------------------------------------\n",
      "Max loss: 0.034963011741638184\n",
      "Min loss: 0.008612415753304958\n",
      "Mean loss: 0.01907315415640672\n",
      "Std loss: 0.00802501961356014\n",
      "Total Loss: 0.11443892493844032\n",
      "------------------------------------ epoch 3518 (21102 steps) ------------------------------------\n",
      "Max loss: 0.04790554195642471\n",
      "Min loss: 0.00777295371517539\n",
      "Mean loss: 0.017528375688319404\n",
      "Std loss: 0.013831971737691445\n",
      "Total Loss: 0.10517025412991643\n",
      "------------------------------------ epoch 3519 (21108 steps) ------------------------------------\n",
      "Max loss: 0.022225305438041687\n",
      "Min loss: 0.009826071560382843\n",
      "Mean loss: 0.01700592723985513\n",
      "Std loss: 0.003666995857583449\n",
      "Total Loss: 0.10203556343913078\n",
      "------------------------------------ epoch 3520 (21114 steps) ------------------------------------\n",
      "Max loss: 0.01984037272632122\n",
      "Min loss: 0.009005960077047348\n",
      "Mean loss: 0.014062659038851658\n",
      "Std loss: 0.003784542952410436\n",
      "Total Loss: 0.08437595423310995\n",
      "------------------------------------ epoch 3521 (21120 steps) ------------------------------------\n",
      "Max loss: 0.027952980250120163\n",
      "Min loss: 0.01045512780547142\n",
      "Mean loss: 0.017855212247620027\n",
      "Std loss: 0.00679021611861809\n",
      "Total Loss: 0.10713127348572016\n",
      "------------------------------------ epoch 3522 (21126 steps) ------------------------------------\n",
      "Max loss: 0.02244197390973568\n",
      "Min loss: 0.008454354479908943\n",
      "Mean loss: 0.014057210180908442\n",
      "Std loss: 0.004677444006063437\n",
      "Total Loss: 0.08434326108545065\n",
      "------------------------------------ epoch 3523 (21132 steps) ------------------------------------\n",
      "Max loss: 0.034882158041000366\n",
      "Min loss: 0.009244635701179504\n",
      "Mean loss: 0.019131487235426903\n",
      "Std loss: 0.009484434618296036\n",
      "Total Loss: 0.11478892341256142\n",
      "------------------------------------ epoch 3524 (21138 steps) ------------------------------------\n",
      "Max loss: 0.045304395258426666\n",
      "Min loss: 0.009760282002389431\n",
      "Mean loss: 0.018294349623223145\n",
      "Std loss: 0.012785313419966938\n",
      "Total Loss: 0.10976609773933887\n",
      "------------------------------------ epoch 3525 (21144 steps) ------------------------------------\n",
      "Max loss: 0.03577079623937607\n",
      "Min loss: 0.008792979642748833\n",
      "Mean loss: 0.02106302495424946\n",
      "Std loss: 0.010992645623692093\n",
      "Total Loss: 0.12637814972549677\n",
      "------------------------------------ epoch 3526 (21150 steps) ------------------------------------\n",
      "Max loss: 0.021997030824422836\n",
      "Min loss: 0.010033976286649704\n",
      "Mean loss: 0.014796607661992311\n",
      "Std loss: 0.004451022459851405\n",
      "Total Loss: 0.08877964597195387\n",
      "------------------------------------ epoch 3527 (21156 steps) ------------------------------------\n",
      "Max loss: 0.01858695223927498\n",
      "Min loss: 0.008358268067240715\n",
      "Mean loss: 0.01384696705887715\n",
      "Std loss: 0.004445995472238784\n",
      "Total Loss: 0.0830818023532629\n",
      "------------------------------------ epoch 3528 (21162 steps) ------------------------------------\n",
      "Max loss: 0.019159432500600815\n",
      "Min loss: 0.010085915215313435\n",
      "Mean loss: 0.014962220719705025\n",
      "Std loss: 0.002814044327262658\n",
      "Total Loss: 0.08977332431823015\n",
      "------------------------------------ epoch 3529 (21168 steps) ------------------------------------\n",
      "Max loss: 0.011504094116389751\n",
      "Min loss: 0.0072924233973026276\n",
      "Mean loss: 0.00939047352100412\n",
      "Std loss: 0.0015277188479232168\n",
      "Total Loss: 0.05634284112602472\n",
      "------------------------------------ epoch 3530 (21174 steps) ------------------------------------\n",
      "Max loss: 0.024048810824751854\n",
      "Min loss: 0.008062778040766716\n",
      "Mean loss: 0.014930450667937597\n",
      "Std loss: 0.005594461808372459\n",
      "Total Loss: 0.08958270400762558\n",
      "------------------------------------ epoch 3531 (21180 steps) ------------------------------------\n",
      "Max loss: 0.01836770586669445\n",
      "Min loss: 0.009265398606657982\n",
      "Mean loss: 0.012513434514403343\n",
      "Std loss: 0.0029691195949641466\n",
      "Total Loss: 0.07508060708642006\n",
      "------------------------------------ epoch 3532 (21186 steps) ------------------------------------\n",
      "Max loss: 0.013960926793515682\n",
      "Min loss: 0.009903318248689175\n",
      "Mean loss: 0.01107065798714757\n",
      "Std loss: 0.0013948050855308522\n",
      "Total Loss: 0.06642394792288542\n",
      "------------------------------------ epoch 3533 (21192 steps) ------------------------------------\n",
      "Max loss: 0.031624890863895416\n",
      "Min loss: 0.010851884260773659\n",
      "Mean loss: 0.015086710142592588\n",
      "Std loss: 0.007447844321576984\n",
      "Total Loss: 0.09052026085555553\n",
      "------------------------------------ epoch 3534 (21198 steps) ------------------------------------\n",
      "Max loss: 0.023466473445296288\n",
      "Min loss: 0.012437049299478531\n",
      "Mean loss: 0.016796857118606567\n",
      "Std loss: 0.0042743301796442775\n",
      "Total Loss: 0.1007811427116394\n",
      "------------------------------------ epoch 3535 (21204 steps) ------------------------------------\n",
      "Max loss: 0.03158669173717499\n",
      "Min loss: 0.006746135652065277\n",
      "Mean loss: 0.019431767364343006\n",
      "Std loss: 0.008233465925294196\n",
      "Total Loss: 0.11659060418605804\n",
      "------------------------------------ epoch 3536 (21210 steps) ------------------------------------\n",
      "Max loss: 0.022476008161902428\n",
      "Min loss: 0.009434686973690987\n",
      "Mean loss: 0.013782932423055172\n",
      "Std loss: 0.004249000256777629\n",
      "Total Loss: 0.08269759453833103\n",
      "------------------------------------ epoch 3537 (21216 steps) ------------------------------------\n",
      "Max loss: 0.04413669928908348\n",
      "Min loss: 0.008689211681485176\n",
      "Mean loss: 0.019396752429505188\n",
      "Std loss: 0.01195069193037898\n",
      "Total Loss: 0.11638051457703114\n",
      "------------------------------------ epoch 3538 (21222 steps) ------------------------------------\n",
      "Max loss: 0.023131921887397766\n",
      "Min loss: 0.008178620599210262\n",
      "Mean loss: 0.01458372346435984\n",
      "Std loss: 0.005326662065698647\n",
      "Total Loss: 0.08750234078615904\n",
      "------------------------------------ epoch 3539 (21228 steps) ------------------------------------\n",
      "Max loss: 0.03147254139184952\n",
      "Min loss: 0.011669201776385307\n",
      "Mean loss: 0.018102434153358143\n",
      "Std loss: 0.006555770945459108\n",
      "Total Loss: 0.10861460492014885\n",
      "------------------------------------ epoch 3540 (21234 steps) ------------------------------------\n",
      "Max loss: 0.021796736866235733\n",
      "Min loss: 0.009276296012103558\n",
      "Mean loss: 0.015148804057389498\n",
      "Std loss: 0.004086740846873383\n",
      "Total Loss: 0.09089282434433699\n",
      "------------------------------------ epoch 3541 (21240 steps) ------------------------------------\n",
      "Max loss: 0.03434119373559952\n",
      "Min loss: 0.009131068363785744\n",
      "Mean loss: 0.016622843376050394\n",
      "Std loss: 0.009159293935083095\n",
      "Total Loss: 0.09973706025630236\n",
      "------------------------------------ epoch 3542 (21246 steps) ------------------------------------\n",
      "Max loss: 0.05068938434123993\n",
      "Min loss: 0.008954416960477829\n",
      "Mean loss: 0.023258469843616087\n",
      "Std loss: 0.014993534740306808\n",
      "Total Loss: 0.13955081906169653\n",
      "------------------------------------ epoch 3543 (21252 steps) ------------------------------------\n",
      "Max loss: 0.021962955594062805\n",
      "Min loss: 0.008441969752311707\n",
      "Mean loss: 0.01659504898513357\n",
      "Std loss: 0.0047995209458062005\n",
      "Total Loss: 0.09957029391080141\n",
      "------------------------------------ epoch 3544 (21258 steps) ------------------------------------\n",
      "Max loss: 0.019094986841082573\n",
      "Min loss: 0.009831467643380165\n",
      "Mean loss: 0.013195816427469254\n",
      "Std loss: 0.00314424259700242\n",
      "Total Loss: 0.07917489856481552\n",
      "------------------------------------ epoch 3545 (21264 steps) ------------------------------------\n",
      "Max loss: 0.02094261348247528\n",
      "Min loss: 0.00893015693873167\n",
      "Mean loss: 0.013884426094591618\n",
      "Std loss: 0.003734508984031489\n",
      "Total Loss: 0.0833065565675497\n",
      "------------------------------------ epoch 3546 (21270 steps) ------------------------------------\n",
      "Max loss: 0.0184251107275486\n",
      "Min loss: 0.0079243965446949\n",
      "Mean loss: 0.013770288011680046\n",
      "Std loss: 0.004251080113047931\n",
      "Total Loss: 0.08262172807008028\n",
      "------------------------------------ epoch 3547 (21276 steps) ------------------------------------\n",
      "Max loss: 0.042332723736763\n",
      "Min loss: 0.010630061849951744\n",
      "Mean loss: 0.020706442029525835\n",
      "Std loss: 0.010953584055477588\n",
      "Total Loss: 0.12423865217715502\n",
      "------------------------------------ epoch 3548 (21282 steps) ------------------------------------\n",
      "Max loss: 0.0290329959243536\n",
      "Min loss: 0.009566979482769966\n",
      "Mean loss: 0.015159271657466888\n",
      "Std loss: 0.006506596374234369\n",
      "Total Loss: 0.09095562994480133\n",
      "------------------------------------ epoch 3549 (21288 steps) ------------------------------------\n",
      "Max loss: 0.023370608687400818\n",
      "Min loss: 0.008729076944291592\n",
      "Mean loss: 0.014044040037939945\n",
      "Std loss: 0.004623398621531613\n",
      "Total Loss: 0.08426424022763968\n",
      "------------------------------------ epoch 3550 (21294 steps) ------------------------------------\n",
      "Max loss: 0.016268325969576836\n",
      "Min loss: 0.010187769308686256\n",
      "Mean loss: 0.013235609512776136\n",
      "Std loss: 0.0020081113755202086\n",
      "Total Loss: 0.07941365707665682\n",
      "------------------------------------ epoch 3551 (21300 steps) ------------------------------------\n",
      "Max loss: 0.019046790897846222\n",
      "Min loss: 0.007463260553777218\n",
      "Mean loss: 0.012707201453546682\n",
      "Std loss: 0.003577638925121732\n",
      "Total Loss: 0.0762432087212801\n",
      "------------------------------------ epoch 3552 (21306 steps) ------------------------------------\n",
      "Max loss: 0.01647518202662468\n",
      "Min loss: 0.009280002675950527\n",
      "Mean loss: 0.013071142913152775\n",
      "Std loss: 0.0030949147640292668\n",
      "Total Loss: 0.07842685747891665\n",
      "------------------------------------ epoch 3553 (21312 steps) ------------------------------------\n",
      "Max loss: 0.013962882570922375\n",
      "Min loss: 0.0078111994080245495\n",
      "Mean loss: 0.011712730939810475\n",
      "Std loss: 0.0023858010350784878\n",
      "Total Loss: 0.07027638563886285\n",
      "------------------------------------ epoch 3554 (21318 steps) ------------------------------------\n",
      "Max loss: 0.015256374143064022\n",
      "Min loss: 0.007500986568629742\n",
      "Mean loss: 0.011336396370703975\n",
      "Std loss: 0.00305944805523598\n",
      "Total Loss: 0.06801837822422385\n",
      "------------------------------------ epoch 3555 (21324 steps) ------------------------------------\n",
      "Max loss: 0.014575280249118805\n",
      "Min loss: 0.00739827286452055\n",
      "Mean loss: 0.01116810009504358\n",
      "Std loss: 0.0026311650601316718\n",
      "Total Loss: 0.06700860057026148\n",
      "------------------------------------ epoch 3556 (21330 steps) ------------------------------------\n",
      "Max loss: 0.042344339191913605\n",
      "Min loss: 0.014121219515800476\n",
      "Mean loss: 0.025656161985049646\n",
      "Std loss: 0.010450284633296203\n",
      "Total Loss: 0.15393697191029787\n",
      "------------------------------------ epoch 3557 (21336 steps) ------------------------------------\n",
      "Max loss: 0.025097165256738663\n",
      "Min loss: 0.009284000843763351\n",
      "Mean loss: 0.012918147103240093\n",
      "Std loss: 0.005639797765223028\n",
      "Total Loss: 0.07750888261944056\n",
      "------------------------------------ epoch 3558 (21342 steps) ------------------------------------\n",
      "Max loss: 0.022042756900191307\n",
      "Min loss: 0.009778739884495735\n",
      "Mean loss: 0.015297896228730679\n",
      "Std loss: 0.003838673630424009\n",
      "Total Loss: 0.09178737737238407\n",
      "------------------------------------ epoch 3559 (21348 steps) ------------------------------------\n",
      "Max loss: 0.031174279749393463\n",
      "Min loss: 0.009376771748065948\n",
      "Mean loss: 0.016134541171292465\n",
      "Std loss: 0.007569927275224266\n",
      "Total Loss: 0.09680724702775478\n",
      "------------------------------------ epoch 3560 (21354 steps) ------------------------------------\n",
      "Max loss: 0.025810271501541138\n",
      "Min loss: 0.007875164039433002\n",
      "Mean loss: 0.01582063843185703\n",
      "Std loss: 0.006619461279007575\n",
      "Total Loss: 0.09492383059114218\n",
      "------------------------------------ epoch 3561 (21360 steps) ------------------------------------\n",
      "Max loss: 0.02030307613313198\n",
      "Min loss: 0.008086532354354858\n",
      "Mean loss: 0.012318544089794159\n",
      "Std loss: 0.005047800269943509\n",
      "Total Loss: 0.07391126453876495\n",
      "------------------------------------ epoch 3562 (21366 steps) ------------------------------------\n",
      "Max loss: 0.01350165531039238\n",
      "Min loss: 0.007645830512046814\n",
      "Mean loss: 0.010511964559555054\n",
      "Std loss: 0.0021035772407530955\n",
      "Total Loss: 0.06307178735733032\n",
      "------------------------------------ epoch 3563 (21372 steps) ------------------------------------\n",
      "Max loss: 0.03869117423892021\n",
      "Min loss: 0.008232123218476772\n",
      "Mean loss: 0.018574465687076252\n",
      "Std loss: 0.011755368743992795\n",
      "Total Loss: 0.1114467941224575\n",
      "------------------------------------ epoch 3564 (21378 steps) ------------------------------------\n",
      "Max loss: 0.02507495880126953\n",
      "Min loss: 0.007892610505223274\n",
      "Mean loss: 0.014586375250170628\n",
      "Std loss: 0.007039948843750099\n",
      "Total Loss: 0.08751825150102377\n",
      "------------------------------------ epoch 3565 (21384 steps) ------------------------------------\n",
      "Max loss: 0.029193755239248276\n",
      "Min loss: 0.01362032350152731\n",
      "Mean loss: 0.018926914936552446\n",
      "Std loss: 0.005073889004690031\n",
      "Total Loss: 0.11356148961931467\n",
      "------------------------------------ epoch 3566 (21390 steps) ------------------------------------\n",
      "Max loss: 0.019860664382576942\n",
      "Min loss: 0.010209443047642708\n",
      "Mean loss: 0.013187333786239227\n",
      "Std loss: 0.003245246130167886\n",
      "Total Loss: 0.07912400271743536\n",
      "------------------------------------ epoch 3567 (21396 steps) ------------------------------------\n",
      "Max loss: 0.028655482456088066\n",
      "Min loss: 0.00801222212612629\n",
      "Mean loss: 0.015166075434535742\n",
      "Std loss: 0.006612111916406451\n",
      "Total Loss: 0.09099645260721445\n",
      "------------------------------------ epoch 3568 (21402 steps) ------------------------------------\n",
      "Max loss: 0.020816147327423096\n",
      "Min loss: 0.008955306373536587\n",
      "Mean loss: 0.01362261145065228\n",
      "Std loss: 0.0037529107812858385\n",
      "Total Loss: 0.08173566870391369\n",
      "------------------------------------ epoch 3569 (21408 steps) ------------------------------------\n",
      "Max loss: 0.04098305478692055\n",
      "Min loss: 0.01088155247271061\n",
      "Mean loss: 0.020277905743569136\n",
      "Std loss: 0.010583621368715383\n",
      "Total Loss: 0.12166743446141481\n",
      "------------------------------------ epoch 3570 (21414 steps) ------------------------------------\n",
      "Max loss: 0.0277390293776989\n",
      "Min loss: 0.007836367934942245\n",
      "Mean loss: 0.01979997909317414\n",
      "Std loss: 0.007556399081444844\n",
      "Total Loss: 0.11879987455904484\n",
      "------------------------------------ epoch 3571 (21420 steps) ------------------------------------\n",
      "Max loss: 0.01977038010954857\n",
      "Min loss: 0.007835166528820992\n",
      "Mean loss: 0.01319078542292118\n",
      "Std loss: 0.003880259276077527\n",
      "Total Loss: 0.07914471253752708\n",
      "------------------------------------ epoch 3572 (21426 steps) ------------------------------------\n",
      "Max loss: 0.018308138474822044\n",
      "Min loss: 0.007859605364501476\n",
      "Mean loss: 0.01367263946061333\n",
      "Std loss: 0.004197977199526942\n",
      "Total Loss: 0.08203583676367998\n",
      "------------------------------------ epoch 3573 (21432 steps) ------------------------------------\n",
      "Max loss: 0.02846742793917656\n",
      "Min loss: 0.008549364283680916\n",
      "Mean loss: 0.019216375115017097\n",
      "Std loss: 0.0069846871264464955\n",
      "Total Loss: 0.11529825069010258\n",
      "------------------------------------ epoch 3574 (21438 steps) ------------------------------------\n",
      "Max loss: 0.03071603737771511\n",
      "Min loss: 0.00824244599789381\n",
      "Mean loss: 0.013900527420143286\n",
      "Std loss: 0.00771034769935545\n",
      "Total Loss: 0.08340316452085972\n",
      "------------------------------------ epoch 3575 (21444 steps) ------------------------------------\n",
      "Max loss: 0.021216895431280136\n",
      "Min loss: 0.007831104099750519\n",
      "Mean loss: 0.01408160508920749\n",
      "Std loss: 0.003926986757661623\n",
      "Total Loss: 0.08448963053524494\n",
      "------------------------------------ epoch 3576 (21450 steps) ------------------------------------\n",
      "Max loss: 0.04217900335788727\n",
      "Min loss: 0.006597074680030346\n",
      "Mean loss: 0.017105527222156525\n",
      "Std loss: 0.011991958964043706\n",
      "Total Loss: 0.10263316333293915\n",
      "------------------------------------ epoch 3577 (21456 steps) ------------------------------------\n",
      "Max loss: 0.023439845070242882\n",
      "Min loss: 0.007147684693336487\n",
      "Mean loss: 0.01218230944747726\n",
      "Std loss: 0.005626814688281952\n",
      "Total Loss: 0.07309385668486357\n",
      "------------------------------------ epoch 3578 (21462 steps) ------------------------------------\n",
      "Max loss: 0.024230726063251495\n",
      "Min loss: 0.00809900090098381\n",
      "Mean loss: 0.013552961871027946\n",
      "Std loss: 0.005994464654452401\n",
      "Total Loss: 0.08131777122616768\n",
      "------------------------------------ epoch 3579 (21468 steps) ------------------------------------\n",
      "Max loss: 0.015439344570040703\n",
      "Min loss: 0.008540130220353603\n",
      "Mean loss: 0.01072505908086896\n",
      "Std loss: 0.0025102737318846774\n",
      "Total Loss: 0.06435035448521376\n",
      "------------------------------------ epoch 3580 (21474 steps) ------------------------------------\n",
      "Max loss: 0.04956026375293732\n",
      "Min loss: 0.008750754408538342\n",
      "Mean loss: 0.01844718710829814\n",
      "Std loss: 0.01421219250516441\n",
      "Total Loss: 0.11068312264978886\n",
      "------------------------------------ epoch 3581 (21480 steps) ------------------------------------\n",
      "Max loss: 0.05174533277750015\n",
      "Min loss: 0.007611685898154974\n",
      "Mean loss: 0.02056996268220246\n",
      "Std loss: 0.014991119392523186\n",
      "Total Loss: 0.12341977609321475\n",
      "------------------------------------ epoch 3582 (21486 steps) ------------------------------------\n",
      "Max loss: 0.022119125351309776\n",
      "Min loss: 0.008036887273192406\n",
      "Mean loss: 0.014532486442476511\n",
      "Std loss: 0.005149405495518343\n",
      "Total Loss: 0.08719491865485907\n",
      "------------------------------------ epoch 3583 (21492 steps) ------------------------------------\n",
      "Max loss: 0.017868343740701675\n",
      "Min loss: 0.00881103053689003\n",
      "Mean loss: 0.013519434413562218\n",
      "Std loss: 0.003231712030290455\n",
      "Total Loss: 0.08111660648137331\n",
      "------------------------------------ epoch 3584 (21498 steps) ------------------------------------\n",
      "Max loss: 0.03342614322900772\n",
      "Min loss: 0.008787990547716618\n",
      "Mean loss: 0.016359507106244564\n",
      "Std loss: 0.008313266497707078\n",
      "Total Loss: 0.09815704263746738\n",
      "------------------------------------ epoch 3585 (21504 steps) ------------------------------------\n",
      "Max loss: 0.024923652410507202\n",
      "Min loss: 0.010467074811458588\n",
      "Mean loss: 0.013822778904189667\n",
      "Std loss: 0.005104654243787596\n",
      "Total Loss: 0.082936673425138\n",
      "------------------------------------ epoch 3586 (21510 steps) ------------------------------------\n",
      "Max loss: 0.018925853073596954\n",
      "Min loss: 0.009074343368411064\n",
      "Mean loss: 0.012144650177409252\n",
      "Std loss: 0.0033480259385986924\n",
      "Total Loss: 0.07286790106445551\n",
      "------------------------------------ epoch 3587 (21516 steps) ------------------------------------\n",
      "Max loss: 0.025936152786016464\n",
      "Min loss: 0.006632203236222267\n",
      "Mean loss: 0.012403104842330018\n",
      "Std loss: 0.00653145651503638\n",
      "Total Loss: 0.07441862905398011\n",
      "------------------------------------ epoch 3588 (21522 steps) ------------------------------------\n",
      "Max loss: 0.021640611812472343\n",
      "Min loss: 0.0076357340440154076\n",
      "Mean loss: 0.01258546548585097\n",
      "Std loss: 0.004850111496710117\n",
      "Total Loss: 0.07551279291510582\n",
      "------------------------------------ epoch 3589 (21528 steps) ------------------------------------\n",
      "Max loss: 0.01774006523191929\n",
      "Min loss: 0.006499134935438633\n",
      "Mean loss: 0.011382108088582754\n",
      "Std loss: 0.0034774793887904594\n",
      "Total Loss: 0.06829264853149652\n",
      "------------------------------------ epoch 3590 (21534 steps) ------------------------------------\n",
      "Max loss: 0.032305046916007996\n",
      "Min loss: 0.011653000488877296\n",
      "Mean loss: 0.017234451447923977\n",
      "Std loss: 0.007091380558097083\n",
      "Total Loss: 0.10340670868754387\n",
      "------------------------------------ epoch 3591 (21540 steps) ------------------------------------\n",
      "Max loss: 0.025815345346927643\n",
      "Min loss: 0.00767288263887167\n",
      "Mean loss: 0.012722809488574663\n",
      "Std loss: 0.006085026010336819\n",
      "Total Loss: 0.07633685693144798\n",
      "------------------------------------ epoch 3592 (21546 steps) ------------------------------------\n",
      "Max loss: 0.050581902265548706\n",
      "Min loss: 0.009797857142984867\n",
      "Mean loss: 0.021243615852048\n",
      "Std loss: 0.013695379587523343\n",
      "Total Loss: 0.127461695112288\n",
      "------------------------------------ epoch 3593 (21552 steps) ------------------------------------\n",
      "Max loss: 0.023324575275182724\n",
      "Min loss: 0.010182279162108898\n",
      "Mean loss: 0.016345388100792963\n",
      "Std loss: 0.004275119106507051\n",
      "Total Loss: 0.09807232860475779\n",
      "------------------------------------ epoch 3594 (21558 steps) ------------------------------------\n",
      "Max loss: 0.02343083918094635\n",
      "Min loss: 0.010718289762735367\n",
      "Mean loss: 0.016060288064181805\n",
      "Std loss: 0.004345919261618781\n",
      "Total Loss: 0.09636172838509083\n",
      "------------------------------------ epoch 3595 (21564 steps) ------------------------------------\n",
      "Max loss: 0.015992583706974983\n",
      "Min loss: 0.010771340690553188\n",
      "Mean loss: 0.013144776535530886\n",
      "Std loss: 0.0019088270522146184\n",
      "Total Loss: 0.07886865921318531\n",
      "------------------------------------ epoch 3596 (21570 steps) ------------------------------------\n",
      "Max loss: 0.027339518070220947\n",
      "Min loss: 0.008872395381331444\n",
      "Mean loss: 0.014588817954063416\n",
      "Std loss: 0.0063815734386426706\n",
      "Total Loss: 0.0875329077243805\n",
      "------------------------------------ epoch 3597 (21576 steps) ------------------------------------\n",
      "Max loss: 0.02090177685022354\n",
      "Min loss: 0.007665402255952358\n",
      "Mean loss: 0.010800410682956377\n",
      "Std loss: 0.004548833567019095\n",
      "Total Loss: 0.06480246409773827\n",
      "------------------------------------ epoch 3598 (21582 steps) ------------------------------------\n",
      "Max loss: 0.024119241163134575\n",
      "Min loss: 0.007159918546676636\n",
      "Mean loss: 0.013265348194787899\n",
      "Std loss: 0.005475495180409892\n",
      "Total Loss: 0.0795920891687274\n",
      "------------------------------------ epoch 3599 (21588 steps) ------------------------------------\n",
      "Max loss: 0.023114047944545746\n",
      "Min loss: 0.006911435630172491\n",
      "Mean loss: 0.016449976634855073\n",
      "Std loss: 0.00513326470234016\n",
      "Total Loss: 0.09869985980913043\n",
      "------------------------------------ epoch 3600 (21594 steps) ------------------------------------\n",
      "Max loss: 0.018505264073610306\n",
      "Min loss: 0.008002071641385555\n",
      "Mean loss: 0.012685682158917189\n",
      "Std loss: 0.003917790742427367\n",
      "Total Loss: 0.07611409295350313\n",
      "------------------------------------ epoch 3601 (21600 steps) ------------------------------------\n",
      "Max loss: 0.01764393039047718\n",
      "Min loss: 0.009979095309972763\n",
      "Mean loss: 0.013902120136966309\n",
      "Std loss: 0.0028908725836142555\n",
      "Total Loss: 0.08341272082179785\n",
      "saved model at ./weights/model_3601.pth\n",
      "------------------------------------ epoch 3602 (21606 steps) ------------------------------------\n",
      "Max loss: 0.015409356914460659\n",
      "Min loss: 0.007465042173862457\n",
      "Mean loss: 0.01149414200335741\n",
      "Std loss: 0.00257926629058827\n",
      "Total Loss: 0.06896485202014446\n",
      "------------------------------------ epoch 3603 (21612 steps) ------------------------------------\n",
      "Max loss: 0.014106426388025284\n",
      "Min loss: 0.006636916659772396\n",
      "Mean loss: 0.009747523814439774\n",
      "Std loss: 0.0027799888387831477\n",
      "Total Loss: 0.05848514288663864\n",
      "------------------------------------ epoch 3604 (21618 steps) ------------------------------------\n",
      "Max loss: 0.043902769684791565\n",
      "Min loss: 0.006551263388246298\n",
      "Mean loss: 0.015632915853833158\n",
      "Std loss: 0.013173115473130626\n",
      "Total Loss: 0.09379749512299895\n",
      "------------------------------------ epoch 3605 (21624 steps) ------------------------------------\n",
      "Max loss: 0.029084837064146996\n",
      "Min loss: 0.012755529955029488\n",
      "Mean loss: 0.01621533619860808\n",
      "Std loss: 0.005868206328527192\n",
      "Total Loss: 0.09729201719164848\n",
      "------------------------------------ epoch 3606 (21630 steps) ------------------------------------\n",
      "Max loss: 0.013006454333662987\n",
      "Min loss: 0.008953677490353584\n",
      "Mean loss: 0.010535259575893482\n",
      "Std loss: 0.0013459277281315796\n",
      "Total Loss: 0.06321155745536089\n",
      "------------------------------------ epoch 3607 (21636 steps) ------------------------------------\n",
      "Max loss: 0.022702381014823914\n",
      "Min loss: 0.008590240962803364\n",
      "Mean loss: 0.015294751462837061\n",
      "Std loss: 0.005154164106771261\n",
      "Total Loss: 0.09176850877702236\n",
      "------------------------------------ epoch 3608 (21642 steps) ------------------------------------\n",
      "Max loss: 0.014537565410137177\n",
      "Min loss: 0.006838692352175713\n",
      "Mean loss: 0.012259867042303085\n",
      "Std loss: 0.0025505368421266054\n",
      "Total Loss: 0.07355920225381851\n",
      "------------------------------------ epoch 3609 (21648 steps) ------------------------------------\n",
      "Max loss: 0.03999537229537964\n",
      "Min loss: 0.009941305965185165\n",
      "Mean loss: 0.021613558133443195\n",
      "Std loss: 0.012725470697528593\n",
      "Total Loss: 0.12968134880065918\n",
      "------------------------------------ epoch 3610 (21654 steps) ------------------------------------\n",
      "Max loss: 0.025138799101114273\n",
      "Min loss: 0.00950080156326294\n",
      "Mean loss: 0.015625316184014082\n",
      "Std loss: 0.005432537841505837\n",
      "Total Loss: 0.09375189710408449\n",
      "------------------------------------ epoch 3611 (21660 steps) ------------------------------------\n",
      "Max loss: 0.02385999634861946\n",
      "Min loss: 0.0077757202088832855\n",
      "Mean loss: 0.012563559537132582\n",
      "Std loss: 0.0052687163377665635\n",
      "Total Loss: 0.07538135722279549\n",
      "------------------------------------ epoch 3612 (21666 steps) ------------------------------------\n",
      "Max loss: 0.02118077129125595\n",
      "Min loss: 0.008424932137131691\n",
      "Mean loss: 0.01287419597307841\n",
      "Std loss: 0.0040357157146654965\n",
      "Total Loss: 0.07724517583847046\n",
      "------------------------------------ epoch 3613 (21672 steps) ------------------------------------\n",
      "Max loss: 0.02081451565027237\n",
      "Min loss: 0.007341732736676931\n",
      "Mean loss: 0.012619871413335204\n",
      "Std loss: 0.004726711847530987\n",
      "Total Loss: 0.07571922848001122\n",
      "------------------------------------ epoch 3614 (21678 steps) ------------------------------------\n",
      "Max loss: 0.03625206649303436\n",
      "Min loss: 0.01062803715467453\n",
      "Mean loss: 0.01734492074077328\n",
      "Std loss: 0.008659026644136442\n",
      "Total Loss: 0.10406952444463968\n",
      "------------------------------------ epoch 3615 (21684 steps) ------------------------------------\n",
      "Max loss: 0.01341914664953947\n",
      "Min loss: 0.008151693269610405\n",
      "Mean loss: 0.009828200563788414\n",
      "Std loss: 0.001918934148201355\n",
      "Total Loss: 0.058969203382730484\n",
      "------------------------------------ epoch 3616 (21690 steps) ------------------------------------\n",
      "Max loss: 0.02837870828807354\n",
      "Min loss: 0.0076784915290772915\n",
      "Mean loss: 0.014382578354949752\n",
      "Std loss: 0.007575105732149097\n",
      "Total Loss: 0.08629547012969851\n",
      "------------------------------------ epoch 3617 (21696 steps) ------------------------------------\n",
      "Max loss: 0.0193228330463171\n",
      "Min loss: 0.008027257397770882\n",
      "Mean loss: 0.012440308928489685\n",
      "Std loss: 0.0042416263919656284\n",
      "Total Loss: 0.07464185357093811\n",
      "------------------------------------ epoch 3618 (21702 steps) ------------------------------------\n",
      "Max loss: 0.02012316696345806\n",
      "Min loss: 0.010256106033921242\n",
      "Mean loss: 0.014352912548929453\n",
      "Std loss: 0.0033081471141534853\n",
      "Total Loss: 0.08611747529357672\n",
      "------------------------------------ epoch 3619 (21708 steps) ------------------------------------\n",
      "Max loss: 0.025095773860812187\n",
      "Min loss: 0.008472420275211334\n",
      "Mean loss: 0.012943309421340624\n",
      "Std loss: 0.005730650834152927\n",
      "Total Loss: 0.07765985652804375\n",
      "------------------------------------ epoch 3620 (21714 steps) ------------------------------------\n",
      "Max loss: 0.012768765911459923\n",
      "Min loss: 0.006202852353453636\n",
      "Mean loss: 0.008983666154866418\n",
      "Std loss: 0.0025859076400222203\n",
      "Total Loss: 0.053901996929198503\n",
      "------------------------------------ epoch 3621 (21720 steps) ------------------------------------\n",
      "Max loss: 0.02377150021493435\n",
      "Min loss: 0.007096949033439159\n",
      "Mean loss: 0.012897451252986988\n",
      "Std loss: 0.00564568486308747\n",
      "Total Loss: 0.07738470751792192\n",
      "------------------------------------ epoch 3622 (21726 steps) ------------------------------------\n",
      "Max loss: 0.0314565934240818\n",
      "Min loss: 0.008071652613580227\n",
      "Mean loss: 0.01932895400871833\n",
      "Std loss: 0.009917852062053542\n",
      "Total Loss: 0.11597372405230999\n",
      "------------------------------------ epoch 3623 (21732 steps) ------------------------------------\n",
      "Max loss: 0.01663043722510338\n",
      "Min loss: 0.00708045344799757\n",
      "Mean loss: 0.01221424030760924\n",
      "Std loss: 0.0029532799552086115\n",
      "Total Loss: 0.07328544184565544\n",
      "------------------------------------ epoch 3624 (21738 steps) ------------------------------------\n",
      "Max loss: 0.022907156497240067\n",
      "Min loss: 0.006429060362279415\n",
      "Mean loss: 0.012208879925310612\n",
      "Std loss: 0.0052254612816362705\n",
      "Total Loss: 0.07325327955186367\n",
      "------------------------------------ epoch 3625 (21744 steps) ------------------------------------\n",
      "Max loss: 0.014208287000656128\n",
      "Min loss: 0.008413881994783878\n",
      "Mean loss: 0.010344816527018944\n",
      "Std loss: 0.002111224860516531\n",
      "Total Loss: 0.06206889916211367\n",
      "------------------------------------ epoch 3626 (21750 steps) ------------------------------------\n",
      "Max loss: 0.035439081490039825\n",
      "Min loss: 0.008398394100368023\n",
      "Mean loss: 0.015317478061964115\n",
      "Std loss: 0.009219041667333424\n",
      "Total Loss: 0.09190486837178469\n",
      "------------------------------------ epoch 3627 (21756 steps) ------------------------------------\n",
      "Max loss: 0.0778140276670456\n",
      "Min loss: 0.007054167799651623\n",
      "Mean loss: 0.02373383240774274\n",
      "Std loss: 0.024697557656516964\n",
      "Total Loss: 0.14240299444645643\n",
      "------------------------------------ epoch 3628 (21762 steps) ------------------------------------\n",
      "Max loss: 0.02209535799920559\n",
      "Min loss: 0.00955650769174099\n",
      "Mean loss: 0.014214773507167896\n",
      "Std loss: 0.004328067871226977\n",
      "Total Loss: 0.08528864104300737\n",
      "------------------------------------ epoch 3629 (21768 steps) ------------------------------------\n",
      "Max loss: 0.019280418753623962\n",
      "Min loss: 0.01099348347634077\n",
      "Mean loss: 0.01597788215925296\n",
      "Std loss: 0.002910215712929289\n",
      "Total Loss: 0.09586729295551777\n",
      "------------------------------------ epoch 3630 (21774 steps) ------------------------------------\n",
      "Max loss: 0.012578929774463177\n",
      "Min loss: 0.006778310984373093\n",
      "Mean loss: 0.010640886767456928\n",
      "Std loss: 0.0019523417662870662\n",
      "Total Loss: 0.06384532060474157\n",
      "------------------------------------ epoch 3631 (21780 steps) ------------------------------------\n",
      "Max loss: 0.014857362024486065\n",
      "Min loss: 0.006916593760251999\n",
      "Mean loss: 0.00868667894974351\n",
      "Std loss: 0.002780061736156882\n",
      "Total Loss: 0.052120073698461056\n",
      "------------------------------------ epoch 3632 (21786 steps) ------------------------------------\n",
      "Max loss: 0.05174168944358826\n",
      "Min loss: 0.007706552743911743\n",
      "Mean loss: 0.018251055851578712\n",
      "Std loss: 0.015313589499949105\n",
      "Total Loss: 0.10950633510947227\n",
      "------------------------------------ epoch 3633 (21792 steps) ------------------------------------\n",
      "Max loss: 0.025671279057860374\n",
      "Min loss: 0.00766270374879241\n",
      "Mean loss: 0.0135116222469757\n",
      "Std loss: 0.006044260861307409\n",
      "Total Loss: 0.0810697334818542\n",
      "------------------------------------ epoch 3634 (21798 steps) ------------------------------------\n",
      "Max loss: 0.013373097404837608\n",
      "Min loss: 0.009039490483701229\n",
      "Mean loss: 0.010850922825435797\n",
      "Std loss: 0.001675730172013628\n",
      "Total Loss: 0.06510553695261478\n",
      "------------------------------------ epoch 3635 (21804 steps) ------------------------------------\n",
      "Max loss: 0.02771771512925625\n",
      "Min loss: 0.007297299802303314\n",
      "Mean loss: 0.015909495608260233\n",
      "Std loss: 0.006233617852695757\n",
      "Total Loss: 0.0954569736495614\n",
      "------------------------------------ epoch 3636 (21810 steps) ------------------------------------\n",
      "Max loss: 0.01798049360513687\n",
      "Min loss: 0.006766028702259064\n",
      "Mean loss: 0.01408619387075305\n",
      "Std loss: 0.003982084328051781\n",
      "Total Loss: 0.0845171632245183\n",
      "------------------------------------ epoch 3637 (21816 steps) ------------------------------------\n",
      "Max loss: 0.025856081396341324\n",
      "Min loss: 0.00803951732814312\n",
      "Mean loss: 0.016166713709632557\n",
      "Std loss: 0.006732802801783043\n",
      "Total Loss: 0.09700028225779533\n",
      "------------------------------------ epoch 3638 (21822 steps) ------------------------------------\n",
      "Max loss: 0.026741594076156616\n",
      "Min loss: 0.010273909196257591\n",
      "Mean loss: 0.016064047813415527\n",
      "Std loss: 0.006420528684008\n",
      "Total Loss: 0.09638428688049316\n",
      "------------------------------------ epoch 3639 (21828 steps) ------------------------------------\n",
      "Max loss: 0.014527909457683563\n",
      "Min loss: 0.0075263045728206635\n",
      "Mean loss: 0.010489870018015305\n",
      "Std loss: 0.002418041358898502\n",
      "Total Loss: 0.06293922010809183\n",
      "------------------------------------ epoch 3640 (21834 steps) ------------------------------------\n",
      "Max loss: 0.023266538977622986\n",
      "Min loss: 0.0069283172488212585\n",
      "Mean loss: 0.012902776710689068\n",
      "Std loss: 0.005389694317338486\n",
      "Total Loss: 0.07741666026413441\n",
      "------------------------------------ epoch 3641 (21840 steps) ------------------------------------\n",
      "Max loss: 0.014606684446334839\n",
      "Min loss: 0.007330076768994331\n",
      "Mean loss: 0.010182424370820323\n",
      "Std loss: 0.003007154009836325\n",
      "Total Loss: 0.06109454622492194\n",
      "------------------------------------ epoch 3642 (21846 steps) ------------------------------------\n",
      "Max loss: 0.03064054064452648\n",
      "Min loss: 0.006402031984180212\n",
      "Mean loss: 0.014479842114572724\n",
      "Std loss: 0.008004059197116187\n",
      "Total Loss: 0.08687905268743634\n",
      "------------------------------------ epoch 3643 (21852 steps) ------------------------------------\n",
      "Max loss: 0.03367533162236214\n",
      "Min loss: 0.009191530756652355\n",
      "Mean loss: 0.017719516220192116\n",
      "Std loss: 0.008496438398943429\n",
      "Total Loss: 0.10631709732115269\n",
      "------------------------------------ epoch 3644 (21858 steps) ------------------------------------\n",
      "Max loss: 0.025691330432891846\n",
      "Min loss: 0.0061945123597979546\n",
      "Mean loss: 0.01088109596942862\n",
      "Std loss: 0.006679979129324092\n",
      "Total Loss: 0.06528657581657171\n",
      "------------------------------------ epoch 3645 (21864 steps) ------------------------------------\n",
      "Max loss: 0.013087747618556023\n",
      "Min loss: 0.006441078614443541\n",
      "Mean loss: 0.008942179381847382\n",
      "Std loss: 0.0022482435721252808\n",
      "Total Loss: 0.05365307629108429\n",
      "------------------------------------ epoch 3646 (21870 steps) ------------------------------------\n",
      "Max loss: 0.011350652202963829\n",
      "Min loss: 0.007042868994176388\n",
      "Mean loss: 0.009175073200215897\n",
      "Std loss: 0.001682982640870765\n",
      "Total Loss: 0.055050439201295376\n",
      "------------------------------------ epoch 3647 (21876 steps) ------------------------------------\n",
      "Max loss: 0.017918001860380173\n",
      "Min loss: 0.008710969239473343\n",
      "Mean loss: 0.011740253617366156\n",
      "Std loss: 0.0031192050713693214\n",
      "Total Loss: 0.07044152170419693\n",
      "------------------------------------ epoch 3648 (21882 steps) ------------------------------------\n",
      "Max loss: 0.022587042301893234\n",
      "Min loss: 0.00745384581387043\n",
      "Mean loss: 0.01647499467556675\n",
      "Std loss: 0.00567973601298273\n",
      "Total Loss: 0.09884996805340052\n",
      "------------------------------------ epoch 3649 (21888 steps) ------------------------------------\n",
      "Max loss: 0.022436123341321945\n",
      "Min loss: 0.008572518825531006\n",
      "Mean loss: 0.013259104453027248\n",
      "Std loss: 0.004470975637707438\n",
      "Total Loss: 0.07955462671816349\n",
      "------------------------------------ epoch 3650 (21894 steps) ------------------------------------\n",
      "Max loss: 0.05811178311705589\n",
      "Min loss: 0.0072571886703372\n",
      "Mean loss: 0.01919288607314229\n",
      "Std loss: 0.01769234034780953\n",
      "Total Loss: 0.11515731643885374\n",
      "------------------------------------ epoch 3651 (21900 steps) ------------------------------------\n",
      "Max loss: 0.048690374940633774\n",
      "Min loss: 0.009549797512590885\n",
      "Mean loss: 0.0234826587451001\n",
      "Std loss: 0.013343962724372565\n",
      "Total Loss: 0.1408959524706006\n",
      "------------------------------------ epoch 3652 (21906 steps) ------------------------------------\n",
      "Max loss: 0.016217423602938652\n",
      "Min loss: 0.0073965732008218765\n",
      "Mean loss: 0.010872644372284412\n",
      "Std loss: 0.0032746115770164526\n",
      "Total Loss: 0.06523586623370647\n",
      "------------------------------------ epoch 3653 (21912 steps) ------------------------------------\n",
      "Max loss: 0.021994508802890778\n",
      "Min loss: 0.007842829450964928\n",
      "Mean loss: 0.011082136382659277\n",
      "Std loss: 0.005028792227878891\n",
      "Total Loss: 0.06649281829595566\n",
      "------------------------------------ epoch 3654 (21918 steps) ------------------------------------\n",
      "Max loss: 0.02815730683505535\n",
      "Min loss: 0.007211224641650915\n",
      "Mean loss: 0.015584855573251843\n",
      "Std loss: 0.008443480069035302\n",
      "Total Loss: 0.09350913343951106\n",
      "------------------------------------ epoch 3655 (21924 steps) ------------------------------------\n",
      "Max loss: 0.03832380101084709\n",
      "Min loss: 0.008918150328099728\n",
      "Mean loss: 0.01942048796142141\n",
      "Std loss: 0.009711673881637921\n",
      "Total Loss: 0.11652292776852846\n",
      "------------------------------------ epoch 3656 (21930 steps) ------------------------------------\n",
      "Max loss: 0.031166162341833115\n",
      "Min loss: 0.00690848333761096\n",
      "Mean loss: 0.013841353279228011\n",
      "Std loss: 0.00799932258669335\n",
      "Total Loss: 0.08304811967536807\n",
      "------------------------------------ epoch 3657 (21936 steps) ------------------------------------\n",
      "Max loss: 0.027724049985408783\n",
      "Min loss: 0.009617490693926811\n",
      "Mean loss: 0.015567342595507702\n",
      "Std loss: 0.00647631509480917\n",
      "Total Loss: 0.09340405557304621\n",
      "------------------------------------ epoch 3658 (21942 steps) ------------------------------------\n",
      "Max loss: 0.035056229680776596\n",
      "Min loss: 0.010138541460037231\n",
      "Mean loss: 0.019274770592649777\n",
      "Std loss: 0.008246932948388066\n",
      "Total Loss: 0.11564862355589867\n",
      "------------------------------------ epoch 3659 (21948 steps) ------------------------------------\n",
      "Max loss: 0.04127410054206848\n",
      "Min loss: 0.006518205162137747\n",
      "Mean loss: 0.015894425644849736\n",
      "Std loss: 0.012118846877000872\n",
      "Total Loss: 0.09536655386909842\n",
      "------------------------------------ epoch 3660 (21954 steps) ------------------------------------\n",
      "Max loss: 0.031650640070438385\n",
      "Min loss: 0.010357867926359177\n",
      "Mean loss: 0.01901086637129386\n",
      "Std loss: 0.00836231251771195\n",
      "Total Loss: 0.11406519822776318\n",
      "------------------------------------ epoch 3661 (21960 steps) ------------------------------------\n",
      "Max loss: 0.03498425334692001\n",
      "Min loss: 0.013192689046263695\n",
      "Mean loss: 0.01967673283070326\n",
      "Std loss: 0.007113640798443027\n",
      "Total Loss: 0.11806039698421955\n",
      "------------------------------------ epoch 3662 (21966 steps) ------------------------------------\n",
      "Max loss: 0.03910742700099945\n",
      "Min loss: 0.008469190448522568\n",
      "Mean loss: 0.020257522041598957\n",
      "Std loss: 0.00971036634378188\n",
      "Total Loss: 0.12154513224959373\n",
      "------------------------------------ epoch 3663 (21972 steps) ------------------------------------\n",
      "Max loss: 0.03426944464445114\n",
      "Min loss: 0.0074003166519105434\n",
      "Mean loss: 0.01753472932614386\n",
      "Std loss: 0.011281230413586935\n",
      "Total Loss: 0.10520837595686316\n",
      "------------------------------------ epoch 3664 (21978 steps) ------------------------------------\n",
      "Max loss: 0.0538412481546402\n",
      "Min loss: 0.008221430703997612\n",
      "Mean loss: 0.020193688261012237\n",
      "Std loss: 0.015529382282433526\n",
      "Total Loss: 0.12116212956607342\n",
      "------------------------------------ epoch 3665 (21984 steps) ------------------------------------\n",
      "Max loss: 0.03909323737025261\n",
      "Min loss: 0.010654985904693604\n",
      "Mean loss: 0.02020828907067577\n",
      "Std loss: 0.009146957253190398\n",
      "Total Loss: 0.12124973442405462\n",
      "------------------------------------ epoch 3666 (21990 steps) ------------------------------------\n",
      "Max loss: 0.05195992812514305\n",
      "Min loss: 0.008947957307100296\n",
      "Mean loss: 0.01996921996275584\n",
      "Std loss: 0.014991939737446608\n",
      "Total Loss: 0.11981531977653503\n",
      "------------------------------------ epoch 3667 (21996 steps) ------------------------------------\n",
      "Max loss: 0.028933625668287277\n",
      "Min loss: 0.0112751554697752\n",
      "Mean loss: 0.019977827556431293\n",
      "Std loss: 0.006500818289230666\n",
      "Total Loss: 0.11986696533858776\n",
      "------------------------------------ epoch 3668 (22002 steps) ------------------------------------\n",
      "Max loss: 0.024057289585471153\n",
      "Min loss: 0.009168622083961964\n",
      "Mean loss: 0.012636274099349976\n",
      "Std loss: 0.005185394638590076\n",
      "Total Loss: 0.07581764459609985\n",
      "------------------------------------ epoch 3669 (22008 steps) ------------------------------------\n",
      "Max loss: 0.023279305547475815\n",
      "Min loss: 0.008836545050144196\n",
      "Mean loss: 0.01737510583673914\n",
      "Std loss: 0.004935515364738374\n",
      "Total Loss: 0.10425063502043486\n",
      "------------------------------------ epoch 3670 (22014 steps) ------------------------------------\n",
      "Max loss: 0.016993926838040352\n",
      "Min loss: 0.008408053778111935\n",
      "Mean loss: 0.01025013749798139\n",
      "Std loss: 0.00303352499842912\n",
      "Total Loss: 0.061500824987888336\n",
      "------------------------------------ epoch 3671 (22020 steps) ------------------------------------\n",
      "Max loss: 0.012824243865907192\n",
      "Min loss: 0.007458038628101349\n",
      "Mean loss: 0.009580246328065792\n",
      "Std loss: 0.001882447992394497\n",
      "Total Loss: 0.057481477968394756\n",
      "------------------------------------ epoch 3672 (22026 steps) ------------------------------------\n",
      "Max loss: 0.015312637202441692\n",
      "Min loss: 0.007737790234386921\n",
      "Mean loss: 0.010447458829730749\n",
      "Std loss: 0.0024763712135160306\n",
      "Total Loss: 0.0626847529783845\n",
      "------------------------------------ epoch 3673 (22032 steps) ------------------------------------\n",
      "Max loss: 0.06319992244243622\n",
      "Min loss: 0.0066346521489322186\n",
      "Mean loss: 0.02766766562126577\n",
      "Std loss: 0.023194510066350753\n",
      "Total Loss: 0.16600599372759461\n",
      "------------------------------------ epoch 3674 (22038 steps) ------------------------------------\n",
      "Max loss: 0.031091799959540367\n",
      "Min loss: 0.007494089659303427\n",
      "Mean loss: 0.014907802687957883\n",
      "Std loss: 0.007924855744154144\n",
      "Total Loss: 0.0894468161277473\n",
      "------------------------------------ epoch 3675 (22044 steps) ------------------------------------\n",
      "Max loss: 0.016240285709500313\n",
      "Min loss: 0.007737141102552414\n",
      "Mean loss: 0.010811703279614449\n",
      "Std loss: 0.0029386450332637578\n",
      "Total Loss: 0.06487021967768669\n",
      "------------------------------------ epoch 3676 (22050 steps) ------------------------------------\n",
      "Max loss: 0.02307584136724472\n",
      "Min loss: 0.006231750827282667\n",
      "Mean loss: 0.015169859786207477\n",
      "Std loss: 0.005073094371272719\n",
      "Total Loss: 0.09101915871724486\n",
      "------------------------------------ epoch 3677 (22056 steps) ------------------------------------\n",
      "Max loss: 0.010859077796339989\n",
      "Min loss: 0.007899296469986439\n",
      "Mean loss: 0.00943201050783197\n",
      "Std loss: 0.0010387918494096167\n",
      "Total Loss: 0.056592063046991825\n",
      "------------------------------------ epoch 3678 (22062 steps) ------------------------------------\n",
      "Max loss: 0.030592449009418488\n",
      "Min loss: 0.006978766527026892\n",
      "Mean loss: 0.01698493359920879\n",
      "Std loss: 0.008629014911938841\n",
      "Total Loss: 0.10190960159525275\n",
      "------------------------------------ epoch 3679 (22068 steps) ------------------------------------\n",
      "Max loss: 0.015354945324361324\n",
      "Min loss: 0.007043833844363689\n",
      "Mean loss: 0.012136803474277258\n",
      "Std loss: 0.0033470553501798493\n",
      "Total Loss: 0.07282082084566355\n",
      "------------------------------------ epoch 3680 (22074 steps) ------------------------------------\n",
      "Max loss: 0.019565705209970474\n",
      "Min loss: 0.006549240555614233\n",
      "Mean loss: 0.009988392625624934\n",
      "Std loss: 0.004342442926752346\n",
      "Total Loss: 0.05993035575374961\n",
      "------------------------------------ epoch 3681 (22080 steps) ------------------------------------\n",
      "Max loss: 0.031000301241874695\n",
      "Min loss: 0.0062175327911973\n",
      "Mean loss: 0.01563413456703226\n",
      "Std loss: 0.009584408023604779\n",
      "Total Loss: 0.09380480740219355\n",
      "------------------------------------ epoch 3682 (22086 steps) ------------------------------------\n",
      "Max loss: 0.01620480604469776\n",
      "Min loss: 0.00668783625587821\n",
      "Mean loss: 0.01103899891798695\n",
      "Std loss: 0.004005323979483979\n",
      "Total Loss: 0.0662339935079217\n",
      "------------------------------------ epoch 3683 (22092 steps) ------------------------------------\n",
      "Max loss: 0.01704309694468975\n",
      "Min loss: 0.007074571214616299\n",
      "Mean loss: 0.01138298554966847\n",
      "Std loss: 0.003327126972686432\n",
      "Total Loss: 0.06829791329801083\n",
      "------------------------------------ epoch 3684 (22098 steps) ------------------------------------\n",
      "Max loss: 0.01487704273313284\n",
      "Min loss: 0.007228405214846134\n",
      "Mean loss: 0.011317223000029722\n",
      "Std loss: 0.0027309989227731167\n",
      "Total Loss: 0.06790333800017834\n",
      "------------------------------------ epoch 3685 (22104 steps) ------------------------------------\n",
      "Max loss: 0.02264849655330181\n",
      "Min loss: 0.006801140960305929\n",
      "Mean loss: 0.011104993289336562\n",
      "Std loss: 0.005584192769990763\n",
      "Total Loss: 0.06662995973601937\n",
      "------------------------------------ epoch 3686 (22110 steps) ------------------------------------\n",
      "Max loss: 0.017285719513893127\n",
      "Min loss: 0.00580549705773592\n",
      "Mean loss: 0.011847027577459812\n",
      "Std loss: 0.00456267958623612\n",
      "Total Loss: 0.07108216546475887\n",
      "------------------------------------ epoch 3687 (22116 steps) ------------------------------------\n",
      "Max loss: 0.01638992503285408\n",
      "Min loss: 0.006876492872834206\n",
      "Mean loss: 0.010949364320064584\n",
      "Std loss: 0.003340204925742676\n",
      "Total Loss: 0.0656961859203875\n",
      "------------------------------------ epoch 3688 (22122 steps) ------------------------------------\n",
      "Max loss: 0.03287980705499649\n",
      "Min loss: 0.008399762213230133\n",
      "Mean loss: 0.019542223463455837\n",
      "Std loss: 0.009321292895162843\n",
      "Total Loss: 0.11725334078073502\n",
      "------------------------------------ epoch 3689 (22128 steps) ------------------------------------\n",
      "Max loss: 0.015098134987056255\n",
      "Min loss: 0.006413928233087063\n",
      "Mean loss: 0.010032732893402377\n",
      "Std loss: 0.0032012033265897545\n",
      "Total Loss: 0.06019639736041427\n",
      "------------------------------------ epoch 3690 (22134 steps) ------------------------------------\n",
      "Max loss: 0.03290252387523651\n",
      "Min loss: 0.00839188601821661\n",
      "Mean loss: 0.016885610297322273\n",
      "Std loss: 0.00933330752817142\n",
      "Total Loss: 0.10131366178393364\n",
      "------------------------------------ epoch 3691 (22140 steps) ------------------------------------\n",
      "Max loss: 0.03723134845495224\n",
      "Min loss: 0.011244530789554119\n",
      "Mean loss: 0.021734524207810562\n",
      "Std loss: 0.009256348377255854\n",
      "Total Loss: 0.13040714524686337\n",
      "------------------------------------ epoch 3692 (22146 steps) ------------------------------------\n",
      "Max loss: 0.022347334772348404\n",
      "Min loss: 0.008554314263164997\n",
      "Mean loss: 0.013888439629226923\n",
      "Std loss: 0.004724265716183139\n",
      "Total Loss: 0.08333063777536154\n",
      "------------------------------------ epoch 3693 (22152 steps) ------------------------------------\n",
      "Max loss: 0.03035079315304756\n",
      "Min loss: 0.006864885799586773\n",
      "Mean loss: 0.012646315774569908\n",
      "Std loss: 0.00806623239432829\n",
      "Total Loss: 0.07587789464741945\n",
      "------------------------------------ epoch 3694 (22158 steps) ------------------------------------\n",
      "Max loss: 0.01643434725701809\n",
      "Min loss: 0.007153834216296673\n",
      "Mean loss: 0.01279782496082286\n",
      "Std loss: 0.0039284860176718945\n",
      "Total Loss: 0.07678694976493716\n",
      "------------------------------------ epoch 3695 (22164 steps) ------------------------------------\n",
      "Max loss: 0.04309540614485741\n",
      "Min loss: 0.007040848024189472\n",
      "Mean loss: 0.014830683435623845\n",
      "Std loss: 0.012727912780387226\n",
      "Total Loss: 0.08898410061374307\n",
      "------------------------------------ epoch 3696 (22170 steps) ------------------------------------\n",
      "Max loss: 0.021615352481603622\n",
      "Min loss: 0.007724408060312271\n",
      "Mean loss: 0.013448389867941538\n",
      "Std loss: 0.005261471995393498\n",
      "Total Loss: 0.08069033920764923\n",
      "------------------------------------ epoch 3697 (22176 steps) ------------------------------------\n",
      "Max loss: 0.026897726580500603\n",
      "Min loss: 0.009028686210513115\n",
      "Mean loss: 0.014373228419572115\n",
      "Std loss: 0.006268260568647288\n",
      "Total Loss: 0.08623937051743269\n",
      "------------------------------------ epoch 3698 (22182 steps) ------------------------------------\n",
      "Max loss: 0.016820186749100685\n",
      "Min loss: 0.007174074649810791\n",
      "Mean loss: 0.011556014263381561\n",
      "Std loss: 0.0037468802346883948\n",
      "Total Loss: 0.06933608558028936\n",
      "------------------------------------ epoch 3699 (22188 steps) ------------------------------------\n",
      "Max loss: 0.019019251689314842\n",
      "Min loss: 0.006440352648496628\n",
      "Mean loss: 0.012128733719388643\n",
      "Std loss: 0.004148940649180582\n",
      "Total Loss: 0.07277240231633186\n",
      "------------------------------------ epoch 3700 (22194 steps) ------------------------------------\n",
      "Max loss: 0.0355122834444046\n",
      "Min loss: 0.006973144598305225\n",
      "Mean loss: 0.020787083078175783\n",
      "Std loss: 0.00922368788844866\n",
      "Total Loss: 0.1247224984690547\n",
      "------------------------------------ epoch 3701 (22200 steps) ------------------------------------\n",
      "Max loss: 0.018245872110128403\n",
      "Min loss: 0.006412630435079336\n",
      "Mean loss: 0.01191862067207694\n",
      "Std loss: 0.0045655915374980165\n",
      "Total Loss: 0.07151172403246164\n",
      "saved model at ./weights/model_3701.pth\n",
      "------------------------------------ epoch 3702 (22206 steps) ------------------------------------\n",
      "Max loss: 0.035410091280937195\n",
      "Min loss: 0.007248771842569113\n",
      "Mean loss: 0.017325250975166757\n",
      "Std loss: 0.009773809096735746\n",
      "Total Loss: 0.10395150585100055\n",
      "------------------------------------ epoch 3703 (22212 steps) ------------------------------------\n",
      "Max loss: 0.030415978282690048\n",
      "Min loss: 0.0080118328332901\n",
      "Mean loss: 0.01488293893635273\n",
      "Std loss: 0.008002658065778661\n",
      "Total Loss: 0.08929763361811638\n",
      "------------------------------------ epoch 3704 (22218 steps) ------------------------------------\n",
      "Max loss: 0.027805805206298828\n",
      "Min loss: 0.008810054510831833\n",
      "Mean loss: 0.018276384721199673\n",
      "Std loss: 0.008493524310230446\n",
      "Total Loss: 0.10965830832719803\n",
      "------------------------------------ epoch 3705 (22224 steps) ------------------------------------\n",
      "Max loss: 0.019994020462036133\n",
      "Min loss: 0.008969010785222054\n",
      "Mean loss: 0.012669136437276999\n",
      "Std loss: 0.0036672439951275526\n",
      "Total Loss: 0.076014818623662\n",
      "------------------------------------ epoch 3706 (22230 steps) ------------------------------------\n",
      "Max loss: 0.035906992852687836\n",
      "Min loss: 0.008639125153422356\n",
      "Mean loss: 0.019068564133097727\n",
      "Std loss: 0.011572415896409071\n",
      "Total Loss: 0.11441138479858637\n",
      "------------------------------------ epoch 3707 (22236 steps) ------------------------------------\n",
      "Max loss: 0.019909022375941277\n",
      "Min loss: 0.008112884126603603\n",
      "Mean loss: 0.012526439968496561\n",
      "Std loss: 0.004489356948567915\n",
      "Total Loss: 0.07515863981097937\n",
      "------------------------------------ epoch 3708 (22242 steps) ------------------------------------\n",
      "Max loss: 0.02706747315824032\n",
      "Min loss: 0.006793903186917305\n",
      "Mean loss: 0.01229858227695028\n",
      "Std loss: 0.007435971220421023\n",
      "Total Loss: 0.07379149366170168\n",
      "------------------------------------ epoch 3709 (22248 steps) ------------------------------------\n",
      "Max loss: 0.020408280193805695\n",
      "Min loss: 0.007032499648630619\n",
      "Mean loss: 0.011758971959352493\n",
      "Std loss: 0.004149694442547819\n",
      "Total Loss: 0.07055383175611496\n",
      "------------------------------------ epoch 3710 (22254 steps) ------------------------------------\n",
      "Max loss: 0.0199459008872509\n",
      "Min loss: 0.007772891782224178\n",
      "Mean loss: 0.012042090917627016\n",
      "Std loss: 0.004733730781152515\n",
      "Total Loss: 0.0722525455057621\n",
      "------------------------------------ epoch 3711 (22260 steps) ------------------------------------\n",
      "Max loss: 0.014778591692447662\n",
      "Min loss: 0.008573606610298157\n",
      "Mean loss: 0.011504799903680881\n",
      "Std loss: 0.0023533463918392135\n",
      "Total Loss: 0.06902879942208529\n",
      "------------------------------------ epoch 3712 (22266 steps) ------------------------------------\n",
      "Max loss: 0.019783850759267807\n",
      "Min loss: 0.008033269084990025\n",
      "Mean loss: 0.012988049692163864\n",
      "Std loss: 0.004895285310114646\n",
      "Total Loss: 0.07792829815298319\n",
      "------------------------------------ epoch 3713 (22272 steps) ------------------------------------\n",
      "Max loss: 0.026503290981054306\n",
      "Min loss: 0.008223496377468109\n",
      "Mean loss: 0.014479038305580616\n",
      "Std loss: 0.006474808878171468\n",
      "Total Loss: 0.0868742298334837\n",
      "------------------------------------ epoch 3714 (22278 steps) ------------------------------------\n",
      "Max loss: 0.012969288975000381\n",
      "Min loss: 0.006552703212946653\n",
      "Mean loss: 0.00923873158171773\n",
      "Std loss: 0.002241089569857775\n",
      "Total Loss: 0.05543238949030638\n",
      "------------------------------------ epoch 3715 (22284 steps) ------------------------------------\n",
      "Max loss: 0.018107792362570763\n",
      "Min loss: 0.006571081466972828\n",
      "Mean loss: 0.012040977521489063\n",
      "Std loss: 0.00347987135962431\n",
      "Total Loss: 0.07224586512893438\n",
      "------------------------------------ epoch 3716 (22290 steps) ------------------------------------\n",
      "Max loss: 0.024940356612205505\n",
      "Min loss: 0.007256015669554472\n",
      "Mean loss: 0.014229466672986746\n",
      "Std loss: 0.007100194626504243\n",
      "Total Loss: 0.08537680003792048\n",
      "------------------------------------ epoch 3717 (22296 steps) ------------------------------------\n",
      "Max loss: 0.01497077476233244\n",
      "Min loss: 0.007018678821623325\n",
      "Mean loss: 0.010400131267185012\n",
      "Std loss: 0.003226708904986313\n",
      "Total Loss: 0.062400787603110075\n",
      "------------------------------------ epoch 3718 (22302 steps) ------------------------------------\n",
      "Max loss: 0.051494061946868896\n",
      "Min loss: 0.007664795964956284\n",
      "Mean loss: 0.018585088352362316\n",
      "Std loss: 0.01587577192827283\n",
      "Total Loss: 0.11151053011417389\n",
      "------------------------------------ epoch 3719 (22308 steps) ------------------------------------\n",
      "Max loss: 0.019333865493535995\n",
      "Min loss: 0.007446855306625366\n",
      "Mean loss: 0.013499088895817598\n",
      "Std loss: 0.0038694009633672903\n",
      "Total Loss: 0.08099453337490559\n",
      "------------------------------------ epoch 3720 (22314 steps) ------------------------------------\n",
      "Max loss: 0.027120089158415794\n",
      "Min loss: 0.006251160055398941\n",
      "Mean loss: 0.012776238533357779\n",
      "Std loss: 0.006719149473312199\n",
      "Total Loss: 0.07665743120014668\n",
      "------------------------------------ epoch 3721 (22320 steps) ------------------------------------\n",
      "Max loss: 0.013255867175757885\n",
      "Min loss: 0.006718473043292761\n",
      "Mean loss: 0.010231819857532779\n",
      "Std loss: 0.0023118354556227066\n",
      "Total Loss: 0.061390919145196676\n",
      "------------------------------------ epoch 3722 (22326 steps) ------------------------------------\n",
      "Max loss: 0.02060188353061676\n",
      "Min loss: 0.005922581069171429\n",
      "Mean loss: 0.012867212217922011\n",
      "Std loss: 0.006592919757395162\n",
      "Total Loss: 0.07720327330753207\n",
      "------------------------------------ epoch 3723 (22332 steps) ------------------------------------\n",
      "Max loss: 0.020534716546535492\n",
      "Min loss: 0.006476889364421368\n",
      "Mean loss: 0.01039641055588921\n",
      "Std loss: 0.004884179947714168\n",
      "Total Loss: 0.062378463335335255\n",
      "------------------------------------ epoch 3724 (22338 steps) ------------------------------------\n",
      "Max loss: 0.03045826405286789\n",
      "Min loss: 0.006103258114308119\n",
      "Mean loss: 0.013653220531220237\n",
      "Std loss: 0.008686439213032235\n",
      "Total Loss: 0.08191932318732142\n",
      "------------------------------------ epoch 3725 (22344 steps) ------------------------------------\n",
      "Max loss: 0.022419661283493042\n",
      "Min loss: 0.006185994483530521\n",
      "Mean loss: 0.0114323355567952\n",
      "Std loss: 0.0051188806755930495\n",
      "Total Loss: 0.0685940133407712\n",
      "------------------------------------ epoch 3726 (22350 steps) ------------------------------------\n",
      "Max loss: 0.02054639160633087\n",
      "Min loss: 0.006674367934465408\n",
      "Mean loss: 0.011317001345256964\n",
      "Std loss: 0.0044284340081712234\n",
      "Total Loss: 0.06790200807154179\n",
      "------------------------------------ epoch 3727 (22356 steps) ------------------------------------\n",
      "Max loss: 0.024644017219543457\n",
      "Min loss: 0.0098729208111763\n",
      "Mean loss: 0.01306439113492767\n",
      "Std loss: 0.00523840612880392\n",
      "Total Loss: 0.07838634680956602\n",
      "------------------------------------ epoch 3728 (22362 steps) ------------------------------------\n",
      "Max loss: 0.021072743460536003\n",
      "Min loss: 0.007525294553488493\n",
      "Mean loss: 0.013308048869172731\n",
      "Std loss: 0.006035044925824194\n",
      "Total Loss: 0.07984829321503639\n",
      "------------------------------------ epoch 3729 (22368 steps) ------------------------------------\n",
      "Max loss: 0.013033956289291382\n",
      "Min loss: 0.00917736254632473\n",
      "Mean loss: 0.011072353149453798\n",
      "Std loss: 0.0013804134425142252\n",
      "Total Loss: 0.0664341188967228\n",
      "------------------------------------ epoch 3730 (22374 steps) ------------------------------------\n",
      "Max loss: 0.01564643532037735\n",
      "Min loss: 0.006253443192690611\n",
      "Mean loss: 0.01145817463596662\n",
      "Std loss: 0.0035790526453926205\n",
      "Total Loss: 0.06874904781579971\n",
      "------------------------------------ epoch 3731 (22380 steps) ------------------------------------\n",
      "Max loss: 0.032747067511081696\n",
      "Min loss: 0.009432531893253326\n",
      "Mean loss: 0.019194329157471657\n",
      "Std loss: 0.009964831055955104\n",
      "Total Loss: 0.11516597494482994\n",
      "------------------------------------ epoch 3732 (22386 steps) ------------------------------------\n",
      "Max loss: 0.02306785248219967\n",
      "Min loss: 0.007180368527770042\n",
      "Mean loss: 0.014994975024213394\n",
      "Std loss: 0.005656692048670124\n",
      "Total Loss: 0.08996985014528036\n",
      "------------------------------------ epoch 3733 (22392 steps) ------------------------------------\n",
      "Max loss: 0.040911927819252014\n",
      "Min loss: 0.009248612448573112\n",
      "Mean loss: 0.01811034760127465\n",
      "Std loss: 0.010822866776972138\n",
      "Total Loss: 0.1086620856076479\n",
      "------------------------------------ epoch 3734 (22398 steps) ------------------------------------\n",
      "Max loss: 0.019230183213949203\n",
      "Min loss: 0.007833164185285568\n",
      "Mean loss: 0.012109956393639246\n",
      "Std loss: 0.003903616876278214\n",
      "Total Loss: 0.07265973836183548\n",
      "------------------------------------ epoch 3735 (22404 steps) ------------------------------------\n",
      "Max loss: 0.015522058121860027\n",
      "Min loss: 0.007732927333563566\n",
      "Mean loss: 0.011540794977918267\n",
      "Std loss: 0.002904362925361902\n",
      "Total Loss: 0.0692447698675096\n",
      "------------------------------------ epoch 3736 (22410 steps) ------------------------------------\n",
      "Max loss: 0.024156447499990463\n",
      "Min loss: 0.009366855025291443\n",
      "Mean loss: 0.014813451562076807\n",
      "Std loss: 0.005276569203343417\n",
      "Total Loss: 0.08888070937246084\n",
      "------------------------------------ epoch 3737 (22416 steps) ------------------------------------\n",
      "Max loss: 0.011362619698047638\n",
      "Min loss: 0.006782229989767075\n",
      "Mean loss: 0.009692575316876173\n",
      "Std loss: 0.0018495736828115811\n",
      "Total Loss: 0.05815545190125704\n",
      "------------------------------------ epoch 3738 (22422 steps) ------------------------------------\n",
      "Max loss: 0.03448272496461868\n",
      "Min loss: 0.007340929936617613\n",
      "Mean loss: 0.015346635831519961\n",
      "Std loss: 0.009038240949066545\n",
      "Total Loss: 0.09207981498911977\n",
      "------------------------------------ epoch 3739 (22428 steps) ------------------------------------\n",
      "Max loss: 0.015502914786338806\n",
      "Min loss: 0.008602814748883247\n",
      "Mean loss: 0.011251968952516714\n",
      "Std loss: 0.0023555235192957457\n",
      "Total Loss: 0.06751181371510029\n",
      "------------------------------------ epoch 3740 (22434 steps) ------------------------------------\n",
      "Max loss: 0.016288280487060547\n",
      "Min loss: 0.006507384590804577\n",
      "Mean loss: 0.01142856323470672\n",
      "Std loss: 0.0037931391863476036\n",
      "Total Loss: 0.06857137940824032\n",
      "------------------------------------ epoch 3741 (22440 steps) ------------------------------------\n",
      "Max loss: 0.014986909925937653\n",
      "Min loss: 0.006346563808619976\n",
      "Mean loss: 0.010224713943898678\n",
      "Std loss: 0.002871558222966768\n",
      "Total Loss: 0.06134828366339207\n",
      "------------------------------------ epoch 3742 (22446 steps) ------------------------------------\n",
      "Max loss: 0.014396300539374352\n",
      "Min loss: 0.006223071366548538\n",
      "Mean loss: 0.008278859235967198\n",
      "Std loss: 0.0027867622504937277\n",
      "Total Loss: 0.049673155415803194\n",
      "------------------------------------ epoch 3743 (22452 steps) ------------------------------------\n",
      "Max loss: 0.02220095321536064\n",
      "Min loss: 0.006988996174186468\n",
      "Mean loss: 0.011144405696541071\n",
      "Std loss: 0.005365024799932174\n",
      "Total Loss: 0.06686643417924643\n",
      "------------------------------------ epoch 3744 (22458 steps) ------------------------------------\n",
      "Max loss: 0.02867024950683117\n",
      "Min loss: 0.008782954886555672\n",
      "Mean loss: 0.015932307268182438\n",
      "Std loss: 0.008303612267169663\n",
      "Total Loss: 0.09559384360909462\n",
      "------------------------------------ epoch 3745 (22464 steps) ------------------------------------\n",
      "Max loss: 0.018031282350420952\n",
      "Min loss: 0.007180043961852789\n",
      "Mean loss: 0.010944475963090857\n",
      "Std loss: 0.003923545517310917\n",
      "Total Loss: 0.06566685577854514\n",
      "------------------------------------ epoch 3746 (22470 steps) ------------------------------------\n",
      "Max loss: 0.018211711198091507\n",
      "Min loss: 0.008535696193575859\n",
      "Mean loss: 0.01133683742955327\n",
      "Std loss: 0.00346498263653239\n",
      "Total Loss: 0.06802102457731962\n",
      "------------------------------------ epoch 3747 (22476 steps) ------------------------------------\n",
      "Max loss: 0.029752545058727264\n",
      "Min loss: 0.006438467651605606\n",
      "Mean loss: 0.011434050897757212\n",
      "Std loss: 0.00825438234845128\n",
      "Total Loss: 0.06860430538654327\n",
      "------------------------------------ epoch 3748 (22482 steps) ------------------------------------\n",
      "Max loss: 0.016808051615953445\n",
      "Min loss: 0.007587485946714878\n",
      "Mean loss: 0.011931702649841705\n",
      "Std loss: 0.0029282035135900475\n",
      "Total Loss: 0.07159021589905024\n",
      "------------------------------------ epoch 3749 (22488 steps) ------------------------------------\n",
      "Max loss: 0.015855927020311356\n",
      "Min loss: 0.007365145720541477\n",
      "Mean loss: 0.011401342072834572\n",
      "Std loss: 0.002684528338591295\n",
      "Total Loss: 0.06840805243700743\n",
      "------------------------------------ epoch 3750 (22494 steps) ------------------------------------\n",
      "Max loss: 0.06584954261779785\n",
      "Min loss: 0.009409318678081036\n",
      "Mean loss: 0.02271095508088668\n",
      "Std loss: 0.019804717406961533\n",
      "Total Loss: 0.1362657304853201\n",
      "------------------------------------ epoch 3751 (22500 steps) ------------------------------------\n",
      "Max loss: 0.022542810067534447\n",
      "Min loss: 0.012142529711127281\n",
      "Mean loss: 0.01604679459705949\n",
      "Std loss: 0.0035228700718941894\n",
      "Total Loss: 0.09628076758235693\n",
      "------------------------------------ epoch 3752 (22506 steps) ------------------------------------\n",
      "Max loss: 0.024353157728910446\n",
      "Min loss: 0.00655474653467536\n",
      "Mean loss: 0.011882104212418199\n",
      "Std loss: 0.005832678775981994\n",
      "Total Loss: 0.07129262527450919\n",
      "------------------------------------ epoch 3753 (22512 steps) ------------------------------------\n",
      "Max loss: 0.05978534370660782\n",
      "Min loss: 0.009918554686009884\n",
      "Mean loss: 0.024044345288227003\n",
      "Std loss: 0.018864033810284273\n",
      "Total Loss: 0.144266071729362\n",
      "------------------------------------ epoch 3754 (22518 steps) ------------------------------------\n",
      "Max loss: 0.02931039035320282\n",
      "Min loss: 0.009585794061422348\n",
      "Mean loss: 0.015374084934592247\n",
      "Std loss: 0.007004502160926241\n",
      "Total Loss: 0.09224450960755348\n",
      "------------------------------------ epoch 3755 (22524 steps) ------------------------------------\n",
      "Max loss: 0.030279967933893204\n",
      "Min loss: 0.01045229285955429\n",
      "Mean loss: 0.015037133047978083\n",
      "Std loss: 0.006972368941469816\n",
      "Total Loss: 0.0902227982878685\n",
      "------------------------------------ epoch 3756 (22530 steps) ------------------------------------\n",
      "Max loss: 0.02735294960439205\n",
      "Min loss: 0.008658718317747116\n",
      "Mean loss: 0.015551526875545582\n",
      "Std loss: 0.006484376456278603\n",
      "Total Loss: 0.09330916125327349\n",
      "------------------------------------ epoch 3757 (22536 steps) ------------------------------------\n",
      "Max loss: 0.03168576955795288\n",
      "Min loss: 0.007153339684009552\n",
      "Mean loss: 0.019452717310438555\n",
      "Std loss: 0.009090154611352782\n",
      "Total Loss: 0.11671630386263132\n",
      "------------------------------------ epoch 3758 (22542 steps) ------------------------------------\n",
      "Max loss: 0.04216538369655609\n",
      "Min loss: 0.0095223318785429\n",
      "Mean loss: 0.01803593011572957\n",
      "Std loss: 0.011368028831500013\n",
      "Total Loss: 0.10821558069437742\n",
      "------------------------------------ epoch 3759 (22548 steps) ------------------------------------\n",
      "Max loss: 0.0458819679915905\n",
      "Min loss: 0.008974825032055378\n",
      "Mean loss: 0.022327238383392494\n",
      "Std loss: 0.012260840676626375\n",
      "Total Loss: 0.13396343030035496\n",
      "------------------------------------ epoch 3760 (22554 steps) ------------------------------------\n",
      "Max loss: 0.019869033247232437\n",
      "Min loss: 0.007366134785115719\n",
      "Mean loss: 0.01258601356918613\n",
      "Std loss: 0.0039685441456282725\n",
      "Total Loss: 0.07551608141511679\n",
      "------------------------------------ epoch 3761 (22560 steps) ------------------------------------\n",
      "Max loss: 0.011896207928657532\n",
      "Min loss: 0.006809299346059561\n",
      "Mean loss: 0.00910216950190564\n",
      "Std loss: 0.001706136432497956\n",
      "Total Loss: 0.05461301701143384\n",
      "------------------------------------ epoch 3762 (22566 steps) ------------------------------------\n",
      "Max loss: 0.02461460791528225\n",
      "Min loss: 0.008087407797574997\n",
      "Mean loss: 0.011437014521410068\n",
      "Std loss: 0.005931334158526246\n",
      "Total Loss: 0.06862208712846041\n",
      "------------------------------------ epoch 3763 (22572 steps) ------------------------------------\n",
      "Max loss: 0.03173290938138962\n",
      "Min loss: 0.007473892532289028\n",
      "Mean loss: 0.013396230836709341\n",
      "Std loss: 0.008265436740910416\n",
      "Total Loss: 0.08037738502025604\n",
      "------------------------------------ epoch 3764 (22578 steps) ------------------------------------\n",
      "Max loss: 0.023103004321455956\n",
      "Min loss: 0.007820472121238708\n",
      "Mean loss: 0.013185723374287287\n",
      "Std loss: 0.005184041252146159\n",
      "Total Loss: 0.07911434024572372\n",
      "------------------------------------ epoch 3765 (22584 steps) ------------------------------------\n",
      "Max loss: 0.012496039271354675\n",
      "Min loss: 0.007028323598206043\n",
      "Mean loss: 0.008986458259945115\n",
      "Std loss: 0.0017775951621765527\n",
      "Total Loss: 0.05391874955967069\n",
      "------------------------------------ epoch 3766 (22590 steps) ------------------------------------\n",
      "Max loss: 0.018550138920545578\n",
      "Min loss: 0.007052060216665268\n",
      "Mean loss: 0.010335265115524331\n",
      "Std loss: 0.004396425100710737\n",
      "Total Loss: 0.06201159069314599\n",
      "------------------------------------ epoch 3767 (22596 steps) ------------------------------------\n",
      "Max loss: 0.013369128108024597\n",
      "Min loss: 0.006967809982597828\n",
      "Mean loss: 0.010360956968118748\n",
      "Std loss: 0.002192278051767431\n",
      "Total Loss: 0.06216574180871248\n",
      "------------------------------------ epoch 3768 (22602 steps) ------------------------------------\n",
      "Max loss: 0.018529951572418213\n",
      "Min loss: 0.0059365928173065186\n",
      "Mean loss: 0.010781577633072933\n",
      "Std loss: 0.004191271830023233\n",
      "Total Loss: 0.0646894657984376\n",
      "------------------------------------ epoch 3769 (22608 steps) ------------------------------------\n",
      "Max loss: 0.012144094333052635\n",
      "Min loss: 0.005668206140398979\n",
      "Mean loss: 0.00870566675439477\n",
      "Std loss: 0.0023822664884869263\n",
      "Total Loss: 0.05223400052636862\n",
      "------------------------------------ epoch 3770 (22614 steps) ------------------------------------\n",
      "Max loss: 0.04100644215941429\n",
      "Min loss: 0.005506107117980719\n",
      "Mean loss: 0.015380486147478223\n",
      "Std loss: 0.011924197736271135\n",
      "Total Loss: 0.09228291688486934\n",
      "------------------------------------ epoch 3771 (22620 steps) ------------------------------------\n",
      "Max loss: 0.027400534600019455\n",
      "Min loss: 0.012070043943822384\n",
      "Mean loss: 0.016264538746327162\n",
      "Std loss: 0.005354431651198194\n",
      "Total Loss: 0.09758723247796297\n",
      "------------------------------------ epoch 3772 (22626 steps) ------------------------------------\n",
      "Max loss: 0.01744784787297249\n",
      "Min loss: 0.009549974463880062\n",
      "Mean loss: 0.012261332633594671\n",
      "Std loss: 0.002647601742059688\n",
      "Total Loss: 0.07356799580156803\n",
      "------------------------------------ epoch 3773 (22632 steps) ------------------------------------\n",
      "Max loss: 0.02290688082575798\n",
      "Min loss: 0.007896218448877335\n",
      "Mean loss: 0.01410376699641347\n",
      "Std loss: 0.004900206347122914\n",
      "Total Loss: 0.08462260197848082\n",
      "------------------------------------ epoch 3774 (22638 steps) ------------------------------------\n",
      "Max loss: 0.02288278192281723\n",
      "Min loss: 0.005965257994830608\n",
      "Mean loss: 0.01290838218604525\n",
      "Std loss: 0.005881735998672008\n",
      "Total Loss: 0.0774502931162715\n",
      "------------------------------------ epoch 3775 (22644 steps) ------------------------------------\n",
      "Max loss: 0.02788252755999565\n",
      "Min loss: 0.013016710989177227\n",
      "Mean loss: 0.01892088803773125\n",
      "Std loss: 0.005123862660099259\n",
      "Total Loss: 0.1135253282263875\n",
      "------------------------------------ epoch 3776 (22650 steps) ------------------------------------\n",
      "Max loss: 0.013809899799525738\n",
      "Min loss: 0.006887000985443592\n",
      "Mean loss: 0.009335196111351252\n",
      "Std loss: 0.0023094602966180025\n",
      "Total Loss: 0.05601117666810751\n",
      "------------------------------------ epoch 3777 (22656 steps) ------------------------------------\n",
      "Max loss: 0.013616316020488739\n",
      "Min loss: 0.010592499747872353\n",
      "Mean loss: 0.012029932967076698\n",
      "Std loss: 0.0010632406305734127\n",
      "Total Loss: 0.0721795978024602\n",
      "------------------------------------ epoch 3778 (22662 steps) ------------------------------------\n",
      "Max loss: 0.012597757391631603\n",
      "Min loss: 0.005319926422089338\n",
      "Mean loss: 0.007832683545226852\n",
      "Std loss: 0.0022859706621846825\n",
      "Total Loss: 0.04699610127136111\n",
      "------------------------------------ epoch 3779 (22668 steps) ------------------------------------\n",
      "Max loss: 0.01968102529644966\n",
      "Min loss: 0.007851057685911655\n",
      "Mean loss: 0.011689959714810053\n",
      "Std loss: 0.003982914806679401\n",
      "Total Loss: 0.07013975828886032\n",
      "------------------------------------ epoch 3780 (22674 steps) ------------------------------------\n",
      "Max loss: 0.02670096419751644\n",
      "Min loss: 0.006229361519217491\n",
      "Mean loss: 0.011495416207859913\n",
      "Std loss: 0.0070163182911930164\n",
      "Total Loss: 0.06897249724715948\n",
      "------------------------------------ epoch 3781 (22680 steps) ------------------------------------\n",
      "Max loss: 0.03213553875684738\n",
      "Min loss: 0.00619437824934721\n",
      "Mean loss: 0.012415477385123571\n",
      "Std loss: 0.008974955843675872\n",
      "Total Loss: 0.07449286431074142\n",
      "------------------------------------ epoch 3782 (22686 steps) ------------------------------------\n",
      "Max loss: 0.03212441876530647\n",
      "Min loss: 0.008145189844071865\n",
      "Mean loss: 0.017844129198541243\n",
      "Std loss: 0.009850558763256276\n",
      "Total Loss: 0.10706477519124746\n",
      "------------------------------------ epoch 3783 (22692 steps) ------------------------------------\n",
      "Max loss: 0.018560633063316345\n",
      "Min loss: 0.007979512214660645\n",
      "Mean loss: 0.011883984630306562\n",
      "Std loss: 0.0034906503711468456\n",
      "Total Loss: 0.07130390778183937\n",
      "------------------------------------ epoch 3784 (22698 steps) ------------------------------------\n",
      "Max loss: 0.028838198632001877\n",
      "Min loss: 0.0065955473110079765\n",
      "Mean loss: 0.015323257073760033\n",
      "Std loss: 0.00756973722224912\n",
      "Total Loss: 0.0919395424425602\n",
      "------------------------------------ epoch 3785 (22704 steps) ------------------------------------\n",
      "Max loss: 0.01738184504210949\n",
      "Min loss: 0.008812526240944862\n",
      "Mean loss: 0.012385143587986628\n",
      "Std loss: 0.002660341845821819\n",
      "Total Loss: 0.07431086152791977\n",
      "------------------------------------ epoch 3786 (22710 steps) ------------------------------------\n",
      "Max loss: 0.011929592117667198\n",
      "Min loss: 0.006806967314332724\n",
      "Mean loss: 0.009987927274778485\n",
      "Std loss: 0.001938097176368841\n",
      "Total Loss: 0.05992756364867091\n",
      "------------------------------------ epoch 3787 (22716 steps) ------------------------------------\n",
      "Max loss: 0.017903631553053856\n",
      "Min loss: 0.007259914185851812\n",
      "Mean loss: 0.012680621584877372\n",
      "Std loss: 0.004227861815971016\n",
      "Total Loss: 0.07608372950926423\n",
      "------------------------------------ epoch 3788 (22722 steps) ------------------------------------\n",
      "Max loss: 0.021066509187221527\n",
      "Min loss: 0.006726140156388283\n",
      "Mean loss: 0.011681510756413141\n",
      "Std loss: 0.005037511237079532\n",
      "Total Loss: 0.07008906453847885\n",
      "------------------------------------ epoch 3789 (22728 steps) ------------------------------------\n",
      "Max loss: 0.011839387938380241\n",
      "Min loss: 0.006716347299516201\n",
      "Mean loss: 0.009278421911100546\n",
      "Std loss: 0.0015824036965337174\n",
      "Total Loss: 0.05567053146660328\n",
      "------------------------------------ epoch 3790 (22734 steps) ------------------------------------\n",
      "Max loss: 0.020370185375213623\n",
      "Min loss: 0.007172050885856152\n",
      "Mean loss: 0.013697827390084663\n",
      "Std loss: 0.003919359938351753\n",
      "Total Loss: 0.08218696434050798\n",
      "------------------------------------ epoch 3791 (22740 steps) ------------------------------------\n",
      "Max loss: 0.028583496809005737\n",
      "Min loss: 0.006535233464092016\n",
      "Mean loss: 0.013488325523212552\n",
      "Std loss: 0.007245613039777916\n",
      "Total Loss: 0.08092995313927531\n",
      "------------------------------------ epoch 3792 (22746 steps) ------------------------------------\n",
      "Max loss: 0.01871311478316784\n",
      "Min loss: 0.006825103424489498\n",
      "Mean loss: 0.013153609043608109\n",
      "Std loss: 0.004051343740359317\n",
      "Total Loss: 0.07892165426164865\n",
      "------------------------------------ epoch 3793 (22752 steps) ------------------------------------\n",
      "Max loss: 0.05229932814836502\n",
      "Min loss: 0.008577747270464897\n",
      "Mean loss: 0.018252636461208265\n",
      "Std loss: 0.015359775746699443\n",
      "Total Loss: 0.10951581876724958\n",
      "------------------------------------ epoch 3794 (22758 steps) ------------------------------------\n",
      "Max loss: 0.028404921293258667\n",
      "Min loss: 0.011392025277018547\n",
      "Mean loss: 0.016652923232565325\n",
      "Std loss: 0.005998670797714903\n",
      "Total Loss: 0.09991753939539194\n",
      "------------------------------------ epoch 3795 (22764 steps) ------------------------------------\n",
      "Max loss: 0.03470255434513092\n",
      "Min loss: 0.009655320085585117\n",
      "Mean loss: 0.01683682653432091\n",
      "Std loss: 0.008327040890245921\n",
      "Total Loss: 0.10102095920592546\n",
      "------------------------------------ epoch 3796 (22770 steps) ------------------------------------\n",
      "Max loss: 0.015421314164996147\n",
      "Min loss: 0.0072633009403944016\n",
      "Mean loss: 0.011536876515795788\n",
      "Std loss: 0.0025447866277730595\n",
      "Total Loss: 0.06922125909477472\n",
      "------------------------------------ epoch 3797 (22776 steps) ------------------------------------\n",
      "Max loss: 0.02305305004119873\n",
      "Min loss: 0.0055883098393678665\n",
      "Mean loss: 0.010971294095118841\n",
      "Std loss: 0.00596504473882073\n",
      "Total Loss: 0.06582776457071304\n",
      "------------------------------------ epoch 3798 (22782 steps) ------------------------------------\n",
      "Max loss: 0.03752728924155235\n",
      "Min loss: 0.007301269099116325\n",
      "Mean loss: 0.01982710634668668\n",
      "Std loss: 0.011300862881636377\n",
      "Total Loss: 0.11896263808012009\n",
      "------------------------------------ epoch 3799 (22788 steps) ------------------------------------\n",
      "Max loss: 0.017746074125170708\n",
      "Min loss: 0.00655198423191905\n",
      "Mean loss: 0.010313026684646806\n",
      "Std loss: 0.004283221880792032\n",
      "Total Loss: 0.06187816010788083\n",
      "------------------------------------ epoch 3800 (22794 steps) ------------------------------------\n",
      "Max loss: 0.013630736619234085\n",
      "Min loss: 0.0062418412417173386\n",
      "Mean loss: 0.008735170044625798\n",
      "Std loss: 0.002688271977332092\n",
      "Total Loss: 0.05241102026775479\n",
      "------------------------------------ epoch 3801 (22800 steps) ------------------------------------\n",
      "Max loss: 0.029795613139867783\n",
      "Min loss: 0.009337704628705978\n",
      "Mean loss: 0.014874032388130823\n",
      "Std loss: 0.006844813457306579\n",
      "Total Loss: 0.08924419432878494\n",
      "saved model at ./weights/model_3801.pth\n",
      "------------------------------------ epoch 3802 (22806 steps) ------------------------------------\n",
      "Max loss: 0.01519993506371975\n",
      "Min loss: 0.006306800991296768\n",
      "Mean loss: 0.009706181474030018\n",
      "Std loss: 0.0032046191045407074\n",
      "Total Loss: 0.05823708884418011\n",
      "------------------------------------ epoch 3803 (22812 steps) ------------------------------------\n",
      "Max loss: 0.017495302483439445\n",
      "Min loss: 0.0061963871121406555\n",
      "Mean loss: 0.010676611059655746\n",
      "Std loss: 0.0036889781342674395\n",
      "Total Loss: 0.06405966635793447\n",
      "------------------------------------ epoch 3804 (22818 steps) ------------------------------------\n",
      "Max loss: 0.023271407932043076\n",
      "Min loss: 0.009184984490275383\n",
      "Mean loss: 0.013733987230807543\n",
      "Std loss: 0.005310371055257287\n",
      "Total Loss: 0.08240392338484526\n",
      "------------------------------------ epoch 3805 (22824 steps) ------------------------------------\n",
      "Max loss: 0.0170915424823761\n",
      "Min loss: 0.0060596708208322525\n",
      "Mean loss: 0.010789466090500355\n",
      "Std loss: 0.0036041175681927142\n",
      "Total Loss: 0.06473679654300213\n",
      "------------------------------------ epoch 3806 (22830 steps) ------------------------------------\n",
      "Max loss: 0.013884767889976501\n",
      "Min loss: 0.005729702766984701\n",
      "Mean loss: 0.008579505452265343\n",
      "Std loss: 0.002704422612860921\n",
      "Total Loss: 0.05147703271359205\n",
      "------------------------------------ epoch 3807 (22836 steps) ------------------------------------\n",
      "Max loss: 0.01622658222913742\n",
      "Min loss: 0.005968241952359676\n",
      "Mean loss: 0.010225636108467976\n",
      "Std loss: 0.0033694123515143103\n",
      "Total Loss: 0.06135381665080786\n",
      "------------------------------------ epoch 3808 (22842 steps) ------------------------------------\n",
      "Max loss: 0.028902817517518997\n",
      "Min loss: 0.005587083753198385\n",
      "Mean loss: 0.01521874552903076\n",
      "Std loss: 0.007531895297720573\n",
      "Total Loss: 0.09131247317418456\n",
      "------------------------------------ epoch 3809 (22848 steps) ------------------------------------\n",
      "Max loss: 0.02887248992919922\n",
      "Min loss: 0.0060237012803554535\n",
      "Mean loss: 0.01721191235507528\n",
      "Std loss: 0.008475100700576008\n",
      "Total Loss: 0.10327147413045168\n",
      "------------------------------------ epoch 3810 (22854 steps) ------------------------------------\n",
      "Max loss: 0.01888509839773178\n",
      "Min loss: 0.007293367292732\n",
      "Mean loss: 0.01178517728112638\n",
      "Std loss: 0.004145859348013739\n",
      "Total Loss: 0.07071106368675828\n",
      "------------------------------------ epoch 3811 (22860 steps) ------------------------------------\n",
      "Max loss: 0.01796926185488701\n",
      "Min loss: 0.006139952689409256\n",
      "Mean loss: 0.011852193856611848\n",
      "Std loss: 0.004633377070270699\n",
      "Total Loss: 0.07111316313967109\n",
      "------------------------------------ epoch 3812 (22866 steps) ------------------------------------\n",
      "Max loss: 0.01683804765343666\n",
      "Min loss: 0.00824768003076315\n",
      "Mean loss: 0.012643565113345781\n",
      "Std loss: 0.0032441900374861475\n",
      "Total Loss: 0.07586139068007469\n",
      "------------------------------------ epoch 3813 (22872 steps) ------------------------------------\n",
      "Max loss: 0.02003050036728382\n",
      "Min loss: 0.0067351809702813625\n",
      "Mean loss: 0.01109440935154756\n",
      "Std loss: 0.004669026096242381\n",
      "Total Loss: 0.06656645610928535\n",
      "------------------------------------ epoch 3814 (22878 steps) ------------------------------------\n",
      "Max loss: 0.02860897034406662\n",
      "Min loss: 0.005712173879146576\n",
      "Mean loss: 0.015576377355804047\n",
      "Std loss: 0.008209991439116049\n",
      "Total Loss: 0.09345826413482428\n",
      "------------------------------------ epoch 3815 (22884 steps) ------------------------------------\n",
      "Max loss: 0.022938765585422516\n",
      "Min loss: 0.006500117480754852\n",
      "Mean loss: 0.012569286084423462\n",
      "Std loss: 0.0057093310268581015\n",
      "Total Loss: 0.07541571650654078\n",
      "------------------------------------ epoch 3816 (22890 steps) ------------------------------------\n",
      "Max loss: 0.06371022015810013\n",
      "Min loss: 0.007589370012283325\n",
      "Mean loss: 0.0194168573555847\n",
      "Std loss: 0.01995431005464439\n",
      "Total Loss: 0.1165011441335082\n",
      "------------------------------------ epoch 3817 (22896 steps) ------------------------------------\n",
      "Max loss: 0.014519929885864258\n",
      "Min loss: 0.008370671421289444\n",
      "Mean loss: 0.011760355904698372\n",
      "Std loss: 0.002322798032212613\n",
      "Total Loss: 0.07056213542819023\n",
      "------------------------------------ epoch 3818 (22902 steps) ------------------------------------\n",
      "Max loss: 0.02974594756960869\n",
      "Min loss: 0.009172092191874981\n",
      "Mean loss: 0.015274668267617622\n",
      "Std loss: 0.007188012200122426\n",
      "Total Loss: 0.09164800960570574\n",
      "------------------------------------ epoch 3819 (22908 steps) ------------------------------------\n",
      "Max loss: 0.051176946610212326\n",
      "Min loss: 0.008834137581288815\n",
      "Mean loss: 0.018120318030317623\n",
      "Std loss: 0.014954734532293637\n",
      "Total Loss: 0.10872190818190575\n",
      "------------------------------------ epoch 3820 (22914 steps) ------------------------------------\n",
      "Max loss: 0.051031261682510376\n",
      "Min loss: 0.007361777126789093\n",
      "Mean loss: 0.02450510486960411\n",
      "Std loss: 0.017930331945239266\n",
      "Total Loss: 0.14703062921762466\n",
      "------------------------------------ epoch 3821 (22920 steps) ------------------------------------\n",
      "Max loss: 0.03869367763400078\n",
      "Min loss: 0.010545900091528893\n",
      "Mean loss: 0.017329853493720293\n",
      "Std loss: 0.009776464774473631\n",
      "Total Loss: 0.10397912096232176\n",
      "------------------------------------ epoch 3822 (22926 steps) ------------------------------------\n",
      "Max loss: 0.03833860158920288\n",
      "Min loss: 0.006609126925468445\n",
      "Mean loss: 0.020141837187111378\n",
      "Std loss: 0.009563538154012771\n",
      "Total Loss: 0.12085102312266827\n",
      "------------------------------------ epoch 3823 (22932 steps) ------------------------------------\n",
      "Max loss: 0.051420774310827255\n",
      "Min loss: 0.008436367847025394\n",
      "Mean loss: 0.0185747096935908\n",
      "Std loss: 0.01487290700267531\n",
      "Total Loss: 0.1114482581615448\n",
      "------------------------------------ epoch 3824 (22938 steps) ------------------------------------\n",
      "Max loss: 0.02997654303908348\n",
      "Min loss: 0.00703031150624156\n",
      "Mean loss: 0.01603361762439211\n",
      "Std loss: 0.009841351059480488\n",
      "Total Loss: 0.09620170574635267\n",
      "------------------------------------ epoch 3825 (22944 steps) ------------------------------------\n",
      "Max loss: 0.01855451986193657\n",
      "Min loss: 0.007697844412177801\n",
      "Mean loss: 0.013598651858046651\n",
      "Std loss: 0.0036382385600492585\n",
      "Total Loss: 0.0815919111482799\n",
      "------------------------------------ epoch 3826 (22950 steps) ------------------------------------\n",
      "Max loss: 0.035158880054950714\n",
      "Min loss: 0.006268227472901344\n",
      "Mean loss: 0.013805003215869268\n",
      "Std loss: 0.0097661922161937\n",
      "Total Loss: 0.0828300192952156\n",
      "------------------------------------ epoch 3827 (22956 steps) ------------------------------------\n",
      "Max loss: 0.01945323869585991\n",
      "Min loss: 0.0067343637347221375\n",
      "Mean loss: 0.01220513042062521\n",
      "Std loss: 0.004216595712200988\n",
      "Total Loss: 0.07323078252375126\n",
      "------------------------------------ epoch 3828 (22962 steps) ------------------------------------\n",
      "Max loss: 0.01250616367906332\n",
      "Min loss: 0.006992024835199118\n",
      "Mean loss: 0.009824574536954364\n",
      "Std loss: 0.0019429256916813173\n",
      "Total Loss: 0.05894744722172618\n",
      "------------------------------------ epoch 3829 (22968 steps) ------------------------------------\n",
      "Max loss: 0.023251067847013474\n",
      "Min loss: 0.010286379605531693\n",
      "Mean loss: 0.01604548655450344\n",
      "Std loss: 0.00533821385630691\n",
      "Total Loss: 0.09627291932702065\n",
      "------------------------------------ epoch 3830 (22974 steps) ------------------------------------\n",
      "Max loss: 0.01987389288842678\n",
      "Min loss: 0.006607480347156525\n",
      "Mean loss: 0.010520337459941706\n",
      "Std loss: 0.004358464927213754\n",
      "Total Loss: 0.06312202475965023\n",
      "------------------------------------ epoch 3831 (22980 steps) ------------------------------------\n",
      "Max loss: 0.02085365727543831\n",
      "Min loss: 0.006757783703505993\n",
      "Mean loss: 0.011213729623705149\n",
      "Std loss: 0.005269732385895473\n",
      "Total Loss: 0.06728237774223089\n",
      "------------------------------------ epoch 3832 (22986 steps) ------------------------------------\n",
      "Max loss: 0.02807290479540825\n",
      "Min loss: 0.005584369413554668\n",
      "Mean loss: 0.01202357622484366\n",
      "Std loss: 0.007550448267993267\n",
      "Total Loss: 0.07214145734906197\n",
      "------------------------------------ epoch 3833 (22992 steps) ------------------------------------\n",
      "Max loss: 0.020051732659339905\n",
      "Min loss: 0.012319441884756088\n",
      "Mean loss: 0.014645816292613745\n",
      "Std loss: 0.0025777694635579496\n",
      "Total Loss: 0.08787489775568247\n",
      "------------------------------------ epoch 3834 (22998 steps) ------------------------------------\n",
      "Max loss: 0.05184612795710564\n",
      "Min loss: 0.00593700772151351\n",
      "Mean loss: 0.017861629681040842\n",
      "Std loss: 0.016090474650022005\n",
      "Total Loss: 0.10716977808624506\n",
      "------------------------------------ epoch 3835 (23004 steps) ------------------------------------\n",
      "Max loss: 0.03719135746359825\n",
      "Min loss: 0.006290538236498833\n",
      "Mean loss: 0.020884728369613487\n",
      "Std loss: 0.010409496816702522\n",
      "Total Loss: 0.12530837021768093\n",
      "------------------------------------ epoch 3836 (23010 steps) ------------------------------------\n",
      "Max loss: 0.03662560507655144\n",
      "Min loss: 0.010079042986035347\n",
      "Mean loss: 0.017209344853957493\n",
      "Std loss: 0.00955235409242007\n",
      "Total Loss: 0.10325606912374496\n",
      "------------------------------------ epoch 3837 (23016 steps) ------------------------------------\n",
      "Max loss: 0.05828791484236717\n",
      "Min loss: 0.014044389128684998\n",
      "Mean loss: 0.027246478013694286\n",
      "Std loss: 0.015595540542174898\n",
      "Total Loss: 0.16347886808216572\n",
      "------------------------------------ epoch 3838 (23022 steps) ------------------------------------\n",
      "Max loss: 0.033128175884485245\n",
      "Min loss: 0.010752088390290737\n",
      "Mean loss: 0.02175350223357479\n",
      "Std loss: 0.00854584976339246\n",
      "Total Loss: 0.13052101340144873\n",
      "------------------------------------ epoch 3839 (23028 steps) ------------------------------------\n",
      "Max loss: 0.03091662935912609\n",
      "Min loss: 0.00886123813688755\n",
      "Mean loss: 0.018110689086218674\n",
      "Std loss: 0.00853188464147126\n",
      "Total Loss: 0.10866413451731205\n",
      "------------------------------------ epoch 3840 (23034 steps) ------------------------------------\n",
      "Max loss: 0.029525935649871826\n",
      "Min loss: 0.0113233532756567\n",
      "Mean loss: 0.016255441742638748\n",
      "Std loss: 0.006107265278679986\n",
      "Total Loss: 0.09753265045583248\n",
      "------------------------------------ epoch 3841 (23040 steps) ------------------------------------\n",
      "Max loss: 0.022028308361768723\n",
      "Min loss: 0.007781164720654488\n",
      "Mean loss: 0.015525585661331812\n",
      "Std loss: 0.005007749678141926\n",
      "Total Loss: 0.09315351396799088\n",
      "------------------------------------ epoch 3842 (23046 steps) ------------------------------------\n",
      "Max loss: 0.014180919155478477\n",
      "Min loss: 0.008421109989285469\n",
      "Mean loss: 0.010634689591825008\n",
      "Std loss: 0.002161660984761118\n",
      "Total Loss: 0.06380813755095005\n",
      "------------------------------------ epoch 3843 (23052 steps) ------------------------------------\n",
      "Max loss: 0.02738962322473526\n",
      "Min loss: 0.00882015936076641\n",
      "Mean loss: 0.013906703175355991\n",
      "Std loss: 0.006132695111111938\n",
      "Total Loss: 0.08344021905213594\n",
      "------------------------------------ epoch 3844 (23058 steps) ------------------------------------\n",
      "Max loss: 0.019797123968601227\n",
      "Min loss: 0.008414107374846935\n",
      "Mean loss: 0.012541831471025944\n",
      "Std loss: 0.004313099682567397\n",
      "Total Loss: 0.07525098882615566\n",
      "------------------------------------ epoch 3845 (23064 steps) ------------------------------------\n",
      "Max loss: 0.013470890931785107\n",
      "Min loss: 0.0066254171542823315\n",
      "Mean loss: 0.010164707355822125\n",
      "Std loss: 0.002646331694890659\n",
      "Total Loss: 0.060988244134932756\n",
      "------------------------------------ epoch 3846 (23070 steps) ------------------------------------\n",
      "Max loss: 0.025263693183660507\n",
      "Min loss: 0.010439161211252213\n",
      "Mean loss: 0.014498571865260601\n",
      "Std loss: 0.005249148245247032\n",
      "Total Loss: 0.0869914311915636\n",
      "------------------------------------ epoch 3847 (23076 steps) ------------------------------------\n",
      "Max loss: 0.04270438849925995\n",
      "Min loss: 0.006325910799205303\n",
      "Mean loss: 0.013660175803427895\n",
      "Std loss: 0.01304442196796449\n",
      "Total Loss: 0.08196105482056737\n",
      "------------------------------------ epoch 3848 (23082 steps) ------------------------------------\n",
      "Max loss: 0.03227115049958229\n",
      "Min loss: 0.007808253169059753\n",
      "Mean loss: 0.016765545743207138\n",
      "Std loss: 0.009468061432901001\n",
      "Total Loss: 0.10059327445924282\n",
      "------------------------------------ epoch 3849 (23088 steps) ------------------------------------\n",
      "Max loss: 0.020126856863498688\n",
      "Min loss: 0.009332705289125443\n",
      "Mean loss: 0.014601216030617556\n",
      "Std loss: 0.004295804545639438\n",
      "Total Loss: 0.08760729618370533\n",
      "------------------------------------ epoch 3850 (23094 steps) ------------------------------------\n",
      "Max loss: 0.024734947830438614\n",
      "Min loss: 0.00838670413941145\n",
      "Mean loss: 0.015291996610661348\n",
      "Std loss: 0.006728460750101595\n",
      "Total Loss: 0.09175197966396809\n",
      "------------------------------------ epoch 3851 (23100 steps) ------------------------------------\n",
      "Max loss: 0.015722092241048813\n",
      "Min loss: 0.006650499999523163\n",
      "Mean loss: 0.008850405691191554\n",
      "Std loss: 0.0031160233448135764\n",
      "Total Loss: 0.053102434147149324\n",
      "------------------------------------ epoch 3852 (23106 steps) ------------------------------------\n",
      "Max loss: 0.025104306638240814\n",
      "Min loss: 0.006266406737267971\n",
      "Mean loss: 0.012505310898025831\n",
      "Std loss: 0.006549640831207426\n",
      "Total Loss: 0.07503186538815498\n",
      "------------------------------------ epoch 3853 (23112 steps) ------------------------------------\n",
      "Max loss: 0.02264491841197014\n",
      "Min loss: 0.008752528578042984\n",
      "Mean loss: 0.013564654936393103\n",
      "Std loss: 0.005020015560900784\n",
      "Total Loss: 0.08138792961835861\n",
      "------------------------------------ epoch 3854 (23118 steps) ------------------------------------\n",
      "Max loss: 0.03033667616546154\n",
      "Min loss: 0.009802471846342087\n",
      "Mean loss: 0.015781294864912827\n",
      "Std loss: 0.006881585077784331\n",
      "Total Loss: 0.09468776918947697\n",
      "------------------------------------ epoch 3855 (23124 steps) ------------------------------------\n",
      "Max loss: 0.02874670922756195\n",
      "Min loss: 0.006815236061811447\n",
      "Mean loss: 0.015522627004732689\n",
      "Std loss: 0.009526578281490831\n",
      "Total Loss: 0.09313576202839613\n",
      "------------------------------------ epoch 3856 (23130 steps) ------------------------------------\n",
      "Max loss: 0.021520299836993217\n",
      "Min loss: 0.010296049527823925\n",
      "Mean loss: 0.013929400593042374\n",
      "Std loss: 0.00395565929088709\n",
      "Total Loss: 0.08357640355825424\n",
      "------------------------------------ epoch 3857 (23136 steps) ------------------------------------\n",
      "Max loss: 0.01847219094634056\n",
      "Min loss: 0.007417734712362289\n",
      "Mean loss: 0.011459878490616878\n",
      "Std loss: 0.004175879135167858\n",
      "Total Loss: 0.06875927094370127\n",
      "------------------------------------ epoch 3858 (23142 steps) ------------------------------------\n",
      "Max loss: 0.020678596571087837\n",
      "Min loss: 0.007242606021463871\n",
      "Mean loss: 0.011141563765704632\n",
      "Std loss: 0.004585450458718591\n",
      "Total Loss: 0.06684938259422779\n",
      "------------------------------------ epoch 3859 (23148 steps) ------------------------------------\n",
      "Max loss: 0.023626577109098434\n",
      "Min loss: 0.0056230900809168816\n",
      "Mean loss: 0.012786601049204668\n",
      "Std loss: 0.007160749777505865\n",
      "Total Loss: 0.076719606295228\n",
      "------------------------------------ epoch 3860 (23154 steps) ------------------------------------\n",
      "Max loss: 0.017113571986556053\n",
      "Min loss: 0.006432222202420235\n",
      "Mean loss: 0.009647245441252986\n",
      "Std loss: 0.003518838182971363\n",
      "Total Loss: 0.05788347264751792\n",
      "------------------------------------ epoch 3861 (23160 steps) ------------------------------------\n",
      "Max loss: 0.01859496533870697\n",
      "Min loss: 0.007392185740172863\n",
      "Mean loss: 0.011769378402580818\n",
      "Std loss: 0.003695055667489467\n",
      "Total Loss: 0.0706162704154849\n",
      "------------------------------------ epoch 3862 (23166 steps) ------------------------------------\n",
      "Max loss: 0.03707584738731384\n",
      "Min loss: 0.007398461457341909\n",
      "Mean loss: 0.01849298217954735\n",
      "Std loss: 0.011096775106584227\n",
      "Total Loss: 0.1109578930772841\n",
      "------------------------------------ epoch 3863 (23172 steps) ------------------------------------\n",
      "Max loss: 0.051288098096847534\n",
      "Min loss: 0.007644570432603359\n",
      "Mean loss: 0.02278362788880865\n",
      "Std loss: 0.013966694776415376\n",
      "Total Loss: 0.1367017673328519\n",
      "------------------------------------ epoch 3864 (23178 steps) ------------------------------------\n",
      "Max loss: 0.024326158687472343\n",
      "Min loss: 0.011683743447065353\n",
      "Mean loss: 0.018019760803629953\n",
      "Std loss: 0.004097139284957094\n",
      "Total Loss: 0.10811856482177973\n",
      "------------------------------------ epoch 3865 (23184 steps) ------------------------------------\n",
      "Max loss: 0.02476413920521736\n",
      "Min loss: 0.009605877101421356\n",
      "Mean loss: 0.016352029672513407\n",
      "Std loss: 0.005736678859269718\n",
      "Total Loss: 0.09811217803508043\n",
      "------------------------------------ epoch 3866 (23190 steps) ------------------------------------\n",
      "Max loss: 0.019758252426981926\n",
      "Min loss: 0.006816582288593054\n",
      "Mean loss: 0.012675700786833962\n",
      "Std loss: 0.004032007261486764\n",
      "Total Loss: 0.07605420472100377\n",
      "------------------------------------ epoch 3867 (23196 steps) ------------------------------------\n",
      "Max loss: 0.02268981747329235\n",
      "Min loss: 0.008899437263607979\n",
      "Mean loss: 0.014599066227674484\n",
      "Std loss: 0.0048588386665158806\n",
      "Total Loss: 0.0875943973660469\n",
      "------------------------------------ epoch 3868 (23202 steps) ------------------------------------\n",
      "Max loss: 0.020527243614196777\n",
      "Min loss: 0.007921959273517132\n",
      "Mean loss: 0.014116247495015463\n",
      "Std loss: 0.003983458676987406\n",
      "Total Loss: 0.08469748497009277\n",
      "------------------------------------ epoch 3869 (23208 steps) ------------------------------------\n",
      "Max loss: 0.013519070111215115\n",
      "Min loss: 0.0071675656363368034\n",
      "Mean loss: 0.010789415954301754\n",
      "Std loss: 0.00209469258345356\n",
      "Total Loss: 0.06473649572581053\n",
      "------------------------------------ epoch 3870 (23214 steps) ------------------------------------\n",
      "Max loss: 0.022702302783727646\n",
      "Min loss: 0.009926684200763702\n",
      "Mean loss: 0.013439632486552\n",
      "Std loss: 0.004536248989816436\n",
      "Total Loss: 0.080637794919312\n",
      "------------------------------------ epoch 3871 (23220 steps) ------------------------------------\n",
      "Max loss: 0.016435656696558\n",
      "Min loss: 0.00948744174093008\n",
      "Mean loss: 0.012491447230180105\n",
      "Std loss: 0.0020737589561427025\n",
      "Total Loss: 0.07494868338108063\n",
      "------------------------------------ epoch 3872 (23226 steps) ------------------------------------\n",
      "Max loss: 0.015815459191799164\n",
      "Min loss: 0.006232358515262604\n",
      "Mean loss: 0.010608049031967917\n",
      "Std loss: 0.003221062592299684\n",
      "Total Loss: 0.06364829419180751\n",
      "------------------------------------ epoch 3873 (23232 steps) ------------------------------------\n",
      "Max loss: 0.02241712063550949\n",
      "Min loss: 0.008379295468330383\n",
      "Mean loss: 0.012330534867942333\n",
      "Std loss: 0.004998400447508865\n",
      "Total Loss: 0.073983209207654\n",
      "------------------------------------ epoch 3874 (23238 steps) ------------------------------------\n",
      "Max loss: 0.020251115784049034\n",
      "Min loss: 0.006176159717142582\n",
      "Mean loss: 0.011525093655412396\n",
      "Std loss: 0.005763924000261205\n",
      "Total Loss: 0.06915056193247437\n",
      "------------------------------------ epoch 3875 (23244 steps) ------------------------------------\n",
      "Max loss: 0.022914402186870575\n",
      "Min loss: 0.007674313150346279\n",
      "Mean loss: 0.015889949165284634\n",
      "Std loss: 0.005750855588690275\n",
      "Total Loss: 0.0953396949917078\n",
      "------------------------------------ epoch 3876 (23250 steps) ------------------------------------\n",
      "Max loss: 0.02040342055261135\n",
      "Min loss: 0.007887735962867737\n",
      "Mean loss: 0.014670557032028833\n",
      "Std loss: 0.0046372281914017705\n",
      "Total Loss: 0.088023342192173\n",
      "------------------------------------ epoch 3877 (23256 steps) ------------------------------------\n",
      "Max loss: 0.03154538571834564\n",
      "Min loss: 0.00676534790545702\n",
      "Mean loss: 0.019269664927075308\n",
      "Std loss: 0.008883502197527543\n",
      "Total Loss: 0.11561798956245184\n",
      "------------------------------------ epoch 3878 (23262 steps) ------------------------------------\n",
      "Max loss: 0.031193656846880913\n",
      "Min loss: 0.008017823100090027\n",
      "Mean loss: 0.017934547426799934\n",
      "Std loss: 0.009313022137590025\n",
      "Total Loss: 0.1076072845607996\n",
      "------------------------------------ epoch 3879 (23268 steps) ------------------------------------\n",
      "Max loss: 0.05663114786148071\n",
      "Min loss: 0.006901380605995655\n",
      "Mean loss: 0.018673153438915808\n",
      "Std loss: 0.017358057490891077\n",
      "Total Loss: 0.11203892063349485\n",
      "------------------------------------ epoch 3880 (23274 steps) ------------------------------------\n",
      "Max loss: 0.019693132489919662\n",
      "Min loss: 0.007017131429165602\n",
      "Mean loss: 0.011299650417640805\n",
      "Std loss: 0.004006591401042821\n",
      "Total Loss: 0.06779790250584483\n",
      "------------------------------------ epoch 3881 (23280 steps) ------------------------------------\n",
      "Max loss: 0.012459350749850273\n",
      "Min loss: 0.0071633863262832165\n",
      "Mean loss: 0.010563806087399522\n",
      "Std loss: 0.002001558741307045\n",
      "Total Loss: 0.06338283652439713\n",
      "------------------------------------ epoch 3882 (23286 steps) ------------------------------------\n",
      "Max loss: 0.022212734445929527\n",
      "Min loss: 0.007092677056789398\n",
      "Mean loss: 0.01196487465252479\n",
      "Std loss: 0.004940125027101265\n",
      "Total Loss: 0.07178924791514874\n",
      "------------------------------------ epoch 3883 (23292 steps) ------------------------------------\n",
      "Max loss: 0.028036024421453476\n",
      "Min loss: 0.008050035685300827\n",
      "Mean loss: 0.014996905345469713\n",
      "Std loss: 0.006463573438319075\n",
      "Total Loss: 0.08998143207281828\n",
      "------------------------------------ epoch 3884 (23298 steps) ------------------------------------\n",
      "Max loss: 0.033287666738033295\n",
      "Min loss: 0.006854521110653877\n",
      "Mean loss: 0.012294800175974766\n",
      "Std loss: 0.009430342234923646\n",
      "Total Loss: 0.0737688010558486\n",
      "------------------------------------ epoch 3885 (23304 steps) ------------------------------------\n",
      "Max loss: 0.03729848191142082\n",
      "Min loss: 0.008237657137215137\n",
      "Mean loss: 0.020403869294871885\n",
      "Std loss: 0.010389849671124614\n",
      "Total Loss: 0.12242321576923132\n",
      "------------------------------------ epoch 3886 (23310 steps) ------------------------------------\n",
      "Max loss: 0.016381761059165\n",
      "Min loss: 0.005918349605053663\n",
      "Mean loss: 0.01105742521273593\n",
      "Std loss: 0.0036847225706120317\n",
      "Total Loss: 0.06634455127641559\n",
      "------------------------------------ epoch 3887 (23316 steps) ------------------------------------\n",
      "Max loss: 0.01632915809750557\n",
      "Min loss: 0.006106329616159201\n",
      "Mean loss: 0.01113982304620246\n",
      "Std loss: 0.00401656190269073\n",
      "Total Loss: 0.06683893827721477\n",
      "------------------------------------ epoch 3888 (23322 steps) ------------------------------------\n",
      "Max loss: 0.011505763046443462\n",
      "Min loss: 0.005888433661311865\n",
      "Mean loss: 0.007994417023534576\n",
      "Std loss: 0.001843564603099927\n",
      "Total Loss: 0.04796650214120746\n",
      "------------------------------------ epoch 3889 (23328 steps) ------------------------------------\n",
      "Max loss: 0.012107154354453087\n",
      "Min loss: 0.006061556749045849\n",
      "Mean loss: 0.008150007265309492\n",
      "Std loss: 0.0022344367325567134\n",
      "Total Loss: 0.048900043591856956\n",
      "------------------------------------ epoch 3890 (23334 steps) ------------------------------------\n",
      "Max loss: 0.016989190131425858\n",
      "Min loss: 0.007381666451692581\n",
      "Mean loss: 0.011447476067890724\n",
      "Std loss: 0.0036473317473365075\n",
      "Total Loss: 0.06868485640734434\n",
      "------------------------------------ epoch 3891 (23340 steps) ------------------------------------\n",
      "Max loss: 0.04137084633111954\n",
      "Min loss: 0.011568717658519745\n",
      "Mean loss: 0.021077783623089392\n",
      "Std loss: 0.011184555401979816\n",
      "Total Loss: 0.12646670173853636\n",
      "------------------------------------ epoch 3892 (23346 steps) ------------------------------------\n",
      "Max loss: 0.04926201328635216\n",
      "Min loss: 0.007490175776183605\n",
      "Mean loss: 0.02019317743058006\n",
      "Std loss: 0.014066820145343942\n",
      "Total Loss: 0.12115906458348036\n",
      "------------------------------------ epoch 3893 (23352 steps) ------------------------------------\n",
      "Max loss: 0.013834429904818535\n",
      "Min loss: 0.00852400716394186\n",
      "Mean loss: 0.011326942592859268\n",
      "Std loss: 0.0020006725585173224\n",
      "Total Loss: 0.06796165555715561\n",
      "------------------------------------ epoch 3894 (23358 steps) ------------------------------------\n",
      "Max loss: 0.0342896394431591\n",
      "Min loss: 0.008174862712621689\n",
      "Mean loss: 0.015259736217558384\n",
      "Std loss: 0.008651506501648823\n",
      "Total Loss: 0.0915584173053503\n",
      "------------------------------------ epoch 3895 (23364 steps) ------------------------------------\n",
      "Max loss: 0.015212595462799072\n",
      "Min loss: 0.008238084614276886\n",
      "Mean loss: 0.011068242602050304\n",
      "Std loss: 0.002389014104527818\n",
      "Total Loss: 0.06640945561230183\n",
      "------------------------------------ epoch 3896 (23370 steps) ------------------------------------\n",
      "Max loss: 0.035631150007247925\n",
      "Min loss: 0.006559917703270912\n",
      "Mean loss: 0.014398531212160984\n",
      "Std loss: 0.010348687171415698\n",
      "Total Loss: 0.08639118727296591\n",
      "------------------------------------ epoch 3897 (23376 steps) ------------------------------------\n",
      "Max loss: 0.018889274448156357\n",
      "Min loss: 0.00786803849041462\n",
      "Mean loss: 0.01412443226824204\n",
      "Std loss: 0.004494203903322046\n",
      "Total Loss: 0.08474659360945225\n",
      "------------------------------------ epoch 3898 (23382 steps) ------------------------------------\n",
      "Max loss: 0.017155848443508148\n",
      "Min loss: 0.009619302116334438\n",
      "Mean loss: 0.012311229016631842\n",
      "Std loss: 0.002548996297251639\n",
      "Total Loss: 0.07386737409979105\n",
      "------------------------------------ epoch 3899 (23388 steps) ------------------------------------\n",
      "Max loss: 0.017355643212795258\n",
      "Min loss: 0.006736134644597769\n",
      "Mean loss: 0.009954934862131873\n",
      "Std loss: 0.003816117288939734\n",
      "Total Loss: 0.05972960917279124\n",
      "------------------------------------ epoch 3900 (23394 steps) ------------------------------------\n",
      "Max loss: 0.04042872041463852\n",
      "Min loss: 0.008037196472287178\n",
      "Mean loss: 0.021052845908949774\n",
      "Std loss: 0.011460979110446787\n",
      "Total Loss: 0.12631707545369864\n",
      "------------------------------------ epoch 3901 (23400 steps) ------------------------------------\n",
      "Max loss: 0.02725345268845558\n",
      "Min loss: 0.006482980214059353\n",
      "Mean loss: 0.015353371699651083\n",
      "Std loss: 0.007523321011077018\n",
      "Total Loss: 0.0921202301979065\n",
      "saved model at ./weights/model_3901.pth\n",
      "------------------------------------ epoch 3902 (23406 steps) ------------------------------------\n",
      "Max loss: 0.022898659110069275\n",
      "Min loss: 0.005737106315791607\n",
      "Mean loss: 0.010976690333336592\n",
      "Std loss: 0.00596757858566843\n",
      "Total Loss: 0.06586014200001955\n",
      "------------------------------------ epoch 3903 (23412 steps) ------------------------------------\n",
      "Max loss: 0.015450399369001389\n",
      "Min loss: 0.006612899713218212\n",
      "Mean loss: 0.010143561055883765\n",
      "Std loss: 0.0031483482686706283\n",
      "Total Loss: 0.06086136633530259\n",
      "------------------------------------ epoch 3904 (23418 steps) ------------------------------------\n",
      "Max loss: 0.020964523777365685\n",
      "Min loss: 0.007471847347915173\n",
      "Mean loss: 0.014716677988568941\n",
      "Std loss: 0.0047993594744943115\n",
      "Total Loss: 0.08830006793141365\n",
      "------------------------------------ epoch 3905 (23424 steps) ------------------------------------\n",
      "Max loss: 0.01655271090567112\n",
      "Min loss: 0.005923771765083075\n",
      "Mean loss: 0.009821476181969047\n",
      "Std loss: 0.004007049513482995\n",
      "Total Loss: 0.05892885709181428\n",
      "------------------------------------ epoch 3906 (23430 steps) ------------------------------------\n",
      "Max loss: 0.025873996317386627\n",
      "Min loss: 0.005369608756154776\n",
      "Mean loss: 0.013723722736661633\n",
      "Std loss: 0.006403496400947123\n",
      "Total Loss: 0.0823423364199698\n",
      "------------------------------------ epoch 3907 (23436 steps) ------------------------------------\n",
      "Max loss: 0.015140879899263382\n",
      "Min loss: 0.006849866360425949\n",
      "Mean loss: 0.010868659863869349\n",
      "Std loss: 0.0029042232436306352\n",
      "Total Loss: 0.0652119591832161\n",
      "------------------------------------ epoch 3908 (23442 steps) ------------------------------------\n",
      "Max loss: 0.022342141717672348\n",
      "Min loss: 0.008530745282769203\n",
      "Mean loss: 0.013211544447888931\n",
      "Std loss: 0.004462237411196317\n",
      "Total Loss: 0.07926926668733358\n",
      "------------------------------------ epoch 3909 (23448 steps) ------------------------------------\n",
      "Max loss: 0.015944108366966248\n",
      "Min loss: 0.010104730725288391\n",
      "Mean loss: 0.013693933064738909\n",
      "Std loss: 0.0018438985860625666\n",
      "Total Loss: 0.08216359838843346\n",
      "------------------------------------ epoch 3910 (23454 steps) ------------------------------------\n",
      "Max loss: 0.027323542162775993\n",
      "Min loss: 0.007099553011357784\n",
      "Mean loss: 0.015149821216861406\n",
      "Std loss: 0.008547693126938444\n",
      "Total Loss: 0.09089892730116844\n",
      "------------------------------------ epoch 3911 (23460 steps) ------------------------------------\n",
      "Max loss: 0.02405504137277603\n",
      "Min loss: 0.0068334368988871574\n",
      "Mean loss: 0.014551490234831968\n",
      "Std loss: 0.006735403103146134\n",
      "Total Loss: 0.08730894140899181\n",
      "------------------------------------ epoch 3912 (23466 steps) ------------------------------------\n",
      "Max loss: 0.024186095222830772\n",
      "Min loss: 0.01216173730790615\n",
      "Mean loss: 0.016022777340064447\n",
      "Std loss: 0.004040929458881035\n",
      "Total Loss: 0.09613666404038668\n",
      "------------------------------------ epoch 3913 (23472 steps) ------------------------------------\n",
      "Max loss: 0.017569635063409805\n",
      "Min loss: 0.007728361990302801\n",
      "Mean loss: 0.011445599685733518\n",
      "Std loss: 0.0036519893696885714\n",
      "Total Loss: 0.0686735981144011\n",
      "------------------------------------ epoch 3914 (23478 steps) ------------------------------------\n",
      "Max loss: 0.023093990981578827\n",
      "Min loss: 0.007647053338587284\n",
      "Mean loss: 0.013268219617505869\n",
      "Std loss: 0.005134654559920298\n",
      "Total Loss: 0.07960931770503521\n",
      "------------------------------------ epoch 3915 (23484 steps) ------------------------------------\n",
      "Max loss: 0.01183764636516571\n",
      "Min loss: 0.006181842647492886\n",
      "Mean loss: 0.008511421658719579\n",
      "Std loss: 0.002075147612531331\n",
      "Total Loss: 0.051068529952317476\n",
      "------------------------------------ epoch 3916 (23490 steps) ------------------------------------\n",
      "Max loss: 0.014714568853378296\n",
      "Min loss: 0.005868837237358093\n",
      "Mean loss: 0.0089140924004217\n",
      "Std loss: 0.0029098180121514346\n",
      "Total Loss: 0.05348455440253019\n",
      "------------------------------------ epoch 3917 (23496 steps) ------------------------------------\n",
      "Max loss: 0.017773570492863655\n",
      "Min loss: 0.0074298083782196045\n",
      "Mean loss: 0.011186742689460516\n",
      "Std loss: 0.003566640429591763\n",
      "Total Loss: 0.0671204561367631\n",
      "------------------------------------ epoch 3918 (23502 steps) ------------------------------------\n",
      "Max loss: 0.014236681163311005\n",
      "Min loss: 0.005662915762513876\n",
      "Mean loss: 0.008769241239254674\n",
      "Std loss: 0.0027903922337348605\n",
      "Total Loss: 0.05261544743552804\n",
      "------------------------------------ epoch 3919 (23508 steps) ------------------------------------\n",
      "Max loss: 0.023234762251377106\n",
      "Min loss: 0.006582885514944792\n",
      "Mean loss: 0.012883282344167432\n",
      "Std loss: 0.0055646679351973585\n",
      "Total Loss: 0.07729969406500459\n",
      "------------------------------------ epoch 3920 (23514 steps) ------------------------------------\n",
      "Max loss: 0.011113481596112251\n",
      "Min loss: 0.0052133603021502495\n",
      "Mean loss: 0.008577218744903803\n",
      "Std loss: 0.002058055585886012\n",
      "Total Loss: 0.05146331246942282\n",
      "------------------------------------ epoch 3921 (23520 steps) ------------------------------------\n",
      "Max loss: 0.01869392767548561\n",
      "Min loss: 0.007471649907529354\n",
      "Mean loss: 0.01145978132262826\n",
      "Std loss: 0.0037938552136134104\n",
      "Total Loss: 0.06875868793576956\n",
      "------------------------------------ epoch 3922 (23526 steps) ------------------------------------\n",
      "Max loss: 0.01811552606523037\n",
      "Min loss: 0.007295604795217514\n",
      "Mean loss: 0.012009404599666595\n",
      "Std loss: 0.0038553169879219037\n",
      "Total Loss: 0.07205642759799957\n",
      "------------------------------------ epoch 3923 (23532 steps) ------------------------------------\n",
      "Max loss: 0.022848814725875854\n",
      "Min loss: 0.0059557585045695305\n",
      "Mean loss: 0.010323874497165283\n",
      "Std loss: 0.005805030958580696\n",
      "Total Loss: 0.061943246982991695\n",
      "------------------------------------ epoch 3924 (23538 steps) ------------------------------------\n",
      "Max loss: 0.02208772860467434\n",
      "Min loss: 0.0062951291911304\n",
      "Mean loss: 0.013308305138101181\n",
      "Std loss: 0.00608615957160116\n",
      "Total Loss: 0.07984983082860708\n",
      "------------------------------------ epoch 3925 (23544 steps) ------------------------------------\n",
      "Max loss: 0.019335053861141205\n",
      "Min loss: 0.006131963338702917\n",
      "Mean loss: 0.010794244629020492\n",
      "Std loss: 0.0045047464292551674\n",
      "Total Loss: 0.06476546777412295\n",
      "------------------------------------ epoch 3926 (23550 steps) ------------------------------------\n",
      "Max loss: 0.023835917934775352\n",
      "Min loss: 0.0072955358773469925\n",
      "Mean loss: 0.013299255321423212\n",
      "Std loss: 0.00644095259700648\n",
      "Total Loss: 0.07979553192853928\n",
      "------------------------------------ epoch 3927 (23556 steps) ------------------------------------\n",
      "Max loss: 0.09330469369888306\n",
      "Min loss: 0.006155852228403091\n",
      "Mean loss: 0.02671729912981391\n",
      "Std loss: 0.031046405056397896\n",
      "Total Loss: 0.16030379477888346\n",
      "------------------------------------ epoch 3928 (23562 steps) ------------------------------------\n",
      "Max loss: 0.026005104184150696\n",
      "Min loss: 0.007648139260709286\n",
      "Mean loss: 0.016673974227160215\n",
      "Std loss: 0.007871388066415907\n",
      "Total Loss: 0.10004384536296129\n",
      "------------------------------------ epoch 3929 (23568 steps) ------------------------------------\n",
      "Max loss: 0.026747018098831177\n",
      "Min loss: 0.006705787032842636\n",
      "Mean loss: 0.016985745790104072\n",
      "Std loss: 0.007248779683445845\n",
      "Total Loss: 0.10191447474062443\n",
      "------------------------------------ epoch 3930 (23574 steps) ------------------------------------\n",
      "Max loss: 0.014174810610711575\n",
      "Min loss: 0.00896256510168314\n",
      "Mean loss: 0.012205610206971565\n",
      "Std loss: 0.0018297253197545498\n",
      "Total Loss: 0.0732336612418294\n",
      "------------------------------------ epoch 3931 (23580 steps) ------------------------------------\n",
      "Max loss: 0.01767391711473465\n",
      "Min loss: 0.008174488320946693\n",
      "Mean loss: 0.011661996288845936\n",
      "Std loss: 0.0032465406815356786\n",
      "Total Loss: 0.06997197773307562\n",
      "------------------------------------ epoch 3932 (23586 steps) ------------------------------------\n",
      "Max loss: 0.045992739498615265\n",
      "Min loss: 0.006725347600877285\n",
      "Mean loss: 0.014999896520748734\n",
      "Std loss: 0.014063002448438638\n",
      "Total Loss: 0.0899993791244924\n",
      "------------------------------------ epoch 3933 (23592 steps) ------------------------------------\n",
      "Max loss: 0.021783513948321342\n",
      "Min loss: 0.006657326128333807\n",
      "Mean loss: 0.013143291464075446\n",
      "Std loss: 0.004625917712913446\n",
      "Total Loss: 0.07885974878445268\n",
      "------------------------------------ epoch 3934 (23598 steps) ------------------------------------\n",
      "Max loss: 0.015802375972270966\n",
      "Min loss: 0.006516887806355953\n",
      "Mean loss: 0.009669839559743801\n",
      "Std loss: 0.0031776751872834253\n",
      "Total Loss: 0.05801903735846281\n",
      "------------------------------------ epoch 3935 (23604 steps) ------------------------------------\n",
      "Max loss: 0.011371994391083717\n",
      "Min loss: 0.006943180225789547\n",
      "Mean loss: 0.009377064649015665\n",
      "Std loss: 0.0016036732802016053\n",
      "Total Loss: 0.05626238789409399\n",
      "------------------------------------ epoch 3936 (23610 steps) ------------------------------------\n",
      "Max loss: 0.021298378705978394\n",
      "Min loss: 0.005234175827354193\n",
      "Mean loss: 0.009610720366860429\n",
      "Std loss: 0.005357104431637671\n",
      "Total Loss: 0.05766432220116258\n",
      "------------------------------------ epoch 3937 (23616 steps) ------------------------------------\n",
      "Max loss: 0.013064864091575146\n",
      "Min loss: 0.0062370761297643185\n",
      "Mean loss: 0.009234652621671557\n",
      "Std loss: 0.002067143303006164\n",
      "Total Loss: 0.055407915730029345\n",
      "------------------------------------ epoch 3938 (23622 steps) ------------------------------------\n",
      "Max loss: 0.022217383608222008\n",
      "Min loss: 0.00712111359462142\n",
      "Mean loss: 0.013457619507486621\n",
      "Std loss: 0.0058451827483593675\n",
      "Total Loss: 0.08074571704491973\n",
      "------------------------------------ epoch 3939 (23628 steps) ------------------------------------\n",
      "Max loss: 0.038536496460437775\n",
      "Min loss: 0.006280745379626751\n",
      "Mean loss: 0.01851513935253024\n",
      "Std loss: 0.013447875888065692\n",
      "Total Loss: 0.11109083611518145\n",
      "------------------------------------ epoch 3940 (23634 steps) ------------------------------------\n",
      "Max loss: 0.03321293741464615\n",
      "Min loss: 0.00827791728079319\n",
      "Mean loss: 0.021382630337029696\n",
      "Std loss: 0.008860232750468183\n",
      "Total Loss: 0.12829578202217817\n",
      "------------------------------------ epoch 3941 (23640 steps) ------------------------------------\n",
      "Max loss: 0.02266976237297058\n",
      "Min loss: 0.009425736963748932\n",
      "Mean loss: 0.016013269778341055\n",
      "Std loss: 0.004070731017082318\n",
      "Total Loss: 0.09607961867004633\n",
      "------------------------------------ epoch 3942 (23646 steps) ------------------------------------\n",
      "Max loss: 0.01558742206543684\n",
      "Min loss: 0.008749878033995628\n",
      "Mean loss: 0.01170755255346497\n",
      "Std loss: 0.002490028928008929\n",
      "Total Loss: 0.07024531532078981\n",
      "------------------------------------ epoch 3943 (23652 steps) ------------------------------------\n",
      "Max loss: 0.022816216573119164\n",
      "Min loss: 0.006976943463087082\n",
      "Mean loss: 0.015244818913439909\n",
      "Std loss: 0.005840320901340745\n",
      "Total Loss: 0.09146891348063946\n",
      "------------------------------------ epoch 3944 (23658 steps) ------------------------------------\n",
      "Max loss: 0.02466222643852234\n",
      "Min loss: 0.009666121564805508\n",
      "Mean loss: 0.01736995003496607\n",
      "Std loss: 0.005316244773308239\n",
      "Total Loss: 0.10421970020979643\n",
      "------------------------------------ epoch 3945 (23664 steps) ------------------------------------\n",
      "Max loss: 0.01879565790295601\n",
      "Min loss: 0.0078415647149086\n",
      "Mean loss: 0.012137870614727339\n",
      "Std loss: 0.004697228407782778\n",
      "Total Loss: 0.07282722368836403\n",
      "------------------------------------ epoch 3946 (23670 steps) ------------------------------------\n",
      "Max loss: 0.06237535923719406\n",
      "Min loss: 0.006979977246373892\n",
      "Mean loss: 0.018983061658218503\n",
      "Std loss: 0.01969979736664058\n",
      "Total Loss: 0.11389836994931102\n",
      "------------------------------------ epoch 3947 (23676 steps) ------------------------------------\n",
      "Max loss: 0.013814996927976608\n",
      "Min loss: 0.006848660297691822\n",
      "Mean loss: 0.01062747680892547\n",
      "Std loss: 0.0026893349601312074\n",
      "Total Loss: 0.06376486085355282\n",
      "------------------------------------ epoch 3948 (23682 steps) ------------------------------------\n",
      "Max loss: 0.035711921751499176\n",
      "Min loss: 0.00880774948745966\n",
      "Mean loss: 0.016307112605621416\n",
      "Std loss: 0.009144381983014222\n",
      "Total Loss: 0.0978426756337285\n",
      "------------------------------------ epoch 3949 (23688 steps) ------------------------------------\n",
      "Max loss: 0.02788081392645836\n",
      "Min loss: 0.010518703609704971\n",
      "Mean loss: 0.01871282048523426\n",
      "Std loss: 0.00655636696138357\n",
      "Total Loss: 0.11227692291140556\n",
      "------------------------------------ epoch 3950 (23694 steps) ------------------------------------\n",
      "Max loss: 0.0526505783200264\n",
      "Min loss: 0.008551951497793198\n",
      "Mean loss: 0.02237177329758803\n",
      "Std loss: 0.01517268888315988\n",
      "Total Loss: 0.13423063978552818\n",
      "------------------------------------ epoch 3951 (23700 steps) ------------------------------------\n",
      "Max loss: 0.029549766331911087\n",
      "Min loss: 0.009088585153222084\n",
      "Mean loss: 0.014529592202355465\n",
      "Std loss: 0.007034981167118582\n",
      "Total Loss: 0.08717755321413279\n",
      "------------------------------------ epoch 3952 (23706 steps) ------------------------------------\n",
      "Max loss: 0.0551331602036953\n",
      "Min loss: 0.008063694462180138\n",
      "Mean loss: 0.0191122245353957\n",
      "Std loss: 0.016497654868046465\n",
      "Total Loss: 0.11467334721237421\n",
      "------------------------------------ epoch 3953 (23712 steps) ------------------------------------\n",
      "Max loss: 0.01784444786608219\n",
      "Min loss: 0.00906846858561039\n",
      "Mean loss: 0.013161856681108475\n",
      "Std loss: 0.0033438052271322753\n",
      "Total Loss: 0.07897114008665085\n",
      "------------------------------------ epoch 3954 (23718 steps) ------------------------------------\n",
      "Max loss: 0.03896138817071915\n",
      "Min loss: 0.006731304340064526\n",
      "Mean loss: 0.014627169041583935\n",
      "Std loss: 0.011065738528949839\n",
      "Total Loss: 0.08776301424950361\n",
      "------------------------------------ epoch 3955 (23724 steps) ------------------------------------\n",
      "Max loss: 0.02021768130362034\n",
      "Min loss: 0.007085868623107672\n",
      "Mean loss: 0.012181904554987947\n",
      "Std loss: 0.004340448643688323\n",
      "Total Loss: 0.07309142732992768\n",
      "------------------------------------ epoch 3956 (23730 steps) ------------------------------------\n",
      "Max loss: 0.014358257874846458\n",
      "Min loss: 0.008123962208628654\n",
      "Mean loss: 0.010739175292352835\n",
      "Std loss: 0.0025506508212966134\n",
      "Total Loss: 0.06443505175411701\n",
      "------------------------------------ epoch 3957 (23736 steps) ------------------------------------\n",
      "Max loss: 0.05357368290424347\n",
      "Min loss: 0.00808860082179308\n",
      "Mean loss: 0.01762091259782513\n",
      "Std loss: 0.01629911871865746\n",
      "Total Loss: 0.10572547558695078\n",
      "------------------------------------ epoch 3958 (23742 steps) ------------------------------------\n",
      "Max loss: 0.01617332547903061\n",
      "Min loss: 0.0061531937681138515\n",
      "Mean loss: 0.009386010002344847\n",
      "Std loss: 0.003629628077946521\n",
      "Total Loss: 0.05631606001406908\n",
      "------------------------------------ epoch 3959 (23748 steps) ------------------------------------\n",
      "Max loss: 0.016455303877592087\n",
      "Min loss: 0.0071357861161231995\n",
      "Mean loss: 0.010340059098477164\n",
      "Std loss: 0.0033450067488097293\n",
      "Total Loss: 0.06204035459086299\n",
      "------------------------------------ epoch 3960 (23754 steps) ------------------------------------\n",
      "Max loss: 0.015824686735868454\n",
      "Min loss: 0.008339460007846355\n",
      "Mean loss: 0.011925104229400555\n",
      "Std loss: 0.002418885197273543\n",
      "Total Loss: 0.07155062537640333\n",
      "------------------------------------ epoch 3961 (23760 steps) ------------------------------------\n",
      "Max loss: 0.01053696870803833\n",
      "Min loss: 0.006337512284517288\n",
      "Mean loss: 0.008686618568996588\n",
      "Std loss: 0.0014972756997183383\n",
      "Total Loss: 0.05211971141397953\n",
      "------------------------------------ epoch 3962 (23766 steps) ------------------------------------\n",
      "Max loss: 0.02039496973156929\n",
      "Min loss: 0.006623617373406887\n",
      "Mean loss: 0.014089679811149836\n",
      "Std loss: 0.005357793214279041\n",
      "Total Loss: 0.08453807886689901\n",
      "------------------------------------ epoch 3963 (23772 steps) ------------------------------------\n",
      "Max loss: 0.01758994348347187\n",
      "Min loss: 0.006055392324924469\n",
      "Mean loss: 0.013342098177721104\n",
      "Std loss: 0.0037406160887307233\n",
      "Total Loss: 0.08005258906632662\n",
      "------------------------------------ epoch 3964 (23778 steps) ------------------------------------\n",
      "Max loss: 0.02578982524573803\n",
      "Min loss: 0.006356182508170605\n",
      "Mean loss: 0.013886011050393185\n",
      "Std loss: 0.007953778055968901\n",
      "Total Loss: 0.0833160663023591\n",
      "------------------------------------ epoch 3965 (23784 steps) ------------------------------------\n",
      "Max loss: 0.027701161801815033\n",
      "Min loss: 0.01063729077577591\n",
      "Mean loss: 0.018199999661495287\n",
      "Std loss: 0.006649479632917607\n",
      "Total Loss: 0.10919999796897173\n",
      "------------------------------------ epoch 3966 (23790 steps) ------------------------------------\n",
      "Max loss: 0.017336729913949966\n",
      "Min loss: 0.008310933597385883\n",
      "Mean loss: 0.012694911876072487\n",
      "Std loss: 0.003676372106488299\n",
      "Total Loss: 0.07616947125643492\n",
      "------------------------------------ epoch 3967 (23796 steps) ------------------------------------\n",
      "Max loss: 0.01750408671796322\n",
      "Min loss: 0.007220791187137365\n",
      "Mean loss: 0.011614361234630147\n",
      "Std loss: 0.003626240870987441\n",
      "Total Loss: 0.06968616740778089\n",
      "------------------------------------ epoch 3968 (23802 steps) ------------------------------------\n",
      "Max loss: 0.017355503514409065\n",
      "Min loss: 0.0066164517775177956\n",
      "Mean loss: 0.010258865232268969\n",
      "Std loss: 0.0034280434354665643\n",
      "Total Loss: 0.061553191393613815\n",
      "------------------------------------ epoch 3969 (23808 steps) ------------------------------------\n",
      "Max loss: 0.018886636942625046\n",
      "Min loss: 0.006505655124783516\n",
      "Mean loss: 0.010426159870500365\n",
      "Std loss: 0.004361987397127191\n",
      "Total Loss: 0.0625569592230022\n",
      "------------------------------------ epoch 3970 (23814 steps) ------------------------------------\n",
      "Max loss: 0.02020372450351715\n",
      "Min loss: 0.005474480800330639\n",
      "Mean loss: 0.011639972605432073\n",
      "Std loss: 0.005290453426431805\n",
      "Total Loss: 0.06983983563259244\n",
      "------------------------------------ epoch 3971 (23820 steps) ------------------------------------\n",
      "Max loss: 0.0333758108317852\n",
      "Min loss: 0.0075890920124948025\n",
      "Mean loss: 0.015004084367925921\n",
      "Std loss: 0.009457152767271352\n",
      "Total Loss: 0.09002450620755553\n",
      "------------------------------------ epoch 3972 (23826 steps) ------------------------------------\n",
      "Max loss: 0.027596987783908844\n",
      "Min loss: 0.007417680229991674\n",
      "Mean loss: 0.012656891020014882\n",
      "Std loss: 0.006868604765220528\n",
      "Total Loss: 0.07594134612008929\n",
      "------------------------------------ epoch 3973 (23832 steps) ------------------------------------\n",
      "Max loss: 0.012069381773471832\n",
      "Min loss: 0.006711668334901333\n",
      "Mean loss: 0.008667744075258573\n",
      "Std loss: 0.0018412889859610393\n",
      "Total Loss: 0.05200646445155144\n",
      "------------------------------------ epoch 3974 (23838 steps) ------------------------------------\n",
      "Max loss: 0.02533685974776745\n",
      "Min loss: 0.006442082580178976\n",
      "Mean loss: 0.013823977283512553\n",
      "Std loss: 0.007973729880344851\n",
      "Total Loss: 0.08294386370107532\n",
      "------------------------------------ epoch 3975 (23844 steps) ------------------------------------\n",
      "Max loss: 0.017210062593221664\n",
      "Min loss: 0.005437587387859821\n",
      "Mean loss: 0.01147941934565703\n",
      "Std loss: 0.00412851796927614\n",
      "Total Loss: 0.06887651607394218\n",
      "------------------------------------ epoch 3976 (23850 steps) ------------------------------------\n",
      "Max loss: 0.017392929643392563\n",
      "Min loss: 0.0070875706151127815\n",
      "Mean loss: 0.012080812361091375\n",
      "Std loss: 0.004197345271102548\n",
      "Total Loss: 0.07248487416654825\n",
      "------------------------------------ epoch 3977 (23856 steps) ------------------------------------\n",
      "Max loss: 0.03512175381183624\n",
      "Min loss: 0.006868201307952404\n",
      "Mean loss: 0.013944884606947502\n",
      "Std loss: 0.009728155891960905\n",
      "Total Loss: 0.08366930764168501\n",
      "------------------------------------ epoch 3978 (23862 steps) ------------------------------------\n",
      "Max loss: 0.024408278986811638\n",
      "Min loss: 0.008672891184687614\n",
      "Mean loss: 0.012287116919954618\n",
      "Std loss: 0.005753882903020431\n",
      "Total Loss: 0.0737227015197277\n",
      "------------------------------------ epoch 3979 (23868 steps) ------------------------------------\n",
      "Max loss: 0.011865897104144096\n",
      "Min loss: 0.006135802716016769\n",
      "Mean loss: 0.009360051403443018\n",
      "Std loss: 0.001820681883846823\n",
      "Total Loss: 0.05616030842065811\n",
      "------------------------------------ epoch 3980 (23874 steps) ------------------------------------\n",
      "Max loss: 0.012047423981130123\n",
      "Min loss: 0.006443431135267019\n",
      "Mean loss: 0.009753192542120814\n",
      "Std loss: 0.001837824692831539\n",
      "Total Loss: 0.058519155252724886\n",
      "------------------------------------ epoch 3981 (23880 steps) ------------------------------------\n",
      "Max loss: 0.020656678825616837\n",
      "Min loss: 0.006342776119709015\n",
      "Mean loss: 0.013282415922731161\n",
      "Std loss: 0.004513417304221532\n",
      "Total Loss: 0.07969449553638697\n",
      "------------------------------------ epoch 3982 (23886 steps) ------------------------------------\n",
      "Max loss: 0.04457888752222061\n",
      "Min loss: 0.006812745239585638\n",
      "Mean loss: 0.017052144510671496\n",
      "Std loss: 0.01334248844319633\n",
      "Total Loss: 0.10231286706402898\n",
      "------------------------------------ epoch 3983 (23892 steps) ------------------------------------\n",
      "Max loss: 0.018047412857413292\n",
      "Min loss: 0.010006172582507133\n",
      "Mean loss: 0.013500041017929712\n",
      "Std loss: 0.002669530387291243\n",
      "Total Loss: 0.08100024610757828\n",
      "------------------------------------ epoch 3984 (23898 steps) ------------------------------------\n",
      "Max loss: 0.01992018334567547\n",
      "Min loss: 0.007069885730743408\n",
      "Mean loss: 0.0115226109046489\n",
      "Std loss: 0.004593406404840907\n",
      "Total Loss: 0.0691356654278934\n",
      "------------------------------------ epoch 3985 (23904 steps) ------------------------------------\n",
      "Max loss: 0.0180011298507452\n",
      "Min loss: 0.008416736498475075\n",
      "Mean loss: 0.012810767938693365\n",
      "Std loss: 0.004270194190216009\n",
      "Total Loss: 0.07686460763216019\n",
      "------------------------------------ epoch 3986 (23910 steps) ------------------------------------\n",
      "Max loss: 0.0223117433488369\n",
      "Min loss: 0.006996799260377884\n",
      "Mean loss: 0.012652940582484007\n",
      "Std loss: 0.004788687691103578\n",
      "Total Loss: 0.07591764349490404\n",
      "------------------------------------ epoch 3987 (23916 steps) ------------------------------------\n",
      "Max loss: 0.04679989814758301\n",
      "Min loss: 0.008136292919516563\n",
      "Mean loss: 0.02184756596883138\n",
      "Std loss: 0.013803799431359912\n",
      "Total Loss: 0.13108539581298828\n",
      "------------------------------------ epoch 3988 (23922 steps) ------------------------------------\n",
      "Max loss: 0.01262810081243515\n",
      "Min loss: 0.007543724961578846\n",
      "Mean loss: 0.010105315440644821\n",
      "Std loss: 0.001812671225846767\n",
      "Total Loss: 0.06063189264386892\n",
      "------------------------------------ epoch 3989 (23928 steps) ------------------------------------\n",
      "Max loss: 0.02077515795826912\n",
      "Min loss: 0.009301809594035149\n",
      "Mean loss: 0.014203897366921106\n",
      "Std loss: 0.0038681160976804414\n",
      "Total Loss: 0.08522338420152664\n",
      "------------------------------------ epoch 3990 (23934 steps) ------------------------------------\n",
      "Max loss: 0.01836954802274704\n",
      "Min loss: 0.007852999493479729\n",
      "Mean loss: 0.011983008279154697\n",
      "Std loss: 0.0037452732982522254\n",
      "Total Loss: 0.07189804967492819\n",
      "------------------------------------ epoch 3991 (23940 steps) ------------------------------------\n",
      "Max loss: 0.04777203127741814\n",
      "Min loss: 0.006577576510608196\n",
      "Mean loss: 0.017095702855537336\n",
      "Std loss: 0.015548316217605499\n",
      "Total Loss: 0.10257421713322401\n",
      "------------------------------------ epoch 3992 (23946 steps) ------------------------------------\n",
      "Max loss: 0.04194159433245659\n",
      "Min loss: 0.008695699274539948\n",
      "Mean loss: 0.019891051885982353\n",
      "Std loss: 0.01096679918237058\n",
      "Total Loss: 0.11934631131589413\n",
      "------------------------------------ epoch 3993 (23952 steps) ------------------------------------\n",
      "Max loss: 0.027598625048995018\n",
      "Min loss: 0.0070881894789636135\n",
      "Mean loss: 0.016154713385427993\n",
      "Std loss: 0.006575079584906747\n",
      "Total Loss: 0.09692828031256795\n",
      "------------------------------------ epoch 3994 (23958 steps) ------------------------------------\n",
      "Max loss: 0.02170754037797451\n",
      "Min loss: 0.007281515281647444\n",
      "Mean loss: 0.012352840897316733\n",
      "Std loss: 0.0045164673719836615\n",
      "Total Loss: 0.0741170453839004\n",
      "------------------------------------ epoch 3995 (23964 steps) ------------------------------------\n",
      "Max loss: 0.019234521314501762\n",
      "Min loss: 0.006582162342965603\n",
      "Mean loss: 0.011547358008101583\n",
      "Std loss: 0.004427451228087339\n",
      "Total Loss: 0.0692841480486095\n",
      "------------------------------------ epoch 3996 (23970 steps) ------------------------------------\n",
      "Max loss: 0.03169406205415726\n",
      "Min loss: 0.00907073076814413\n",
      "Mean loss: 0.017217111463348072\n",
      "Std loss: 0.009463328559883992\n",
      "Total Loss: 0.10330266878008842\n",
      "------------------------------------ epoch 3997 (23976 steps) ------------------------------------\n",
      "Max loss: 0.013182727620005608\n",
      "Min loss: 0.00782586820423603\n",
      "Mean loss: 0.00941298653682073\n",
      "Std loss: 0.0019254261029277677\n",
      "Total Loss: 0.05647791922092438\n",
      "------------------------------------ epoch 3998 (23982 steps) ------------------------------------\n",
      "Max loss: 0.014144301414489746\n",
      "Min loss: 0.006602490320801735\n",
      "Mean loss: 0.009844542325784763\n",
      "Std loss: 0.0027540485375722964\n",
      "Total Loss: 0.059067253954708576\n",
      "------------------------------------ epoch 3999 (23988 steps) ------------------------------------\n",
      "Max loss: 0.030694138258695602\n",
      "Min loss: 0.007439705543220043\n",
      "Mean loss: 0.013988114117334286\n",
      "Std loss: 0.00793289929419571\n",
      "Total Loss: 0.08392868470400572\n",
      "------------------------------------ epoch 4000 (23994 steps) ------------------------------------\n",
      "Max loss: 0.016842197626829147\n",
      "Min loss: 0.005961672868579626\n",
      "Mean loss: 0.011306783262019357\n",
      "Std loss: 0.0034574007841410644\n",
      "Total Loss: 0.06784069957211614\n",
      "------------------------------------ epoch 4001 (24000 steps) ------------------------------------\n",
      "Max loss: 0.0179356150329113\n",
      "Min loss: 0.006519985385239124\n",
      "Mean loss: 0.00965289631858468\n",
      "Std loss: 0.003805796248271525\n",
      "Total Loss: 0.05791737791150808\n",
      "saved model at ./weights/model_4001.pth\n",
      "------------------------------------ epoch 4002 (24006 steps) ------------------------------------\n",
      "Max loss: 0.02684376947581768\n",
      "Min loss: 0.005851113237440586\n",
      "Mean loss: 0.015920332322518032\n",
      "Std loss: 0.00842970833687976\n",
      "Total Loss: 0.09552199393510818\n",
      "------------------------------------ epoch 4003 (24012 steps) ------------------------------------\n",
      "Max loss: 0.02354533225297928\n",
      "Min loss: 0.007643633522093296\n",
      "Mean loss: 0.014723838617404303\n",
      "Std loss: 0.005534813521453762\n",
      "Total Loss: 0.08834303170442581\n",
      "------------------------------------ epoch 4004 (24018 steps) ------------------------------------\n",
      "Max loss: 0.01977478712797165\n",
      "Min loss: 0.009127030149102211\n",
      "Mean loss: 0.012682138631741205\n",
      "Std loss: 0.003445424349978102\n",
      "Total Loss: 0.07609283179044724\n",
      "------------------------------------ epoch 4005 (24024 steps) ------------------------------------\n",
      "Max loss: 0.019935574382543564\n",
      "Min loss: 0.005591356661170721\n",
      "Mean loss: 0.01135441823862493\n",
      "Std loss: 0.004346726208091081\n",
      "Total Loss: 0.06812650943174958\n",
      "------------------------------------ epoch 4006 (24030 steps) ------------------------------------\n",
      "Max loss: 0.024593234062194824\n",
      "Min loss: 0.005815436132252216\n",
      "Mean loss: 0.01389557138706247\n",
      "Std loss: 0.006810047409343405\n",
      "Total Loss: 0.08337342832237482\n",
      "------------------------------------ epoch 4007 (24036 steps) ------------------------------------\n",
      "Max loss: 0.02863084338605404\n",
      "Min loss: 0.0062305619940161705\n",
      "Mean loss: 0.012517201869438091\n",
      "Std loss: 0.007416436075342472\n",
      "Total Loss: 0.07510321121662855\n",
      "------------------------------------ epoch 4008 (24042 steps) ------------------------------------\n",
      "Max loss: 0.0151411397382617\n",
      "Min loss: 0.0070355599746108055\n",
      "Mean loss: 0.010599580127745867\n",
      "Std loss: 0.0027242391625071534\n",
      "Total Loss: 0.0635974807664752\n",
      "------------------------------------ epoch 4009 (24048 steps) ------------------------------------\n",
      "Max loss: 0.023992519825696945\n",
      "Min loss: 0.007455950137227774\n",
      "Mean loss: 0.01295147843969365\n",
      "Std loss: 0.00553949429068594\n",
      "Total Loss: 0.0777088706381619\n",
      "------------------------------------ epoch 4010 (24054 steps) ------------------------------------\n",
      "Max loss: 0.018377207219600677\n",
      "Min loss: 0.007682173978537321\n",
      "Mean loss: 0.013442487378294269\n",
      "Std loss: 0.00382222812768454\n",
      "Total Loss: 0.08065492426976562\n",
      "------------------------------------ epoch 4011 (24060 steps) ------------------------------------\n",
      "Max loss: 0.017156753689050674\n",
      "Min loss: 0.006068411748856306\n",
      "Mean loss: 0.011678489002709588\n",
      "Std loss: 0.00400141170081202\n",
      "Total Loss: 0.07007093401625752\n",
      "------------------------------------ epoch 4012 (24066 steps) ------------------------------------\n",
      "Max loss: 0.031068086624145508\n",
      "Min loss: 0.006045598071068525\n",
      "Mean loss: 0.011427706650768718\n",
      "Std loss: 0.008864271802216332\n",
      "Total Loss: 0.0685662399046123\n",
      "------------------------------------ epoch 4013 (24072 steps) ------------------------------------\n",
      "Max loss: 0.03380659222602844\n",
      "Min loss: 0.006608609110116959\n",
      "Mean loss: 0.019001618182907503\n",
      "Std loss: 0.010388679329838144\n",
      "Total Loss: 0.11400970909744501\n",
      "------------------------------------ epoch 4014 (24078 steps) ------------------------------------\n",
      "Max loss: 0.033318500965833664\n",
      "Min loss: 0.008178910240530968\n",
      "Mean loss: 0.014187726968278488\n",
      "Std loss: 0.008807282179693465\n",
      "Total Loss: 0.08512636180967093\n",
      "------------------------------------ epoch 4015 (24084 steps) ------------------------------------\n",
      "Max loss: 0.02216896414756775\n",
      "Min loss: 0.00850469246506691\n",
      "Mean loss: 0.014848253379265467\n",
      "Std loss: 0.004995326072222988\n",
      "Total Loss: 0.0890895202755928\n",
      "------------------------------------ epoch 4016 (24090 steps) ------------------------------------\n",
      "Max loss: 0.024855149909853935\n",
      "Min loss: 0.00873846560716629\n",
      "Mean loss: 0.013105638325214386\n",
      "Std loss: 0.005473929160205854\n",
      "Total Loss: 0.07863382995128632\n",
      "------------------------------------ epoch 4017 (24096 steps) ------------------------------------\n",
      "Max loss: 0.021920088678598404\n",
      "Min loss: 0.008985263295471668\n",
      "Mean loss: 0.013533261294166246\n",
      "Std loss: 0.004985626268682861\n",
      "Total Loss: 0.08119956776499748\n",
      "------------------------------------ epoch 4018 (24102 steps) ------------------------------------\n",
      "Max loss: 0.025707170367240906\n",
      "Min loss: 0.006731521338224411\n",
      "Mean loss: 0.015285379563768705\n",
      "Std loss: 0.006856243504737831\n",
      "Total Loss: 0.09171227738261223\n",
      "------------------------------------ epoch 4019 (24108 steps) ------------------------------------\n",
      "Max loss: 0.02477460727095604\n",
      "Min loss: 0.009199701249599457\n",
      "Mean loss: 0.01566324693461259\n",
      "Std loss: 0.005110712893682607\n",
      "Total Loss: 0.09397948160767555\n",
      "------------------------------------ epoch 4020 (24114 steps) ------------------------------------\n",
      "Max loss: 0.013849040493369102\n",
      "Min loss: 0.0068437485024333\n",
      "Mean loss: 0.010936538378397623\n",
      "Std loss: 0.0028797550009118183\n",
      "Total Loss: 0.06561923027038574\n",
      "------------------------------------ epoch 4021 (24120 steps) ------------------------------------\n",
      "Max loss: 0.04862023890018463\n",
      "Min loss: 0.008980413898825645\n",
      "Mean loss: 0.022301093985637028\n",
      "Std loss: 0.012785480242359567\n",
      "Total Loss: 0.13380656391382217\n",
      "------------------------------------ epoch 4022 (24126 steps) ------------------------------------\n",
      "Max loss: 0.01877458393573761\n",
      "Min loss: 0.0077048903331160545\n",
      "Mean loss: 0.01270431357746323\n",
      "Std loss: 0.004187405807028953\n",
      "Total Loss: 0.07622588146477938\n",
      "------------------------------------ epoch 4023 (24132 steps) ------------------------------------\n",
      "Max loss: 0.026686718687415123\n",
      "Min loss: 0.007642625831067562\n",
      "Mean loss: 0.017384831483165424\n",
      "Std loss: 0.007347978092055241\n",
      "Total Loss: 0.10430898889899254\n",
      "------------------------------------ epoch 4024 (24138 steps) ------------------------------------\n",
      "Max loss: 0.01943003013730049\n",
      "Min loss: 0.00913710705935955\n",
      "Mean loss: 0.012455745444943508\n",
      "Std loss: 0.0035018388959391343\n",
      "Total Loss: 0.07473447266966105\n",
      "------------------------------------ epoch 4025 (24144 steps) ------------------------------------\n",
      "Max loss: 0.02142351120710373\n",
      "Min loss: 0.009363172575831413\n",
      "Mean loss: 0.014892923490454754\n",
      "Std loss: 0.004331074796081872\n",
      "Total Loss: 0.08935754094272852\n",
      "------------------------------------ epoch 4026 (24150 steps) ------------------------------------\n",
      "Max loss: 0.015036064200103283\n",
      "Min loss: 0.006745832972228527\n",
      "Mean loss: 0.009979312773793936\n",
      "Std loss: 0.002774500543358761\n",
      "Total Loss: 0.059875876642763615\n",
      "------------------------------------ epoch 4027 (24156 steps) ------------------------------------\n",
      "Max loss: 0.02352953515946865\n",
      "Min loss: 0.006457689218223095\n",
      "Mean loss: 0.013786265471329292\n",
      "Std loss: 0.0053229410800518205\n",
      "Total Loss: 0.08271759282797575\n",
      "------------------------------------ epoch 4028 (24162 steps) ------------------------------------\n",
      "Max loss: 0.015295702032744884\n",
      "Min loss: 0.005933677311986685\n",
      "Mean loss: 0.010990069791053733\n",
      "Std loss: 0.0030330613322759044\n",
      "Total Loss: 0.0659404187463224\n",
      "------------------------------------ epoch 4029 (24168 steps) ------------------------------------\n",
      "Max loss: 0.045807890594005585\n",
      "Min loss: 0.00848371908068657\n",
      "Mean loss: 0.01734324125573039\n",
      "Std loss: 0.013192469321022665\n",
      "Total Loss: 0.10405944753438234\n",
      "------------------------------------ epoch 4030 (24174 steps) ------------------------------------\n",
      "Max loss: 0.024175889790058136\n",
      "Min loss: 0.006901971995830536\n",
      "Mean loss: 0.01249909633770585\n",
      "Std loss: 0.005807602793401857\n",
      "Total Loss: 0.0749945780262351\n",
      "------------------------------------ epoch 4031 (24180 steps) ------------------------------------\n",
      "Max loss: 0.01217104122042656\n",
      "Min loss: 0.007651265244930983\n",
      "Mean loss: 0.010168406724308928\n",
      "Std loss: 0.001524305901162194\n",
      "Total Loss: 0.06101044034585357\n",
      "------------------------------------ epoch 4032 (24186 steps) ------------------------------------\n",
      "Max loss: 0.024522844702005386\n",
      "Min loss: 0.006992958020418882\n",
      "Mean loss: 0.01205902174115181\n",
      "Std loss: 0.006089811898436801\n",
      "Total Loss: 0.07235413044691086\n",
      "------------------------------------ epoch 4033 (24192 steps) ------------------------------------\n",
      "Max loss: 0.027452029287815094\n",
      "Min loss: 0.005384302698075771\n",
      "Mean loss: 0.01241620226452748\n",
      "Std loss: 0.008108417105769391\n",
      "Total Loss: 0.07449721358716488\n",
      "------------------------------------ epoch 4034 (24198 steps) ------------------------------------\n",
      "Max loss: 0.02328607067465782\n",
      "Min loss: 0.006780548952519894\n",
      "Mean loss: 0.012996895083536705\n",
      "Std loss: 0.006761041597442696\n",
      "Total Loss: 0.07798137050122023\n",
      "------------------------------------ epoch 4035 (24204 steps) ------------------------------------\n",
      "Max loss: 0.02276531420648098\n",
      "Min loss: 0.007647150661796331\n",
      "Mean loss: 0.010506198819105824\n",
      "Std loss: 0.005487838670724539\n",
      "Total Loss: 0.06303719291463494\n",
      "------------------------------------ epoch 4036 (24210 steps) ------------------------------------\n",
      "Max loss: 0.013492549769580364\n",
      "Min loss: 0.006264036055654287\n",
      "Mean loss: 0.009624068858101964\n",
      "Std loss: 0.0027442033050155195\n",
      "Total Loss: 0.057744413148611784\n",
      "------------------------------------ epoch 4037 (24216 steps) ------------------------------------\n",
      "Max loss: 0.009051915258169174\n",
      "Min loss: 0.007081184536218643\n",
      "Mean loss: 0.008351796772331\n",
      "Std loss: 0.0006341901343849197\n",
      "Total Loss: 0.050110780633985996\n",
      "------------------------------------ epoch 4038 (24222 steps) ------------------------------------\n",
      "Max loss: 0.02207927033305168\n",
      "Min loss: 0.006103004328906536\n",
      "Mean loss: 0.015749650541692972\n",
      "Std loss: 0.0059043515468348\n",
      "Total Loss: 0.09449790325015783\n",
      "------------------------------------ epoch 4039 (24228 steps) ------------------------------------\n",
      "Max loss: 0.018036913126707077\n",
      "Min loss: 0.008355237543582916\n",
      "Mean loss: 0.010806971695274115\n",
      "Std loss: 0.003329283093440449\n",
      "Total Loss: 0.06484183017164469\n",
      "------------------------------------ epoch 4040 (24234 steps) ------------------------------------\n",
      "Max loss: 0.038700170814991\n",
      "Min loss: 0.007049355655908585\n",
      "Mean loss: 0.01946335774846375\n",
      "Std loss: 0.012133875417191799\n",
      "Total Loss: 0.1167801464907825\n",
      "------------------------------------ epoch 4041 (24240 steps) ------------------------------------\n",
      "Max loss: 0.04163544625043869\n",
      "Min loss: 0.00748403137549758\n",
      "Mean loss: 0.014348675574486455\n",
      "Std loss: 0.012241394888364256\n",
      "Total Loss: 0.08609205344691873\n",
      "------------------------------------ epoch 4042 (24246 steps) ------------------------------------\n",
      "Max loss: 0.052098195999860764\n",
      "Min loss: 0.009405354969203472\n",
      "Mean loss: 0.02388053433969617\n",
      "Std loss: 0.018177534163178473\n",
      "Total Loss: 0.143283206038177\n",
      "------------------------------------ epoch 4043 (24252 steps) ------------------------------------\n",
      "Max loss: 0.022708987817168236\n",
      "Min loss: 0.006919588893651962\n",
      "Mean loss: 0.013284154158706466\n",
      "Std loss: 0.006418826547845079\n",
      "Total Loss: 0.0797049249522388\n",
      "------------------------------------ epoch 4044 (24258 steps) ------------------------------------\n",
      "Max loss: 0.02569068782031536\n",
      "Min loss: 0.008785538375377655\n",
      "Mean loss: 0.0168229218882819\n",
      "Std loss: 0.006432462882043008\n",
      "Total Loss: 0.10093753132969141\n",
      "------------------------------------ epoch 4045 (24264 steps) ------------------------------------\n",
      "Max loss: 0.016283676028251648\n",
      "Min loss: 0.007180945016443729\n",
      "Mean loss: 0.010761136344323555\n",
      "Std loss: 0.0032034106397897592\n",
      "Total Loss: 0.06456681806594133\n",
      "------------------------------------ epoch 4046 (24270 steps) ------------------------------------\n",
      "Max loss: 0.016660571098327637\n",
      "Min loss: 0.00688908901065588\n",
      "Mean loss: 0.010192421730607748\n",
      "Std loss: 0.0032723838299727336\n",
      "Total Loss: 0.06115453038364649\n",
      "------------------------------------ epoch 4047 (24276 steps) ------------------------------------\n",
      "Max loss: 0.0234609954059124\n",
      "Min loss: 0.006416420452296734\n",
      "Mean loss: 0.013265052965531746\n",
      "Std loss: 0.005920722910750286\n",
      "Total Loss: 0.07959031779319048\n",
      "------------------------------------ epoch 4048 (24282 steps) ------------------------------------\n",
      "Max loss: 0.06544578075408936\n",
      "Min loss: 0.007310205604881048\n",
      "Mean loss: 0.02125092432834208\n",
      "Std loss: 0.020105089269579525\n",
      "Total Loss: 0.12750554597005248\n",
      "------------------------------------ epoch 4049 (24288 steps) ------------------------------------\n",
      "Max loss: 0.017378024756908417\n",
      "Min loss: 0.00660242885351181\n",
      "Mean loss: 0.011497626701990763\n",
      "Std loss: 0.0043148168138031325\n",
      "Total Loss: 0.06898576021194458\n",
      "------------------------------------ epoch 4050 (24294 steps) ------------------------------------\n",
      "Max loss: 0.016552699729800224\n",
      "Min loss: 0.006997830234467983\n",
      "Mean loss: 0.01022831816226244\n",
      "Std loss: 0.003099998355829661\n",
      "Total Loss: 0.06136990897357464\n",
      "------------------------------------ epoch 4051 (24300 steps) ------------------------------------\n",
      "Max loss: 0.03430788591504097\n",
      "Min loss: 0.0071252090856432915\n",
      "Mean loss: 0.017960781852404278\n",
      "Std loss: 0.010522968903200132\n",
      "Total Loss: 0.10776469111442566\n",
      "------------------------------------ epoch 4052 (24306 steps) ------------------------------------\n",
      "Max loss: 0.01733548752963543\n",
      "Min loss: 0.0076489984057843685\n",
      "Mean loss: 0.011276015313342214\n",
      "Std loss: 0.003100529668953393\n",
      "Total Loss: 0.06765609188005328\n",
      "------------------------------------ epoch 4053 (24312 steps) ------------------------------------\n",
      "Max loss: 0.07070458680391312\n",
      "Min loss: 0.006893420126289129\n",
      "Mean loss: 0.023726259591057897\n",
      "Std loss: 0.021714715393797827\n",
      "Total Loss: 0.14235755754634738\n",
      "------------------------------------ epoch 4054 (24318 steps) ------------------------------------\n",
      "Max loss: 0.016785575076937675\n",
      "Min loss: 0.006088522728532553\n",
      "Mean loss: 0.010224680261065563\n",
      "Std loss: 0.004087547393224645\n",
      "Total Loss: 0.061348081566393375\n",
      "------------------------------------ epoch 4055 (24324 steps) ------------------------------------\n",
      "Max loss: 0.04073657840490341\n",
      "Min loss: 0.007877249270677567\n",
      "Mean loss: 0.017458235068867605\n",
      "Std loss: 0.011297993818691605\n",
      "Total Loss: 0.10474941041320562\n",
      "------------------------------------ epoch 4056 (24330 steps) ------------------------------------\n",
      "Max loss: 0.04853270575404167\n",
      "Min loss: 0.011783232912421227\n",
      "Mean loss: 0.022060407636066277\n",
      "Std loss: 0.01284537056526637\n",
      "Total Loss: 0.13236244581639767\n",
      "------------------------------------ epoch 4057 (24336 steps) ------------------------------------\n",
      "Max loss: 0.01839175820350647\n",
      "Min loss: 0.007482835557311773\n",
      "Mean loss: 0.012792832916602492\n",
      "Std loss: 0.0034157346800803433\n",
      "Total Loss: 0.07675699749961495\n",
      "------------------------------------ epoch 4058 (24342 steps) ------------------------------------\n",
      "Max loss: 0.03331150859594345\n",
      "Min loss: 0.006627073511481285\n",
      "Mean loss: 0.014607027483483156\n",
      "Std loss: 0.009077447977527032\n",
      "Total Loss: 0.08764216490089893\n",
      "------------------------------------ epoch 4059 (24348 steps) ------------------------------------\n",
      "Max loss: 0.02748391032218933\n",
      "Min loss: 0.0080507667735219\n",
      "Mean loss: 0.015733318403363228\n",
      "Std loss: 0.006954534344021616\n",
      "Total Loss: 0.09439991042017937\n",
      "------------------------------------ epoch 4060 (24354 steps) ------------------------------------\n",
      "Max loss: 0.035018905997276306\n",
      "Min loss: 0.008314023725688457\n",
      "Mean loss: 0.016684740937004488\n",
      "Std loss: 0.00889883617900149\n",
      "Total Loss: 0.10010844562202692\n",
      "------------------------------------ epoch 4061 (24360 steps) ------------------------------------\n",
      "Max loss: 0.02337566576898098\n",
      "Min loss: 0.007763882167637348\n",
      "Mean loss: 0.017084649298340082\n",
      "Std loss: 0.005220237294800855\n",
      "Total Loss: 0.10250789579004049\n",
      "------------------------------------ epoch 4062 (24366 steps) ------------------------------------\n",
      "Max loss: 0.030693983659148216\n",
      "Min loss: 0.0072382064536213875\n",
      "Mean loss: 0.014763486571609974\n",
      "Std loss: 0.007979155341514594\n",
      "Total Loss: 0.08858091942965984\n",
      "------------------------------------ epoch 4063 (24372 steps) ------------------------------------\n",
      "Max loss: 0.01438977476209402\n",
      "Min loss: 0.007439286448061466\n",
      "Mean loss: 0.010715466924011707\n",
      "Std loss: 0.0024157520016318745\n",
      "Total Loss: 0.06429280154407024\n",
      "------------------------------------ epoch 4064 (24378 steps) ------------------------------------\n",
      "Max loss: 0.026270300149917603\n",
      "Min loss: 0.006356765981763601\n",
      "Mean loss: 0.011134544542680183\n",
      "Std loss: 0.006845911533694609\n",
      "Total Loss: 0.0668072672560811\n",
      "------------------------------------ epoch 4065 (24384 steps) ------------------------------------\n",
      "Max loss: 0.01449069194495678\n",
      "Min loss: 0.007822509855031967\n",
      "Mean loss: 0.009870504184315601\n",
      "Std loss: 0.002341605578828791\n",
      "Total Loss: 0.05922302510589361\n",
      "------------------------------------ epoch 4066 (24390 steps) ------------------------------------\n",
      "Max loss: 0.02993173897266388\n",
      "Min loss: 0.007063372991979122\n",
      "Mean loss: 0.015609039148936668\n",
      "Std loss: 0.008590926054327283\n",
      "Total Loss: 0.09365423489362001\n",
      "------------------------------------ epoch 4067 (24396 steps) ------------------------------------\n",
      "Max loss: 0.024260079488158226\n",
      "Min loss: 0.008171754889190197\n",
      "Mean loss: 0.015625447190056246\n",
      "Std loss: 0.005941363675627448\n",
      "Total Loss: 0.09375268314033747\n",
      "------------------------------------ epoch 4068 (24402 steps) ------------------------------------\n",
      "Max loss: 0.04079226404428482\n",
      "Min loss: 0.009645109996199608\n",
      "Mean loss: 0.019733263955761988\n",
      "Std loss: 0.012362917160692795\n",
      "Total Loss: 0.11839958373457193\n",
      "------------------------------------ epoch 4069 (24408 steps) ------------------------------------\n",
      "Max loss: 0.03787868842482567\n",
      "Min loss: 0.007354946341365576\n",
      "Mean loss: 0.01956995630947252\n",
      "Std loss: 0.01188659327665884\n",
      "Total Loss: 0.11741973785683513\n",
      "------------------------------------ epoch 4070 (24414 steps) ------------------------------------\n",
      "Max loss: 0.02913559041917324\n",
      "Min loss: 0.008073296397924423\n",
      "Mean loss: 0.015220134053379297\n",
      "Std loss: 0.006747103349124804\n",
      "Total Loss: 0.09132080432027578\n",
      "------------------------------------ epoch 4071 (24420 steps) ------------------------------------\n",
      "Max loss: 0.02365785837173462\n",
      "Min loss: 0.008430360816419125\n",
      "Mean loss: 0.013832293295611938\n",
      "Std loss: 0.004825201619872068\n",
      "Total Loss: 0.08299375977367163\n",
      "------------------------------------ epoch 4072 (24426 steps) ------------------------------------\n",
      "Max loss: 0.022048061713576317\n",
      "Min loss: 0.00702076917514205\n",
      "Mean loss: 0.011279166443273425\n",
      "Std loss: 0.005001866604301939\n",
      "Total Loss: 0.06767499865964055\n",
      "------------------------------------ epoch 4073 (24432 steps) ------------------------------------\n",
      "Max loss: 0.022041037678718567\n",
      "Min loss: 0.00686699990183115\n",
      "Mean loss: 0.012149978894740343\n",
      "Std loss: 0.0049390967909932835\n",
      "Total Loss: 0.07289987336844206\n",
      "------------------------------------ epoch 4074 (24438 steps) ------------------------------------\n",
      "Max loss: 0.02208670601248741\n",
      "Min loss: 0.007695636712014675\n",
      "Mean loss: 0.01341689998904864\n",
      "Std loss: 0.005012689695823511\n",
      "Total Loss: 0.08050139993429184\n",
      "------------------------------------ epoch 4075 (24444 steps) ------------------------------------\n",
      "Max loss: 0.01968073844909668\n",
      "Min loss: 0.006056708749383688\n",
      "Mean loss: 0.011558130771542588\n",
      "Std loss: 0.004381780072002547\n",
      "Total Loss: 0.06934878462925553\n",
      "------------------------------------ epoch 4076 (24450 steps) ------------------------------------\n",
      "Max loss: 0.021334007382392883\n",
      "Min loss: 0.00525591429322958\n",
      "Mean loss: 0.010116651964684328\n",
      "Std loss: 0.006204609390397892\n",
      "Total Loss: 0.060699911788105965\n",
      "------------------------------------ epoch 4077 (24456 steps) ------------------------------------\n",
      "Max loss: 0.011527873575687408\n",
      "Min loss: 0.0065808845683932304\n",
      "Mean loss: 0.008295185475920638\n",
      "Std loss: 0.001706851179788521\n",
      "Total Loss: 0.049771112855523825\n",
      "------------------------------------ epoch 4078 (24462 steps) ------------------------------------\n",
      "Max loss: 0.011696319095790386\n",
      "Min loss: 0.005147207994014025\n",
      "Mean loss: 0.007318704000984629\n",
      "Std loss: 0.00207495052379423\n",
      "Total Loss: 0.043912224005907774\n",
      "------------------------------------ epoch 4079 (24468 steps) ------------------------------------\n",
      "Max loss: 0.020129499956965446\n",
      "Min loss: 0.006182562094181776\n",
      "Mean loss: 0.013306110398843884\n",
      "Std loss: 0.004977866898270661\n",
      "Total Loss: 0.0798366623930633\n",
      "------------------------------------ epoch 4080 (24474 steps) ------------------------------------\n",
      "Max loss: 0.03953348472714424\n",
      "Min loss: 0.005731026642024517\n",
      "Mean loss: 0.01799395742515723\n",
      "Std loss: 0.013264330232636552\n",
      "Total Loss: 0.10796374455094337\n",
      "------------------------------------ epoch 4081 (24480 steps) ------------------------------------\n",
      "Max loss: 0.011449767276644707\n",
      "Min loss: 0.006001044064760208\n",
      "Mean loss: 0.009375306156774363\n",
      "Std loss: 0.0017609234434842144\n",
      "Total Loss: 0.05625183694064617\n",
      "------------------------------------ epoch 4082 (24486 steps) ------------------------------------\n",
      "Max loss: 0.014647643081843853\n",
      "Min loss: 0.005428049713373184\n",
      "Mean loss: 0.00992590623597304\n",
      "Std loss: 0.0033723568781646555\n",
      "Total Loss: 0.05955543741583824\n",
      "------------------------------------ epoch 4083 (24492 steps) ------------------------------------\n",
      "Max loss: 0.012050206772983074\n",
      "Min loss: 0.005532360635697842\n",
      "Mean loss: 0.008680519337455431\n",
      "Std loss: 0.0021753778692720337\n",
      "Total Loss: 0.05208311602473259\n",
      "------------------------------------ epoch 4084 (24498 steps) ------------------------------------\n",
      "Max loss: 0.018136128783226013\n",
      "Min loss: 0.006724953651428223\n",
      "Mean loss: 0.009999518593152365\n",
      "Std loss: 0.003917210167057207\n",
      "Total Loss: 0.059997111558914185\n",
      "------------------------------------ epoch 4085 (24504 steps) ------------------------------------\n",
      "Max loss: 0.02349093370139599\n",
      "Min loss: 0.009340873919427395\n",
      "Mean loss: 0.016350098730375368\n",
      "Std loss: 0.004979104233933132\n",
      "Total Loss: 0.09810059238225222\n",
      "------------------------------------ epoch 4086 (24510 steps) ------------------------------------\n",
      "Max loss: 0.009939477778971195\n",
      "Min loss: 0.006547859869897366\n",
      "Mean loss: 0.008094084992383918\n",
      "Std loss: 0.0012707984672300193\n",
      "Total Loss: 0.0485645099543035\n",
      "------------------------------------ epoch 4087 (24516 steps) ------------------------------------\n",
      "Max loss: 0.047006528824567795\n",
      "Min loss: 0.00912158377468586\n",
      "Mean loss: 0.022995477852722008\n",
      "Std loss: 0.012624126510897322\n",
      "Total Loss: 0.13797286711633205\n",
      "------------------------------------ epoch 4088 (24522 steps) ------------------------------------\n",
      "Max loss: 0.027005812153220177\n",
      "Min loss: 0.006795905530452728\n",
      "Mean loss: 0.014279098560412725\n",
      "Std loss: 0.006340274142291448\n",
      "Total Loss: 0.08567459136247635\n",
      "------------------------------------ epoch 4089 (24528 steps) ------------------------------------\n",
      "Max loss: 0.021216824650764465\n",
      "Min loss: 0.007671134080737829\n",
      "Mean loss: 0.01272562681697309\n",
      "Std loss: 0.004389585474461958\n",
      "Total Loss: 0.07635376090183854\n",
      "------------------------------------ epoch 4090 (24534 steps) ------------------------------------\n",
      "Max loss: 0.026156853884458542\n",
      "Min loss: 0.007558803539723158\n",
      "Mean loss: 0.014348732074722648\n",
      "Std loss: 0.007890539149646481\n",
      "Total Loss: 0.08609239244833589\n",
      "------------------------------------ epoch 4091 (24540 steps) ------------------------------------\n",
      "Max loss: 0.03525242954492569\n",
      "Min loss: 0.006509461905807257\n",
      "Mean loss: 0.014539771480485797\n",
      "Std loss: 0.010753324676571417\n",
      "Total Loss: 0.08723862888291478\n",
      "------------------------------------ epoch 4092 (24546 steps) ------------------------------------\n",
      "Max loss: 0.021400323137640953\n",
      "Min loss: 0.007391480728983879\n",
      "Mean loss: 0.0133927040733397\n",
      "Std loss: 0.004512991208392615\n",
      "Total Loss: 0.0803562244400382\n",
      "------------------------------------ epoch 4093 (24552 steps) ------------------------------------\n",
      "Max loss: 0.027089346200227737\n",
      "Min loss: 0.006297278217971325\n",
      "Mean loss: 0.014450799518575272\n",
      "Std loss: 0.007782363913865777\n",
      "Total Loss: 0.08670479711145163\n",
      "------------------------------------ epoch 4094 (24558 steps) ------------------------------------\n",
      "Max loss: 0.01281716488301754\n",
      "Min loss: 0.006118427962064743\n",
      "Mean loss: 0.009140843215088049\n",
      "Std loss: 0.002469188049296006\n",
      "Total Loss: 0.0548450592905283\n",
      "------------------------------------ epoch 4095 (24564 steps) ------------------------------------\n",
      "Max loss: 0.048089802265167236\n",
      "Min loss: 0.006241393741220236\n",
      "Mean loss: 0.020338831391806405\n",
      "Std loss: 0.016907499178563178\n",
      "Total Loss: 0.12203298835083842\n",
      "------------------------------------ epoch 4096 (24570 steps) ------------------------------------\n",
      "Max loss: 0.019805889576673508\n",
      "Min loss: 0.008495748043060303\n",
      "Mean loss: 0.012501616341372332\n",
      "Std loss: 0.004040839664897143\n",
      "Total Loss: 0.07500969804823399\n",
      "------------------------------------ epoch 4097 (24576 steps) ------------------------------------\n",
      "Max loss: 0.022081997245550156\n",
      "Min loss: 0.006825375370681286\n",
      "Mean loss: 0.01231467816978693\n",
      "Std loss: 0.0050238727487354915\n",
      "Total Loss: 0.07388806901872158\n",
      "------------------------------------ epoch 4098 (24582 steps) ------------------------------------\n",
      "Max loss: 0.03558666631579399\n",
      "Min loss: 0.008157267235219479\n",
      "Mean loss: 0.019178097912420828\n",
      "Std loss: 0.008880466227958664\n",
      "Total Loss: 0.11506858747452497\n",
      "------------------------------------ epoch 4099 (24588 steps) ------------------------------------\n",
      "Max loss: 0.0658918023109436\n",
      "Min loss: 0.00787296798080206\n",
      "Mean loss: 0.029337297348926466\n",
      "Std loss: 0.020285734327863722\n",
      "Total Loss: 0.1760237840935588\n",
      "------------------------------------ epoch 4100 (24594 steps) ------------------------------------\n",
      "Max loss: 0.028717007488012314\n",
      "Min loss: 0.007244376465678215\n",
      "Mean loss: 0.015183010914673408\n",
      "Std loss: 0.007064289098059737\n",
      "Total Loss: 0.09109806548804045\n",
      "------------------------------------ epoch 4101 (24600 steps) ------------------------------------\n",
      "Max loss: 0.014439523220062256\n",
      "Min loss: 0.007059789728373289\n",
      "Mean loss: 0.009852812547857562\n",
      "Std loss: 0.0029789904740440236\n",
      "Total Loss: 0.059116875287145376\n",
      "saved model at ./weights/model_4101.pth\n",
      "------------------------------------ epoch 4102 (24606 steps) ------------------------------------\n",
      "Max loss: 0.027035411447286606\n",
      "Min loss: 0.008259618654847145\n",
      "Mean loss: 0.01612142426893115\n",
      "Std loss: 0.007546704679645096\n",
      "Total Loss: 0.0967285456135869\n",
      "------------------------------------ epoch 4103 (24612 steps) ------------------------------------\n",
      "Max loss: 0.03714542090892792\n",
      "Min loss: 0.007768522948026657\n",
      "Mean loss: 0.019618299168845017\n",
      "Std loss: 0.01067031370487979\n",
      "Total Loss: 0.1177097950130701\n",
      "------------------------------------ epoch 4104 (24618 steps) ------------------------------------\n",
      "Max loss: 0.01885075494647026\n",
      "Min loss: 0.006767197512090206\n",
      "Mean loss: 0.011812223897626003\n",
      "Std loss: 0.0041602378120873295\n",
      "Total Loss: 0.07087334338575602\n",
      "------------------------------------ epoch 4105 (24624 steps) ------------------------------------\n",
      "Max loss: 0.03229725360870361\n",
      "Min loss: 0.009122008457779884\n",
      "Mean loss: 0.016730261966586113\n",
      "Std loss: 0.009688987720801233\n",
      "Total Loss: 0.10038157179951668\n",
      "------------------------------------ epoch 4106 (24630 steps) ------------------------------------\n",
      "Max loss: 0.03608422726392746\n",
      "Min loss: 0.008137919008731842\n",
      "Mean loss: 0.016905892950793106\n",
      "Std loss: 0.009837467340759201\n",
      "Total Loss: 0.10143535770475864\n",
      "------------------------------------ epoch 4107 (24636 steps) ------------------------------------\n",
      "Max loss: 0.026252321898937225\n",
      "Min loss: 0.006076354067772627\n",
      "Mean loss: 0.013766967924311757\n",
      "Std loss: 0.006941264073652199\n",
      "Total Loss: 0.08260180754587054\n",
      "------------------------------------ epoch 4108 (24642 steps) ------------------------------------\n",
      "Max loss: 0.018654435873031616\n",
      "Min loss: 0.007544736843556166\n",
      "Mean loss: 0.011366398461783925\n",
      "Std loss: 0.003950988721181033\n",
      "Total Loss: 0.06819839077070355\n",
      "------------------------------------ epoch 4109 (24648 steps) ------------------------------------\n",
      "Max loss: 0.02972007542848587\n",
      "Min loss: 0.007516900077462196\n",
      "Mean loss: 0.012013175679991642\n",
      "Std loss: 0.007940286547587116\n",
      "Total Loss: 0.07207905407994986\n",
      "------------------------------------ epoch 4110 (24654 steps) ------------------------------------\n",
      "Max loss: 0.02241693250834942\n",
      "Min loss: 0.00695963716134429\n",
      "Mean loss: 0.013830750016495585\n",
      "Std loss: 0.006015028321584446\n",
      "Total Loss: 0.08298450009897351\n",
      "------------------------------------ epoch 4111 (24660 steps) ------------------------------------\n",
      "Max loss: 0.02720259130001068\n",
      "Min loss: 0.005705907940864563\n",
      "Mean loss: 0.012762178123618165\n",
      "Std loss: 0.00733315033143471\n",
      "Total Loss: 0.076573068741709\n",
      "------------------------------------ epoch 4112 (24666 steps) ------------------------------------\n",
      "Max loss: 0.017278574407100677\n",
      "Min loss: 0.007454131729900837\n",
      "Mean loss: 0.013331638649106026\n",
      "Std loss: 0.0040135936366911765\n",
      "Total Loss: 0.07998983189463615\n",
      "------------------------------------ epoch 4113 (24672 steps) ------------------------------------\n",
      "Max loss: 0.06437622755765915\n",
      "Min loss: 0.006424064747989178\n",
      "Mean loss: 0.024105217463026445\n",
      "Std loss: 0.01908748390876312\n",
      "Total Loss: 0.14463130477815866\n",
      "------------------------------------ epoch 4114 (24678 steps) ------------------------------------\n",
      "Max loss: 0.037703365087509155\n",
      "Min loss: 0.007354922592639923\n",
      "Mean loss: 0.019136403997739155\n",
      "Std loss: 0.009984511264312549\n",
      "Total Loss: 0.11481842398643494\n",
      "------------------------------------ epoch 4115 (24684 steps) ------------------------------------\n",
      "Max loss: 0.021318992599844933\n",
      "Min loss: 0.008967018686234951\n",
      "Mean loss: 0.014529491153856119\n",
      "Std loss: 0.004173328969223801\n",
      "Total Loss: 0.08717694692313671\n",
      "------------------------------------ epoch 4116 (24690 steps) ------------------------------------\n",
      "Max loss: 0.010328857228159904\n",
      "Min loss: 0.007613669149577618\n",
      "Mean loss: 0.009246005521466335\n",
      "Std loss: 0.0011228631395076897\n",
      "Total Loss: 0.05547603312879801\n",
      "------------------------------------ epoch 4117 (24696 steps) ------------------------------------\n",
      "Max loss: 0.03867676854133606\n",
      "Min loss: 0.008761505596339703\n",
      "Mean loss: 0.01882584175715844\n",
      "Std loss: 0.012026706300452033\n",
      "Total Loss: 0.11295505054295063\n",
      "------------------------------------ epoch 4118 (24702 steps) ------------------------------------\n",
      "Max loss: 0.02010234072804451\n",
      "Min loss: 0.006772590335458517\n",
      "Mean loss: 0.01293014152906835\n",
      "Std loss: 0.005360792349630781\n",
      "Total Loss: 0.0775808491744101\n",
      "------------------------------------ epoch 4119 (24708 steps) ------------------------------------\n",
      "Max loss: 0.010434262454509735\n",
      "Min loss: 0.007045873906463385\n",
      "Mean loss: 0.00870415063885351\n",
      "Std loss: 0.0011931934095912248\n",
      "Total Loss: 0.05222490383312106\n",
      "------------------------------------ epoch 4120 (24714 steps) ------------------------------------\n",
      "Max loss: 0.023560471832752228\n",
      "Min loss: 0.006455534137785435\n",
      "Mean loss: 0.012554803242286047\n",
      "Std loss: 0.005516963584663562\n",
      "Total Loss: 0.07532881945371628\n",
      "------------------------------------ epoch 4121 (24720 steps) ------------------------------------\n",
      "Max loss: 0.028682317584753036\n",
      "Min loss: 0.007486720569431782\n",
      "Mean loss: 0.01403647925083836\n",
      "Std loss: 0.007353834970866613\n",
      "Total Loss: 0.08421887550503016\n",
      "------------------------------------ epoch 4122 (24726 steps) ------------------------------------\n",
      "Max loss: 0.015125892125070095\n",
      "Min loss: 0.0060166772454977036\n",
      "Mean loss: 0.00984825884612898\n",
      "Std loss: 0.003439587313156641\n",
      "Total Loss: 0.05908955307677388\n",
      "------------------------------------ epoch 4123 (24732 steps) ------------------------------------\n",
      "Max loss: 0.015029743313789368\n",
      "Min loss: 0.007825170643627644\n",
      "Mean loss: 0.011755835730582476\n",
      "Std loss: 0.0029265879286987117\n",
      "Total Loss: 0.07053501438349485\n",
      "------------------------------------ epoch 4124 (24738 steps) ------------------------------------\n",
      "Max loss: 0.04612774774432182\n",
      "Min loss: 0.008783137425780296\n",
      "Mean loss: 0.017888038729627926\n",
      "Std loss: 0.01281064732589588\n",
      "Total Loss: 0.10732823237776756\n",
      "------------------------------------ epoch 4125 (24744 steps) ------------------------------------\n",
      "Max loss: 0.03831592947244644\n",
      "Min loss: 0.007052664645016193\n",
      "Mean loss: 0.014948937886705002\n",
      "Std loss: 0.010914558655185517\n",
      "Total Loss: 0.08969362732023001\n",
      "------------------------------------ epoch 4126 (24750 steps) ------------------------------------\n",
      "Max loss: 0.015116485767066479\n",
      "Min loss: 0.0076544932089746\n",
      "Mean loss: 0.011888879584148526\n",
      "Std loss: 0.002476832905577973\n",
      "Total Loss: 0.07133327750489116\n",
      "------------------------------------ epoch 4127 (24756 steps) ------------------------------------\n",
      "Max loss: 0.02267514169216156\n",
      "Min loss: 0.005537714809179306\n",
      "Mean loss: 0.014756956758598486\n",
      "Std loss: 0.006100760006490015\n",
      "Total Loss: 0.08854174055159092\n",
      "------------------------------------ epoch 4128 (24762 steps) ------------------------------------\n",
      "Max loss: 0.018979739397764206\n",
      "Min loss: 0.007848914712667465\n",
      "Mean loss: 0.011258876572052637\n",
      "Std loss: 0.003602599234569381\n",
      "Total Loss: 0.06755325943231583\n",
      "------------------------------------ epoch 4129 (24768 steps) ------------------------------------\n",
      "Max loss: 0.02576439082622528\n",
      "Min loss: 0.0064029875211417675\n",
      "Mean loss: 0.012772416463121772\n",
      "Std loss: 0.006368913702377904\n",
      "Total Loss: 0.07663449877873063\n",
      "------------------------------------ epoch 4130 (24774 steps) ------------------------------------\n",
      "Max loss: 0.016685541719198227\n",
      "Min loss: 0.006089642643928528\n",
      "Mean loss: 0.011529424382994572\n",
      "Std loss: 0.00370126650308183\n",
      "Total Loss: 0.06917654629796743\n",
      "------------------------------------ epoch 4131 (24780 steps) ------------------------------------\n",
      "Max loss: 0.0325981043279171\n",
      "Min loss: 0.007842287421226501\n",
      "Mean loss: 0.01765013299882412\n",
      "Std loss: 0.009270200367581822\n",
      "Total Loss: 0.10590079799294472\n",
      "------------------------------------ epoch 4132 (24786 steps) ------------------------------------\n",
      "Max loss: 0.016181007027626038\n",
      "Min loss: 0.009057054296135902\n",
      "Mean loss: 0.012002431942770878\n",
      "Std loss: 0.002443828770559425\n",
      "Total Loss: 0.07201459165662527\n",
      "------------------------------------ epoch 4133 (24792 steps) ------------------------------------\n",
      "Max loss: 0.023912031203508377\n",
      "Min loss: 0.011131860315799713\n",
      "Mean loss: 0.015666827093809843\n",
      "Std loss: 0.0050371089463448985\n",
      "Total Loss: 0.09400096256285906\n",
      "------------------------------------ epoch 4134 (24798 steps) ------------------------------------\n",
      "Max loss: 0.031167326495051384\n",
      "Min loss: 0.008711807429790497\n",
      "Mean loss: 0.015153112510840097\n",
      "Std loss: 0.007594951265593099\n",
      "Total Loss: 0.09091867506504059\n",
      "------------------------------------ epoch 4135 (24804 steps) ------------------------------------\n",
      "Max loss: 0.030543457716703415\n",
      "Min loss: 0.0075117452070117\n",
      "Mean loss: 0.01364533727367719\n",
      "Std loss: 0.007847069772531374\n",
      "Total Loss: 0.08187202364206314\n",
      "------------------------------------ epoch 4136 (24810 steps) ------------------------------------\n",
      "Max loss: 0.021398603916168213\n",
      "Min loss: 0.005816998891532421\n",
      "Mean loss: 0.011995718271161119\n",
      "Std loss: 0.00509458427951368\n",
      "Total Loss: 0.07197430962696671\n",
      "------------------------------------ epoch 4137 (24816 steps) ------------------------------------\n",
      "Max loss: 0.016980288550257683\n",
      "Min loss: 0.006239201407879591\n",
      "Mean loss: 0.010890310242151221\n",
      "Std loss: 0.003981660827085408\n",
      "Total Loss: 0.06534186145290732\n",
      "------------------------------------ epoch 4138 (24822 steps) ------------------------------------\n",
      "Max loss: 0.028618503361940384\n",
      "Min loss: 0.007320918142795563\n",
      "Mean loss: 0.01321305613964796\n",
      "Std loss: 0.007490037091441879\n",
      "Total Loss: 0.07927833683788776\n",
      "------------------------------------ epoch 4139 (24828 steps) ------------------------------------\n",
      "Max loss: 0.04822276160120964\n",
      "Min loss: 0.00954832136631012\n",
      "Mean loss: 0.018682793558885653\n",
      "Std loss: 0.013373726904899866\n",
      "Total Loss: 0.11209676135331392\n",
      "------------------------------------ epoch 4140 (24834 steps) ------------------------------------\n",
      "Max loss: 0.011374211870133877\n",
      "Min loss: 0.00724458321928978\n",
      "Mean loss: 0.009056379863371452\n",
      "Std loss: 0.0016844970376675918\n",
      "Total Loss: 0.05433827918022871\n",
      "------------------------------------ epoch 4141 (24840 steps) ------------------------------------\n",
      "Max loss: 0.031068868935108185\n",
      "Min loss: 0.007887437008321285\n",
      "Mean loss: 0.01681836120163401\n",
      "Std loss: 0.007200366244054222\n",
      "Total Loss: 0.10091016720980406\n",
      "------------------------------------ epoch 4142 (24846 steps) ------------------------------------\n",
      "Max loss: 0.02515585720539093\n",
      "Min loss: 0.008355379104614258\n",
      "Mean loss: 0.012343399226665497\n",
      "Std loss: 0.005893293045973942\n",
      "Total Loss: 0.07406039535999298\n",
      "------------------------------------ epoch 4143 (24852 steps) ------------------------------------\n",
      "Max loss: 0.013283434323966503\n",
      "Min loss: 0.006702043116092682\n",
      "Mean loss: 0.009200232569128275\n",
      "Std loss: 0.0022574194518086067\n",
      "Total Loss: 0.05520139541476965\n",
      "------------------------------------ epoch 4144 (24858 steps) ------------------------------------\n",
      "Max loss: 0.05515701323747635\n",
      "Min loss: 0.007225735113024712\n",
      "Mean loss: 0.020255003900577623\n",
      "Std loss: 0.017604899027340776\n",
      "Total Loss: 0.12153002340346575\n",
      "------------------------------------ epoch 4145 (24864 steps) ------------------------------------\n",
      "Max loss: 0.019834764301776886\n",
      "Min loss: 0.00584276020526886\n",
      "Mean loss: 0.011627769252906242\n",
      "Std loss: 0.00591780003974228\n",
      "Total Loss: 0.06976661551743746\n",
      "------------------------------------ epoch 4146 (24870 steps) ------------------------------------\n",
      "Max loss: 0.012672817334532738\n",
      "Min loss: 0.006755502428859472\n",
      "Mean loss: 0.009246483833218614\n",
      "Std loss: 0.0020535323152923828\n",
      "Total Loss: 0.055478902999311686\n",
      "------------------------------------ epoch 4147 (24876 steps) ------------------------------------\n",
      "Max loss: 0.018803974613547325\n",
      "Min loss: 0.007708599790930748\n",
      "Mean loss: 0.011993647087365389\n",
      "Std loss: 0.004121416051740936\n",
      "Total Loss: 0.07196188252419233\n",
      "------------------------------------ epoch 4148 (24882 steps) ------------------------------------\n",
      "Max loss: 0.03416970372200012\n",
      "Min loss: 0.008716357871890068\n",
      "Mean loss: 0.015950301972528298\n",
      "Std loss: 0.008406789768584586\n",
      "Total Loss: 0.09570181183516979\n",
      "------------------------------------ epoch 4149 (24888 steps) ------------------------------------\n",
      "Max loss: 0.011498842388391495\n",
      "Min loss: 0.00698065897449851\n",
      "Mean loss: 0.009255560580641031\n",
      "Std loss: 0.0018560560584715232\n",
      "Total Loss: 0.05553336348384619\n",
      "------------------------------------ epoch 4150 (24894 steps) ------------------------------------\n",
      "Max loss: 0.017415165901184082\n",
      "Min loss: 0.006567440927028656\n",
      "Mean loss: 0.011567345820367336\n",
      "Std loss: 0.00407244425649788\n",
      "Total Loss: 0.06940407492220402\n",
      "------------------------------------ epoch 4151 (24900 steps) ------------------------------------\n",
      "Max loss: 0.010185189545154572\n",
      "Min loss: 0.007717350497841835\n",
      "Mean loss: 0.008955113589763641\n",
      "Std loss: 0.0009154727680781851\n",
      "Total Loss: 0.05373068153858185\n",
      "------------------------------------ epoch 4152 (24906 steps) ------------------------------------\n",
      "Max loss: 0.040363553911447525\n",
      "Min loss: 0.009788383729755878\n",
      "Mean loss: 0.017189934228857357\n",
      "Std loss: 0.01069624581116287\n",
      "Total Loss: 0.10313960537314415\n",
      "------------------------------------ epoch 4153 (24912 steps) ------------------------------------\n",
      "Max loss: 0.04562348127365112\n",
      "Min loss: 0.005760989151895046\n",
      "Mean loss: 0.016158423541734617\n",
      "Std loss: 0.013746049971552475\n",
      "Total Loss: 0.0969505412504077\n",
      "------------------------------------ epoch 4154 (24918 steps) ------------------------------------\n",
      "Max loss: 0.042297329753637314\n",
      "Min loss: 0.008046934381127357\n",
      "Mean loss: 0.017781687589983147\n",
      "Std loss: 0.01191256055627559\n",
      "Total Loss: 0.10669012553989887\n",
      "------------------------------------ epoch 4155 (24924 steps) ------------------------------------\n",
      "Max loss: 0.03442786633968353\n",
      "Min loss: 0.006593199446797371\n",
      "Mean loss: 0.01573628482098381\n",
      "Std loss: 0.008990666307317595\n",
      "Total Loss: 0.09441770892590284\n",
      "------------------------------------ epoch 4156 (24930 steps) ------------------------------------\n",
      "Max loss: 0.029833894222974777\n",
      "Min loss: 0.008019791916012764\n",
      "Mean loss: 0.013708737989266714\n",
      "Std loss: 0.007423120385067743\n",
      "Total Loss: 0.08225242793560028\n",
      "------------------------------------ epoch 4157 (24936 steps) ------------------------------------\n",
      "Max loss: 0.01818086951971054\n",
      "Min loss: 0.006828955840319395\n",
      "Mean loss: 0.009979674980665246\n",
      "Std loss: 0.003833606840839895\n",
      "Total Loss: 0.05987804988399148\n",
      "------------------------------------ epoch 4158 (24942 steps) ------------------------------------\n",
      "Max loss: 0.022269077599048615\n",
      "Min loss: 0.007512575015425682\n",
      "Mean loss: 0.015185825526714325\n",
      "Std loss: 0.00534044995831822\n",
      "Total Loss: 0.09111495316028595\n",
      "------------------------------------ epoch 4159 (24948 steps) ------------------------------------\n",
      "Max loss: 0.0375397726893425\n",
      "Min loss: 0.008954828605055809\n",
      "Mean loss: 0.01685151333610217\n",
      "Std loss: 0.009598522159072993\n",
      "Total Loss: 0.101109080016613\n",
      "------------------------------------ epoch 4160 (24954 steps) ------------------------------------\n",
      "Max loss: 0.013007985427975655\n",
      "Min loss: 0.007984863594174385\n",
      "Mean loss: 0.010138843053330978\n",
      "Std loss: 0.0019611288011168273\n",
      "Total Loss: 0.06083305831998587\n",
      "------------------------------------ epoch 4161 (24960 steps) ------------------------------------\n",
      "Max loss: 0.02203810214996338\n",
      "Min loss: 0.007696238812059164\n",
      "Mean loss: 0.011702150028819839\n",
      "Std loss: 0.004804268661517107\n",
      "Total Loss: 0.07021290017291903\n",
      "------------------------------------ epoch 4162 (24966 steps) ------------------------------------\n",
      "Max loss: 0.020536202937364578\n",
      "Min loss: 0.006788303609937429\n",
      "Mean loss: 0.014858725713565946\n",
      "Std loss: 0.005299023857771884\n",
      "Total Loss: 0.08915235428139567\n",
      "------------------------------------ epoch 4163 (24972 steps) ------------------------------------\n",
      "Max loss: 0.029725005850195885\n",
      "Min loss: 0.007058675866574049\n",
      "Mean loss: 0.012828066401804486\n",
      "Std loss: 0.00783699638538257\n",
      "Total Loss: 0.07696839841082692\n",
      "------------------------------------ epoch 4164 (24978 steps) ------------------------------------\n",
      "Max loss: 0.036074623465538025\n",
      "Min loss: 0.009783882647752762\n",
      "Mean loss: 0.018948706643035013\n",
      "Std loss: 0.008385881087642842\n",
      "Total Loss: 0.11369223985821009\n",
      "------------------------------------ epoch 4165 (24984 steps) ------------------------------------\n",
      "Max loss: 0.02956458181142807\n",
      "Min loss: 0.007404550909996033\n",
      "Mean loss: 0.01604065935437878\n",
      "Std loss: 0.008572893614928467\n",
      "Total Loss: 0.09624395612627268\n",
      "------------------------------------ epoch 4166 (24990 steps) ------------------------------------\n",
      "Max loss: 0.03039378486573696\n",
      "Min loss: 0.011498494073748589\n",
      "Mean loss: 0.022512779881556828\n",
      "Std loss: 0.006614143959694869\n",
      "Total Loss: 0.13507667928934097\n",
      "------------------------------------ epoch 4167 (24996 steps) ------------------------------------\n",
      "Max loss: 0.02832687459886074\n",
      "Min loss: 0.00900455191731453\n",
      "Mean loss: 0.01442109482983748\n",
      "Std loss: 0.006789919311166194\n",
      "Total Loss: 0.08652656897902489\n",
      "------------------------------------ epoch 4168 (25002 steps) ------------------------------------\n",
      "Max loss: 0.02676127478480339\n",
      "Min loss: 0.008087733760476112\n",
      "Mean loss: 0.0160936345346272\n",
      "Std loss: 0.007433149756638542\n",
      "Total Loss: 0.0965618072077632\n",
      "------------------------------------ epoch 4169 (25008 steps) ------------------------------------\n",
      "Max loss: 0.02549862675368786\n",
      "Min loss: 0.00833679549396038\n",
      "Mean loss: 0.016082042983422678\n",
      "Std loss: 0.0056552159677803575\n",
      "Total Loss: 0.09649225790053606\n",
      "------------------------------------ epoch 4170 (25014 steps) ------------------------------------\n",
      "Max loss: 0.029840795323252678\n",
      "Min loss: 0.010145196691155434\n",
      "Mean loss: 0.015834213079263765\n",
      "Std loss: 0.007529127981947268\n",
      "Total Loss: 0.0950052784755826\n",
      "------------------------------------ epoch 4171 (25020 steps) ------------------------------------\n",
      "Max loss: 0.04464792460203171\n",
      "Min loss: 0.009427543729543686\n",
      "Mean loss: 0.021145998500287533\n",
      "Std loss: 0.011781624674194278\n",
      "Total Loss: 0.1268759910017252\n",
      "------------------------------------ epoch 4172 (25026 steps) ------------------------------------\n",
      "Max loss: 0.02229103446006775\n",
      "Min loss: 0.00906350091099739\n",
      "Mean loss: 0.015260378985355297\n",
      "Std loss: 0.0048642888026271805\n",
      "Total Loss: 0.09156227391213179\n",
      "------------------------------------ epoch 4173 (25032 steps) ------------------------------------\n",
      "Max loss: 0.01930742710828781\n",
      "Min loss: 0.008570711128413677\n",
      "Mean loss: 0.012077522464096546\n",
      "Std loss: 0.0044294914754045185\n",
      "Total Loss: 0.07246513478457928\n",
      "------------------------------------ epoch 4174 (25038 steps) ------------------------------------\n",
      "Max loss: 0.025435850024223328\n",
      "Min loss: 0.009648794308304787\n",
      "Mean loss: 0.016341437585651875\n",
      "Std loss: 0.005580792011770383\n",
      "Total Loss: 0.09804862551391125\n",
      "------------------------------------ epoch 4175 (25044 steps) ------------------------------------\n",
      "Max loss: 0.016338147222995758\n",
      "Min loss: 0.008388514630496502\n",
      "Mean loss: 0.013490172879149517\n",
      "Std loss: 0.003207952609166668\n",
      "Total Loss: 0.0809410372748971\n",
      "------------------------------------ epoch 4176 (25050 steps) ------------------------------------\n",
      "Max loss: 0.04000795632600784\n",
      "Min loss: 0.0068664466962218285\n",
      "Mean loss: 0.01725325562680761\n",
      "Std loss: 0.011283242735875403\n",
      "Total Loss: 0.10351953376084566\n",
      "------------------------------------ epoch 4177 (25056 steps) ------------------------------------\n",
      "Max loss: 0.015903685241937637\n",
      "Min loss: 0.006256185006350279\n",
      "Mean loss: 0.01012007643779119\n",
      "Std loss: 0.0035037347847476255\n",
      "Total Loss: 0.06072045862674713\n",
      "------------------------------------ epoch 4178 (25062 steps) ------------------------------------\n",
      "Max loss: 0.022369738668203354\n",
      "Min loss: 0.006578657776117325\n",
      "Mean loss: 0.01738830531636874\n",
      "Std loss: 0.005056382775350682\n",
      "Total Loss: 0.10432983189821243\n",
      "------------------------------------ epoch 4179 (25068 steps) ------------------------------------\n",
      "Max loss: 0.044986285269260406\n",
      "Min loss: 0.008773669600486755\n",
      "Mean loss: 0.01953925285488367\n",
      "Std loss: 0.012308360249707542\n",
      "Total Loss: 0.11723551712930202\n",
      "------------------------------------ epoch 4180 (25074 steps) ------------------------------------\n",
      "Max loss: 0.01829329878091812\n",
      "Min loss: 0.007372771855443716\n",
      "Mean loss: 0.011186778467769424\n",
      "Std loss: 0.004428568075088303\n",
      "Total Loss: 0.06712067080661654\n",
      "------------------------------------ epoch 4181 (25080 steps) ------------------------------------\n",
      "Max loss: 0.013310838490724564\n",
      "Min loss: 0.006022073328495026\n",
      "Mean loss: 0.008925483872493109\n",
      "Std loss: 0.0026547245257981036\n",
      "Total Loss: 0.05355290323495865\n",
      "------------------------------------ epoch 4182 (25086 steps) ------------------------------------\n",
      "Max loss: 0.01757676899433136\n",
      "Min loss: 0.006791604217141867\n",
      "Mean loss: 0.011331637933229407\n",
      "Std loss: 0.004095234003699291\n",
      "Total Loss: 0.06798982759937644\n",
      "------------------------------------ epoch 4183 (25092 steps) ------------------------------------\n",
      "Max loss: 0.03442874178290367\n",
      "Min loss: 0.007056842558085918\n",
      "Mean loss: 0.015810451082264382\n",
      "Std loss: 0.010621068723273779\n",
      "Total Loss: 0.0948627064935863\n",
      "------------------------------------ epoch 4184 (25098 steps) ------------------------------------\n",
      "Max loss: 0.04596009850502014\n",
      "Min loss: 0.007259230129420757\n",
      "Mean loss: 0.017761285261561472\n",
      "Std loss: 0.013249524359259557\n",
      "Total Loss: 0.10656771156936884\n",
      "------------------------------------ epoch 4185 (25104 steps) ------------------------------------\n",
      "Max loss: 0.02403096668422222\n",
      "Min loss: 0.00902877189218998\n",
      "Mean loss: 0.014263856535156568\n",
      "Std loss: 0.004867939461714731\n",
      "Total Loss: 0.08558313921093941\n",
      "------------------------------------ epoch 4186 (25110 steps) ------------------------------------\n",
      "Max loss: 0.02697669342160225\n",
      "Min loss: 0.007688192185014486\n",
      "Mean loss: 0.014086596279715499\n",
      "Std loss: 0.006495783513964467\n",
      "Total Loss: 0.08451957767829299\n",
      "------------------------------------ epoch 4187 (25116 steps) ------------------------------------\n",
      "Max loss: 0.03345203399658203\n",
      "Min loss: 0.0075197359547019005\n",
      "Mean loss: 0.01440976265196999\n",
      "Std loss: 0.009093269167880079\n",
      "Total Loss: 0.08645857591181993\n",
      "------------------------------------ epoch 4188 (25122 steps) ------------------------------------\n",
      "Max loss: 0.034946467727422714\n",
      "Min loss: 0.007641869597136974\n",
      "Mean loss: 0.016435353085398674\n",
      "Std loss: 0.008966735219107073\n",
      "Total Loss: 0.09861211851239204\n",
      "------------------------------------ epoch 4189 (25128 steps) ------------------------------------\n",
      "Max loss: 0.012632033787667751\n",
      "Min loss: 0.006562334951013327\n",
      "Mean loss: 0.00885208152855436\n",
      "Std loss: 0.0019825613110903365\n",
      "Total Loss: 0.05311248917132616\n",
      "------------------------------------ epoch 4190 (25134 steps) ------------------------------------\n",
      "Max loss: 0.011069269850850105\n",
      "Min loss: 0.006638991646468639\n",
      "Mean loss: 0.00929811162253221\n",
      "Std loss: 0.0014472839657241224\n",
      "Total Loss: 0.05578866973519325\n",
      "------------------------------------ epoch 4191 (25140 steps) ------------------------------------\n",
      "Max loss: 0.019435392692685127\n",
      "Min loss: 0.009026221930980682\n",
      "Mean loss: 0.013788999679187933\n",
      "Std loss: 0.0035299816061500973\n",
      "Total Loss: 0.0827339980751276\n",
      "------------------------------------ epoch 4192 (25146 steps) ------------------------------------\n",
      "Max loss: 0.027936041355133057\n",
      "Min loss: 0.006838317960500717\n",
      "Mean loss: 0.014044894215961298\n",
      "Std loss: 0.006580575964454767\n",
      "Total Loss: 0.08426936529576778\n",
      "------------------------------------ epoch 4193 (25152 steps) ------------------------------------\n",
      "Max loss: 0.03363191708922386\n",
      "Min loss: 0.008233711123466492\n",
      "Mean loss: 0.014982247414688269\n",
      "Std loss: 0.008676087021305078\n",
      "Total Loss: 0.08989348448812962\n",
      "------------------------------------ epoch 4194 (25158 steps) ------------------------------------\n",
      "Max loss: 0.03713660314679146\n",
      "Min loss: 0.0072557032108306885\n",
      "Mean loss: 0.015310914100458225\n",
      "Std loss: 0.010173203043264125\n",
      "Total Loss: 0.09186548460274935\n",
      "------------------------------------ epoch 4195 (25164 steps) ------------------------------------\n",
      "Max loss: 0.06238897889852524\n",
      "Min loss: 0.005145473871380091\n",
      "Mean loss: 0.018976988658929866\n",
      "Std loss: 0.020006418877306045\n",
      "Total Loss: 0.11386193195357919\n",
      "------------------------------------ epoch 4196 (25170 steps) ------------------------------------\n",
      "Max loss: 0.019738757982850075\n",
      "Min loss: 0.008887710981070995\n",
      "Mean loss: 0.011428424157202244\n",
      "Std loss: 0.00374374627001021\n",
      "Total Loss: 0.06857054494321346\n",
      "------------------------------------ epoch 4197 (25176 steps) ------------------------------------\n",
      "Max loss: 0.022916382178664207\n",
      "Min loss: 0.007497066166251898\n",
      "Mean loss: 0.011872145424907407\n",
      "Std loss: 0.005163971115351987\n",
      "Total Loss: 0.07123287254944444\n",
      "------------------------------------ epoch 4198 (25182 steps) ------------------------------------\n",
      "Max loss: 0.015957120805978775\n",
      "Min loss: 0.0082231555134058\n",
      "Mean loss: 0.011764824079970518\n",
      "Std loss: 0.0030041819439455015\n",
      "Total Loss: 0.07058894447982311\n",
      "------------------------------------ epoch 4199 (25188 steps) ------------------------------------\n",
      "Max loss: 0.021114416420459747\n",
      "Min loss: 0.007677447982132435\n",
      "Mean loss: 0.01229911313081781\n",
      "Std loss: 0.004448290450302691\n",
      "Total Loss: 0.07379467878490686\n",
      "------------------------------------ epoch 4200 (25194 steps) ------------------------------------\n",
      "Max loss: 0.02593599073588848\n",
      "Min loss: 0.008570119738578796\n",
      "Mean loss: 0.016009067340443533\n",
      "Std loss: 0.0055420476857306975\n",
      "Total Loss: 0.09605440404266119\n",
      "------------------------------------ epoch 4201 (25200 steps) ------------------------------------\n",
      "Max loss: 0.021075015887618065\n",
      "Min loss: 0.0065032849088311195\n",
      "Mean loss: 0.01436899338538448\n",
      "Std loss: 0.005621191796239587\n",
      "Total Loss: 0.08621396031230688\n",
      "saved model at ./weights/model_4201.pth\n",
      "------------------------------------ epoch 4202 (25206 steps) ------------------------------------\n",
      "Max loss: 0.02484661340713501\n",
      "Min loss: 0.008054332807660103\n",
      "Mean loss: 0.014222153772910437\n",
      "Std loss: 0.005753578668164969\n",
      "Total Loss: 0.08533292263746262\n",
      "------------------------------------ epoch 4203 (25212 steps) ------------------------------------\n",
      "Max loss: 0.056470394134521484\n",
      "Min loss: 0.0069580404087901115\n",
      "Mean loss: 0.016561754668752354\n",
      "Std loss: 0.017871640742195848\n",
      "Total Loss: 0.09937052801251411\n",
      "------------------------------------ epoch 4204 (25218 steps) ------------------------------------\n",
      "Max loss: 0.019643202424049377\n",
      "Min loss: 0.007469989359378815\n",
      "Mean loss: 0.01236734710012873\n",
      "Std loss: 0.004656017255750519\n",
      "Total Loss: 0.07420408260077238\n",
      "------------------------------------ epoch 4205 (25224 steps) ------------------------------------\n",
      "Max loss: 0.020489176735281944\n",
      "Min loss: 0.006734314374625683\n",
      "Mean loss: 0.013841142257054647\n",
      "Std loss: 0.0054560799942292414\n",
      "Total Loss: 0.08304685354232788\n",
      "------------------------------------ epoch 4206 (25230 steps) ------------------------------------\n",
      "Max loss: 0.018546616658568382\n",
      "Min loss: 0.006807405035942793\n",
      "Mean loss: 0.010298945552979907\n",
      "Std loss: 0.004029742259254694\n",
      "Total Loss: 0.06179367331787944\n",
      "------------------------------------ epoch 4207 (25236 steps) ------------------------------------\n",
      "Max loss: 0.009333403781056404\n",
      "Min loss: 0.006159546785056591\n",
      "Mean loss: 0.007497770789389809\n",
      "Std loss: 0.0013032012318436898\n",
      "Total Loss: 0.044986624736338854\n",
      "------------------------------------ epoch 4208 (25242 steps) ------------------------------------\n",
      "Max loss: 0.024820759892463684\n",
      "Min loss: 0.010718236677348614\n",
      "Mean loss: 0.015297477909674248\n",
      "Std loss: 0.004812786625611749\n",
      "Total Loss: 0.09178486745804548\n",
      "------------------------------------ epoch 4209 (25248 steps) ------------------------------------\n",
      "Max loss: 0.024578571319580078\n",
      "Min loss: 0.007119837217032909\n",
      "Mean loss: 0.012580174952745438\n",
      "Std loss: 0.00596159436084344\n",
      "Total Loss: 0.07548104971647263\n",
      "------------------------------------ epoch 4210 (25254 steps) ------------------------------------\n",
      "Max loss: 0.02320743165910244\n",
      "Min loss: 0.007059997878968716\n",
      "Mean loss: 0.014114280076076588\n",
      "Std loss: 0.006264483693729402\n",
      "Total Loss: 0.08468568045645952\n",
      "------------------------------------ epoch 4211 (25260 steps) ------------------------------------\n",
      "Max loss: 0.008876549080014229\n",
      "Min loss: 0.005667325109243393\n",
      "Mean loss: 0.007429569649199645\n",
      "Std loss: 0.0010686127508290346\n",
      "Total Loss: 0.04457741789519787\n",
      "------------------------------------ epoch 4212 (25266 steps) ------------------------------------\n",
      "Max loss: 0.021274669095873833\n",
      "Min loss: 0.006279184482991695\n",
      "Mean loss: 0.015115829029430946\n",
      "Std loss: 0.004929974640088446\n",
      "Total Loss: 0.09069497417658567\n",
      "------------------------------------ epoch 4213 (25272 steps) ------------------------------------\n",
      "Max loss: 0.03561270236968994\n",
      "Min loss: 0.006652479525655508\n",
      "Mean loss: 0.021550048685943086\n",
      "Std loss: 0.009615939510675329\n",
      "Total Loss: 0.12930029211565852\n",
      "------------------------------------ epoch 4214 (25278 steps) ------------------------------------\n",
      "Max loss: 0.017312288284301758\n",
      "Min loss: 0.006490736734122038\n",
      "Mean loss: 0.010363114609693488\n",
      "Std loss: 0.003929913480946119\n",
      "Total Loss: 0.062178687658160925\n",
      "------------------------------------ epoch 4215 (25284 steps) ------------------------------------\n",
      "Max loss: 0.019737854599952698\n",
      "Min loss: 0.006971972994506359\n",
      "Mean loss: 0.010803822583208481\n",
      "Std loss: 0.004272701678846583\n",
      "Total Loss: 0.06482293549925089\n",
      "------------------------------------ epoch 4216 (25290 steps) ------------------------------------\n",
      "Max loss: 0.040315285325050354\n",
      "Min loss: 0.006319560110569\n",
      "Mean loss: 0.019608695370455582\n",
      "Std loss: 0.012317679329539592\n",
      "Total Loss: 0.1176521722227335\n",
      "------------------------------------ epoch 4217 (25296 steps) ------------------------------------\n",
      "Max loss: 0.02822994813323021\n",
      "Min loss: 0.005790017545223236\n",
      "Mean loss: 0.01175923109985888\n",
      "Std loss: 0.008007407185735022\n",
      "Total Loss: 0.07055538659915328\n",
      "------------------------------------ epoch 4218 (25302 steps) ------------------------------------\n",
      "Max loss: 0.01805640384554863\n",
      "Min loss: 0.007028342690318823\n",
      "Mean loss: 0.012013871145124236\n",
      "Std loss: 0.004486419119572102\n",
      "Total Loss: 0.07208322687074542\n",
      "------------------------------------ epoch 4219 (25308 steps) ------------------------------------\n",
      "Max loss: 0.015540480613708496\n",
      "Min loss: 0.006128467619419098\n",
      "Mean loss: 0.0089840036816895\n",
      "Std loss: 0.003081168459662444\n",
      "Total Loss: 0.053904022090137005\n",
      "------------------------------------ epoch 4220 (25314 steps) ------------------------------------\n",
      "Max loss: 0.014509772881865501\n",
      "Min loss: 0.005654636770486832\n",
      "Mean loss: 0.008364298070470491\n",
      "Std loss: 0.002908352370443552\n",
      "Total Loss: 0.05018578842282295\n",
      "------------------------------------ epoch 4221 (25320 steps) ------------------------------------\n",
      "Max loss: 0.013798508793115616\n",
      "Min loss: 0.00656106835231185\n",
      "Mean loss: 0.00908874262434741\n",
      "Std loss: 0.0023443971160059425\n",
      "Total Loss: 0.05453245574608445\n",
      "------------------------------------ epoch 4222 (25326 steps) ------------------------------------\n",
      "Max loss: 0.020588304847478867\n",
      "Min loss: 0.00877358578145504\n",
      "Mean loss: 0.011827677488327026\n",
      "Std loss: 0.004214333537329793\n",
      "Total Loss: 0.07096606492996216\n",
      "------------------------------------ epoch 4223 (25332 steps) ------------------------------------\n",
      "Max loss: 0.028151482343673706\n",
      "Min loss: 0.005059023387730122\n",
      "Mean loss: 0.016028460736076038\n",
      "Std loss: 0.007826159720876913\n",
      "Total Loss: 0.09617076441645622\n",
      "------------------------------------ epoch 4224 (25338 steps) ------------------------------------\n",
      "Max loss: 0.01778509095311165\n",
      "Min loss: 0.007333084009587765\n",
      "Mean loss: 0.012085287986944119\n",
      "Std loss: 0.003805643201912727\n",
      "Total Loss: 0.07251172792166471\n",
      "------------------------------------ epoch 4225 (25344 steps) ------------------------------------\n",
      "Max loss: 0.014469625428318977\n",
      "Min loss: 0.0074721514247357845\n",
      "Mean loss: 0.011665137717500329\n",
      "Std loss: 0.002717961079468641\n",
      "Total Loss: 0.06999082630500197\n",
      "------------------------------------ epoch 4226 (25350 steps) ------------------------------------\n",
      "Max loss: 0.025625361129641533\n",
      "Min loss: 0.005631269421428442\n",
      "Mean loss: 0.011502904817461967\n",
      "Std loss: 0.0065840132404121595\n",
      "Total Loss: 0.0690174289047718\n",
      "------------------------------------ epoch 4227 (25356 steps) ------------------------------------\n",
      "Max loss: 0.016499044373631477\n",
      "Min loss: 0.005422211717814207\n",
      "Mean loss: 0.010787126841023564\n",
      "Std loss: 0.003934015418754509\n",
      "Total Loss: 0.06472276104614139\n",
      "------------------------------------ epoch 4228 (25362 steps) ------------------------------------\n",
      "Max loss: 0.017077382653951645\n",
      "Min loss: 0.005277623422443867\n",
      "Mean loss: 0.012691946544994911\n",
      "Std loss: 0.004709841879192447\n",
      "Total Loss: 0.07615167926996946\n",
      "------------------------------------ epoch 4229 (25368 steps) ------------------------------------\n",
      "Max loss: 0.019014474004507065\n",
      "Min loss: 0.007564707659184933\n",
      "Mean loss: 0.012834370912363132\n",
      "Std loss: 0.003885813902996002\n",
      "Total Loss: 0.07700622547417879\n",
      "------------------------------------ epoch 4230 (25374 steps) ------------------------------------\n",
      "Max loss: 0.01644357666373253\n",
      "Min loss: 0.007318367715924978\n",
      "Mean loss: 0.01110288694811364\n",
      "Std loss: 0.003637594680455469\n",
      "Total Loss: 0.06661732168868184\n",
      "------------------------------------ epoch 4231 (25380 steps) ------------------------------------\n",
      "Max loss: 0.043727807700634\n",
      "Min loss: 0.006663747131824493\n",
      "Mean loss: 0.01715421552459399\n",
      "Std loss: 0.01237960144380227\n",
      "Total Loss: 0.10292529314756393\n",
      "------------------------------------ epoch 4232 (25386 steps) ------------------------------------\n",
      "Max loss: 0.013496743515133858\n",
      "Min loss: 0.006097885780036449\n",
      "Mean loss: 0.010077250810960928\n",
      "Std loss: 0.002465999633560965\n",
      "Total Loss: 0.06046350486576557\n",
      "------------------------------------ epoch 4233 (25392 steps) ------------------------------------\n",
      "Max loss: 0.028869306668639183\n",
      "Min loss: 0.006334972567856312\n",
      "Mean loss: 0.01596056856215\n",
      "Std loss: 0.00898489458247911\n",
      "Total Loss: 0.09576341137290001\n",
      "------------------------------------ epoch 4234 (25398 steps) ------------------------------------\n",
      "Max loss: 0.01780599169433117\n",
      "Min loss: 0.006978115998208523\n",
      "Mean loss: 0.011742719914764166\n",
      "Std loss: 0.0038020951925277478\n",
      "Total Loss: 0.070456319488585\n",
      "------------------------------------ epoch 4235 (25404 steps) ------------------------------------\n",
      "Max loss: 0.020399993285536766\n",
      "Min loss: 0.006740149110555649\n",
      "Mean loss: 0.011647825750211874\n",
      "Std loss: 0.00480282877390264\n",
      "Total Loss: 0.06988695450127125\n",
      "------------------------------------ epoch 4236 (25410 steps) ------------------------------------\n",
      "Max loss: 0.01692686788737774\n",
      "Min loss: 0.0059315818361938\n",
      "Mean loss: 0.010647409517938891\n",
      "Std loss: 0.0036534279371190337\n",
      "Total Loss: 0.06388445710763335\n",
      "------------------------------------ epoch 4237 (25416 steps) ------------------------------------\n",
      "Max loss: 0.011762605980038643\n",
      "Min loss: 0.007439854554831982\n",
      "Mean loss: 0.00892598181962967\n",
      "Std loss: 0.0013981170664179403\n",
      "Total Loss: 0.053555890917778015\n",
      "------------------------------------ epoch 4238 (25422 steps) ------------------------------------\n",
      "Max loss: 0.022371066734194756\n",
      "Min loss: 0.007463719230145216\n",
      "Mean loss: 0.013247874798253179\n",
      "Std loss: 0.005211461696995662\n",
      "Total Loss: 0.07948724878951907\n",
      "------------------------------------ epoch 4239 (25428 steps) ------------------------------------\n",
      "Max loss: 0.014879381284117699\n",
      "Min loss: 0.006653040647506714\n",
      "Mean loss: 0.010507499799132347\n",
      "Std loss: 0.0028331147460913283\n",
      "Total Loss: 0.06304499879479408\n",
      "------------------------------------ epoch 4240 (25434 steps) ------------------------------------\n",
      "Max loss: 0.02934352122247219\n",
      "Min loss: 0.006306362804025412\n",
      "Mean loss: 0.011693358654156327\n",
      "Std loss: 0.008049458482055535\n",
      "Total Loss: 0.07016015192493796\n",
      "------------------------------------ epoch 4241 (25440 steps) ------------------------------------\n",
      "Max loss: 0.020489156246185303\n",
      "Min loss: 0.007759777829051018\n",
      "Mean loss: 0.01537311046073834\n",
      "Std loss: 0.004251784817620905\n",
      "Total Loss: 0.09223866276443005\n",
      "------------------------------------ epoch 4242 (25446 steps) ------------------------------------\n",
      "Max loss: 0.03659422695636749\n",
      "Min loss: 0.009845275431871414\n",
      "Mean loss: 0.017903299691776436\n",
      "Std loss: 0.008716167260835473\n",
      "Total Loss: 0.10741979815065861\n",
      "------------------------------------ epoch 4243 (25452 steps) ------------------------------------\n",
      "Max loss: 0.01110523846000433\n",
      "Min loss: 0.006428116466850042\n",
      "Mean loss: 0.009596338883663217\n",
      "Std loss: 0.0016691991613786874\n",
      "Total Loss: 0.0575780333019793\n",
      "------------------------------------ epoch 4244 (25458 steps) ------------------------------------\n",
      "Max loss: 0.014018992893397808\n",
      "Min loss: 0.005966467317193747\n",
      "Mean loss: 0.008863300085067749\n",
      "Std loss: 0.0026810806863786767\n",
      "Total Loss: 0.053179800510406494\n",
      "------------------------------------ epoch 4245 (25464 steps) ------------------------------------\n",
      "Max loss: 0.01039443351328373\n",
      "Min loss: 0.006332974880933762\n",
      "Mean loss: 0.00801569347580274\n",
      "Std loss: 0.0011998342402399686\n",
      "Total Loss: 0.04809416085481644\n",
      "------------------------------------ epoch 4246 (25470 steps) ------------------------------------\n",
      "Max loss: 0.016313010826706886\n",
      "Min loss: 0.0059065669775009155\n",
      "Mean loss: 0.009029287612065673\n",
      "Std loss: 0.003591458066865431\n",
      "Total Loss: 0.05417572567239404\n",
      "------------------------------------ epoch 4247 (25476 steps) ------------------------------------\n",
      "Max loss: 0.03894936293363571\n",
      "Min loss: 0.005882515572011471\n",
      "Mean loss: 0.014878615038469434\n",
      "Std loss: 0.01173044907653314\n",
      "Total Loss: 0.0892716902308166\n",
      "------------------------------------ epoch 4248 (25482 steps) ------------------------------------\n",
      "Max loss: 0.012769227847456932\n",
      "Min loss: 0.0076423464342951775\n",
      "Mean loss: 0.011120398994535208\n",
      "Std loss: 0.001726009088225305\n",
      "Total Loss: 0.06672239396721125\n",
      "------------------------------------ epoch 4249 (25488 steps) ------------------------------------\n",
      "Max loss: 0.03655710816383362\n",
      "Min loss: 0.006776156835258007\n",
      "Mean loss: 0.012455991779764494\n",
      "Std loss: 0.010786446743505734\n",
      "Total Loss: 0.07473595067858696\n",
      "------------------------------------ epoch 4250 (25494 steps) ------------------------------------\n",
      "Max loss: 0.017508311197161674\n",
      "Min loss: 0.008294722996652126\n",
      "Mean loss: 0.0118659862006704\n",
      "Std loss: 0.0031755858064823773\n",
      "Total Loss: 0.07119591720402241\n",
      "------------------------------------ epoch 4251 (25500 steps) ------------------------------------\n",
      "Max loss: 0.04227791726589203\n",
      "Min loss: 0.008382073603570461\n",
      "Mean loss: 0.015682786392668884\n",
      "Std loss: 0.012111201806163194\n",
      "Total Loss: 0.0940967183560133\n",
      "------------------------------------ epoch 4252 (25506 steps) ------------------------------------\n",
      "Max loss: 0.034385617822408676\n",
      "Min loss: 0.00691720936447382\n",
      "Mean loss: 0.014725849808504185\n",
      "Std loss: 0.00934523377097174\n",
      "Total Loss: 0.0883550988510251\n",
      "------------------------------------ epoch 4253 (25512 steps) ------------------------------------\n",
      "Max loss: 0.033413246273994446\n",
      "Min loss: 0.006078177131712437\n",
      "Mean loss: 0.014892325270920992\n",
      "Std loss: 0.009124320280036\n",
      "Total Loss: 0.08935395162552595\n",
      "------------------------------------ epoch 4254 (25518 steps) ------------------------------------\n",
      "Max loss: 0.03120758756995201\n",
      "Min loss: 0.007215577177703381\n",
      "Mean loss: 0.013398732369144758\n",
      "Std loss: 0.008292476161105776\n",
      "Total Loss: 0.08039239421486855\n",
      "------------------------------------ epoch 4255 (25524 steps) ------------------------------------\n",
      "Max loss: 0.01921919360756874\n",
      "Min loss: 0.005820276215672493\n",
      "Mean loss: 0.011024475252876679\n",
      "Std loss: 0.0053072023309321025\n",
      "Total Loss: 0.06614685151726007\n",
      "------------------------------------ epoch 4256 (25530 steps) ------------------------------------\n",
      "Max loss: 0.018086550757288933\n",
      "Min loss: 0.0067613385617733\n",
      "Mean loss: 0.013638417702168226\n",
      "Std loss: 0.0036393271787920154\n",
      "Total Loss: 0.08183050621300936\n",
      "------------------------------------ epoch 4257 (25536 steps) ------------------------------------\n",
      "Max loss: 0.05054871365427971\n",
      "Min loss: 0.007557167671620846\n",
      "Mean loss: 0.018535226893921692\n",
      "Std loss: 0.015423865186301301\n",
      "Total Loss: 0.11121136136353016\n",
      "------------------------------------ epoch 4258 (25542 steps) ------------------------------------\n",
      "Max loss: 0.018982062116265297\n",
      "Min loss: 0.006341010332107544\n",
      "Mean loss: 0.011202423212428888\n",
      "Std loss: 0.005008527342526718\n",
      "Total Loss: 0.06721453927457333\n",
      "------------------------------------ epoch 4259 (25548 steps) ------------------------------------\n",
      "Max loss: 0.012452821247279644\n",
      "Min loss: 0.0064924354664981365\n",
      "Mean loss: 0.009181809999669591\n",
      "Std loss: 0.002467216160316324\n",
      "Total Loss: 0.05509085999801755\n",
      "------------------------------------ epoch 4260 (25554 steps) ------------------------------------\n",
      "Max loss: 0.0507151298224926\n",
      "Min loss: 0.007416429929435253\n",
      "Mean loss: 0.018735589769979317\n",
      "Std loss: 0.01503649106344952\n",
      "Total Loss: 0.11241353861987591\n",
      "------------------------------------ epoch 4261 (25560 steps) ------------------------------------\n",
      "Max loss: 0.03872251883149147\n",
      "Min loss: 0.006090652197599411\n",
      "Mean loss: 0.014345396310091019\n",
      "Std loss: 0.011187799592829149\n",
      "Total Loss: 0.08607237786054611\n",
      "------------------------------------ epoch 4262 (25566 steps) ------------------------------------\n",
      "Max loss: 0.016370553523302078\n",
      "Min loss: 0.007150582037866116\n",
      "Mean loss: 0.012106971660008034\n",
      "Std loss: 0.0029875208001673564\n",
      "Total Loss: 0.0726418299600482\n",
      "------------------------------------ epoch 4263 (25572 steps) ------------------------------------\n",
      "Max loss: 0.011836755089461803\n",
      "Min loss: 0.008192583918571472\n",
      "Mean loss: 0.010102529078722\n",
      "Std loss: 0.0011507693032761339\n",
      "Total Loss: 0.060615174472332\n",
      "------------------------------------ epoch 4264 (25578 steps) ------------------------------------\n",
      "Max loss: 0.023816263303160667\n",
      "Min loss: 0.007023563142865896\n",
      "Mean loss: 0.013539203365022937\n",
      "Std loss: 0.005489135896984944\n",
      "Total Loss: 0.08123522019013762\n",
      "------------------------------------ epoch 4265 (25584 steps) ------------------------------------\n",
      "Max loss: 0.039574041962623596\n",
      "Min loss: 0.005720357410609722\n",
      "Mean loss: 0.013228082874168953\n",
      "Std loss: 0.011836472809178397\n",
      "Total Loss: 0.07936849724501371\n",
      "------------------------------------ epoch 4266 (25590 steps) ------------------------------------\n",
      "Max loss: 0.03557851165533066\n",
      "Min loss: 0.0058771762996912\n",
      "Mean loss: 0.015554934662456313\n",
      "Std loss: 0.01024325816599316\n",
      "Total Loss: 0.09332960797473788\n",
      "------------------------------------ epoch 4267 (25596 steps) ------------------------------------\n",
      "Max loss: 0.029187489300966263\n",
      "Min loss: 0.009480534121394157\n",
      "Mean loss: 0.017317892983555794\n",
      "Std loss: 0.006621496089719541\n",
      "Total Loss: 0.10390735790133476\n",
      "------------------------------------ epoch 4268 (25602 steps) ------------------------------------\n",
      "Max loss: 0.028543610125780106\n",
      "Min loss: 0.007950290106236935\n",
      "Mean loss: 0.016500271391123533\n",
      "Std loss: 0.0087400835099229\n",
      "Total Loss: 0.0990016283467412\n",
      "------------------------------------ epoch 4269 (25608 steps) ------------------------------------\n",
      "Max loss: 0.037978120148181915\n",
      "Min loss: 0.008344168774783611\n",
      "Mean loss: 0.017059088374177616\n",
      "Std loss: 0.009670473918534988\n",
      "Total Loss: 0.10235453024506569\n",
      "------------------------------------ epoch 4270 (25614 steps) ------------------------------------\n",
      "Max loss: 0.04218047857284546\n",
      "Min loss: 0.007334668189287186\n",
      "Mean loss: 0.01698348787613213\n",
      "Std loss: 0.01266114218440225\n",
      "Total Loss: 0.10190092725679278\n",
      "------------------------------------ epoch 4271 (25620 steps) ------------------------------------\n",
      "Max loss: 0.025147875770926476\n",
      "Min loss: 0.007843418046832085\n",
      "Mean loss: 0.014539434729764858\n",
      "Std loss: 0.005517461818960519\n",
      "Total Loss: 0.08723660837858915\n",
      "------------------------------------ epoch 4272 (25626 steps) ------------------------------------\n",
      "Max loss: 0.02068614587187767\n",
      "Min loss: 0.006610089913010597\n",
      "Mean loss: 0.012735583120957017\n",
      "Std loss: 0.005553035036359761\n",
      "Total Loss: 0.0764134987257421\n",
      "------------------------------------ epoch 4273 (25632 steps) ------------------------------------\n",
      "Max loss: 0.016755666583776474\n",
      "Min loss: 0.006107244174927473\n",
      "Mean loss: 0.009092101206382116\n",
      "Std loss: 0.003519436959513566\n",
      "Total Loss: 0.054552607238292694\n",
      "------------------------------------ epoch 4274 (25638 steps) ------------------------------------\n",
      "Max loss: 0.05053577572107315\n",
      "Min loss: 0.00686529278755188\n",
      "Mean loss: 0.019714595439533394\n",
      "Std loss: 0.014472525173443522\n",
      "Total Loss: 0.11828757263720036\n",
      "------------------------------------ epoch 4275 (25644 steps) ------------------------------------\n",
      "Max loss: 0.016766570508480072\n",
      "Min loss: 0.008610434830188751\n",
      "Mean loss: 0.010975468748559555\n",
      "Std loss: 0.0027778584196012338\n",
      "Total Loss: 0.06585281249135733\n",
      "------------------------------------ epoch 4276 (25650 steps) ------------------------------------\n",
      "Max loss: 0.014940237626433372\n",
      "Min loss: 0.0072670746594667435\n",
      "Mean loss: 0.011526347448428472\n",
      "Std loss: 0.0026201375662697705\n",
      "Total Loss: 0.06915808469057083\n",
      "------------------------------------ epoch 4277 (25656 steps) ------------------------------------\n",
      "Max loss: 0.016436543315649033\n",
      "Min loss: 0.0064459084533154964\n",
      "Mean loss: 0.011066402851914367\n",
      "Std loss: 0.0033627968057277848\n",
      "Total Loss: 0.0663984171114862\n",
      "------------------------------------ epoch 4278 (25662 steps) ------------------------------------\n",
      "Max loss: 0.024032164365053177\n",
      "Min loss: 0.005634302273392677\n",
      "Mean loss: 0.010825934897487363\n",
      "Std loss: 0.006223321124250323\n",
      "Total Loss: 0.06495560938492417\n",
      "------------------------------------ epoch 4279 (25668 steps) ------------------------------------\n",
      "Max loss: 0.01660045050084591\n",
      "Min loss: 0.0072768679820001125\n",
      "Mean loss: 0.010067575999225179\n",
      "Std loss: 0.003583407235382623\n",
      "Total Loss: 0.060405455995351076\n",
      "------------------------------------ epoch 4280 (25674 steps) ------------------------------------\n",
      "Max loss: 0.025729335844516754\n",
      "Min loss: 0.005598397459834814\n",
      "Mean loss: 0.012578836486985287\n",
      "Std loss: 0.007162889846407896\n",
      "Total Loss: 0.07547301892191172\n",
      "------------------------------------ epoch 4281 (25680 steps) ------------------------------------\n",
      "Max loss: 0.01489043515175581\n",
      "Min loss: 0.006928853690624237\n",
      "Mean loss: 0.011156934003035227\n",
      "Std loss: 0.003454867792403313\n",
      "Total Loss: 0.06694160401821136\n",
      "------------------------------------ epoch 4282 (25686 steps) ------------------------------------\n",
      "Max loss: 0.03404129296541214\n",
      "Min loss: 0.00701790489256382\n",
      "Mean loss: 0.015011725947260857\n",
      "Std loss: 0.009082208139147143\n",
      "Total Loss: 0.09007035568356514\n",
      "------------------------------------ epoch 4283 (25692 steps) ------------------------------------\n",
      "Max loss: 0.045398205518722534\n",
      "Min loss: 0.005787956528365612\n",
      "Mean loss: 0.01582255307585001\n",
      "Std loss: 0.013406605335950023\n",
      "Total Loss: 0.09493531845510006\n",
      "------------------------------------ epoch 4284 (25698 steps) ------------------------------------\n",
      "Max loss: 0.02559736929833889\n",
      "Min loss: 0.006019511725753546\n",
      "Mean loss: 0.013672476711993417\n",
      "Std loss: 0.007103306546408509\n",
      "Total Loss: 0.0820348602719605\n",
      "------------------------------------ epoch 4285 (25704 steps) ------------------------------------\n",
      "Max loss: 0.0326312780380249\n",
      "Min loss: 0.007048299070447683\n",
      "Mean loss: 0.014380178957556685\n",
      "Std loss: 0.0094013771866277\n",
      "Total Loss: 0.08628107374534011\n",
      "------------------------------------ epoch 4286 (25710 steps) ------------------------------------\n",
      "Max loss: 0.03308381885290146\n",
      "Min loss: 0.007675906177610159\n",
      "Mean loss: 0.01903366988214354\n",
      "Std loss: 0.010008832808380946\n",
      "Total Loss: 0.11420201929286122\n",
      "------------------------------------ epoch 4287 (25716 steps) ------------------------------------\n",
      "Max loss: 0.0350765660405159\n",
      "Min loss: 0.006499074399471283\n",
      "Mean loss: 0.013608737538258234\n",
      "Std loss: 0.009911948540728775\n",
      "Total Loss: 0.08165242522954941\n",
      "------------------------------------ epoch 4288 (25722 steps) ------------------------------------\n",
      "Max loss: 0.031151391565799713\n",
      "Min loss: 0.008109468966722488\n",
      "Mean loss: 0.015267146285623312\n",
      "Std loss: 0.008402676043277645\n",
      "Total Loss: 0.09160287771373987\n",
      "------------------------------------ epoch 4289 (25728 steps) ------------------------------------\n",
      "Max loss: 0.04812134802341461\n",
      "Min loss: 0.009104056283831596\n",
      "Mean loss: 0.01733699720352888\n",
      "Std loss: 0.013917695050726469\n",
      "Total Loss: 0.10402198322117329\n",
      "------------------------------------ epoch 4290 (25734 steps) ------------------------------------\n",
      "Max loss: 0.03751049190759659\n",
      "Min loss: 0.011570176109671593\n",
      "Mean loss: 0.01752654354398449\n",
      "Std loss: 0.009109136120954459\n",
      "Total Loss: 0.10515926126390696\n",
      "------------------------------------ epoch 4291 (25740 steps) ------------------------------------\n",
      "Max loss: 0.013116473332047462\n",
      "Min loss: 0.006435954011976719\n",
      "Mean loss: 0.011379600347330173\n",
      "Std loss: 0.0022873072972620753\n",
      "Total Loss: 0.06827760208398104\n",
      "------------------------------------ epoch 4292 (25746 steps) ------------------------------------\n",
      "Max loss: 0.02639583870768547\n",
      "Min loss: 0.0073889195919036865\n",
      "Mean loss: 0.01566887383038799\n",
      "Std loss: 0.006242039555437724\n",
      "Total Loss: 0.09401324298232794\n",
      "------------------------------------ epoch 4293 (25752 steps) ------------------------------------\n",
      "Max loss: 0.021196147426962852\n",
      "Min loss: 0.0066404580138623714\n",
      "Mean loss: 0.014191093776995936\n",
      "Std loss: 0.0052792839642780065\n",
      "Total Loss: 0.08514656266197562\n",
      "------------------------------------ epoch 4294 (25758 steps) ------------------------------------\n",
      "Max loss: 0.019162800163030624\n",
      "Min loss: 0.009180048480629921\n",
      "Mean loss: 0.012570764093349377\n",
      "Std loss: 0.0035640636753391177\n",
      "Total Loss: 0.07542458456009626\n",
      "------------------------------------ epoch 4295 (25764 steps) ------------------------------------\n",
      "Max loss: 0.024166030809283257\n",
      "Min loss: 0.008929445408284664\n",
      "Mean loss: 0.013137506476293007\n",
      "Std loss: 0.00521363297221441\n",
      "Total Loss: 0.07882503885775805\n",
      "------------------------------------ epoch 4296 (25770 steps) ------------------------------------\n",
      "Max loss: 0.0592346154153347\n",
      "Min loss: 0.007386025041341782\n",
      "Mean loss: 0.01996452718352278\n",
      "Std loss: 0.018064427290351008\n",
      "Total Loss: 0.11978716310113668\n",
      "------------------------------------ epoch 4297 (25776 steps) ------------------------------------\n",
      "Max loss: 0.017706481739878654\n",
      "Min loss: 0.0064896224066615105\n",
      "Mean loss: 0.011752784873048464\n",
      "Std loss: 0.004853720966435245\n",
      "Total Loss: 0.07051670923829079\n",
      "------------------------------------ epoch 4298 (25782 steps) ------------------------------------\n",
      "Max loss: 0.0267295204102993\n",
      "Min loss: 0.005840539000928402\n",
      "Mean loss: 0.012454005889594555\n",
      "Std loss: 0.007536260497225501\n",
      "Total Loss: 0.07472403533756733\n",
      "------------------------------------ epoch 4299 (25788 steps) ------------------------------------\n",
      "Max loss: 0.050127848982810974\n",
      "Min loss: 0.007978533394634724\n",
      "Mean loss: 0.016966585845996935\n",
      "Std loss: 0.015071479974514744\n",
      "Total Loss: 0.10179951507598162\n",
      "------------------------------------ epoch 4300 (25794 steps) ------------------------------------\n",
      "Max loss: 0.03748767450451851\n",
      "Min loss: 0.007040801458060741\n",
      "Mean loss: 0.018276679795235395\n",
      "Std loss: 0.010354481432055637\n",
      "Total Loss: 0.10966007877141237\n",
      "------------------------------------ epoch 4301 (25800 steps) ------------------------------------\n",
      "Max loss: 0.03781818598508835\n",
      "Min loss: 0.007946640253067017\n",
      "Mean loss: 0.021932707478602726\n",
      "Std loss: 0.010398536056231193\n",
      "Total Loss: 0.13159624487161636\n",
      "saved model at ./weights/model_4301.pth\n",
      "------------------------------------ epoch 4302 (25806 steps) ------------------------------------\n",
      "Max loss: 0.017844734713435173\n",
      "Min loss: 0.008400933817029\n",
      "Mean loss: 0.012559758188823858\n",
      "Std loss: 0.003950842006063181\n",
      "Total Loss: 0.07535854913294315\n",
      "------------------------------------ epoch 4303 (25812 steps) ------------------------------------\n",
      "Max loss: 0.0333675853908062\n",
      "Min loss: 0.009494039230048656\n",
      "Mean loss: 0.015117154146234194\n",
      "Std loss: 0.008256570789132636\n",
      "Total Loss: 0.09070292487740517\n",
      "------------------------------------ epoch 4304 (25818 steps) ------------------------------------\n",
      "Max loss: 0.025495026260614395\n",
      "Min loss: 0.011391118168830872\n",
      "Mean loss: 0.015729966418196756\n",
      "Std loss: 0.004674274204379465\n",
      "Total Loss: 0.09437979850918055\n",
      "------------------------------------ epoch 4305 (25824 steps) ------------------------------------\n",
      "Max loss: 0.014386221766471863\n",
      "Min loss: 0.007396522909402847\n",
      "Mean loss: 0.011025095824152231\n",
      "Std loss: 0.0024985772939139397\n",
      "Total Loss: 0.06615057494491339\n",
      "------------------------------------ epoch 4306 (25830 steps) ------------------------------------\n",
      "Max loss: 0.020774420350790024\n",
      "Min loss: 0.00692433025687933\n",
      "Mean loss: 0.01111571645985047\n",
      "Std loss: 0.004598996379357391\n",
      "Total Loss: 0.06669429875910282\n",
      "------------------------------------ epoch 4307 (25836 steps) ------------------------------------\n",
      "Max loss: 0.011733449064195156\n",
      "Min loss: 0.006215560715645552\n",
      "Mean loss: 0.008860380699237188\n",
      "Std loss: 0.0017430126593833545\n",
      "Total Loss: 0.053162284195423126\n",
      "------------------------------------ epoch 4308 (25842 steps) ------------------------------------\n",
      "Max loss: 0.020905381068587303\n",
      "Min loss: 0.009327122941613197\n",
      "Mean loss: 0.013232250697910786\n",
      "Std loss: 0.003970335677107724\n",
      "Total Loss: 0.07939350418746471\n",
      "------------------------------------ epoch 4309 (25848 steps) ------------------------------------\n",
      "Max loss: 0.030492335557937622\n",
      "Min loss: 0.005939397495239973\n",
      "Mean loss: 0.012272059607009092\n",
      "Std loss: 0.008473477347733458\n",
      "Total Loss: 0.07363235764205456\n",
      "------------------------------------ epoch 4310 (25854 steps) ------------------------------------\n",
      "Max loss: 0.018070213496685028\n",
      "Min loss: 0.006610540207475424\n",
      "Mean loss: 0.011468406999483705\n",
      "Std loss: 0.004248211980309952\n",
      "Total Loss: 0.06881044199690223\n",
      "------------------------------------ epoch 4311 (25860 steps) ------------------------------------\n",
      "Max loss: 0.01850711554288864\n",
      "Min loss: 0.006779135670512915\n",
      "Mean loss: 0.009574134213229021\n",
      "Std loss: 0.00409431240816447\n",
      "Total Loss: 0.05744480527937412\n",
      "------------------------------------ epoch 4312 (25866 steps) ------------------------------------\n",
      "Max loss: 0.027865014970302582\n",
      "Min loss: 0.0067951492965221405\n",
      "Mean loss: 0.015495235100388527\n",
      "Std loss: 0.008314572625208173\n",
      "Total Loss: 0.09297141060233116\n",
      "------------------------------------ epoch 4313 (25872 steps) ------------------------------------\n",
      "Max loss: 0.019164741039276123\n",
      "Min loss: 0.00820812676101923\n",
      "Mean loss: 0.01199831192692121\n",
      "Std loss: 0.003678766410444021\n",
      "Total Loss: 0.07198987156152725\n",
      "------------------------------------ epoch 4314 (25878 steps) ------------------------------------\n",
      "Max loss: 0.027609890326857567\n",
      "Min loss: 0.006476717069745064\n",
      "Mean loss: 0.012852276054521402\n",
      "Std loss: 0.007242390838883341\n",
      "Total Loss: 0.07711365632712841\n",
      "------------------------------------ epoch 4315 (25884 steps) ------------------------------------\n",
      "Max loss: 0.03370283171534538\n",
      "Min loss: 0.008661068975925446\n",
      "Mean loss: 0.015273906290531158\n",
      "Std loss: 0.008396117331303512\n",
      "Total Loss: 0.09164343774318695\n",
      "------------------------------------ epoch 4316 (25890 steps) ------------------------------------\n",
      "Max loss: 0.022030828520655632\n",
      "Min loss: 0.0065248585306108\n",
      "Mean loss: 0.010566239322846135\n",
      "Std loss: 0.005214082370056039\n",
      "Total Loss: 0.06339743593707681\n",
      "------------------------------------ epoch 4317 (25896 steps) ------------------------------------\n",
      "Max loss: 0.025000009685754776\n",
      "Min loss: 0.006556973792612553\n",
      "Mean loss: 0.01245554614191254\n",
      "Std loss: 0.006393300617699856\n",
      "Total Loss: 0.07473327685147524\n",
      "------------------------------------ epoch 4318 (25902 steps) ------------------------------------\n",
      "Max loss: 0.025112127885222435\n",
      "Min loss: 0.006479709874838591\n",
      "Mean loss: 0.014758917270228267\n",
      "Std loss: 0.0077649155620369845\n",
      "Total Loss: 0.0885535036213696\n",
      "------------------------------------ epoch 4319 (25908 steps) ------------------------------------\n",
      "Max loss: 0.012831229716539383\n",
      "Min loss: 0.007417749147862196\n",
      "Mean loss: 0.010393969171370069\n",
      "Std loss: 0.001986596319820535\n",
      "Total Loss: 0.062363815028220415\n",
      "------------------------------------ epoch 4320 (25914 steps) ------------------------------------\n",
      "Max loss: 0.012123922817409039\n",
      "Min loss: 0.00735253794118762\n",
      "Mean loss: 0.009415649576112628\n",
      "Std loss: 0.0015565592149986323\n",
      "Total Loss: 0.05649389745667577\n",
      "------------------------------------ epoch 4321 (25920 steps) ------------------------------------\n",
      "Max loss: 0.05191801115870476\n",
      "Min loss: 0.0058771949261426926\n",
      "Mean loss: 0.016120827523991466\n",
      "Std loss: 0.0162033043837012\n",
      "Total Loss: 0.0967249651439488\n",
      "------------------------------------ epoch 4322 (25926 steps) ------------------------------------\n",
      "Max loss: 0.026403015479445457\n",
      "Min loss: 0.0054696789011359215\n",
      "Mean loss: 0.01047797283778588\n",
      "Std loss: 0.007183915584162692\n",
      "Total Loss: 0.06286783702671528\n",
      "------------------------------------ epoch 4323 (25932 steps) ------------------------------------\n",
      "Max loss: 0.013085775077342987\n",
      "Min loss: 0.006152947898954153\n",
      "Mean loss: 0.009045478655025363\n",
      "Std loss: 0.0021658370520710253\n",
      "Total Loss: 0.05427287193015218\n",
      "------------------------------------ epoch 4324 (25938 steps) ------------------------------------\n",
      "Max loss: 0.025870759040117264\n",
      "Min loss: 0.0049278708174824715\n",
      "Mean loss: 0.010155679270004233\n",
      "Std loss: 0.007198750612353775\n",
      "Total Loss: 0.060934075620025396\n",
      "------------------------------------ epoch 4325 (25944 steps) ------------------------------------\n",
      "Max loss: 0.014206181280314922\n",
      "Min loss: 0.006290230434387922\n",
      "Mean loss: 0.009384972664217154\n",
      "Std loss: 0.002747668774836581\n",
      "Total Loss: 0.056309835985302925\n",
      "------------------------------------ epoch 4326 (25950 steps) ------------------------------------\n",
      "Max loss: 0.014457566663622856\n",
      "Min loss: 0.005894011817872524\n",
      "Mean loss: 0.009313717717304826\n",
      "Std loss: 0.00359889756288108\n",
      "Total Loss: 0.055882306303828955\n",
      "------------------------------------ epoch 4327 (25956 steps) ------------------------------------\n",
      "Max loss: 0.036054812371730804\n",
      "Min loss: 0.00568352360278368\n",
      "Mean loss: 0.012854732184981307\n",
      "Std loss: 0.010595575002461296\n",
      "Total Loss: 0.07712839310988784\n",
      "------------------------------------ epoch 4328 (25962 steps) ------------------------------------\n",
      "Max loss: 0.03672368451952934\n",
      "Min loss: 0.00565829873085022\n",
      "Mean loss: 0.012735411136721572\n",
      "Std loss: 0.010843719563977288\n",
      "Total Loss: 0.07641246682032943\n",
      "------------------------------------ epoch 4329 (25968 steps) ------------------------------------\n",
      "Max loss: 0.02444463036954403\n",
      "Min loss: 0.007696916814893484\n",
      "Mean loss: 0.01321534727079173\n",
      "Std loss: 0.006324554557611101\n",
      "Total Loss: 0.07929208362475038\n",
      "------------------------------------ epoch 4330 (25974 steps) ------------------------------------\n",
      "Max loss: 0.011230993084609509\n",
      "Min loss: 0.005318116396665573\n",
      "Mean loss: 0.007979626068845391\n",
      "Std loss: 0.0021903610116964073\n",
      "Total Loss: 0.04787775641307235\n",
      "------------------------------------ epoch 4331 (25980 steps) ------------------------------------\n",
      "Max loss: 0.017131995409727097\n",
      "Min loss: 0.006848668679594994\n",
      "Mean loss: 0.010180052990714708\n",
      "Std loss: 0.0035626199841482017\n",
      "Total Loss: 0.061080317944288254\n",
      "------------------------------------ epoch 4332 (25986 steps) ------------------------------------\n",
      "Max loss: 0.021894916892051697\n",
      "Min loss: 0.005965532269328833\n",
      "Mean loss: 0.012452453219642242\n",
      "Std loss: 0.0063077235172586945\n",
      "Total Loss: 0.07471471931785345\n",
      "------------------------------------ epoch 4333 (25992 steps) ------------------------------------\n",
      "Max loss: 0.013146614655852318\n",
      "Min loss: 0.006308138370513916\n",
      "Mean loss: 0.009480985890453061\n",
      "Std loss: 0.0024109724665073522\n",
      "Total Loss: 0.05688591534271836\n",
      "------------------------------------ epoch 4334 (25998 steps) ------------------------------------\n",
      "Max loss: 0.025021858513355255\n",
      "Min loss: 0.005115284584462643\n",
      "Mean loss: 0.011862456798553467\n",
      "Std loss: 0.007916417163350708\n",
      "Total Loss: 0.0711747407913208\n",
      "------------------------------------ epoch 4335 (26004 steps) ------------------------------------\n",
      "Max loss: 0.025477416813373566\n",
      "Min loss: 0.006510830484330654\n",
      "Mean loss: 0.012661921326071024\n",
      "Std loss: 0.006762122479320257\n",
      "Total Loss: 0.07597152795642614\n",
      "------------------------------------ epoch 4336 (26010 steps) ------------------------------------\n",
      "Max loss: 0.023439627140760422\n",
      "Min loss: 0.006757840514183044\n",
      "Mean loss: 0.012514847485969463\n",
      "Std loss: 0.005558999580575036\n",
      "Total Loss: 0.07508908491581678\n",
      "------------------------------------ epoch 4337 (26016 steps) ------------------------------------\n",
      "Max loss: 0.016755245625972748\n",
      "Min loss: 0.008368941023945808\n",
      "Mean loss: 0.012569602889319261\n",
      "Std loss: 0.003574525578437956\n",
      "Total Loss: 0.07541761733591557\n",
      "------------------------------------ epoch 4338 (26022 steps) ------------------------------------\n",
      "Max loss: 0.01833409070968628\n",
      "Min loss: 0.006132510956376791\n",
      "Mean loss: 0.012542250178133449\n",
      "Std loss: 0.004381130224510625\n",
      "Total Loss: 0.07525350106880069\n",
      "------------------------------------ epoch 4339 (26028 steps) ------------------------------------\n",
      "Max loss: 0.023912876844406128\n",
      "Min loss: 0.010355815291404724\n",
      "Mean loss: 0.013784105579058329\n",
      "Std loss: 0.004733667885039892\n",
      "Total Loss: 0.08270463347434998\n",
      "------------------------------------ epoch 4340 (26034 steps) ------------------------------------\n",
      "Max loss: 0.01767430640757084\n",
      "Min loss: 0.00711531937122345\n",
      "Mean loss: 0.01235262785727779\n",
      "Std loss: 0.0037088295800642543\n",
      "Total Loss: 0.07411576714366674\n",
      "------------------------------------ epoch 4341 (26040 steps) ------------------------------------\n",
      "Max loss: 0.00916966237127781\n",
      "Min loss: 0.006557074841111898\n",
      "Mean loss: 0.007765126026545961\n",
      "Std loss: 0.0009243197790741655\n",
      "Total Loss: 0.04659075615927577\n",
      "------------------------------------ epoch 4342 (26046 steps) ------------------------------------\n",
      "Max loss: 0.04122564569115639\n",
      "Min loss: 0.00712235551327467\n",
      "Mean loss: 0.01665397205700477\n",
      "Std loss: 0.011603329568383813\n",
      "Total Loss: 0.09992383234202862\n",
      "------------------------------------ epoch 4343 (26052 steps) ------------------------------------\n",
      "Max loss: 0.016270285472273827\n",
      "Min loss: 0.007755338214337826\n",
      "Mean loss: 0.010505597417553266\n",
      "Std loss: 0.00296540971819597\n",
      "Total Loss: 0.0630335845053196\n",
      "------------------------------------ epoch 4344 (26058 steps) ------------------------------------\n",
      "Max loss: 0.034564144909381866\n",
      "Min loss: 0.007019887678325176\n",
      "Mean loss: 0.014354253731047114\n",
      "Std loss: 0.009351410348978958\n",
      "Total Loss: 0.08612552238628268\n",
      "------------------------------------ epoch 4345 (26064 steps) ------------------------------------\n",
      "Max loss: 0.01735110580921173\n",
      "Min loss: 0.006702303886413574\n",
      "Mean loss: 0.011017523473128676\n",
      "Std loss: 0.003760919514813201\n",
      "Total Loss: 0.06610514083877206\n",
      "------------------------------------ epoch 4346 (26070 steps) ------------------------------------\n",
      "Max loss: 0.02050689235329628\n",
      "Min loss: 0.009661500342190266\n",
      "Mean loss: 0.0139203070042034\n",
      "Std loss: 0.004005229008009841\n",
      "Total Loss: 0.0835218420252204\n",
      "------------------------------------ epoch 4347 (26076 steps) ------------------------------------\n",
      "Max loss: 0.022554419934749603\n",
      "Min loss: 0.006954745389521122\n",
      "Mean loss: 0.013611395688106617\n",
      "Std loss: 0.006100380833469106\n",
      "Total Loss: 0.0816683741286397\n",
      "------------------------------------ epoch 4348 (26082 steps) ------------------------------------\n",
      "Max loss: 0.03570073843002319\n",
      "Min loss: 0.005365715827792883\n",
      "Mean loss: 0.014328258112072945\n",
      "Std loss: 0.010328606483844337\n",
      "Total Loss: 0.08596954867243767\n",
      "------------------------------------ epoch 4349 (26088 steps) ------------------------------------\n",
      "Max loss: 0.018634162843227386\n",
      "Min loss: 0.006597449537366629\n",
      "Mean loss: 0.010830420224616924\n",
      "Std loss: 0.004350368438945912\n",
      "Total Loss: 0.06498252134770155\n",
      "------------------------------------ epoch 4350 (26094 steps) ------------------------------------\n",
      "Max loss: 0.014462764374911785\n",
      "Min loss: 0.006706017069518566\n",
      "Mean loss: 0.01088219741359353\n",
      "Std loss: 0.0025011478245874426\n",
      "Total Loss: 0.06529318448156118\n",
      "------------------------------------ epoch 4351 (26100 steps) ------------------------------------\n",
      "Max loss: 0.030462658032774925\n",
      "Min loss: 0.007463798858225346\n",
      "Mean loss: 0.01869379635900259\n",
      "Std loss: 0.00852262990849032\n",
      "Total Loss: 0.11216277815401554\n",
      "------------------------------------ epoch 4352 (26106 steps) ------------------------------------\n",
      "Max loss: 0.014403512701392174\n",
      "Min loss: 0.007532324176281691\n",
      "Mean loss: 0.010853569721803069\n",
      "Std loss: 0.0025475705996109987\n",
      "Total Loss: 0.06512141833081841\n",
      "------------------------------------ epoch 4353 (26112 steps) ------------------------------------\n",
      "Max loss: 0.01821073517203331\n",
      "Min loss: 0.007613789290189743\n",
      "Mean loss: 0.012986096553504467\n",
      "Std loss: 0.0049244521921472705\n",
      "Total Loss: 0.0779165793210268\n",
      "------------------------------------ epoch 4354 (26118 steps) ------------------------------------\n",
      "Max loss: 0.024785785004496574\n",
      "Min loss: 0.005996900610625744\n",
      "Mean loss: 0.01117031485773623\n",
      "Std loss: 0.006312009779022971\n",
      "Total Loss: 0.06702188914641738\n",
      "------------------------------------ epoch 4355 (26124 steps) ------------------------------------\n",
      "Max loss: 0.027466990053653717\n",
      "Min loss: 0.006755282636731863\n",
      "Mean loss: 0.01351787468108038\n",
      "Std loss: 0.007336194373515856\n",
      "Total Loss: 0.08110724808648229\n",
      "------------------------------------ epoch 4356 (26130 steps) ------------------------------------\n",
      "Max loss: 0.015270253643393517\n",
      "Min loss: 0.009132293984293938\n",
      "Mean loss: 0.012225564724455277\n",
      "Std loss: 0.002018476899064315\n",
      "Total Loss: 0.07335338834673166\n",
      "------------------------------------ epoch 4357 (26136 steps) ------------------------------------\n",
      "Max loss: 0.013163572177290916\n",
      "Min loss: 0.006442676298320293\n",
      "Mean loss: 0.01027610075349609\n",
      "Std loss: 0.002551199990983107\n",
      "Total Loss: 0.06165660452097654\n",
      "------------------------------------ epoch 4358 (26142 steps) ------------------------------------\n",
      "Max loss: 0.02509436011314392\n",
      "Min loss: 0.00806218571960926\n",
      "Mean loss: 0.012488571461290121\n",
      "Std loss: 0.006149850483031367\n",
      "Total Loss: 0.07493142876774073\n",
      "------------------------------------ epoch 4359 (26148 steps) ------------------------------------\n",
      "Max loss: 0.013169500045478344\n",
      "Min loss: 0.0053994785994291306\n",
      "Mean loss: 0.009229634966080388\n",
      "Std loss: 0.0027694081432062816\n",
      "Total Loss: 0.055377809796482325\n",
      "------------------------------------ epoch 4360 (26154 steps) ------------------------------------\n",
      "Max loss: 0.015846366062760353\n",
      "Min loss: 0.005705466028302908\n",
      "Mean loss: 0.009503237127015987\n",
      "Std loss: 0.0033756900350375555\n",
      "Total Loss: 0.05701942276209593\n",
      "------------------------------------ epoch 4361 (26160 steps) ------------------------------------\n",
      "Max loss: 0.021007951349020004\n",
      "Min loss: 0.005636633839458227\n",
      "Mean loss: 0.0103031184989959\n",
      "Std loss: 0.005256539669850904\n",
      "Total Loss: 0.0618187109939754\n",
      "------------------------------------ epoch 4362 (26166 steps) ------------------------------------\n",
      "Max loss: 0.03018239699304104\n",
      "Min loss: 0.005864529870450497\n",
      "Mean loss: 0.01634705167574187\n",
      "Std loss: 0.008689539967287174\n",
      "Total Loss: 0.09808231005445123\n",
      "------------------------------------ epoch 4363 (26172 steps) ------------------------------------\n",
      "Max loss: 0.015947725623846054\n",
      "Min loss: 0.00584874302148819\n",
      "Mean loss: 0.009369090975572666\n",
      "Std loss: 0.003196633090432858\n",
      "Total Loss: 0.05621454585343599\n",
      "------------------------------------ epoch 4364 (26178 steps) ------------------------------------\n",
      "Max loss: 0.023482412099838257\n",
      "Min loss: 0.006400629878044128\n",
      "Mean loss: 0.014069012831896544\n",
      "Std loss: 0.006174473875590449\n",
      "Total Loss: 0.08441407699137926\n",
      "------------------------------------ epoch 4365 (26184 steps) ------------------------------------\n",
      "Max loss: 0.015851490199565887\n",
      "Min loss: 0.00585539173334837\n",
      "Mean loss: 0.010689400291691223\n",
      "Std loss: 0.003914481560766925\n",
      "Total Loss: 0.06413640175014734\n",
      "------------------------------------ epoch 4366 (26190 steps) ------------------------------------\n",
      "Max loss: 0.0485607385635376\n",
      "Min loss: 0.006251038517802954\n",
      "Mean loss: 0.017474151371667784\n",
      "Std loss: 0.014335626157626551\n",
      "Total Loss: 0.1048449082300067\n",
      "------------------------------------ epoch 4367 (26196 steps) ------------------------------------\n",
      "Max loss: 0.03808669373393059\n",
      "Min loss: 0.007873807102441788\n",
      "Mean loss: 0.021725503727793694\n",
      "Std loss: 0.011892444617023152\n",
      "Total Loss: 0.13035302236676216\n",
      "------------------------------------ epoch 4368 (26202 steps) ------------------------------------\n",
      "Max loss: 0.052074532955884933\n",
      "Min loss: 0.007819661870598793\n",
      "Mean loss: 0.027836337685585022\n",
      "Std loss: 0.017215729037399208\n",
      "Total Loss: 0.16701802611351013\n",
      "------------------------------------ epoch 4369 (26208 steps) ------------------------------------\n",
      "Max loss: 0.021007250994443893\n",
      "Min loss: 0.00667628925293684\n",
      "Mean loss: 0.013179438033451637\n",
      "Std loss: 0.004902444375844547\n",
      "Total Loss: 0.07907662820070982\n",
      "------------------------------------ epoch 4370 (26214 steps) ------------------------------------\n",
      "Max loss: 0.019258111715316772\n",
      "Min loss: 0.00891005527228117\n",
      "Mean loss: 0.013652404770255089\n",
      "Std loss: 0.0032689089095882117\n",
      "Total Loss: 0.08191442862153053\n",
      "------------------------------------ epoch 4371 (26220 steps) ------------------------------------\n",
      "Max loss: 0.022041141986846924\n",
      "Min loss: 0.0060502043925225735\n",
      "Mean loss: 0.013787727880602082\n",
      "Std loss: 0.006810080670320968\n",
      "Total Loss: 0.08272636728361249\n",
      "------------------------------------ epoch 4372 (26226 steps) ------------------------------------\n",
      "Max loss: 0.013993698172271252\n",
      "Min loss: 0.006051217205822468\n",
      "Mean loss: 0.010859981877729297\n",
      "Std loss: 0.0031000721867614576\n",
      "Total Loss: 0.06515989126637578\n",
      "------------------------------------ epoch 4373 (26232 steps) ------------------------------------\n",
      "Max loss: 0.02820458635687828\n",
      "Min loss: 0.005759245716035366\n",
      "Mean loss: 0.011660647578537464\n",
      "Std loss: 0.0075338101465322336\n",
      "Total Loss: 0.06996388547122478\n",
      "------------------------------------ epoch 4374 (26238 steps) ------------------------------------\n",
      "Max loss: 0.030560839921236038\n",
      "Min loss: 0.005177796818315983\n",
      "Mean loss: 0.014575323245177666\n",
      "Std loss: 0.009185033354972076\n",
      "Total Loss: 0.087451939471066\n",
      "------------------------------------ epoch 4375 (26244 steps) ------------------------------------\n",
      "Max loss: 0.03402238339185715\n",
      "Min loss: 0.009052207693457603\n",
      "Mean loss: 0.014814262433598438\n",
      "Std loss: 0.008712316271021921\n",
      "Total Loss: 0.08888557460159063\n",
      "------------------------------------ epoch 4376 (26250 steps) ------------------------------------\n",
      "Max loss: 0.01530260406434536\n",
      "Min loss: 0.0075836507603526115\n",
      "Mean loss: 0.010971585288643837\n",
      "Std loss: 0.0024991776494356177\n",
      "Total Loss: 0.06582951173186302\n",
      "------------------------------------ epoch 4377 (26256 steps) ------------------------------------\n",
      "Max loss: 0.02490055188536644\n",
      "Min loss: 0.005772666074335575\n",
      "Mean loss: 0.01447955813879768\n",
      "Std loss: 0.006361058820284163\n",
      "Total Loss: 0.08687734883278608\n",
      "------------------------------------ epoch 4378 (26262 steps) ------------------------------------\n",
      "Max loss: 0.044014930725097656\n",
      "Min loss: 0.008461963385343552\n",
      "Mean loss: 0.019946370739489794\n",
      "Std loss: 0.012133356485267737\n",
      "Total Loss: 0.11967822443693876\n",
      "------------------------------------ epoch 4379 (26268 steps) ------------------------------------\n",
      "Max loss: 0.02895612269639969\n",
      "Min loss: 0.0074097709730267525\n",
      "Mean loss: 0.013807831021646658\n",
      "Std loss: 0.007646422351174049\n",
      "Total Loss: 0.08284698612987995\n",
      "------------------------------------ epoch 4380 (26274 steps) ------------------------------------\n",
      "Max loss: 0.010511966422200203\n",
      "Min loss: 0.006433231756091118\n",
      "Mean loss: 0.008382225952421626\n",
      "Std loss: 0.0013965084848922378\n",
      "Total Loss: 0.05029335571452975\n",
      "------------------------------------ epoch 4381 (26280 steps) ------------------------------------\n",
      "Max loss: 0.032071661204099655\n",
      "Min loss: 0.007179665844887495\n",
      "Mean loss: 0.013882176407302419\n",
      "Std loss: 0.008570753293986991\n",
      "Total Loss: 0.08329305844381452\n",
      "------------------------------------ epoch 4382 (26286 steps) ------------------------------------\n",
      "Max loss: 0.021191567182540894\n",
      "Min loss: 0.007703929673880339\n",
      "Mean loss: 0.012582140586649379\n",
      "Std loss: 0.005080644753673566\n",
      "Total Loss: 0.07549284351989627\n",
      "------------------------------------ epoch 4383 (26292 steps) ------------------------------------\n",
      "Max loss: 0.04037626087665558\n",
      "Min loss: 0.006922842003405094\n",
      "Mean loss: 0.017130834981799126\n",
      "Std loss: 0.012035550206751665\n",
      "Total Loss: 0.10278500989079475\n",
      "------------------------------------ epoch 4384 (26298 steps) ------------------------------------\n",
      "Max loss: 0.027890536934137344\n",
      "Min loss: 0.008092444390058517\n",
      "Mean loss: 0.015123185546447834\n",
      "Std loss: 0.0071378843651576296\n",
      "Total Loss: 0.090739113278687\n",
      "------------------------------------ epoch 4385 (26304 steps) ------------------------------------\n",
      "Max loss: 0.012829299084842205\n",
      "Min loss: 0.006592543330043554\n",
      "Mean loss: 0.01013085967861116\n",
      "Std loss: 0.001976362583707971\n",
      "Total Loss: 0.060785158071666956\n",
      "------------------------------------ epoch 4386 (26310 steps) ------------------------------------\n",
      "Max loss: 0.017329545691609383\n",
      "Min loss: 0.005117312539368868\n",
      "Mean loss: 0.01020301024739941\n",
      "Std loss: 0.004764504458001768\n",
      "Total Loss: 0.06121806148439646\n",
      "------------------------------------ epoch 4387 (26316 steps) ------------------------------------\n",
      "Max loss: 0.015451575629413128\n",
      "Min loss: 0.008774182759225368\n",
      "Mean loss: 0.011698060358564058\n",
      "Std loss: 0.0021574866989483616\n",
      "Total Loss: 0.07018836215138435\n",
      "------------------------------------ epoch 4388 (26322 steps) ------------------------------------\n",
      "Max loss: 0.041520897299051285\n",
      "Min loss: 0.005846414715051651\n",
      "Mean loss: 0.01385700392226378\n",
      "Std loss: 0.01259674643732469\n",
      "Total Loss: 0.08314202353358269\n",
      "------------------------------------ epoch 4389 (26328 steps) ------------------------------------\n",
      "Max loss: 0.023590749129652977\n",
      "Min loss: 0.007143493741750717\n",
      "Mean loss: 0.01500385394319892\n",
      "Std loss: 0.005822486356747667\n",
      "Total Loss: 0.09002312365919352\n",
      "------------------------------------ epoch 4390 (26334 steps) ------------------------------------\n",
      "Max loss: 0.033423181623220444\n",
      "Min loss: 0.006897380109876394\n",
      "Mean loss: 0.014447445748373866\n",
      "Std loss: 0.009497097321638386\n",
      "Total Loss: 0.0866846744902432\n",
      "------------------------------------ epoch 4391 (26340 steps) ------------------------------------\n",
      "Max loss: 0.02087099477648735\n",
      "Min loss: 0.006514507345855236\n",
      "Mean loss: 0.010023693398882946\n",
      "Std loss: 0.004973985160772403\n",
      "Total Loss: 0.06014216039329767\n",
      "------------------------------------ epoch 4392 (26346 steps) ------------------------------------\n",
      "Max loss: 0.020315250381827354\n",
      "Min loss: 0.00686683040112257\n",
      "Mean loss: 0.013093553793927034\n",
      "Std loss: 0.005219891054012746\n",
      "Total Loss: 0.0785613227635622\n",
      "------------------------------------ epoch 4393 (26352 steps) ------------------------------------\n",
      "Max loss: 0.015214732848107815\n",
      "Min loss: 0.0067203654907643795\n",
      "Mean loss: 0.01191672783655425\n",
      "Std loss: 0.0029584276643284694\n",
      "Total Loss: 0.0715003670193255\n",
      "------------------------------------ epoch 4394 (26358 steps) ------------------------------------\n",
      "Max loss: 0.04211301729083061\n",
      "Min loss: 0.00608405377715826\n",
      "Mean loss: 0.01579121306228141\n",
      "Std loss: 0.012548975590009292\n",
      "Total Loss: 0.09474727837368846\n",
      "------------------------------------ epoch 4395 (26364 steps) ------------------------------------\n",
      "Max loss: 0.031662747263908386\n",
      "Min loss: 0.0074176788330078125\n",
      "Mean loss: 0.014586318594714006\n",
      "Std loss: 0.008443815299850548\n",
      "Total Loss: 0.08751791156828403\n",
      "------------------------------------ epoch 4396 (26370 steps) ------------------------------------\n",
      "Max loss: 0.02913140133023262\n",
      "Min loss: 0.009460404515266418\n",
      "Mean loss: 0.016207482820997637\n",
      "Std loss: 0.006182055972179651\n",
      "Total Loss: 0.09724489692598581\n",
      "------------------------------------ epoch 4397 (26376 steps) ------------------------------------\n",
      "Max loss: 0.02755027636885643\n",
      "Min loss: 0.007190094795078039\n",
      "Mean loss: 0.012597073257590333\n",
      "Std loss: 0.006784598402183614\n",
      "Total Loss: 0.075582439545542\n",
      "------------------------------------ epoch 4398 (26382 steps) ------------------------------------\n",
      "Max loss: 0.016594544053077698\n",
      "Min loss: 0.005848792847245932\n",
      "Mean loss: 0.010316170907268921\n",
      "Std loss: 0.003659019018813728\n",
      "Total Loss: 0.06189702544361353\n",
      "------------------------------------ epoch 4399 (26388 steps) ------------------------------------\n",
      "Max loss: 0.02021055668592453\n",
      "Min loss: 0.006092562340199947\n",
      "Mean loss: 0.010778383584693074\n",
      "Std loss: 0.005163690942573598\n",
      "Total Loss: 0.06467030150815845\n",
      "------------------------------------ epoch 4400 (26394 steps) ------------------------------------\n",
      "Max loss: 0.03068537637591362\n",
      "Min loss: 0.006541161797940731\n",
      "Mean loss: 0.014136957935988903\n",
      "Std loss: 0.008597971147555597\n",
      "Total Loss: 0.08482174761593342\n",
      "------------------------------------ epoch 4401 (26400 steps) ------------------------------------\n",
      "Max loss: 0.02804817259311676\n",
      "Min loss: 0.006344858556985855\n",
      "Mean loss: 0.01523104589432478\n",
      "Std loss: 0.008562279498909645\n",
      "Total Loss: 0.09138627536594868\n",
      "saved model at ./weights/model_4401.pth\n",
      "------------------------------------ epoch 4402 (26406 steps) ------------------------------------\n",
      "Max loss: 0.028849055990576744\n",
      "Min loss: 0.007385843433439732\n",
      "Mean loss: 0.015417709946632385\n",
      "Std loss: 0.007033652063791001\n",
      "Total Loss: 0.09250625967979431\n",
      "------------------------------------ epoch 4403 (26412 steps) ------------------------------------\n",
      "Max loss: 0.021030712872743607\n",
      "Min loss: 0.005877159535884857\n",
      "Mean loss: 0.012660241685807705\n",
      "Std loss: 0.004802664248024877\n",
      "Total Loss: 0.07596145011484623\n",
      "------------------------------------ epoch 4404 (26418 steps) ------------------------------------\n",
      "Max loss: 0.01193592231720686\n",
      "Min loss: 0.00691788038238883\n",
      "Mean loss: 0.00873780770537754\n",
      "Std loss: 0.0017504524759385676\n",
      "Total Loss: 0.052426846232265234\n",
      "------------------------------------ epoch 4405 (26424 steps) ------------------------------------\n",
      "Max loss: 0.013608654960989952\n",
      "Min loss: 0.008963409811258316\n",
      "Mean loss: 0.010805489805837473\n",
      "Std loss: 0.0016638882504509563\n",
      "Total Loss: 0.06483293883502483\n",
      "------------------------------------ epoch 4406 (26430 steps) ------------------------------------\n",
      "Max loss: 0.011846873909235\n",
      "Min loss: 0.0070994263514876366\n",
      "Mean loss: 0.009399698891987404\n",
      "Std loss: 0.0016096814748284005\n",
      "Total Loss: 0.05639819335192442\n",
      "------------------------------------ epoch 4407 (26436 steps) ------------------------------------\n",
      "Max loss: 0.0316007174551487\n",
      "Min loss: 0.007031113840639591\n",
      "Mean loss: 0.01408701374505957\n",
      "Std loss: 0.008842219832462327\n",
      "Total Loss: 0.08452208247035742\n",
      "------------------------------------ epoch 4408 (26442 steps) ------------------------------------\n",
      "Max loss: 0.016570355743169785\n",
      "Min loss: 0.007096181623637676\n",
      "Mean loss: 0.010588701503972212\n",
      "Std loss: 0.003466797819633144\n",
      "Total Loss: 0.06353220902383327\n",
      "------------------------------------ epoch 4409 (26448 steps) ------------------------------------\n",
      "Max loss: 0.012176322750747204\n",
      "Min loss: 0.0069028581492602825\n",
      "Mean loss: 0.009703150950372219\n",
      "Std loss: 0.002056857339314701\n",
      "Total Loss: 0.058218905702233315\n",
      "------------------------------------ epoch 4410 (26454 steps) ------------------------------------\n",
      "Max loss: 0.01643958128988743\n",
      "Min loss: 0.005136591382324696\n",
      "Mean loss: 0.010130867439632615\n",
      "Std loss: 0.0042600616210089265\n",
      "Total Loss: 0.06078520463779569\n",
      "------------------------------------ epoch 4411 (26460 steps) ------------------------------------\n",
      "Max loss: 0.028969701379537582\n",
      "Min loss: 0.00824696570634842\n",
      "Mean loss: 0.015117173393567404\n",
      "Std loss: 0.006943837880945042\n",
      "Total Loss: 0.09070304036140442\n",
      "------------------------------------ epoch 4412 (26466 steps) ------------------------------------\n",
      "Max loss: 0.016078714281320572\n",
      "Min loss: 0.006659939419478178\n",
      "Mean loss: 0.009878400014713407\n",
      "Std loss: 0.0034061661770962994\n",
      "Total Loss: 0.05927040008828044\n",
      "------------------------------------ epoch 4413 (26472 steps) ------------------------------------\n",
      "Max loss: 0.011250555515289307\n",
      "Min loss: 0.005702651105821133\n",
      "Mean loss: 0.007750063203275204\n",
      "Std loss: 0.0019727530705585345\n",
      "Total Loss: 0.04650037921965122\n",
      "------------------------------------ epoch 4414 (26478 steps) ------------------------------------\n",
      "Max loss: 0.046531375497579575\n",
      "Min loss: 0.005198263563215733\n",
      "Mean loss: 0.013111765185991922\n",
      "Std loss: 0.015000893201086407\n",
      "Total Loss: 0.07867059111595154\n",
      "------------------------------------ epoch 4415 (26484 steps) ------------------------------------\n",
      "Max loss: 0.019934801384806633\n",
      "Min loss: 0.006135069765150547\n",
      "Mean loss: 0.012385001483683785\n",
      "Std loss: 0.005715331717565804\n",
      "Total Loss: 0.07431000890210271\n",
      "------------------------------------ epoch 4416 (26490 steps) ------------------------------------\n",
      "Max loss: 0.03305913135409355\n",
      "Min loss: 0.005823974963277578\n",
      "Mean loss: 0.014071424569313725\n",
      "Std loss: 0.009518046239840384\n",
      "Total Loss: 0.08442854741588235\n",
      "------------------------------------ epoch 4417 (26496 steps) ------------------------------------\n",
      "Max loss: 0.02350516989827156\n",
      "Min loss: 0.007799121551215649\n",
      "Mean loss: 0.014965510461479425\n",
      "Std loss: 0.004778627802193134\n",
      "Total Loss: 0.08979306276887655\n",
      "------------------------------------ epoch 4418 (26502 steps) ------------------------------------\n",
      "Max loss: 0.03596924990415573\n",
      "Min loss: 0.007000146433711052\n",
      "Mean loss: 0.019389008016635973\n",
      "Std loss: 0.010213274061268256\n",
      "Total Loss: 0.11633404809981585\n",
      "------------------------------------ epoch 4419 (26508 steps) ------------------------------------\n",
      "Max loss: 0.02665800414979458\n",
      "Min loss: 0.006353245582431555\n",
      "Mean loss: 0.01520282425917685\n",
      "Std loss: 0.008276134789629738\n",
      "Total Loss: 0.0912169455550611\n",
      "------------------------------------ epoch 4420 (26514 steps) ------------------------------------\n",
      "Max loss: 0.038416508585214615\n",
      "Min loss: 0.007910838350653648\n",
      "Mean loss: 0.0207680716800193\n",
      "Std loss: 0.010946093239238694\n",
      "Total Loss: 0.1246084300801158\n",
      "------------------------------------ epoch 4421 (26520 steps) ------------------------------------\n",
      "Max loss: 0.02268332801759243\n",
      "Min loss: 0.008768626488745213\n",
      "Mean loss: 0.013191180303692818\n",
      "Std loss: 0.005053978333084294\n",
      "Total Loss: 0.0791470818221569\n",
      "------------------------------------ epoch 4422 (26526 steps) ------------------------------------\n",
      "Max loss: 0.05258475989103317\n",
      "Min loss: 0.009590666741132736\n",
      "Mean loss: 0.01782494115953644\n",
      "Std loss: 0.015569210831858247\n",
      "Total Loss: 0.10694964695721865\n",
      "------------------------------------ epoch 4423 (26532 steps) ------------------------------------\n",
      "Max loss: 0.014865967445075512\n",
      "Min loss: 0.005894714500755072\n",
      "Mean loss: 0.008755353434632221\n",
      "Std loss: 0.003179201150805706\n",
      "Total Loss: 0.05253212060779333\n",
      "------------------------------------ epoch 4424 (26538 steps) ------------------------------------\n",
      "Max loss: 0.022152531892061234\n",
      "Min loss: 0.006854583974927664\n",
      "Mean loss: 0.012069555232301354\n",
      "Std loss: 0.004876307448089331\n",
      "Total Loss: 0.07241733139380813\n",
      "------------------------------------ epoch 4425 (26544 steps) ------------------------------------\n",
      "Max loss: 0.03168713301420212\n",
      "Min loss: 0.009053900837898254\n",
      "Mean loss: 0.015829377342015505\n",
      "Std loss: 0.0073600662181960794\n",
      "Total Loss: 0.09497626405209303\n",
      "------------------------------------ epoch 4426 (26550 steps) ------------------------------------\n",
      "Max loss: 0.008103674277663231\n",
      "Min loss: 0.005285652354359627\n",
      "Mean loss: 0.006895181955769658\n",
      "Std loss: 0.0009174784034888526\n",
      "Total Loss: 0.04137109173461795\n",
      "------------------------------------ epoch 4427 (26556 steps) ------------------------------------\n",
      "Max loss: 0.019777674227952957\n",
      "Min loss: 0.006407092325389385\n",
      "Mean loss: 0.010056761869539818\n",
      "Std loss: 0.004517971554249706\n",
      "Total Loss: 0.0603405712172389\n",
      "------------------------------------ epoch 4428 (26562 steps) ------------------------------------\n",
      "Max loss: 0.01867273822426796\n",
      "Min loss: 0.005386629607528448\n",
      "Mean loss: 0.011487676373993358\n",
      "Std loss: 0.004833512687738275\n",
      "Total Loss: 0.06892605824396014\n",
      "------------------------------------ epoch 4429 (26568 steps) ------------------------------------\n",
      "Max loss: 0.015579232946038246\n",
      "Min loss: 0.007507815025746822\n",
      "Mean loss: 0.010072356710831324\n",
      "Std loss: 0.002614176888056908\n",
      "Total Loss: 0.060434140264987946\n",
      "------------------------------------ epoch 4430 (26574 steps) ------------------------------------\n",
      "Max loss: 0.014037872664630413\n",
      "Min loss: 0.006224765442311764\n",
      "Mean loss: 0.009003178604568044\n",
      "Std loss: 0.0026362396665443794\n",
      "Total Loss: 0.054019071627408266\n",
      "------------------------------------ epoch 4431 (26580 steps) ------------------------------------\n",
      "Max loss: 0.01125575602054596\n",
      "Min loss: 0.006730261258780956\n",
      "Mean loss: 0.008567139816780886\n",
      "Std loss: 0.0013557757010214705\n",
      "Total Loss: 0.05140283890068531\n",
      "------------------------------------ epoch 4432 (26586 steps) ------------------------------------\n",
      "Max loss: 0.019839677959680557\n",
      "Min loss: 0.006026610266417265\n",
      "Mean loss: 0.009761601764087876\n",
      "Std loss: 0.004680931224398846\n",
      "Total Loss: 0.058569610584527254\n",
      "------------------------------------ epoch 4433 (26592 steps) ------------------------------------\n",
      "Max loss: 0.015502954833209515\n",
      "Min loss: 0.005327271297574043\n",
      "Mean loss: 0.009257530405496558\n",
      "Std loss: 0.0036551561709665735\n",
      "Total Loss: 0.055545182432979345\n",
      "------------------------------------ epoch 4434 (26598 steps) ------------------------------------\n",
      "Max loss: 0.0261305570602417\n",
      "Min loss: 0.006089901085942984\n",
      "Mean loss: 0.010459370367849866\n",
      "Std loss: 0.007078878841881772\n",
      "Total Loss: 0.0627562222070992\n",
      "------------------------------------ epoch 4435 (26604 steps) ------------------------------------\n",
      "Max loss: 0.03128647059202194\n",
      "Min loss: 0.007516957353800535\n",
      "Mean loss: 0.013793611976628503\n",
      "Std loss: 0.00840946697421301\n",
      "Total Loss: 0.08276167185977101\n",
      "------------------------------------ epoch 4436 (26610 steps) ------------------------------------\n",
      "Max loss: 0.020000003278255463\n",
      "Min loss: 0.006382455583661795\n",
      "Mean loss: 0.012626192222038904\n",
      "Std loss: 0.005707746701279712\n",
      "Total Loss: 0.07575715333223343\n",
      "------------------------------------ epoch 4437 (26616 steps) ------------------------------------\n",
      "Max loss: 0.028024684637784958\n",
      "Min loss: 0.0056244260631501675\n",
      "Mean loss: 0.012095748679712415\n",
      "Std loss: 0.007509904215495548\n",
      "Total Loss: 0.07257449207827449\n",
      "------------------------------------ epoch 4438 (26622 steps) ------------------------------------\n",
      "Max loss: 0.022752948105335236\n",
      "Min loss: 0.005612719338387251\n",
      "Mean loss: 0.012808136874809861\n",
      "Std loss: 0.005442474009050491\n",
      "Total Loss: 0.07684882124885917\n",
      "------------------------------------ epoch 4439 (26628 steps) ------------------------------------\n",
      "Max loss: 0.02230594865977764\n",
      "Min loss: 0.005670332815498114\n",
      "Mean loss: 0.011816429905593395\n",
      "Std loss: 0.007404296723266113\n",
      "Total Loss: 0.07089857943356037\n",
      "------------------------------------ epoch 4440 (26634 steps) ------------------------------------\n",
      "Max loss: 0.024307487532496452\n",
      "Min loss: 0.005825703963637352\n",
      "Mean loss: 0.010539734968915582\n",
      "Std loss: 0.006350901099963971\n",
      "Total Loss: 0.06323840981349349\n",
      "------------------------------------ epoch 4441 (26640 steps) ------------------------------------\n",
      "Max loss: 0.021566059440374374\n",
      "Min loss: 0.0071157533675432205\n",
      "Mean loss: 0.013706428464502096\n",
      "Std loss: 0.0053048760430725725\n",
      "Total Loss: 0.08223857078701258\n",
      "------------------------------------ epoch 4442 (26646 steps) ------------------------------------\n",
      "Max loss: 0.012011094018816948\n",
      "Min loss: 0.006270451005548239\n",
      "Mean loss: 0.008992053180312118\n",
      "Std loss: 0.0018288971182033\n",
      "Total Loss: 0.0539523190818727\n",
      "------------------------------------ epoch 4443 (26652 steps) ------------------------------------\n",
      "Max loss: 0.019932938739657402\n",
      "Min loss: 0.005973679479211569\n",
      "Mean loss: 0.011383921606466174\n",
      "Std loss: 0.005128059772455146\n",
      "Total Loss: 0.06830352963879704\n",
      "------------------------------------ epoch 4444 (26658 steps) ------------------------------------\n",
      "Max loss: 0.042669475078582764\n",
      "Min loss: 0.009476003237068653\n",
      "Mean loss: 0.01857630458349983\n",
      "Std loss: 0.012306679722506998\n",
      "Total Loss: 0.11145782750099897\n",
      "------------------------------------ epoch 4445 (26664 steps) ------------------------------------\n",
      "Max loss: 0.014519302174448967\n",
      "Min loss: 0.0058025941252708435\n",
      "Mean loss: 0.009746439599742493\n",
      "Std loss: 0.0027887985824846385\n",
      "Total Loss: 0.05847863759845495\n",
      "------------------------------------ epoch 4446 (26670 steps) ------------------------------------\n",
      "Max loss: 0.025646936148405075\n",
      "Min loss: 0.006103170569986105\n",
      "Mean loss: 0.016032585486148793\n",
      "Std loss: 0.0072274329188799944\n",
      "Total Loss: 0.09619551291689277\n",
      "------------------------------------ epoch 4447 (26676 steps) ------------------------------------\n",
      "Max loss: 0.02313919924199581\n",
      "Min loss: 0.006764914840459824\n",
      "Mean loss: 0.012354526668787003\n",
      "Std loss: 0.005266136414782615\n",
      "Total Loss: 0.07412716001272202\n",
      "------------------------------------ epoch 4448 (26682 steps) ------------------------------------\n",
      "Max loss: 0.0369342640042305\n",
      "Min loss: 0.006035314407199621\n",
      "Mean loss: 0.017507479758933187\n",
      "Std loss: 0.01375315050009715\n",
      "Total Loss: 0.10504487855359912\n",
      "------------------------------------ epoch 4449 (26688 steps) ------------------------------------\n",
      "Max loss: 0.014544332399964333\n",
      "Min loss: 0.007998226210474968\n",
      "Mean loss: 0.011338282686968645\n",
      "Std loss: 0.0022692929256570614\n",
      "Total Loss: 0.06802969612181187\n",
      "------------------------------------ epoch 4450 (26694 steps) ------------------------------------\n",
      "Max loss: 0.024717170745134354\n",
      "Min loss: 0.00682238070294261\n",
      "Mean loss: 0.01350783440284431\n",
      "Std loss: 0.007176639867596154\n",
      "Total Loss: 0.08104700641706586\n",
      "------------------------------------ epoch 4451 (26700 steps) ------------------------------------\n",
      "Max loss: 0.06708680093288422\n",
      "Min loss: 0.0068623642437160015\n",
      "Mean loss: 0.021589782011384766\n",
      "Std loss: 0.020995751451334192\n",
      "Total Loss: 0.1295386920683086\n",
      "------------------------------------ epoch 4452 (26706 steps) ------------------------------------\n",
      "Max loss: 0.033570170402526855\n",
      "Min loss: 0.006846229545772076\n",
      "Mean loss: 0.014098378596827388\n",
      "Std loss: 0.00929204714595855\n",
      "Total Loss: 0.08459027158096433\n",
      "------------------------------------ epoch 4453 (26712 steps) ------------------------------------\n",
      "Max loss: 0.023606091737747192\n",
      "Min loss: 0.009677644819021225\n",
      "Mean loss: 0.01520889981960257\n",
      "Std loss: 0.005285870338037948\n",
      "Total Loss: 0.09125339891761541\n",
      "------------------------------------ epoch 4454 (26718 steps) ------------------------------------\n",
      "Max loss: 0.021567432209849358\n",
      "Min loss: 0.0066682412289083\n",
      "Mean loss: 0.01140159935069581\n",
      "Std loss: 0.005083757718295923\n",
      "Total Loss: 0.06840959610417485\n",
      "------------------------------------ epoch 4455 (26724 steps) ------------------------------------\n",
      "Max loss: 0.10081528127193451\n",
      "Min loss: 0.009633036330342293\n",
      "Mean loss: 0.0317003404100736\n",
      "Std loss: 0.03194131241961482\n",
      "Total Loss: 0.1902020424604416\n",
      "------------------------------------ epoch 4456 (26730 steps) ------------------------------------\n",
      "Max loss: 0.013409404084086418\n",
      "Min loss: 0.008553740568459034\n",
      "Mean loss: 0.01131286813567082\n",
      "Std loss: 0.0017577820458656497\n",
      "Total Loss: 0.06787720881402493\n",
      "------------------------------------ epoch 4457 (26736 steps) ------------------------------------\n",
      "Max loss: 0.045973241329193115\n",
      "Min loss: 0.007248153444379568\n",
      "Mean loss: 0.02334609852793316\n",
      "Std loss: 0.014150514122678868\n",
      "Total Loss: 0.14007659116759896\n",
      "------------------------------------ epoch 4458 (26742 steps) ------------------------------------\n",
      "Max loss: 0.0274028442800045\n",
      "Min loss: 0.006474657915532589\n",
      "Mean loss: 0.013528012049694857\n",
      "Std loss: 0.00732751808674294\n",
      "Total Loss: 0.08116807229816914\n",
      "------------------------------------ epoch 4459 (26748 steps) ------------------------------------\n",
      "Max loss: 0.027577146887779236\n",
      "Min loss: 0.008501536212861538\n",
      "Mean loss: 0.015265161637216806\n",
      "Std loss: 0.006853522072334589\n",
      "Total Loss: 0.09159096982330084\n",
      "------------------------------------ epoch 4460 (26754 steps) ------------------------------------\n",
      "Max loss: 0.04077037051320076\n",
      "Min loss: 0.008933915756642818\n",
      "Mean loss: 0.01739881979301572\n",
      "Std loss: 0.010692091833994064\n",
      "Total Loss: 0.10439291875809431\n",
      "------------------------------------ epoch 4461 (26760 steps) ------------------------------------\n",
      "Max loss: 0.02140664868056774\n",
      "Min loss: 0.00706903263926506\n",
      "Mean loss: 0.013370024040341377\n",
      "Std loss: 0.004960348759550069\n",
      "Total Loss: 0.08022014424204826\n",
      "------------------------------------ epoch 4462 (26766 steps) ------------------------------------\n",
      "Max loss: 0.10130580514669418\n",
      "Min loss: 0.0071635483764112\n",
      "Mean loss: 0.024729451552654307\n",
      "Std loss: 0.03429794326921328\n",
      "Total Loss: 0.14837670931592584\n",
      "------------------------------------ epoch 4463 (26772 steps) ------------------------------------\n",
      "Max loss: 0.028794359415769577\n",
      "Min loss: 0.0107774268835783\n",
      "Mean loss: 0.015749998080233734\n",
      "Std loss: 0.006025760938764767\n",
      "Total Loss: 0.0944999884814024\n",
      "------------------------------------ epoch 4464 (26778 steps) ------------------------------------\n",
      "Max loss: 0.036055225878953934\n",
      "Min loss: 0.00888826884329319\n",
      "Mean loss: 0.015558628365397453\n",
      "Std loss: 0.009620049678065378\n",
      "Total Loss: 0.09335177019238472\n",
      "------------------------------------ epoch 4465 (26784 steps) ------------------------------------\n",
      "Max loss: 0.021095730364322662\n",
      "Min loss: 0.009912929497659206\n",
      "Mean loss: 0.014762357498208681\n",
      "Std loss: 0.004002002240123026\n",
      "Total Loss: 0.08857414498925209\n",
      "------------------------------------ epoch 4466 (26790 steps) ------------------------------------\n",
      "Max loss: 0.02428964525461197\n",
      "Min loss: 0.008110756985843182\n",
      "Mean loss: 0.013863117123643557\n",
      "Std loss: 0.004975555784514416\n",
      "Total Loss: 0.08317870274186134\n",
      "------------------------------------ epoch 4467 (26796 steps) ------------------------------------\n",
      "Max loss: 0.019631262868642807\n",
      "Min loss: 0.01021232083439827\n",
      "Mean loss: 0.01567134230087201\n",
      "Std loss: 0.003334654093369704\n",
      "Total Loss: 0.09402805380523205\n",
      "------------------------------------ epoch 4468 (26802 steps) ------------------------------------\n",
      "Max loss: 0.024264302104711533\n",
      "Min loss: 0.011478021740913391\n",
      "Mean loss: 0.01570079755038023\n",
      "Std loss: 0.004974472645746636\n",
      "Total Loss: 0.09420478530228138\n",
      "------------------------------------ epoch 4469 (26808 steps) ------------------------------------\n",
      "Max loss: 0.013907825574278831\n",
      "Min loss: 0.006172187626361847\n",
      "Mean loss: 0.009497450509419044\n",
      "Std loss: 0.0023607629274955554\n",
      "Total Loss: 0.05698470305651426\n",
      "------------------------------------ epoch 4470 (26814 steps) ------------------------------------\n",
      "Max loss: 0.023838520050048828\n",
      "Min loss: 0.00803073775023222\n",
      "Mean loss: 0.014163681926826635\n",
      "Std loss: 0.005916841759898627\n",
      "Total Loss: 0.08498209156095982\n",
      "------------------------------------ epoch 4471 (26820 steps) ------------------------------------\n",
      "Max loss: 0.02340730093419552\n",
      "Min loss: 0.006879650056362152\n",
      "Mean loss: 0.012513588337848583\n",
      "Std loss: 0.006004552203365139\n",
      "Total Loss: 0.0750815300270915\n",
      "------------------------------------ epoch 4472 (26826 steps) ------------------------------------\n",
      "Max loss: 0.04308144748210907\n",
      "Min loss: 0.006573240272700787\n",
      "Mean loss: 0.017176994277785223\n",
      "Std loss: 0.013221074144782428\n",
      "Total Loss: 0.10306196566671133\n",
      "------------------------------------ epoch 4473 (26832 steps) ------------------------------------\n",
      "Max loss: 0.03193094581365585\n",
      "Min loss: 0.010134554468095303\n",
      "Mean loss: 0.01757288072258234\n",
      "Std loss: 0.007988268018194852\n",
      "Total Loss: 0.10543728433549404\n",
      "------------------------------------ epoch 4474 (26838 steps) ------------------------------------\n",
      "Max loss: 0.024350522086024284\n",
      "Min loss: 0.006855064071714878\n",
      "Mean loss: 0.01133418750638763\n",
      "Std loss: 0.006004865647006578\n",
      "Total Loss: 0.06800512503832579\n",
      "------------------------------------ epoch 4475 (26844 steps) ------------------------------------\n",
      "Max loss: 0.01892233081161976\n",
      "Min loss: 0.0065592615865170956\n",
      "Mean loss: 0.011983178944016496\n",
      "Std loss: 0.004657711723461103\n",
      "Total Loss: 0.07189907366409898\n",
      "------------------------------------ epoch 4476 (26850 steps) ------------------------------------\n",
      "Max loss: 0.02896018885076046\n",
      "Min loss: 0.006851383484899998\n",
      "Mean loss: 0.013528021518141031\n",
      "Std loss: 0.007183001553321954\n",
      "Total Loss: 0.08116812910884619\n",
      "------------------------------------ epoch 4477 (26856 steps) ------------------------------------\n",
      "Max loss: 0.02576974406838417\n",
      "Min loss: 0.007301866542547941\n",
      "Mean loss: 0.01477368587317566\n",
      "Std loss: 0.007634123985919807\n",
      "Total Loss: 0.08864211523905396\n",
      "------------------------------------ epoch 4478 (26862 steps) ------------------------------------\n",
      "Max loss: 0.011458680033683777\n",
      "Min loss: 0.007402793504297733\n",
      "Mean loss: 0.00979682244360447\n",
      "Std loss: 0.0012356901602471777\n",
      "Total Loss: 0.058780934661626816\n",
      "------------------------------------ epoch 4479 (26868 steps) ------------------------------------\n",
      "Max loss: 0.059341516345739365\n",
      "Min loss: 0.00558919832110405\n",
      "Mean loss: 0.021203901541108888\n",
      "Std loss: 0.01910773240752233\n",
      "Total Loss: 0.12722340924665332\n",
      "------------------------------------ epoch 4480 (26874 steps) ------------------------------------\n",
      "Max loss: 0.016577430069446564\n",
      "Min loss: 0.0066689737141132355\n",
      "Mean loss: 0.010201905543605486\n",
      "Std loss: 0.0034712125513895555\n",
      "Total Loss: 0.06121143326163292\n",
      "------------------------------------ epoch 4481 (26880 steps) ------------------------------------\n",
      "Max loss: 0.049413781613111496\n",
      "Min loss: 0.01020250003784895\n",
      "Mean loss: 0.02103938441723585\n",
      "Std loss: 0.01318356807983809\n",
      "Total Loss: 0.1262363065034151\n",
      "------------------------------------ epoch 4482 (26886 steps) ------------------------------------\n",
      "Max loss: 0.012914407067000866\n",
      "Min loss: 0.007113707717508078\n",
      "Mean loss: 0.00955808931030333\n",
      "Std loss: 0.0018150509413194713\n",
      "Total Loss: 0.05734853586181998\n",
      "------------------------------------ epoch 4483 (26892 steps) ------------------------------------\n",
      "Max loss: 0.013917552307248116\n",
      "Min loss: 0.006942398846149445\n",
      "Mean loss: 0.010056341222176949\n",
      "Std loss: 0.0022105746685279455\n",
      "Total Loss: 0.060338047333061695\n",
      "------------------------------------ epoch 4484 (26898 steps) ------------------------------------\n",
      "Max loss: 0.018755657598376274\n",
      "Min loss: 0.006064782850444317\n",
      "Mean loss: 0.009987954671184221\n",
      "Std loss: 0.004281660484482503\n",
      "Total Loss: 0.05992772802710533\n",
      "------------------------------------ epoch 4485 (26904 steps) ------------------------------------\n",
      "Max loss: 0.011121402494609356\n",
      "Min loss: 0.006882488261908293\n",
      "Mean loss: 0.008915913213665286\n",
      "Std loss: 0.0016115385974505582\n",
      "Total Loss: 0.05349547928199172\n",
      "------------------------------------ epoch 4486 (26910 steps) ------------------------------------\n",
      "Max loss: 0.010145049542188644\n",
      "Min loss: 0.006873191799968481\n",
      "Mean loss: 0.008713655717050036\n",
      "Std loss: 0.0009994558621834444\n",
      "Total Loss: 0.052281934302300215\n",
      "------------------------------------ epoch 4487 (26916 steps) ------------------------------------\n",
      "Max loss: 0.033274829387664795\n",
      "Min loss: 0.005325194448232651\n",
      "Mean loss: 0.012836828284586469\n",
      "Std loss: 0.009457766512715078\n",
      "Total Loss: 0.07702096970751882\n",
      "------------------------------------ epoch 4488 (26922 steps) ------------------------------------\n",
      "Max loss: 0.0183690395206213\n",
      "Min loss: 0.0065049841068685055\n",
      "Mean loss: 0.009637341912214955\n",
      "Std loss: 0.00400840607263094\n",
      "Total Loss: 0.05782405147328973\n",
      "------------------------------------ epoch 4489 (26928 steps) ------------------------------------\n",
      "Max loss: 0.044587522745132446\n",
      "Min loss: 0.0068510472774505615\n",
      "Mean loss: 0.018259055602053802\n",
      "Std loss: 0.012416018100181855\n",
      "Total Loss: 0.10955433361232281\n",
      "------------------------------------ epoch 4490 (26934 steps) ------------------------------------\n",
      "Max loss: 0.02485046535730362\n",
      "Min loss: 0.006383295636624098\n",
      "Mean loss: 0.01323718725082775\n",
      "Std loss: 0.005825976758972599\n",
      "Total Loss: 0.0794231235049665\n",
      "------------------------------------ epoch 4491 (26940 steps) ------------------------------------\n",
      "Max loss: 0.029894281178712845\n",
      "Min loss: 0.006637314334511757\n",
      "Mean loss: 0.012868905439972878\n",
      "Std loss: 0.008118966028686364\n",
      "Total Loss: 0.07721343263983727\n",
      "------------------------------------ epoch 4492 (26946 steps) ------------------------------------\n",
      "Max loss: 0.017095021903514862\n",
      "Min loss: 0.0057948618195950985\n",
      "Mean loss: 0.012289060667778054\n",
      "Std loss: 0.0036369390215141716\n",
      "Total Loss: 0.07373436400666833\n",
      "------------------------------------ epoch 4493 (26952 steps) ------------------------------------\n",
      "Max loss: 0.013104841113090515\n",
      "Min loss: 0.006784828379750252\n",
      "Mean loss: 0.00947364792227745\n",
      "Std loss: 0.001970611135706236\n",
      "Total Loss: 0.0568418875336647\n",
      "------------------------------------ epoch 4494 (26958 steps) ------------------------------------\n",
      "Max loss: 0.03850564360618591\n",
      "Min loss: 0.008883146569132805\n",
      "Mean loss: 0.016475957507888477\n",
      "Std loss: 0.010113677610181034\n",
      "Total Loss: 0.09885574504733086\n",
      "------------------------------------ epoch 4495 (26964 steps) ------------------------------------\n",
      "Max loss: 0.030121978372335434\n",
      "Min loss: 0.006554133724421263\n",
      "Mean loss: 0.01399752194993198\n",
      "Std loss: 0.007846133334261391\n",
      "Total Loss: 0.08398513169959188\n",
      "------------------------------------ epoch 4496 (26970 steps) ------------------------------------\n",
      "Max loss: 0.016659095883369446\n",
      "Min loss: 0.005270249675959349\n",
      "Mean loss: 0.009357929384956757\n",
      "Std loss: 0.003720052341581046\n",
      "Total Loss: 0.05614757630974054\n",
      "------------------------------------ epoch 4497 (26976 steps) ------------------------------------\n",
      "Max loss: 0.01528889499604702\n",
      "Min loss: 0.006542172282934189\n",
      "Mean loss: 0.010342527646571398\n",
      "Std loss: 0.002815933591016719\n",
      "Total Loss: 0.06205516587942839\n",
      "------------------------------------ epoch 4498 (26982 steps) ------------------------------------\n",
      "Max loss: 0.019692281261086464\n",
      "Min loss: 0.007285199128091335\n",
      "Mean loss: 0.013604212086647749\n",
      "Std loss: 0.004810643151547958\n",
      "Total Loss: 0.0816252725198865\n",
      "------------------------------------ epoch 4499 (26988 steps) ------------------------------------\n",
      "Max loss: 0.021613582968711853\n",
      "Min loss: 0.007113510277122259\n",
      "Mean loss: 0.013540978465850154\n",
      "Std loss: 0.004563901957846077\n",
      "Total Loss: 0.08124587079510093\n",
      "------------------------------------ epoch 4500 (26994 steps) ------------------------------------\n",
      "Max loss: 0.01889118179678917\n",
      "Min loss: 0.007543967571109533\n",
      "Mean loss: 0.01334240729920566\n",
      "Std loss: 0.0036864570467806\n",
      "Total Loss: 0.08005444379523396\n",
      "------------------------------------ epoch 4501 (27000 steps) ------------------------------------\n",
      "Max loss: 0.015910254791378975\n",
      "Min loss: 0.00549662858247757\n",
      "Mean loss: 0.010453569081922373\n",
      "Std loss: 0.00312421083991881\n",
      "Total Loss: 0.06272141449153423\n",
      "saved model at ./weights/model_4501.pth\n",
      "------------------------------------ epoch 4502 (27006 steps) ------------------------------------\n",
      "Max loss: 0.03224281594157219\n",
      "Min loss: 0.005829495843499899\n",
      "Mean loss: 0.014864274533465505\n",
      "Std loss: 0.008653297769386888\n",
      "Total Loss: 0.08918564720079303\n",
      "------------------------------------ epoch 4503 (27012 steps) ------------------------------------\n",
      "Max loss: 0.017103606835007668\n",
      "Min loss: 0.006359832361340523\n",
      "Mean loss: 0.009137191344052553\n",
      "Std loss: 0.0037150873280398784\n",
      "Total Loss: 0.05482314806431532\n",
      "------------------------------------ epoch 4504 (27018 steps) ------------------------------------\n",
      "Max loss: 0.012963792309165001\n",
      "Min loss: 0.007694243919104338\n",
      "Mean loss: 0.010344287923847636\n",
      "Std loss: 0.0016298212735635783\n",
      "Total Loss: 0.062065727543085814\n",
      "------------------------------------ epoch 4505 (27024 steps) ------------------------------------\n",
      "Max loss: 0.020629439502954483\n",
      "Min loss: 0.006005441304296255\n",
      "Mean loss: 0.011998933351909121\n",
      "Std loss: 0.004750091388447099\n",
      "Total Loss: 0.07199360011145473\n",
      "------------------------------------ epoch 4506 (27030 steps) ------------------------------------\n",
      "Max loss: 0.021588142961263657\n",
      "Min loss: 0.0071808407083153725\n",
      "Mean loss: 0.013724599964916706\n",
      "Std loss: 0.005368170083334845\n",
      "Total Loss: 0.08234759978950024\n",
      "------------------------------------ epoch 4507 (27036 steps) ------------------------------------\n",
      "Max loss: 0.018189232796430588\n",
      "Min loss: 0.00578821636736393\n",
      "Mean loss: 0.010440703326215347\n",
      "Std loss: 0.004139494568276657\n",
      "Total Loss: 0.06264421995729208\n",
      "------------------------------------ epoch 4508 (27042 steps) ------------------------------------\n",
      "Max loss: 0.01779714971780777\n",
      "Min loss: 0.008051840588450432\n",
      "Mean loss: 0.010849942297985157\n",
      "Std loss: 0.003453590015438086\n",
      "Total Loss: 0.06509965378791094\n",
      "------------------------------------ epoch 4509 (27048 steps) ------------------------------------\n",
      "Max loss: 0.035369955003261566\n",
      "Min loss: 0.007292745169252157\n",
      "Mean loss: 0.01297812513075769\n",
      "Std loss: 0.010053643149617117\n",
      "Total Loss: 0.07786875078454614\n",
      "------------------------------------ epoch 4510 (27054 steps) ------------------------------------\n",
      "Max loss: 0.01864287257194519\n",
      "Min loss: 0.008090279996395111\n",
      "Mean loss: 0.011885255575180054\n",
      "Std loss: 0.003542495522533407\n",
      "Total Loss: 0.07131153345108032\n",
      "------------------------------------ epoch 4511 (27060 steps) ------------------------------------\n",
      "Max loss: 0.02084195241332054\n",
      "Min loss: 0.006119640544056892\n",
      "Mean loss: 0.012902907561510801\n",
      "Std loss: 0.004855255993268385\n",
      "Total Loss: 0.07741744536906481\n",
      "------------------------------------ epoch 4512 (27066 steps) ------------------------------------\n",
      "Max loss: 0.012779834680259228\n",
      "Min loss: 0.007798607461154461\n",
      "Mean loss: 0.010524583825220665\n",
      "Std loss: 0.0020845374884308106\n",
      "Total Loss: 0.06314750295132399\n",
      "------------------------------------ epoch 4513 (27072 steps) ------------------------------------\n",
      "Max loss: 0.01787983812391758\n",
      "Min loss: 0.005749969743192196\n",
      "Mean loss: 0.010009652081256112\n",
      "Std loss: 0.0045415281626327375\n",
      "Total Loss: 0.06005791248753667\n",
      "------------------------------------ epoch 4514 (27078 steps) ------------------------------------\n",
      "Max loss: 0.016209695488214493\n",
      "Min loss: 0.0060228160582482815\n",
      "Mean loss: 0.009968092665076256\n",
      "Std loss: 0.003903605170407093\n",
      "Total Loss: 0.059808555990457535\n",
      "------------------------------------ epoch 4515 (27084 steps) ------------------------------------\n",
      "Max loss: 0.012841885909438133\n",
      "Min loss: 0.006498799659311771\n",
      "Mean loss: 0.009108948986977339\n",
      "Std loss: 0.002057483683550171\n",
      "Total Loss: 0.05465369392186403\n",
      "------------------------------------ epoch 4516 (27090 steps) ------------------------------------\n",
      "Max loss: 0.019278865307569504\n",
      "Min loss: 0.005149147938936949\n",
      "Mean loss: 0.010337677861874303\n",
      "Std loss: 0.004442104818195234\n",
      "Total Loss: 0.06202606717124581\n",
      "------------------------------------ epoch 4517 (27096 steps) ------------------------------------\n",
      "Max loss: 0.011729761026799679\n",
      "Min loss: 0.0042611295357346535\n",
      "Mean loss: 0.008212252287194133\n",
      "Std loss: 0.003244959650551959\n",
      "Total Loss: 0.0492735137231648\n",
      "------------------------------------ epoch 4518 (27102 steps) ------------------------------------\n",
      "Max loss: 0.012810289859771729\n",
      "Min loss: 0.004290827549993992\n",
      "Mean loss: 0.007947250191743175\n",
      "Std loss: 0.003106124650216526\n",
      "Total Loss: 0.04768350115045905\n",
      "------------------------------------ epoch 4519 (27108 steps) ------------------------------------\n",
      "Max loss: 0.011252234689891338\n",
      "Min loss: 0.005225741770118475\n",
      "Mean loss: 0.008283656711379686\n",
      "Std loss: 0.0023154956945901288\n",
      "Total Loss: 0.04970194026827812\n",
      "------------------------------------ epoch 4520 (27114 steps) ------------------------------------\n",
      "Max loss: 0.017672700807452202\n",
      "Min loss: 0.00527867441996932\n",
      "Mean loss: 0.009150191132600108\n",
      "Std loss: 0.004514604928113417\n",
      "Total Loss: 0.05490114679560065\n",
      "------------------------------------ epoch 4521 (27120 steps) ------------------------------------\n",
      "Max loss: 0.042676620185375214\n",
      "Min loss: 0.008067894726991653\n",
      "Mean loss: 0.016093574464321136\n",
      "Std loss: 0.012059352700873561\n",
      "Total Loss: 0.09656144678592682\n",
      "------------------------------------ epoch 4522 (27126 steps) ------------------------------------\n",
      "Max loss: 0.03332215175032616\n",
      "Min loss: 0.0062406244687736034\n",
      "Mean loss: 0.014247430876518289\n",
      "Std loss: 0.009169371446839497\n",
      "Total Loss: 0.08548458525910974\n",
      "------------------------------------ epoch 4523 (27132 steps) ------------------------------------\n",
      "Max loss: 0.03920410946011543\n",
      "Min loss: 0.006185445934534073\n",
      "Mean loss: 0.01934363041073084\n",
      "Std loss: 0.011742657723068668\n",
      "Total Loss: 0.11606178246438503\n",
      "------------------------------------ epoch 4524 (27138 steps) ------------------------------------\n",
      "Max loss: 0.01756991818547249\n",
      "Min loss: 0.008508728817105293\n",
      "Mean loss: 0.012755575124174356\n",
      "Std loss: 0.0033026015155650187\n",
      "Total Loss: 0.07653345074504614\n",
      "------------------------------------ epoch 4525 (27144 steps) ------------------------------------\n",
      "Max loss: 0.02852455899119377\n",
      "Min loss: 0.010028168559074402\n",
      "Mean loss: 0.01485721937691172\n",
      "Std loss: 0.006416033091924245\n",
      "Total Loss: 0.08914331626147032\n",
      "------------------------------------ epoch 4526 (27150 steps) ------------------------------------\n",
      "Max loss: 0.025996601209044456\n",
      "Min loss: 0.005615188740193844\n",
      "Mean loss: 0.0159437811623017\n",
      "Std loss: 0.008816253143146666\n",
      "Total Loss: 0.0956626869738102\n",
      "------------------------------------ epoch 4527 (27156 steps) ------------------------------------\n",
      "Max loss: 0.03900553286075592\n",
      "Min loss: 0.007774601690471172\n",
      "Mean loss: 0.016127505029241245\n",
      "Std loss: 0.011423718433048841\n",
      "Total Loss: 0.09676503017544746\n",
      "------------------------------------ epoch 4528 (27162 steps) ------------------------------------\n",
      "Max loss: 0.01892310380935669\n",
      "Min loss: 0.009207150898873806\n",
      "Mean loss: 0.011975813501824936\n",
      "Std loss: 0.0036204295830330243\n",
      "Total Loss: 0.07185488101094961\n",
      "------------------------------------ epoch 4529 (27168 steps) ------------------------------------\n",
      "Max loss: 0.05394711345434189\n",
      "Min loss: 0.00735682575032115\n",
      "Mean loss: 0.016521670467530686\n",
      "Std loss: 0.016789621116301192\n",
      "Total Loss: 0.09913002280518413\n",
      "------------------------------------ epoch 4530 (27174 steps) ------------------------------------\n",
      "Max loss: 0.02507242187857628\n",
      "Min loss: 0.007857035845518112\n",
      "Mean loss: 0.013160573473821083\n",
      "Std loss: 0.006461250326597878\n",
      "Total Loss: 0.0789634408429265\n",
      "------------------------------------ epoch 4531 (27180 steps) ------------------------------------\n",
      "Max loss: 0.014777427539229393\n",
      "Min loss: 0.00613398989662528\n",
      "Mean loss: 0.010036476810152331\n",
      "Std loss: 0.0027331237493515566\n",
      "Total Loss: 0.06021886086091399\n",
      "------------------------------------ epoch 4532 (27186 steps) ------------------------------------\n",
      "Max loss: 0.014109302312135696\n",
      "Min loss: 0.005601303186267614\n",
      "Mean loss: 0.009177764179185033\n",
      "Std loss: 0.003333147134330659\n",
      "Total Loss: 0.0550665850751102\n",
      "------------------------------------ epoch 4533 (27192 steps) ------------------------------------\n",
      "Max loss: 0.02707168273627758\n",
      "Min loss: 0.005524141248315573\n",
      "Mean loss: 0.010674224855999151\n",
      "Std loss: 0.007457617478868767\n",
      "Total Loss: 0.06404534913599491\n",
      "------------------------------------ epoch 4534 (27198 steps) ------------------------------------\n",
      "Max loss: 0.023484718054533005\n",
      "Min loss: 0.006761542521417141\n",
      "Mean loss: 0.0121542289853096\n",
      "Std loss: 0.0060126012045793676\n",
      "Total Loss: 0.0729253739118576\n",
      "------------------------------------ epoch 4535 (27204 steps) ------------------------------------\n",
      "Max loss: 0.029430247843265533\n",
      "Min loss: 0.006937635131180286\n",
      "Mean loss: 0.014342843710134426\n",
      "Std loss: 0.008479488726097599\n",
      "Total Loss: 0.08605706226080656\n",
      "------------------------------------ epoch 4536 (27210 steps) ------------------------------------\n",
      "Max loss: 0.01702490821480751\n",
      "Min loss: 0.008358646184206009\n",
      "Mean loss: 0.011864771911253532\n",
      "Std loss: 0.002881942364417563\n",
      "Total Loss: 0.07118863146752119\n",
      "------------------------------------ epoch 4537 (27216 steps) ------------------------------------\n",
      "Max loss: 0.013474254868924618\n",
      "Min loss: 0.006467184983193874\n",
      "Mean loss: 0.010147663919876019\n",
      "Std loss: 0.0022913224449128003\n",
      "Total Loss: 0.060885983519256115\n",
      "------------------------------------ epoch 4538 (27222 steps) ------------------------------------\n",
      "Max loss: 0.020473327487707138\n",
      "Min loss: 0.005250576883554459\n",
      "Mean loss: 0.00967938833249112\n",
      "Std loss: 0.005271997809512155\n",
      "Total Loss: 0.05807632999494672\n",
      "------------------------------------ epoch 4539 (27228 steps) ------------------------------------\n",
      "Max loss: 0.03198786452412605\n",
      "Min loss: 0.006066575646400452\n",
      "Mean loss: 0.014493529219180346\n",
      "Std loss: 0.008759591054596111\n",
      "Total Loss: 0.08696117531508207\n",
      "------------------------------------ epoch 4540 (27234 steps) ------------------------------------\n",
      "Max loss: 0.017965976148843765\n",
      "Min loss: 0.00542322127148509\n",
      "Mean loss: 0.00874589430168271\n",
      "Std loss: 0.0044441117886817655\n",
      "Total Loss: 0.052475365810096264\n",
      "------------------------------------ epoch 4541 (27240 steps) ------------------------------------\n",
      "Max loss: 0.017216352745890617\n",
      "Min loss: 0.0067995460703969\n",
      "Mean loss: 0.011817623550693193\n",
      "Std loss: 0.0037854639050279807\n",
      "Total Loss: 0.07090574130415916\n",
      "------------------------------------ epoch 4542 (27246 steps) ------------------------------------\n",
      "Max loss: 0.03546338155865669\n",
      "Min loss: 0.007209816947579384\n",
      "Mean loss: 0.012895863270387053\n",
      "Std loss: 0.010128978896593472\n",
      "Total Loss: 0.07737517962232232\n",
      "------------------------------------ epoch 4543 (27252 steps) ------------------------------------\n",
      "Max loss: 0.025255903601646423\n",
      "Min loss: 0.008526614867150784\n",
      "Mean loss: 0.01697326296319564\n",
      "Std loss: 0.005616535141060212\n",
      "Total Loss: 0.10183957777917385\n",
      "------------------------------------ epoch 4544 (27258 steps) ------------------------------------\n",
      "Max loss: 0.040048837661743164\n",
      "Min loss: 0.006402510683983564\n",
      "Mean loss: 0.015318767400458455\n",
      "Std loss: 0.011747303198771757\n",
      "Total Loss: 0.09191260440275073\n",
      "------------------------------------ epoch 4545 (27264 steps) ------------------------------------\n",
      "Max loss: 0.013553190045058727\n",
      "Min loss: 0.005567604675889015\n",
      "Mean loss: 0.010308691300451756\n",
      "Std loss: 0.0025466918467394396\n",
      "Total Loss: 0.06185214780271053\n",
      "------------------------------------ epoch 4546 (27270 steps) ------------------------------------\n",
      "Max loss: 0.023174187168478966\n",
      "Min loss: 0.005930433981120586\n",
      "Mean loss: 0.01264256564900279\n",
      "Std loss: 0.005319087568897867\n",
      "Total Loss: 0.07585539389401674\n",
      "------------------------------------ epoch 4547 (27276 steps) ------------------------------------\n",
      "Max loss: 0.016735553741455078\n",
      "Min loss: 0.005685898941010237\n",
      "Mean loss: 0.010271634751309952\n",
      "Std loss: 0.003835854980141752\n",
      "Total Loss: 0.06162980850785971\n",
      "------------------------------------ epoch 4548 (27282 steps) ------------------------------------\n",
      "Max loss: 0.012151777744293213\n",
      "Min loss: 0.00530241196975112\n",
      "Mean loss: 0.008375134319067001\n",
      "Std loss: 0.002629435098997778\n",
      "Total Loss: 0.05025080591440201\n",
      "------------------------------------ epoch 4549 (27288 steps) ------------------------------------\n",
      "Max loss: 0.016434840857982635\n",
      "Min loss: 0.007562549319118261\n",
      "Mean loss: 0.010397905095790824\n",
      "Std loss: 0.0029961514426684495\n",
      "Total Loss: 0.06238743057474494\n",
      "------------------------------------ epoch 4550 (27294 steps) ------------------------------------\n",
      "Max loss: 0.02218792401254177\n",
      "Min loss: 0.006922069471329451\n",
      "Mean loss: 0.01206258723201851\n",
      "Std loss: 0.0056128625221348\n",
      "Total Loss: 0.07237552339211106\n",
      "------------------------------------ epoch 4551 (27300 steps) ------------------------------------\n",
      "Max loss: 0.02498873695731163\n",
      "Min loss: 0.005155803635716438\n",
      "Mean loss: 0.012714656147484979\n",
      "Std loss: 0.00788187078059455\n",
      "Total Loss: 0.07628793688490987\n",
      "------------------------------------ epoch 4552 (27306 steps) ------------------------------------\n",
      "Max loss: 0.013990545645356178\n",
      "Min loss: 0.005796806886792183\n",
      "Mean loss: 0.009175109987457594\n",
      "Std loss: 0.003087934992235604\n",
      "Total Loss: 0.05505065992474556\n",
      "------------------------------------ epoch 4553 (27312 steps) ------------------------------------\n",
      "Max loss: 0.021128704771399498\n",
      "Min loss: 0.005818215198814869\n",
      "Mean loss: 0.01216179272159934\n",
      "Std loss: 0.005602348250472024\n",
      "Total Loss: 0.07297075632959604\n",
      "------------------------------------ epoch 4554 (27318 steps) ------------------------------------\n",
      "Max loss: 0.02783709391951561\n",
      "Min loss: 0.007222683634608984\n",
      "Mean loss: 0.013253588850299517\n",
      "Std loss: 0.007065643227349692\n",
      "Total Loss: 0.0795215331017971\n",
      "------------------------------------ epoch 4555 (27324 steps) ------------------------------------\n",
      "Max loss: 0.02508278749883175\n",
      "Min loss: 0.005407251417636871\n",
      "Mean loss: 0.01214046636596322\n",
      "Std loss: 0.006546494529873393\n",
      "Total Loss: 0.07284279819577932\n",
      "------------------------------------ epoch 4556 (27330 steps) ------------------------------------\n",
      "Max loss: 0.023725001141428947\n",
      "Min loss: 0.005310667213052511\n",
      "Mean loss: 0.012603238380203644\n",
      "Std loss: 0.006084597093962157\n",
      "Total Loss: 0.07561943028122187\n",
      "------------------------------------ epoch 4557 (27336 steps) ------------------------------------\n",
      "Max loss: 0.021834101527929306\n",
      "Min loss: 0.00710719358175993\n",
      "Mean loss: 0.012172083836048841\n",
      "Std loss: 0.005447348269937947\n",
      "Total Loss: 0.07303250301629305\n",
      "------------------------------------ epoch 4558 (27342 steps) ------------------------------------\n",
      "Max loss: 0.04760168120265007\n",
      "Min loss: 0.0084028709679842\n",
      "Mean loss: 0.017992626565198105\n",
      "Std loss: 0.013470676436188607\n",
      "Total Loss: 0.10795575939118862\n",
      "------------------------------------ epoch 4559 (27348 steps) ------------------------------------\n",
      "Max loss: 0.01944204792380333\n",
      "Min loss: 0.00792875699698925\n",
      "Mean loss: 0.012069332568595806\n",
      "Std loss: 0.004230376655563692\n",
      "Total Loss: 0.07241599541157484\n",
      "------------------------------------ epoch 4560 (27354 steps) ------------------------------------\n",
      "Max loss: 0.044643089175224304\n",
      "Min loss: 0.0074064964428544044\n",
      "Mean loss: 0.020346767424295347\n",
      "Std loss: 0.013760957502797284\n",
      "Total Loss: 0.12208060454577208\n",
      "------------------------------------ epoch 4561 (27360 steps) ------------------------------------\n",
      "Max loss: 0.01349045243114233\n",
      "Min loss: 0.00861825980246067\n",
      "Mean loss: 0.010726443026214838\n",
      "Std loss: 0.0016595066318130533\n",
      "Total Loss: 0.06435865815728903\n",
      "------------------------------------ epoch 4562 (27366 steps) ------------------------------------\n",
      "Max loss: 0.02019135095179081\n",
      "Min loss: 0.008968534879386425\n",
      "Mean loss: 0.013451655705769857\n",
      "Std loss: 0.003751841797424524\n",
      "Total Loss: 0.08070993423461914\n",
      "------------------------------------ epoch 4563 (27372 steps) ------------------------------------\n",
      "Max loss: 0.024214357137680054\n",
      "Min loss: 0.007905593141913414\n",
      "Mean loss: 0.016684188817938168\n",
      "Std loss: 0.006493863132898564\n",
      "Total Loss: 0.10010513290762901\n",
      "------------------------------------ epoch 4564 (27378 steps) ------------------------------------\n",
      "Max loss: 0.013450085185468197\n",
      "Min loss: 0.006319201551377773\n",
      "Mean loss: 0.009949977820118269\n",
      "Std loss: 0.002689438862508586\n",
      "Total Loss: 0.05969986692070961\n",
      "------------------------------------ epoch 4565 (27384 steps) ------------------------------------\n",
      "Max loss: 0.018039032816886902\n",
      "Min loss: 0.006934880744665861\n",
      "Mean loss: 0.011487541332220038\n",
      "Std loss: 0.0036819902144025955\n",
      "Total Loss: 0.06892524799332023\n",
      "------------------------------------ epoch 4566 (27390 steps) ------------------------------------\n",
      "Max loss: 0.020408017560839653\n",
      "Min loss: 0.007675237022340298\n",
      "Mean loss: 0.01316776763026913\n",
      "Std loss: 0.005112187508012349\n",
      "Total Loss: 0.07900660578161478\n",
      "------------------------------------ epoch 4567 (27396 steps) ------------------------------------\n",
      "Max loss: 0.018976464867591858\n",
      "Min loss: 0.006597178988158703\n",
      "Mean loss: 0.010229373583570123\n",
      "Std loss: 0.004110643622596772\n",
      "Total Loss: 0.061376241501420736\n",
      "------------------------------------ epoch 4568 (27402 steps) ------------------------------------\n",
      "Max loss: 0.039899565279483795\n",
      "Min loss: 0.00640898710116744\n",
      "Mean loss: 0.014736138051375747\n",
      "Std loss: 0.011721733392796304\n",
      "Total Loss: 0.08841682830825448\n",
      "------------------------------------ epoch 4569 (27408 steps) ------------------------------------\n",
      "Max loss: 0.08310900628566742\n",
      "Min loss: 0.007933594286441803\n",
      "Mean loss: 0.0315219348606964\n",
      "Std loss: 0.02955885391745784\n",
      "Total Loss: 0.18913160916417837\n",
      "------------------------------------ epoch 4570 (27414 steps) ------------------------------------\n",
      "Max loss: 0.0195621307939291\n",
      "Min loss: 0.007007046602666378\n",
      "Mean loss: 0.011535986792296171\n",
      "Std loss: 0.003989108877351709\n",
      "Total Loss: 0.06921592075377703\n",
      "------------------------------------ epoch 4571 (27420 steps) ------------------------------------\n",
      "Max loss: 0.027208052575588226\n",
      "Min loss: 0.006052385549992323\n",
      "Mean loss: 0.016424614392841857\n",
      "Std loss: 0.0065740215798696195\n",
      "Total Loss: 0.09854768635705113\n",
      "------------------------------------ epoch 4572 (27426 steps) ------------------------------------\n",
      "Max loss: 0.031364113092422485\n",
      "Min loss: 0.0058298371732234955\n",
      "Mean loss: 0.013604385157426199\n",
      "Std loss: 0.008527051469105276\n",
      "Total Loss: 0.08162631094455719\n",
      "------------------------------------ epoch 4573 (27432 steps) ------------------------------------\n",
      "Max loss: 0.01909358985722065\n",
      "Min loss: 0.0060163638554513454\n",
      "Mean loss: 0.009624933203061422\n",
      "Std loss: 0.004724283171238047\n",
      "Total Loss: 0.05774959921836853\n",
      "------------------------------------ epoch 4574 (27438 steps) ------------------------------------\n",
      "Max loss: 0.05140731856226921\n",
      "Min loss: 0.0065078455954790115\n",
      "Mean loss: 0.021235039147237938\n",
      "Std loss: 0.015306492454354555\n",
      "Total Loss: 0.12741023488342762\n",
      "------------------------------------ epoch 4575 (27444 steps) ------------------------------------\n",
      "Max loss: 0.018719114363193512\n",
      "Min loss: 0.007789842784404755\n",
      "Mean loss: 0.011029801176240047\n",
      "Std loss: 0.0036586266773285096\n",
      "Total Loss: 0.06617880705744028\n",
      "------------------------------------ epoch 4576 (27450 steps) ------------------------------------\n",
      "Max loss: 0.028903108090162277\n",
      "Min loss: 0.008744536899030209\n",
      "Mean loss: 0.013615179962168137\n",
      "Std loss: 0.006962919991153402\n",
      "Total Loss: 0.08169107977300882\n",
      "------------------------------------ epoch 4577 (27456 steps) ------------------------------------\n",
      "Max loss: 0.034698981791734695\n",
      "Min loss: 0.005725587718188763\n",
      "Mean loss: 0.012303779367357492\n",
      "Std loss: 0.010175286114841262\n",
      "Total Loss: 0.07382267620414495\n",
      "------------------------------------ epoch 4578 (27462 steps) ------------------------------------\n",
      "Max loss: 0.030364755541086197\n",
      "Min loss: 0.0057264817878603935\n",
      "Mean loss: 0.012924138146142164\n",
      "Std loss: 0.008087714536256197\n",
      "Total Loss: 0.07754482887685299\n",
      "------------------------------------ epoch 4579 (27468 steps) ------------------------------------\n",
      "Max loss: 0.03142092004418373\n",
      "Min loss: 0.006294462364166975\n",
      "Mean loss: 0.012742460239678621\n",
      "Std loss: 0.008708161747910101\n",
      "Total Loss: 0.07645476143807173\n",
      "------------------------------------ epoch 4580 (27474 steps) ------------------------------------\n",
      "Max loss: 0.0281697865575552\n",
      "Min loss: 0.006955886259675026\n",
      "Mean loss: 0.01087994392340382\n",
      "Std loss: 0.007746888701668581\n",
      "Total Loss: 0.06527966354042292\n",
      "------------------------------------ epoch 4581 (27480 steps) ------------------------------------\n",
      "Max loss: 0.01867685839533806\n",
      "Min loss: 0.005123406182974577\n",
      "Mean loss: 0.009823628778879842\n",
      "Std loss: 0.0045000656870831535\n",
      "Total Loss: 0.05894177267327905\n",
      "------------------------------------ epoch 4582 (27486 steps) ------------------------------------\n",
      "Max loss: 0.033298786729574203\n",
      "Min loss: 0.005359993781894445\n",
      "Mean loss: 0.016168907439957064\n",
      "Std loss: 0.011792223992786425\n",
      "Total Loss: 0.09701344463974237\n",
      "------------------------------------ epoch 4583 (27492 steps) ------------------------------------\n",
      "Max loss: 0.01951894350349903\n",
      "Min loss: 0.005821145139634609\n",
      "Mean loss: 0.013159149326384068\n",
      "Std loss: 0.004982108390094468\n",
      "Total Loss: 0.0789548959583044\n",
      "------------------------------------ epoch 4584 (27498 steps) ------------------------------------\n",
      "Max loss: 0.027383282780647278\n",
      "Min loss: 0.005951891653239727\n",
      "Mean loss: 0.013992392535631856\n",
      "Std loss: 0.008525646424944886\n",
      "Total Loss: 0.08395435521379113\n",
      "------------------------------------ epoch 4585 (27504 steps) ------------------------------------\n",
      "Max loss: 0.01706727035343647\n",
      "Min loss: 0.00845163594931364\n",
      "Mean loss: 0.012527264809856812\n",
      "Std loss: 0.003170061800206605\n",
      "Total Loss: 0.07516358885914087\n",
      "------------------------------------ epoch 4586 (27510 steps) ------------------------------------\n",
      "Max loss: 0.02236984297633171\n",
      "Min loss: 0.005843690596520901\n",
      "Mean loss: 0.009939023526385427\n",
      "Std loss: 0.005748422818086685\n",
      "Total Loss: 0.05963414115831256\n",
      "------------------------------------ epoch 4587 (27516 steps) ------------------------------------\n",
      "Max loss: 0.023819176480174065\n",
      "Min loss: 0.00648470688611269\n",
      "Mean loss: 0.01479640199492375\n",
      "Std loss: 0.005871343175833115\n",
      "Total Loss: 0.0887784119695425\n",
      "------------------------------------ epoch 4588 (27522 steps) ------------------------------------\n",
      "Max loss: 0.03875909745693207\n",
      "Min loss: 0.01198427565395832\n",
      "Mean loss: 0.01907911120603482\n",
      "Std loss: 0.009390352117605012\n",
      "Total Loss: 0.11447466723620892\n",
      "------------------------------------ epoch 4589 (27528 steps) ------------------------------------\n",
      "Max loss: 0.024254772812128067\n",
      "Min loss: 0.007318432442843914\n",
      "Mean loss: 0.013335341432442268\n",
      "Std loss: 0.006148833074870075\n",
      "Total Loss: 0.0800120485946536\n",
      "------------------------------------ epoch 4590 (27534 steps) ------------------------------------\n",
      "Max loss: 0.02079951763153076\n",
      "Min loss: 0.005266824271529913\n",
      "Mean loss: 0.011382529279217124\n",
      "Std loss: 0.005223901888991034\n",
      "Total Loss: 0.06829517567530274\n",
      "------------------------------------ epoch 4591 (27540 steps) ------------------------------------\n",
      "Max loss: 0.0228160098195076\n",
      "Min loss: 0.005055466666817665\n",
      "Mean loss: 0.012798036603877941\n",
      "Std loss: 0.006113984430990923\n",
      "Total Loss: 0.07678821962326765\n",
      "------------------------------------ epoch 4592 (27546 steps) ------------------------------------\n",
      "Max loss: 0.017394183203577995\n",
      "Min loss: 0.007560783065855503\n",
      "Mean loss: 0.012358937412500381\n",
      "Std loss: 0.003943240458549081\n",
      "Total Loss: 0.07415362447500229\n",
      "------------------------------------ epoch 4593 (27552 steps) ------------------------------------\n",
      "Max loss: 0.023052355274558067\n",
      "Min loss: 0.006772778462618589\n",
      "Mean loss: 0.012732278633241853\n",
      "Std loss: 0.005440895607159013\n",
      "Total Loss: 0.07639367179945111\n",
      "------------------------------------ epoch 4594 (27558 steps) ------------------------------------\n",
      "Max loss: 0.013468126766383648\n",
      "Min loss: 0.009528633207082748\n",
      "Mean loss: 0.011417990395178398\n",
      "Std loss: 0.001410268341049184\n",
      "Total Loss: 0.06850794237107038\n",
      "------------------------------------ epoch 4595 (27564 steps) ------------------------------------\n",
      "Max loss: 0.018289588391780853\n",
      "Min loss: 0.007143047638237476\n",
      "Mean loss: 0.010706893634051085\n",
      "Std loss: 0.0036299219790549395\n",
      "Total Loss: 0.06424136180430651\n",
      "------------------------------------ epoch 4596 (27570 steps) ------------------------------------\n",
      "Max loss: 0.017930544912815094\n",
      "Min loss: 0.005822586826980114\n",
      "Mean loss: 0.011403272394090891\n",
      "Std loss: 0.003924490092037329\n",
      "Total Loss: 0.06841963436454535\n",
      "------------------------------------ epoch 4597 (27576 steps) ------------------------------------\n",
      "Max loss: 0.021419836208224297\n",
      "Min loss: 0.004907665774226189\n",
      "Mean loss: 0.00931306229904294\n",
      "Std loss: 0.00560002178803386\n",
      "Total Loss: 0.05587837379425764\n",
      "------------------------------------ epoch 4598 (27582 steps) ------------------------------------\n",
      "Max loss: 0.02171584963798523\n",
      "Min loss: 0.0054530855268239975\n",
      "Mean loss: 0.01235896535217762\n",
      "Std loss: 0.005738269959849181\n",
      "Total Loss: 0.07415379211306572\n",
      "------------------------------------ epoch 4599 (27588 steps) ------------------------------------\n",
      "Max loss: 0.01599062792956829\n",
      "Min loss: 0.006750054657459259\n",
      "Mean loss: 0.009876315171519915\n",
      "Std loss: 0.003492900326320587\n",
      "Total Loss: 0.05925789102911949\n",
      "------------------------------------ epoch 4600 (27594 steps) ------------------------------------\n",
      "Max loss: 0.02960050106048584\n",
      "Min loss: 0.006234775763005018\n",
      "Mean loss: 0.016468116237471502\n",
      "Std loss: 0.009410701838740848\n",
      "Total Loss: 0.098808697424829\n",
      "------------------------------------ epoch 4601 (27600 steps) ------------------------------------\n",
      "Max loss: 0.015424917452037334\n",
      "Min loss: 0.0058965375646948814\n",
      "Mean loss: 0.009986340378721556\n",
      "Std loss: 0.003229232785271271\n",
      "Total Loss: 0.05991804227232933\n",
      "saved model at ./weights/model_4601.pth\n",
      "------------------------------------ epoch 4602 (27606 steps) ------------------------------------\n",
      "Max loss: 0.02128194086253643\n",
      "Min loss: 0.005592044442892075\n",
      "Mean loss: 0.009463889601950845\n",
      "Std loss: 0.0054300327530545376\n",
      "Total Loss: 0.056783337611705065\n",
      "------------------------------------ epoch 4603 (27612 steps) ------------------------------------\n",
      "Max loss: 0.017397955060005188\n",
      "Min loss: 0.006256596650928259\n",
      "Mean loss: 0.010103386283541719\n",
      "Std loss: 0.0037247063273979433\n",
      "Total Loss: 0.060620317701250315\n",
      "------------------------------------ epoch 4604 (27618 steps) ------------------------------------\n",
      "Max loss: 0.018175875768065453\n",
      "Min loss: 0.0050979782827198505\n",
      "Mean loss: 0.008843018906190991\n",
      "Std loss: 0.00458772068086246\n",
      "Total Loss: 0.05305811343714595\n",
      "------------------------------------ epoch 4605 (27624 steps) ------------------------------------\n",
      "Max loss: 0.019823310896754265\n",
      "Min loss: 0.005372983869165182\n",
      "Mean loss: 0.013365692148605982\n",
      "Std loss: 0.0053302633946254695\n",
      "Total Loss: 0.0801941528916359\n",
      "------------------------------------ epoch 4606 (27630 steps) ------------------------------------\n",
      "Max loss: 0.02686839923262596\n",
      "Min loss: 0.005585433449596167\n",
      "Mean loss: 0.01068209003036221\n",
      "Std loss: 0.007326005755527385\n",
      "Total Loss: 0.06409254018217325\n",
      "------------------------------------ epoch 4607 (27636 steps) ------------------------------------\n",
      "Max loss: 0.020128589123487473\n",
      "Min loss: 0.008411023765802383\n",
      "Mean loss: 0.012050162069499493\n",
      "Std loss: 0.0037691509768492944\n",
      "Total Loss: 0.07230097241699696\n",
      "------------------------------------ epoch 4608 (27642 steps) ------------------------------------\n",
      "Max loss: 0.038756538182497025\n",
      "Min loss: 0.007396837696433067\n",
      "Mean loss: 0.014107653560737768\n",
      "Std loss: 0.01118546383959183\n",
      "Total Loss: 0.08464592136442661\n",
      "------------------------------------ epoch 4609 (27648 steps) ------------------------------------\n",
      "Max loss: 0.026387466117739677\n",
      "Min loss: 0.005730225704610348\n",
      "Mean loss: 0.01824234115580718\n",
      "Std loss: 0.00765181316210756\n",
      "Total Loss: 0.10945404693484306\n",
      "------------------------------------ epoch 4610 (27654 steps) ------------------------------------\n",
      "Max loss: 0.017109964042901993\n",
      "Min loss: 0.0065239123068749905\n",
      "Mean loss: 0.012338127552842101\n",
      "Std loss: 0.0033665127604685387\n",
      "Total Loss: 0.0740287653170526\n",
      "------------------------------------ epoch 4611 (27660 steps) ------------------------------------\n",
      "Max loss: 0.03343681991100311\n",
      "Min loss: 0.008698806166648865\n",
      "Mean loss: 0.01690629031509161\n",
      "Std loss: 0.008132577772207057\n",
      "Total Loss: 0.10143774189054966\n",
      "------------------------------------ epoch 4612 (27666 steps) ------------------------------------\n",
      "Max loss: 0.03223702311515808\n",
      "Min loss: 0.010320104658603668\n",
      "Mean loss: 0.02066259862234195\n",
      "Std loss: 0.008352275413774576\n",
      "Total Loss: 0.1239755917340517\n",
      "------------------------------------ epoch 4613 (27672 steps) ------------------------------------\n",
      "Max loss: 0.03199993818998337\n",
      "Min loss: 0.009470081888139248\n",
      "Mean loss: 0.01620053080841899\n",
      "Std loss: 0.007717913757688241\n",
      "Total Loss: 0.09720318485051394\n",
      "------------------------------------ epoch 4614 (27678 steps) ------------------------------------\n",
      "Max loss: 0.020281044766306877\n",
      "Min loss: 0.008510733023285866\n",
      "Mean loss: 0.012946201798816523\n",
      "Std loss: 0.004087566984492427\n",
      "Total Loss: 0.07767721079289913\n",
      "------------------------------------ epoch 4615 (27684 steps) ------------------------------------\n",
      "Max loss: 0.014440886676311493\n",
      "Min loss: 0.007078587077558041\n",
      "Mean loss: 0.010311608202755451\n",
      "Std loss: 0.002519335113955648\n",
      "Total Loss: 0.06186964921653271\n",
      "------------------------------------ epoch 4616 (27690 steps) ------------------------------------\n",
      "Max loss: 0.010219565592706203\n",
      "Min loss: 0.006009586155414581\n",
      "Mean loss: 0.00819335924461484\n",
      "Std loss: 0.0015877884230435148\n",
      "Total Loss: 0.04916015546768904\n",
      "------------------------------------ epoch 4617 (27696 steps) ------------------------------------\n",
      "Max loss: 0.020455628633499146\n",
      "Min loss: 0.00640679057687521\n",
      "Mean loss: 0.012075693812221289\n",
      "Std loss: 0.005590285981366753\n",
      "Total Loss: 0.07245416287332773\n",
      "------------------------------------ epoch 4618 (27702 steps) ------------------------------------\n",
      "Max loss: 0.023843875154852867\n",
      "Min loss: 0.008203942328691483\n",
      "Mean loss: 0.01364465399334828\n",
      "Std loss: 0.005469109177049313\n",
      "Total Loss: 0.08186792396008968\n",
      "------------------------------------ epoch 4619 (27708 steps) ------------------------------------\n",
      "Max loss: 0.018207930028438568\n",
      "Min loss: 0.007667760364711285\n",
      "Mean loss: 0.011611143437524637\n",
      "Std loss: 0.0037968206287051975\n",
      "Total Loss: 0.06966686062514782\n",
      "------------------------------------ epoch 4620 (27714 steps) ------------------------------------\n",
      "Max loss: 0.025435276329517365\n",
      "Min loss: 0.008605275303125381\n",
      "Mean loss: 0.014996647058675686\n",
      "Std loss: 0.006008881909792566\n",
      "Total Loss: 0.08997988235205412\n",
      "------------------------------------ epoch 4621 (27720 steps) ------------------------------------\n",
      "Max loss: 0.05727052688598633\n",
      "Min loss: 0.006804309785366058\n",
      "Mean loss: 0.019408164313063025\n",
      "Std loss: 0.01730269224473301\n",
      "Total Loss: 0.11644898587837815\n",
      "------------------------------------ epoch 4622 (27726 steps) ------------------------------------\n",
      "Max loss: 0.02248392254114151\n",
      "Min loss: 0.009122400544583797\n",
      "Mean loss: 0.01488122840722402\n",
      "Std loss: 0.0058068446420687\n",
      "Total Loss: 0.08928737044334412\n",
      "------------------------------------ epoch 4623 (27732 steps) ------------------------------------\n",
      "Max loss: 0.021049922332167625\n",
      "Min loss: 0.006417722906917334\n",
      "Mean loss: 0.010106760698060194\n",
      "Std loss: 0.005152951816898118\n",
      "Total Loss: 0.06064056418836117\n",
      "------------------------------------ epoch 4624 (27738 steps) ------------------------------------\n",
      "Max loss: 0.01183497067540884\n",
      "Min loss: 0.006111352704465389\n",
      "Mean loss: 0.008582646492868662\n",
      "Std loss: 0.0018588192541716115\n",
      "Total Loss: 0.05149587895721197\n",
      "------------------------------------ epoch 4625 (27744 steps) ------------------------------------\n",
      "Max loss: 0.035790424793958664\n",
      "Min loss: 0.005568932741880417\n",
      "Mean loss: 0.013547403272241354\n",
      "Std loss: 0.01045170576807865\n",
      "Total Loss: 0.08128441963344812\n",
      "------------------------------------ epoch 4626 (27750 steps) ------------------------------------\n",
      "Max loss: 0.02378344163298607\n",
      "Min loss: 0.008201425895094872\n",
      "Mean loss: 0.013496845805396637\n",
      "Std loss: 0.005523165463965457\n",
      "Total Loss: 0.08098107483237982\n",
      "------------------------------------ epoch 4627 (27756 steps) ------------------------------------\n",
      "Max loss: 0.04102298617362976\n",
      "Min loss: 0.007660062052309513\n",
      "Mean loss: 0.016971583012491465\n",
      "Std loss: 0.011216039354192547\n",
      "Total Loss: 0.10182949807494879\n",
      "------------------------------------ epoch 4628 (27762 steps) ------------------------------------\n",
      "Max loss: 0.016818704083561897\n",
      "Min loss: 0.005637607537209988\n",
      "Mean loss: 0.010002699525405964\n",
      "Std loss: 0.0035542452102281354\n",
      "Total Loss: 0.06001619715243578\n",
      "------------------------------------ epoch 4629 (27768 steps) ------------------------------------\n",
      "Max loss: 0.01651025004684925\n",
      "Min loss: 0.006431292742490768\n",
      "Mean loss: 0.010869915829971433\n",
      "Std loss: 0.0037082582105415107\n",
      "Total Loss: 0.0652194949798286\n",
      "------------------------------------ epoch 4630 (27774 steps) ------------------------------------\n",
      "Max loss: 0.027402743697166443\n",
      "Min loss: 0.005346297286450863\n",
      "Mean loss: 0.013911720210065445\n",
      "Std loss: 0.006913836389459018\n",
      "Total Loss: 0.08347032126039267\n",
      "------------------------------------ epoch 4631 (27780 steps) ------------------------------------\n",
      "Max loss: 0.013025193475186825\n",
      "Min loss: 0.006059050559997559\n",
      "Mean loss: 0.008284202621628841\n",
      "Std loss: 0.0022687072065638095\n",
      "Total Loss: 0.049705215729773045\n",
      "------------------------------------ epoch 4632 (27786 steps) ------------------------------------\n",
      "Max loss: 0.017272986471652985\n",
      "Min loss: 0.008228527382016182\n",
      "Mean loss: 0.01114038952315847\n",
      "Std loss: 0.0030272647764070597\n",
      "Total Loss: 0.06684233713895082\n",
      "------------------------------------ epoch 4633 (27792 steps) ------------------------------------\n",
      "Max loss: 0.014733051881194115\n",
      "Min loss: 0.005734249018132687\n",
      "Mean loss: 0.010098207276314497\n",
      "Std loss: 0.0028930598086102754\n",
      "Total Loss: 0.06058924365788698\n",
      "------------------------------------ epoch 4634 (27798 steps) ------------------------------------\n",
      "Max loss: 0.010394962504506111\n",
      "Min loss: 0.00572486175224185\n",
      "Mean loss: 0.008091784470404187\n",
      "Std loss: 0.001701586779727661\n",
      "Total Loss: 0.04855070682242513\n",
      "------------------------------------ epoch 4635 (27804 steps) ------------------------------------\n",
      "Max loss: 0.03502281382679939\n",
      "Min loss: 0.008205673657357693\n",
      "Mean loss: 0.01863009746496876\n",
      "Std loss: 0.009273010006484119\n",
      "Total Loss: 0.11178058478981256\n",
      "------------------------------------ epoch 4636 (27810 steps) ------------------------------------\n",
      "Max loss: 0.015433287248015404\n",
      "Min loss: 0.006116980221122503\n",
      "Mean loss: 0.009922542842105031\n",
      "Std loss: 0.0032821529717366894\n",
      "Total Loss: 0.059535257052630186\n",
      "------------------------------------ epoch 4637 (27816 steps) ------------------------------------\n",
      "Max loss: 0.020626801997423172\n",
      "Min loss: 0.010127141140401363\n",
      "Mean loss: 0.015173939988017082\n",
      "Std loss: 0.003100907005553753\n",
      "Total Loss: 0.0910436399281025\n",
      "------------------------------------ epoch 4638 (27822 steps) ------------------------------------\n",
      "Max loss: 0.014623480848968029\n",
      "Min loss: 0.007070144638419151\n",
      "Mean loss: 0.010960847915460667\n",
      "Std loss: 0.002662464384703724\n",
      "Total Loss: 0.065765087492764\n",
      "------------------------------------ epoch 4639 (27828 steps) ------------------------------------\n",
      "Max loss: 0.023547351360321045\n",
      "Min loss: 0.006009895354509354\n",
      "Mean loss: 0.0143609798202912\n",
      "Std loss: 0.00685186059469787\n",
      "Total Loss: 0.08616587892174721\n",
      "------------------------------------ epoch 4640 (27834 steps) ------------------------------------\n",
      "Max loss: 0.010246245190501213\n",
      "Min loss: 0.005903225392103195\n",
      "Mean loss: 0.007803527793536584\n",
      "Std loss: 0.0013508933670819269\n",
      "Total Loss: 0.0468211667612195\n",
      "------------------------------------ epoch 4641 (27840 steps) ------------------------------------\n",
      "Max loss: 0.011812346056103706\n",
      "Min loss: 0.005416820757091045\n",
      "Mean loss: 0.007478725087518494\n",
      "Std loss: 0.0021921027861004807\n",
      "Total Loss: 0.04487235052511096\n",
      "------------------------------------ epoch 4642 (27846 steps) ------------------------------------\n",
      "Max loss: 0.03540075197815895\n",
      "Min loss: 0.009400739334523678\n",
      "Mean loss: 0.020127892028540373\n",
      "Std loss: 0.008485109913131778\n",
      "Total Loss: 0.12076735217124224\n",
      "------------------------------------ epoch 4643 (27852 steps) ------------------------------------\n",
      "Max loss: 0.01609995774924755\n",
      "Min loss: 0.006879397667944431\n",
      "Mean loss: 0.010476401230941216\n",
      "Std loss: 0.0034927968606061855\n",
      "Total Loss: 0.0628584073856473\n",
      "------------------------------------ epoch 4644 (27858 steps) ------------------------------------\n",
      "Max loss: 0.02473529241979122\n",
      "Min loss: 0.008014748804271221\n",
      "Mean loss: 0.01277095933134357\n",
      "Std loss: 0.005480554344211971\n",
      "Total Loss: 0.07662575598806143\n",
      "------------------------------------ epoch 4645 (27864 steps) ------------------------------------\n",
      "Max loss: 0.023404192179441452\n",
      "Min loss: 0.007250973954796791\n",
      "Mean loss: 0.014131314043576518\n",
      "Std loss: 0.006256876702340325\n",
      "Total Loss: 0.08478788426145911\n",
      "------------------------------------ epoch 4646 (27870 steps) ------------------------------------\n",
      "Max loss: 0.015074996277689934\n",
      "Min loss: 0.007735291495919228\n",
      "Mean loss: 0.010534758058687052\n",
      "Std loss: 0.002633805069110494\n",
      "Total Loss: 0.0632085483521223\n",
      "------------------------------------ epoch 4647 (27876 steps) ------------------------------------\n",
      "Max loss: 0.015043522231280804\n",
      "Min loss: 0.007783952169120312\n",
      "Mean loss: 0.010433105130990347\n",
      "Std loss: 0.0030173481658823517\n",
      "Total Loss: 0.06259863078594208\n",
      "------------------------------------ epoch 4648 (27882 steps) ------------------------------------\n",
      "Max loss: 0.03790302574634552\n",
      "Min loss: 0.008567379787564278\n",
      "Mean loss: 0.01784685766324401\n",
      "Std loss: 0.00953470170376608\n",
      "Total Loss: 0.10708114597946405\n",
      "------------------------------------ epoch 4649 (27888 steps) ------------------------------------\n",
      "Max loss: 0.013458868488669395\n",
      "Min loss: 0.007295913062989712\n",
      "Mean loss: 0.010327759974946579\n",
      "Std loss: 0.002718515990888534\n",
      "Total Loss: 0.06196655984967947\n",
      "------------------------------------ epoch 4650 (27894 steps) ------------------------------------\n",
      "Max loss: 0.011363799683749676\n",
      "Min loss: 0.006100393831729889\n",
      "Mean loss: 0.009138288286825022\n",
      "Std loss: 0.0021229983253403034\n",
      "Total Loss: 0.05482972972095013\n",
      "------------------------------------ epoch 4651 (27900 steps) ------------------------------------\n",
      "Max loss: 0.021692927926778793\n",
      "Min loss: 0.006556802894920111\n",
      "Mean loss: 0.013219575630500913\n",
      "Std loss: 0.0049047026588124355\n",
      "Total Loss: 0.07931745378300548\n",
      "------------------------------------ epoch 4652 (27906 steps) ------------------------------------\n",
      "Max loss: 0.02851627580821514\n",
      "Min loss: 0.007964437827467918\n",
      "Mean loss: 0.01495771249756217\n",
      "Std loss: 0.007380135650689231\n",
      "Total Loss: 0.08974627498537302\n",
      "------------------------------------ epoch 4653 (27912 steps) ------------------------------------\n",
      "Max loss: 0.023540055379271507\n",
      "Min loss: 0.008018511347472668\n",
      "Mean loss: 0.014009723595033089\n",
      "Std loss: 0.006144829532210705\n",
      "Total Loss: 0.08405834157019854\n",
      "------------------------------------ epoch 4654 (27918 steps) ------------------------------------\n",
      "Max loss: 0.01859326846897602\n",
      "Min loss: 0.006124190520495176\n",
      "Mean loss: 0.011978415229047338\n",
      "Std loss: 0.004240069279223702\n",
      "Total Loss: 0.07187049137428403\n",
      "------------------------------------ epoch 4655 (27924 steps) ------------------------------------\n",
      "Max loss: 0.01659116894006729\n",
      "Min loss: 0.0077554937452077866\n",
      "Mean loss: 0.01117309865852197\n",
      "Std loss: 0.003783732054083364\n",
      "Total Loss: 0.06703859195113182\n",
      "------------------------------------ epoch 4656 (27930 steps) ------------------------------------\n",
      "Max loss: 0.012979455292224884\n",
      "Min loss: 0.006178756710141897\n",
      "Mean loss: 0.009501074363167087\n",
      "Std loss: 0.002645048902236868\n",
      "Total Loss: 0.05700644617900252\n",
      "------------------------------------ epoch 4657 (27936 steps) ------------------------------------\n",
      "Max loss: 0.015968233346939087\n",
      "Min loss: 0.006057959049940109\n",
      "Mean loss: 0.010129404254257679\n",
      "Std loss: 0.003519990573047353\n",
      "Total Loss: 0.060776425525546074\n",
      "------------------------------------ epoch 4658 (27942 steps) ------------------------------------\n",
      "Max loss: 0.023142552003264427\n",
      "Min loss: 0.005349527578800917\n",
      "Mean loss: 0.013195876575385531\n",
      "Std loss: 0.006390382218956511\n",
      "Total Loss: 0.07917525945231318\n",
      "------------------------------------ epoch 4659 (27948 steps) ------------------------------------\n",
      "Max loss: 0.025483153760433197\n",
      "Min loss: 0.005715047009289265\n",
      "Mean loss: 0.014929216665526232\n",
      "Std loss: 0.006544181830922276\n",
      "Total Loss: 0.08957529999315739\n",
      "------------------------------------ epoch 4660 (27954 steps) ------------------------------------\n",
      "Max loss: 0.021111294627189636\n",
      "Min loss: 0.005370383616536856\n",
      "Mean loss: 0.010596088521803418\n",
      "Std loss: 0.005594883446519622\n",
      "Total Loss: 0.06357653113082051\n",
      "------------------------------------ epoch 4661 (27960 steps) ------------------------------------\n",
      "Max loss: 0.02359795942902565\n",
      "Min loss: 0.008146237581968307\n",
      "Mean loss: 0.014606461860239506\n",
      "Std loss: 0.005705143856716806\n",
      "Total Loss: 0.08763877116143703\n",
      "------------------------------------ epoch 4662 (27966 steps) ------------------------------------\n",
      "Max loss: 0.06611353158950806\n",
      "Min loss: 0.00744091160595417\n",
      "Mean loss: 0.02168036609267195\n",
      "Std loss: 0.020495848058571915\n",
      "Total Loss: 0.1300821965560317\n",
      "------------------------------------ epoch 4663 (27972 steps) ------------------------------------\n",
      "Max loss: 0.022270116955041885\n",
      "Min loss: 0.006762799806892872\n",
      "Mean loss: 0.01236366294324398\n",
      "Std loss: 0.005020326887355349\n",
      "Total Loss: 0.07418197765946388\n",
      "------------------------------------ epoch 4664 (27978 steps) ------------------------------------\n",
      "Max loss: 0.014237902127206326\n",
      "Min loss: 0.008450524881482124\n",
      "Mean loss: 0.010100314859300852\n",
      "Std loss: 0.002015348795049213\n",
      "Total Loss: 0.06060188915580511\n",
      "------------------------------------ epoch 4665 (27984 steps) ------------------------------------\n",
      "Max loss: 0.017508171498775482\n",
      "Min loss: 0.007794284727424383\n",
      "Mean loss: 0.011941577851151427\n",
      "Std loss: 0.0029514866915799636\n",
      "Total Loss: 0.07164946710690856\n",
      "------------------------------------ epoch 4666 (27990 steps) ------------------------------------\n",
      "Max loss: 0.058394189924001694\n",
      "Min loss: 0.006478229071944952\n",
      "Mean loss: 0.02206193710056444\n",
      "Std loss: 0.018338238968917113\n",
      "Total Loss: 0.13237162260338664\n",
      "------------------------------------ epoch 4667 (27996 steps) ------------------------------------\n",
      "Max loss: 0.033416636288166046\n",
      "Min loss: 0.006781815551221371\n",
      "Mean loss: 0.01467994942019383\n",
      "Std loss: 0.00893223353216787\n",
      "Total Loss: 0.08807969652116299\n",
      "------------------------------------ epoch 4668 (28002 steps) ------------------------------------\n",
      "Max loss: 0.025309592485427856\n",
      "Min loss: 0.008800389245152473\n",
      "Mean loss: 0.01755868565912048\n",
      "Std loss: 0.005680560557337202\n",
      "Total Loss: 0.10535211395472288\n",
      "------------------------------------ epoch 4669 (28008 steps) ------------------------------------\n",
      "Max loss: 0.023600298911333084\n",
      "Min loss: 0.007100346498191357\n",
      "Mean loss: 0.012711046574016413\n",
      "Std loss: 0.007047131266157122\n",
      "Total Loss: 0.07626627944409847\n",
      "------------------------------------ epoch 4670 (28014 steps) ------------------------------------\n",
      "Max loss: 0.016938839107751846\n",
      "Min loss: 0.005300024524331093\n",
      "Mean loss: 0.011760727114354571\n",
      "Std loss: 0.004590891159451297\n",
      "Total Loss: 0.07056436268612742\n",
      "------------------------------------ epoch 4671 (28020 steps) ------------------------------------\n",
      "Max loss: 0.022074222564697266\n",
      "Min loss: 0.007822005078196526\n",
      "Mean loss: 0.010616410213212172\n",
      "Std loss: 0.005139267650662636\n",
      "Total Loss: 0.06369846127927303\n",
      "------------------------------------ epoch 4672 (28026 steps) ------------------------------------\n",
      "Max loss: 0.01587359420955181\n",
      "Min loss: 0.0055113001726567745\n",
      "Mean loss: 0.010008004882062474\n",
      "Std loss: 0.003204311694843567\n",
      "Total Loss: 0.06004802929237485\n",
      "------------------------------------ epoch 4673 (28032 steps) ------------------------------------\n",
      "Max loss: 0.01995265483856201\n",
      "Min loss: 0.006593444850295782\n",
      "Mean loss: 0.012117754124725858\n",
      "Std loss: 0.003982225668645109\n",
      "Total Loss: 0.07270652474835515\n",
      "------------------------------------ epoch 4674 (28038 steps) ------------------------------------\n",
      "Max loss: 0.015085132792592049\n",
      "Min loss: 0.005111614242196083\n",
      "Mean loss: 0.008710929192602634\n",
      "Std loss: 0.003342918709203615\n",
      "Total Loss: 0.05226557515561581\n",
      "------------------------------------ epoch 4675 (28044 steps) ------------------------------------\n",
      "Max loss: 0.015763700008392334\n",
      "Min loss: 0.00609235092997551\n",
      "Mean loss: 0.010599604807794094\n",
      "Std loss: 0.0034786482142803057\n",
      "Total Loss: 0.06359762884676456\n",
      "------------------------------------ epoch 4676 (28050 steps) ------------------------------------\n",
      "Max loss: 0.010345409624278545\n",
      "Min loss: 0.006276117172092199\n",
      "Mean loss: 0.007665119056279461\n",
      "Std loss: 0.00137810900300764\n",
      "Total Loss: 0.045990714337676764\n",
      "------------------------------------ epoch 4677 (28056 steps) ------------------------------------\n",
      "Max loss: 0.018228238448500633\n",
      "Min loss: 0.005426058080047369\n",
      "Mean loss: 0.00996097014285624\n",
      "Std loss: 0.004252859844748564\n",
      "Total Loss: 0.05976582085713744\n",
      "------------------------------------ epoch 4678 (28062 steps) ------------------------------------\n",
      "Max loss: 0.01568690314888954\n",
      "Min loss: 0.007279100827872753\n",
      "Mean loss: 0.010831427294760942\n",
      "Std loss: 0.0030437727711745523\n",
      "Total Loss: 0.06498856376856565\n",
      "------------------------------------ epoch 4679 (28068 steps) ------------------------------------\n",
      "Max loss: 0.02660609409213066\n",
      "Min loss: 0.006987942382693291\n",
      "Mean loss: 0.012864708745231232\n",
      "Std loss: 0.006627295052508166\n",
      "Total Loss: 0.07718825247138739\n",
      "------------------------------------ epoch 4680 (28074 steps) ------------------------------------\n",
      "Max loss: 0.07150642573833466\n",
      "Min loss: 0.006508180871605873\n",
      "Mean loss: 0.020416599232703447\n",
      "Std loss: 0.02301234054019061\n",
      "Total Loss: 0.12249959539622068\n",
      "------------------------------------ epoch 4681 (28080 steps) ------------------------------------\n",
      "Max loss: 0.03723200410604477\n",
      "Min loss: 0.009124858304858208\n",
      "Mean loss: 0.018290198718508083\n",
      "Std loss: 0.010436623623192308\n",
      "Total Loss: 0.10974119231104851\n",
      "------------------------------------ epoch 4682 (28086 steps) ------------------------------------\n",
      "Max loss: 0.01893123798072338\n",
      "Min loss: 0.006438282318413258\n",
      "Mean loss: 0.013859587876747051\n",
      "Std loss: 0.004443835238495288\n",
      "Total Loss: 0.08315752726048231\n",
      "------------------------------------ epoch 4683 (28092 steps) ------------------------------------\n",
      "Max loss: 0.023184429854154587\n",
      "Min loss: 0.005829842295497656\n",
      "Mean loss: 0.013700451158607999\n",
      "Std loss: 0.006535872163760446\n",
      "Total Loss: 0.082202706951648\n",
      "------------------------------------ epoch 4684 (28098 steps) ------------------------------------\n",
      "Max loss: 0.0282154344022274\n",
      "Min loss: 0.006581208668649197\n",
      "Mean loss: 0.014462704459826151\n",
      "Std loss: 0.0076364322610833575\n",
      "Total Loss: 0.08677622675895691\n",
      "------------------------------------ epoch 4685 (28104 steps) ------------------------------------\n",
      "Max loss: 0.01968933641910553\n",
      "Min loss: 0.007379339542239904\n",
      "Mean loss: 0.01221004493224124\n",
      "Std loss: 0.004845384757886566\n",
      "Total Loss: 0.07326026959344745\n",
      "------------------------------------ epoch 4686 (28110 steps) ------------------------------------\n",
      "Max loss: 0.03616562485694885\n",
      "Min loss: 0.005277193151414394\n",
      "Mean loss: 0.01801920464883248\n",
      "Std loss: 0.01225899202166663\n",
      "Total Loss: 0.10811522789299488\n",
      "------------------------------------ epoch 4687 (28116 steps) ------------------------------------\n",
      "Max loss: 0.02098929136991501\n",
      "Min loss: 0.006153757683932781\n",
      "Mean loss: 0.013787833508104086\n",
      "Std loss: 0.004754756662109352\n",
      "Total Loss: 0.08272700104862452\n",
      "------------------------------------ epoch 4688 (28122 steps) ------------------------------------\n",
      "Max loss: 0.019754325971007347\n",
      "Min loss: 0.008036524057388306\n",
      "Mean loss: 0.013378233648836613\n",
      "Std loss: 0.0036499013305197\n",
      "Total Loss: 0.08026940189301968\n",
      "------------------------------------ epoch 4689 (28128 steps) ------------------------------------\n",
      "Max loss: 0.030149465426802635\n",
      "Min loss: 0.006438862066715956\n",
      "Mean loss: 0.013025041591996947\n",
      "Std loss: 0.008024044424142578\n",
      "Total Loss: 0.07815024955198169\n",
      "------------------------------------ epoch 4690 (28134 steps) ------------------------------------\n",
      "Max loss: 0.04255027323961258\n",
      "Min loss: 0.006475821137428284\n",
      "Mean loss: 0.014541354573642215\n",
      "Std loss: 0.01265120954362944\n",
      "Total Loss: 0.08724812744185328\n",
      "------------------------------------ epoch 4691 (28140 steps) ------------------------------------\n",
      "Max loss: 0.024853257462382317\n",
      "Min loss: 0.009333327412605286\n",
      "Mean loss: 0.015575602340201536\n",
      "Std loss: 0.006464901871869962\n",
      "Total Loss: 0.09345361404120922\n",
      "------------------------------------ epoch 4692 (28146 steps) ------------------------------------\n",
      "Max loss: 0.03250141441822052\n",
      "Min loss: 0.00646287202835083\n",
      "Mean loss: 0.013303509137282768\n",
      "Std loss: 0.009015206077741086\n",
      "Total Loss: 0.07982105482369661\n",
      "------------------------------------ epoch 4693 (28152 steps) ------------------------------------\n",
      "Max loss: 0.01565222255885601\n",
      "Min loss: 0.00564716849476099\n",
      "Mean loss: 0.00837446857864658\n",
      "Std loss: 0.003387496933645724\n",
      "Total Loss: 0.05024681147187948\n",
      "------------------------------------ epoch 4694 (28158 steps) ------------------------------------\n",
      "Max loss: 0.05972134694457054\n",
      "Min loss: 0.005736806895583868\n",
      "Mean loss: 0.017625375573212903\n",
      "Std loss: 0.018940450459593982\n",
      "Total Loss: 0.10575225343927741\n",
      "------------------------------------ epoch 4695 (28164 steps) ------------------------------------\n",
      "Max loss: 0.024006551131606102\n",
      "Min loss: 0.006267491262406111\n",
      "Mean loss: 0.013264112562562028\n",
      "Std loss: 0.0062975081140673045\n",
      "Total Loss: 0.07958467537537217\n",
      "------------------------------------ epoch 4696 (28170 steps) ------------------------------------\n",
      "Max loss: 0.021397005766630173\n",
      "Min loss: 0.007097306661307812\n",
      "Mean loss: 0.014015123868981997\n",
      "Std loss: 0.005396881979516861\n",
      "Total Loss: 0.08409074321389198\n",
      "------------------------------------ epoch 4697 (28176 steps) ------------------------------------\n",
      "Max loss: 0.022691821679472923\n",
      "Min loss: 0.011765735223889351\n",
      "Mean loss: 0.016005549269417923\n",
      "Std loss: 0.0043499005928288115\n",
      "Total Loss: 0.09603329561650753\n",
      "------------------------------------ epoch 4698 (28182 steps) ------------------------------------\n",
      "Max loss: 0.011707346886396408\n",
      "Min loss: 0.008891230449080467\n",
      "Mean loss: 0.010481843569626411\n",
      "Std loss: 0.0009133576320287416\n",
      "Total Loss: 0.06289106141775846\n",
      "------------------------------------ epoch 4699 (28188 steps) ------------------------------------\n",
      "Max loss: 0.0519598051905632\n",
      "Min loss: 0.00550122931599617\n",
      "Mean loss: 0.016725329216569662\n",
      "Std loss: 0.016196681890222914\n",
      "Total Loss: 0.10035197529941797\n",
      "------------------------------------ epoch 4700 (28194 steps) ------------------------------------\n",
      "Max loss: 0.030212949961423874\n",
      "Min loss: 0.008151991292834282\n",
      "Mean loss: 0.015591656789183617\n",
      "Std loss: 0.00746742447091982\n",
      "Total Loss: 0.0935499407351017\n",
      "------------------------------------ epoch 4701 (28200 steps) ------------------------------------\n",
      "Max loss: 0.017267439514398575\n",
      "Min loss: 0.0077243344858288765\n",
      "Mean loss: 0.01243237421537439\n",
      "Std loss: 0.0030178791242398873\n",
      "Total Loss: 0.07459424529224634\n",
      "saved model at ./weights/model_4701.pth\n",
      "------------------------------------ epoch 4702 (28206 steps) ------------------------------------\n",
      "Max loss: 0.027574097737669945\n",
      "Min loss: 0.0080440454185009\n",
      "Mean loss: 0.01581882406026125\n",
      "Std loss: 0.006583004962616264\n",
      "Total Loss: 0.0949129443615675\n",
      "------------------------------------ epoch 4703 (28212 steps) ------------------------------------\n",
      "Max loss: 0.01758303865790367\n",
      "Min loss: 0.010438986122608185\n",
      "Mean loss: 0.013614586709688107\n",
      "Std loss: 0.0024617739282145547\n",
      "Total Loss: 0.08168752025812864\n",
      "------------------------------------ epoch 4704 (28218 steps) ------------------------------------\n",
      "Max loss: 0.01949157752096653\n",
      "Min loss: 0.005631088279187679\n",
      "Mean loss: 0.01063477930923303\n",
      "Std loss: 0.004752787071523542\n",
      "Total Loss: 0.06380867585539818\n",
      "------------------------------------ epoch 4705 (28224 steps) ------------------------------------\n",
      "Max loss: 0.010894189588725567\n",
      "Min loss: 0.005405536852777004\n",
      "Mean loss: 0.008058492482329408\n",
      "Std loss: 0.002338809614026961\n",
      "Total Loss: 0.04835095489397645\n",
      "------------------------------------ epoch 4706 (28230 steps) ------------------------------------\n",
      "Max loss: 0.01769915036857128\n",
      "Min loss: 0.00503358943387866\n",
      "Mean loss: 0.008954160458718738\n",
      "Std loss: 0.004195995363290639\n",
      "Total Loss: 0.05372496275231242\n",
      "------------------------------------ epoch 4707 (28236 steps) ------------------------------------\n",
      "Max loss: 0.023014232516288757\n",
      "Min loss: 0.008776068687438965\n",
      "Mean loss: 0.013987180000791946\n",
      "Std loss: 0.004727872429955842\n",
      "Total Loss: 0.08392308000475168\n",
      "------------------------------------ epoch 4708 (28242 steps) ------------------------------------\n",
      "Max loss: 0.02893177792429924\n",
      "Min loss: 0.006272344850003719\n",
      "Mean loss: 0.013901667126143972\n",
      "Std loss: 0.008023124238490375\n",
      "Total Loss: 0.08341000275686383\n",
      "------------------------------------ epoch 4709 (28248 steps) ------------------------------------\n",
      "Max loss: 0.012406518682837486\n",
      "Min loss: 0.007034907117486\n",
      "Mean loss: 0.009538757847622037\n",
      "Std loss: 0.002086148259447502\n",
      "Total Loss: 0.05723254708573222\n",
      "------------------------------------ epoch 4710 (28254 steps) ------------------------------------\n",
      "Max loss: 0.04053939878940582\n",
      "Min loss: 0.006606379058212042\n",
      "Mean loss: 0.015764267261450488\n",
      "Std loss: 0.01144599009111407\n",
      "Total Loss: 0.09458560356870294\n",
      "------------------------------------ epoch 4711 (28260 steps) ------------------------------------\n",
      "Max loss: 0.025347817689180374\n",
      "Min loss: 0.006944294087588787\n",
      "Mean loss: 0.01606721415494879\n",
      "Std loss: 0.006513862214600912\n",
      "Total Loss: 0.09640328492969275\n",
      "------------------------------------ epoch 4712 (28266 steps) ------------------------------------\n",
      "Max loss: 0.02175542525947094\n",
      "Min loss: 0.006347252056002617\n",
      "Mean loss: 0.012123066311081251\n",
      "Std loss: 0.005583172547883235\n",
      "Total Loss: 0.0727383978664875\n",
      "------------------------------------ epoch 4713 (28272 steps) ------------------------------------\n",
      "Max loss: 0.019845716655254364\n",
      "Min loss: 0.0067934561520814896\n",
      "Mean loss: 0.011325349876036247\n",
      "Std loss: 0.004576627997054371\n",
      "Total Loss: 0.06795209925621748\n",
      "------------------------------------ epoch 4714 (28278 steps) ------------------------------------\n",
      "Max loss: 0.012282605282962322\n",
      "Min loss: 0.006854631006717682\n",
      "Mean loss: 0.009608386705319086\n",
      "Std loss: 0.0018907510911112845\n",
      "Total Loss: 0.05765032023191452\n",
      "------------------------------------ epoch 4715 (28284 steps) ------------------------------------\n",
      "Max loss: 0.014168160036206245\n",
      "Min loss: 0.00724163418635726\n",
      "Mean loss: 0.009848096019898852\n",
      "Std loss: 0.0021778773887372043\n",
      "Total Loss: 0.05908857611939311\n",
      "------------------------------------ epoch 4716 (28290 steps) ------------------------------------\n",
      "Max loss: 0.01945354789495468\n",
      "Min loss: 0.0072425431571900845\n",
      "Mean loss: 0.012633303102726737\n",
      "Std loss: 0.004289663808692236\n",
      "Total Loss: 0.07579981861636043\n",
      "------------------------------------ epoch 4717 (28296 steps) ------------------------------------\n",
      "Max loss: 0.017402242869138718\n",
      "Min loss: 0.009876163676381111\n",
      "Mean loss: 0.012304854734490315\n",
      "Std loss: 0.0025073726706583616\n",
      "Total Loss: 0.07382912840694189\n",
      "------------------------------------ epoch 4718 (28302 steps) ------------------------------------\n",
      "Max loss: 0.011514334939420223\n",
      "Min loss: 0.005906624253839254\n",
      "Mean loss: 0.008646876395990452\n",
      "Std loss: 0.0017276088108875939\n",
      "Total Loss: 0.05188125837594271\n",
      "------------------------------------ epoch 4719 (28308 steps) ------------------------------------\n",
      "Max loss: 0.026835616677999496\n",
      "Min loss: 0.005696085747331381\n",
      "Mean loss: 0.014285851114739975\n",
      "Std loss: 0.00785201893035769\n",
      "Total Loss: 0.08571510668843985\n",
      "------------------------------------ epoch 4720 (28314 steps) ------------------------------------\n",
      "Max loss: 0.0182637982070446\n",
      "Min loss: 0.006157932337373495\n",
      "Mean loss: 0.011755437823012471\n",
      "Std loss: 0.00434237329226529\n",
      "Total Loss: 0.07053262693807483\n",
      "------------------------------------ epoch 4721 (28320 steps) ------------------------------------\n",
      "Max loss: 0.01604301854968071\n",
      "Min loss: 0.0060002803802490234\n",
      "Mean loss: 0.009286383943011364\n",
      "Std loss: 0.00317442652982414\n",
      "Total Loss: 0.05571830365806818\n",
      "------------------------------------ epoch 4722 (28326 steps) ------------------------------------\n",
      "Max loss: 0.03759286552667618\n",
      "Min loss: 0.005250169895589352\n",
      "Mean loss: 0.01367229021464785\n",
      "Std loss: 0.01098134544284324\n",
      "Total Loss: 0.0820337412878871\n",
      "------------------------------------ epoch 4723 (28332 steps) ------------------------------------\n",
      "Max loss: 0.021978605538606644\n",
      "Min loss: 0.005686438642442226\n",
      "Mean loss: 0.012474251134941975\n",
      "Std loss: 0.0062736064970974606\n",
      "Total Loss: 0.07484550680965185\n",
      "------------------------------------ epoch 4724 (28338 steps) ------------------------------------\n",
      "Max loss: 0.01056603342294693\n",
      "Min loss: 0.006176052615046501\n",
      "Mean loss: 0.008858893997967243\n",
      "Std loss: 0.0014818726543504492\n",
      "Total Loss: 0.05315336398780346\n",
      "------------------------------------ epoch 4725 (28344 steps) ------------------------------------\n",
      "Max loss: 0.029575210064649582\n",
      "Min loss: 0.006819356232881546\n",
      "Mean loss: 0.014471341079721848\n",
      "Std loss: 0.00790241096362127\n",
      "Total Loss: 0.08682804647833109\n",
      "------------------------------------ epoch 4726 (28350 steps) ------------------------------------\n",
      "Max loss: 0.03895603492856026\n",
      "Min loss: 0.007411448284983635\n",
      "Mean loss: 0.015246473563214144\n",
      "Std loss: 0.01112384277688886\n",
      "Total Loss: 0.09147884137928486\n",
      "------------------------------------ epoch 4727 (28356 steps) ------------------------------------\n",
      "Max loss: 0.014183668419718742\n",
      "Min loss: 0.006493107881397009\n",
      "Mean loss: 0.010746896810208758\n",
      "Std loss: 0.002847930926426336\n",
      "Total Loss: 0.06448138086125255\n",
      "------------------------------------ epoch 4728 (28362 steps) ------------------------------------\n",
      "Max loss: 0.031003132462501526\n",
      "Min loss: 0.005810421891510487\n",
      "Mean loss: 0.012365632380048433\n",
      "Std loss: 0.009004990125555092\n",
      "Total Loss: 0.0741937942802906\n",
      "------------------------------------ epoch 4729 (28368 steps) ------------------------------------\n",
      "Max loss: 0.021461661905050278\n",
      "Min loss: 0.006485171616077423\n",
      "Mean loss: 0.01249536033719778\n",
      "Std loss: 0.00503278917270354\n",
      "Total Loss: 0.07497216202318668\n",
      "------------------------------------ epoch 4730 (28374 steps) ------------------------------------\n",
      "Max loss: 0.01251102052628994\n",
      "Min loss: 0.007354319095611572\n",
      "Mean loss: 0.010676535467306772\n",
      "Std loss: 0.0015799619938149977\n",
      "Total Loss: 0.06405921280384064\n",
      "------------------------------------ epoch 4731 (28380 steps) ------------------------------------\n",
      "Max loss: 0.023009371012449265\n",
      "Min loss: 0.007323773577809334\n",
      "Mean loss: 0.011617260985076427\n",
      "Std loss: 0.005287603238903627\n",
      "Total Loss: 0.06970356591045856\n",
      "------------------------------------ epoch 4732 (28386 steps) ------------------------------------\n",
      "Max loss: 0.026284730061888695\n",
      "Min loss: 0.005654715001583099\n",
      "Mean loss: 0.011989344687511524\n",
      "Std loss: 0.006893673921409348\n",
      "Total Loss: 0.07193606812506914\n",
      "------------------------------------ epoch 4733 (28392 steps) ------------------------------------\n",
      "Max loss: 0.02835860662162304\n",
      "Min loss: 0.005894284695386887\n",
      "Mean loss: 0.011757545017947754\n",
      "Std loss: 0.007946546122385297\n",
      "Total Loss: 0.07054527010768652\n",
      "------------------------------------ epoch 4734 (28398 steps) ------------------------------------\n",
      "Max loss: 0.025390520691871643\n",
      "Min loss: 0.00930812954902649\n",
      "Mean loss: 0.015453727295001348\n",
      "Std loss: 0.005090206661413847\n",
      "Total Loss: 0.09272236377000809\n",
      "------------------------------------ epoch 4735 (28404 steps) ------------------------------------\n",
      "Max loss: 0.02298809215426445\n",
      "Min loss: 0.006237417459487915\n",
      "Mean loss: 0.013664181188990673\n",
      "Std loss: 0.006632690676665218\n",
      "Total Loss: 0.08198508713394403\n",
      "------------------------------------ epoch 4736 (28410 steps) ------------------------------------\n",
      "Max loss: 0.022305846214294434\n",
      "Min loss: 0.007458331994712353\n",
      "Mean loss: 0.013037652087708315\n",
      "Std loss: 0.005404651135701345\n",
      "Total Loss: 0.07822591252624989\n",
      "------------------------------------ epoch 4737 (28416 steps) ------------------------------------\n",
      "Max loss: 0.015111783519387245\n",
      "Min loss: 0.006191957741975784\n",
      "Mean loss: 0.009897034925719103\n",
      "Std loss: 0.003049001937742775\n",
      "Total Loss: 0.05938220955431461\n",
      "------------------------------------ epoch 4738 (28422 steps) ------------------------------------\n",
      "Max loss: 0.012734338641166687\n",
      "Min loss: 0.006326416041702032\n",
      "Mean loss: 0.00853724319798251\n",
      "Std loss: 0.002185822126390879\n",
      "Total Loss: 0.05122345918789506\n",
      "------------------------------------ epoch 4739 (28428 steps) ------------------------------------\n",
      "Max loss: 0.03261243924498558\n",
      "Min loss: 0.006060149520635605\n",
      "Mean loss: 0.01615842618048191\n",
      "Std loss: 0.010466583619924958\n",
      "Total Loss: 0.09695055708289146\n",
      "------------------------------------ epoch 4740 (28434 steps) ------------------------------------\n",
      "Max loss: 0.015881767496466637\n",
      "Min loss: 0.0051419371739029884\n",
      "Mean loss: 0.01132725536202391\n",
      "Std loss: 0.0036583021338837747\n",
      "Total Loss: 0.06796353217214346\n",
      "------------------------------------ epoch 4741 (28440 steps) ------------------------------------\n",
      "Max loss: 0.018964966759085655\n",
      "Min loss: 0.006137042306363583\n",
      "Mean loss: 0.011180359792585174\n",
      "Std loss: 0.0044811885457174475\n",
      "Total Loss: 0.06708215875551105\n",
      "------------------------------------ epoch 4742 (28446 steps) ------------------------------------\n",
      "Max loss: 0.020391013473272324\n",
      "Min loss: 0.005976101383566856\n",
      "Mean loss: 0.011531685323764881\n",
      "Std loss: 0.004667564333980468\n",
      "Total Loss: 0.06919011194258928\n",
      "------------------------------------ epoch 4743 (28452 steps) ------------------------------------\n",
      "Max loss: 0.018511861562728882\n",
      "Min loss: 0.006874812301248312\n",
      "Mean loss: 0.01206104977366825\n",
      "Std loss: 0.004042824645558898\n",
      "Total Loss: 0.0723662986420095\n",
      "------------------------------------ epoch 4744 (28458 steps) ------------------------------------\n",
      "Max loss: 0.048541828989982605\n",
      "Min loss: 0.005221077706664801\n",
      "Mean loss: 0.014609253499656916\n",
      "Std loss: 0.01549088882847885\n",
      "Total Loss: 0.0876555209979415\n",
      "------------------------------------ epoch 4745 (28464 steps) ------------------------------------\n",
      "Max loss: 0.021803081035614014\n",
      "Min loss: 0.0073394132778048515\n",
      "Mean loss: 0.012421865171442429\n",
      "Std loss: 0.00526718785907257\n",
      "Total Loss: 0.07453119102865458\n",
      "------------------------------------ epoch 4746 (28470 steps) ------------------------------------\n",
      "Max loss: 0.016495171934366226\n",
      "Min loss: 0.005887502804398537\n",
      "Mean loss: 0.009817589074373245\n",
      "Std loss: 0.0038589489969532756\n",
      "Total Loss: 0.05890553444623947\n",
      "------------------------------------ epoch 4747 (28476 steps) ------------------------------------\n",
      "Max loss: 0.0529685840010643\n",
      "Min loss: 0.005845668725669384\n",
      "Mean loss: 0.017974624565492075\n",
      "Std loss: 0.016554547936499693\n",
      "Total Loss: 0.10784774739295244\n",
      "------------------------------------ epoch 4748 (28482 steps) ------------------------------------\n",
      "Max loss: 0.05112297087907791\n",
      "Min loss: 0.008168159052729607\n",
      "Mean loss: 0.02220731492464741\n",
      "Std loss: 0.013788025742313251\n",
      "Total Loss: 0.13324388954788446\n",
      "------------------------------------ epoch 4749 (28488 steps) ------------------------------------\n",
      "Max loss: 0.029789820313453674\n",
      "Min loss: 0.012041634880006313\n",
      "Mean loss: 0.020529224071651697\n",
      "Std loss: 0.006918214384518647\n",
      "Total Loss: 0.12317534442991018\n",
      "------------------------------------ epoch 4750 (28494 steps) ------------------------------------\n",
      "Max loss: 0.056489959359169006\n",
      "Min loss: 0.01069614291191101\n",
      "Mean loss: 0.027425328735262156\n",
      "Std loss: 0.01522406160986559\n",
      "Total Loss: 0.16455197241157293\n",
      "------------------------------------ epoch 4751 (28500 steps) ------------------------------------\n",
      "Max loss: 0.02276376634836197\n",
      "Min loss: 0.011685187928378582\n",
      "Mean loss: 0.015943945695956547\n",
      "Std loss: 0.0037823799388672402\n",
      "Total Loss: 0.09566367417573929\n",
      "------------------------------------ epoch 4752 (28506 steps) ------------------------------------\n",
      "Max loss: 0.013521368615329266\n",
      "Min loss: 0.008730082772672176\n",
      "Mean loss: 0.010725708057483038\n",
      "Std loss: 0.0016957153891183113\n",
      "Total Loss: 0.06435424834489822\n",
      "------------------------------------ epoch 4753 (28512 steps) ------------------------------------\n",
      "Max loss: 0.0318027064204216\n",
      "Min loss: 0.008609564043581486\n",
      "Mean loss: 0.020320662297308445\n",
      "Std loss: 0.008902267138919149\n",
      "Total Loss: 0.12192397378385067\n",
      "------------------------------------ epoch 4754 (28518 steps) ------------------------------------\n",
      "Max loss: 0.03379243239760399\n",
      "Min loss: 0.007363922894001007\n",
      "Mean loss: 0.01822177677725752\n",
      "Std loss: 0.008821104990676426\n",
      "Total Loss: 0.10933066066354513\n",
      "------------------------------------ epoch 4755 (28524 steps) ------------------------------------\n",
      "Max loss: 0.028323981910943985\n",
      "Min loss: 0.008165684528648853\n",
      "Mean loss: 0.014406930189579725\n",
      "Std loss: 0.006567760143581117\n",
      "Total Loss: 0.08644158113747835\n",
      "------------------------------------ epoch 4756 (28530 steps) ------------------------------------\n",
      "Max loss: 0.048427097499370575\n",
      "Min loss: 0.007413262035697699\n",
      "Mean loss: 0.0193894087181737\n",
      "Std loss: 0.013587813184347283\n",
      "Total Loss: 0.11633645230904222\n",
      "------------------------------------ epoch 4757 (28536 steps) ------------------------------------\n",
      "Max loss: 0.06349068135023117\n",
      "Min loss: 0.006992303766310215\n",
      "Mean loss: 0.027161603172620136\n",
      "Std loss: 0.0205755787821002\n",
      "Total Loss: 0.16296961903572083\n",
      "------------------------------------ epoch 4758 (28542 steps) ------------------------------------\n",
      "Max loss: 0.019382137805223465\n",
      "Min loss: 0.00664330180734396\n",
      "Mean loss: 0.011116507928818464\n",
      "Std loss: 0.0042295049769624235\n",
      "Total Loss: 0.06669904757291079\n",
      "------------------------------------ epoch 4759 (28548 steps) ------------------------------------\n",
      "Max loss: 0.028301946818828583\n",
      "Min loss: 0.006420026998966932\n",
      "Mean loss: 0.017102883430197835\n",
      "Std loss: 0.007875669143312392\n",
      "Total Loss: 0.10261730058118701\n",
      "------------------------------------ epoch 4760 (28554 steps) ------------------------------------\n",
      "Max loss: 0.02129388228058815\n",
      "Min loss: 0.00651931157335639\n",
      "Mean loss: 0.012100927298888564\n",
      "Std loss: 0.004912498976835856\n",
      "Total Loss: 0.07260556379333138\n",
      "------------------------------------ epoch 4761 (28560 steps) ------------------------------------\n",
      "Max loss: 0.018343988806009293\n",
      "Min loss: 0.007563951425254345\n",
      "Mean loss: 0.011974566305677095\n",
      "Std loss: 0.003937294605059893\n",
      "Total Loss: 0.07184739783406258\n",
      "------------------------------------ epoch 4762 (28566 steps) ------------------------------------\n",
      "Max loss: 0.021562300622463226\n",
      "Min loss: 0.0060365693643689156\n",
      "Mean loss: 0.011927307738612095\n",
      "Std loss: 0.005006392422483825\n",
      "Total Loss: 0.07156384643167257\n",
      "------------------------------------ epoch 4763 (28572 steps) ------------------------------------\n",
      "Max loss: 0.031383953988552094\n",
      "Min loss: 0.005883178673684597\n",
      "Mean loss: 0.0110937284771353\n",
      "Std loss: 0.009125069858105714\n",
      "Total Loss: 0.0665623708628118\n",
      "------------------------------------ epoch 4764 (28578 steps) ------------------------------------\n",
      "Max loss: 0.01866057887673378\n",
      "Min loss: 0.005957860499620438\n",
      "Mean loss: 0.010221559399118027\n",
      "Std loss: 0.005832309653217746\n",
      "Total Loss: 0.06132935639470816\n",
      "------------------------------------ epoch 4765 (28584 steps) ------------------------------------\n",
      "Max loss: 0.012377850711345673\n",
      "Min loss: 0.00496330764144659\n",
      "Mean loss: 0.007831929717212915\n",
      "Std loss: 0.0023759197872856154\n",
      "Total Loss: 0.04699157830327749\n",
      "------------------------------------ epoch 4766 (28590 steps) ------------------------------------\n",
      "Max loss: 0.0077686551958322525\n",
      "Min loss: 0.005234850104898214\n",
      "Mean loss: 0.006165869068354368\n",
      "Std loss: 0.000772048918658748\n",
      "Total Loss: 0.03699521441012621\n",
      "------------------------------------ epoch 4767 (28596 steps) ------------------------------------\n",
      "Max loss: 0.028418712317943573\n",
      "Min loss: 0.006211951375007629\n",
      "Mean loss: 0.01383387817380329\n",
      "Std loss: 0.008171582656419048\n",
      "Total Loss: 0.08300326904281974\n",
      "------------------------------------ epoch 4768 (28602 steps) ------------------------------------\n",
      "Max loss: 0.022913921624422073\n",
      "Min loss: 0.005434745457023382\n",
      "Mean loss: 0.014792024545992414\n",
      "Std loss: 0.0057891094788162815\n",
      "Total Loss: 0.08875214727595448\n",
      "------------------------------------ epoch 4769 (28608 steps) ------------------------------------\n",
      "Max loss: 0.01675247587263584\n",
      "Min loss: 0.005641852505505085\n",
      "Mean loss: 0.010216962856551012\n",
      "Std loss: 0.0041923176121642265\n",
      "Total Loss: 0.06130177713930607\n",
      "------------------------------------ epoch 4770 (28614 steps) ------------------------------------\n",
      "Max loss: 0.018173640593886375\n",
      "Min loss: 0.005371943116188049\n",
      "Mean loss: 0.013996357253442207\n",
      "Std loss: 0.004207860490995867\n",
      "Total Loss: 0.08397814352065325\n",
      "------------------------------------ epoch 4771 (28620 steps) ------------------------------------\n",
      "Max loss: 0.045034490525722504\n",
      "Min loss: 0.007768545299768448\n",
      "Mean loss: 0.016045691755910713\n",
      "Std loss: 0.013044442426488403\n",
      "Total Loss: 0.09627415053546429\n",
      "------------------------------------ epoch 4772 (28626 steps) ------------------------------------\n",
      "Max loss: 0.023701172322034836\n",
      "Min loss: 0.005884822458028793\n",
      "Mean loss: 0.0107280935626477\n",
      "Std loss: 0.006315174983348641\n",
      "Total Loss: 0.0643685613758862\n",
      "------------------------------------ epoch 4773 (28632 steps) ------------------------------------\n",
      "Max loss: 0.0176431592553854\n",
      "Min loss: 0.009849922731518745\n",
      "Mean loss: 0.012345517364641031\n",
      "Std loss: 0.002777648364055249\n",
      "Total Loss: 0.07407310418784618\n",
      "------------------------------------ epoch 4774 (28638 steps) ------------------------------------\n",
      "Max loss: 0.025777511298656464\n",
      "Min loss: 0.007126683369278908\n",
      "Mean loss: 0.012714272054533163\n",
      "Std loss: 0.006355600969793734\n",
      "Total Loss: 0.07628563232719898\n",
      "------------------------------------ epoch 4775 (28644 steps) ------------------------------------\n",
      "Max loss: 0.04085255041718483\n",
      "Min loss: 0.006423638667911291\n",
      "Mean loss: 0.014953275754426917\n",
      "Std loss: 0.011773784176021648\n",
      "Total Loss: 0.0897196545265615\n",
      "------------------------------------ epoch 4776 (28650 steps) ------------------------------------\n",
      "Max loss: 0.02711612544953823\n",
      "Min loss: 0.008084548637270927\n",
      "Mean loss: 0.014819752735396227\n",
      "Std loss: 0.008195215883050428\n",
      "Total Loss: 0.08891851641237736\n",
      "------------------------------------ epoch 4777 (28656 steps) ------------------------------------\n",
      "Max loss: 0.0461486391723156\n",
      "Min loss: 0.00647988636046648\n",
      "Mean loss: 0.018884669213245314\n",
      "Std loss: 0.01271794332493796\n",
      "Total Loss: 0.11330801527947187\n",
      "------------------------------------ epoch 4778 (28662 steps) ------------------------------------\n",
      "Max loss: 0.017408547922968864\n",
      "Min loss: 0.007012953981757164\n",
      "Mean loss: 0.011318676018466553\n",
      "Std loss: 0.0030824341109124903\n",
      "Total Loss: 0.06791205611079931\n",
      "------------------------------------ epoch 4779 (28668 steps) ------------------------------------\n",
      "Max loss: 0.022884352132678032\n",
      "Min loss: 0.0053343018516898155\n",
      "Mean loss: 0.010109106233964363\n",
      "Std loss: 0.005992652299027048\n",
      "Total Loss: 0.06065463740378618\n",
      "------------------------------------ epoch 4780 (28674 steps) ------------------------------------\n",
      "Max loss: 0.020268384367227554\n",
      "Min loss: 0.006427881773561239\n",
      "Mean loss: 0.010335723559061686\n",
      "Std loss: 0.004869166679892112\n",
      "Total Loss: 0.06201434135437012\n",
      "------------------------------------ epoch 4781 (28680 steps) ------------------------------------\n",
      "Max loss: 0.014016171917319298\n",
      "Min loss: 0.006721738260239363\n",
      "Mean loss: 0.010705800804619988\n",
      "Std loss: 0.0027944855395078193\n",
      "Total Loss: 0.06423480482771993\n",
      "------------------------------------ epoch 4782 (28686 steps) ------------------------------------\n",
      "Max loss: 0.01537826657295227\n",
      "Min loss: 0.005482321139425039\n",
      "Mean loss: 0.009639799206828078\n",
      "Std loss: 0.0038146427112310333\n",
      "Total Loss: 0.057838795240968466\n",
      "------------------------------------ epoch 4783 (28692 steps) ------------------------------------\n",
      "Max loss: 0.038266319781541824\n",
      "Min loss: 0.005322571843862534\n",
      "Mean loss: 0.014350358551988998\n",
      "Std loss: 0.011288579881998304\n",
      "Total Loss: 0.086102151311934\n",
      "------------------------------------ epoch 4784 (28698 steps) ------------------------------------\n",
      "Max loss: 0.022908609360456467\n",
      "Min loss: 0.007127206772565842\n",
      "Mean loss: 0.012249894440174103\n",
      "Std loss: 0.005258536176595575\n",
      "Total Loss: 0.07349936664104462\n",
      "------------------------------------ epoch 4785 (28704 steps) ------------------------------------\n",
      "Max loss: 0.015668299049139023\n",
      "Min loss: 0.005365035496652126\n",
      "Mean loss: 0.0096327083495756\n",
      "Std loss: 0.0034367660303410053\n",
      "Total Loss: 0.057796250097453594\n",
      "------------------------------------ epoch 4786 (28710 steps) ------------------------------------\n",
      "Max loss: 0.0202479287981987\n",
      "Min loss: 0.005972152575850487\n",
      "Mean loss: 0.012040980160236359\n",
      "Std loss: 0.005079190025542217\n",
      "Total Loss: 0.07224588096141815\n",
      "------------------------------------ epoch 4787 (28716 steps) ------------------------------------\n",
      "Max loss: 0.01642434298992157\n",
      "Min loss: 0.006941746920347214\n",
      "Mean loss: 0.010399690518776575\n",
      "Std loss: 0.0035794406224839643\n",
      "Total Loss: 0.062398143112659454\n",
      "------------------------------------ epoch 4788 (28722 steps) ------------------------------------\n",
      "Max loss: 0.024888215586543083\n",
      "Min loss: 0.004679567646235228\n",
      "Mean loss: 0.010688561868543426\n",
      "Std loss: 0.006804533226056449\n",
      "Total Loss: 0.06413137121126056\n",
      "------------------------------------ epoch 4789 (28728 steps) ------------------------------------\n",
      "Max loss: 0.021941151469945908\n",
      "Min loss: 0.007467703428119421\n",
      "Mean loss: 0.014689037343487144\n",
      "Std loss: 0.004889035901363635\n",
      "Total Loss: 0.08813422406092286\n",
      "------------------------------------ epoch 4790 (28734 steps) ------------------------------------\n",
      "Max loss: 0.016759665682911873\n",
      "Min loss: 0.005424839910119772\n",
      "Mean loss: 0.010785313788801432\n",
      "Std loss: 0.004134541549798158\n",
      "Total Loss: 0.06471188273280859\n",
      "------------------------------------ epoch 4791 (28740 steps) ------------------------------------\n",
      "Max loss: 0.021576404571533203\n",
      "Min loss: 0.005067700054496527\n",
      "Mean loss: 0.010650648036971688\n",
      "Std loss: 0.005737620211067487\n",
      "Total Loss: 0.06390388822183013\n",
      "------------------------------------ epoch 4792 (28746 steps) ------------------------------------\n",
      "Max loss: 0.015334738418459892\n",
      "Min loss: 0.005674296990036964\n",
      "Mean loss: 0.009393735633542141\n",
      "Std loss: 0.0029231937496574044\n",
      "Total Loss: 0.05636241380125284\n",
      "------------------------------------ epoch 4793 (28752 steps) ------------------------------------\n",
      "Max loss: 0.016768179833889008\n",
      "Min loss: 0.008461585268378258\n",
      "Mean loss: 0.011792487930506468\n",
      "Std loss: 0.0030435058233688225\n",
      "Total Loss: 0.0707549275830388\n",
      "------------------------------------ epoch 4794 (28758 steps) ------------------------------------\n",
      "Max loss: 0.010608012787997723\n",
      "Min loss: 0.005604628473520279\n",
      "Mean loss: 0.007981203806897005\n",
      "Std loss: 0.0017322272888754464\n",
      "Total Loss: 0.04788722284138203\n",
      "------------------------------------ epoch 4795 (28764 steps) ------------------------------------\n",
      "Max loss: 0.013462374918162823\n",
      "Min loss: 0.006675725802779198\n",
      "Mean loss: 0.009361890377476811\n",
      "Std loss: 0.002475056697617623\n",
      "Total Loss: 0.05617134226486087\n",
      "------------------------------------ epoch 4796 (28770 steps) ------------------------------------\n",
      "Max loss: 0.013073131442070007\n",
      "Min loss: 0.007173439022153616\n",
      "Mean loss: 0.010314807062968612\n",
      "Std loss: 0.0020635731404207882\n",
      "Total Loss: 0.06188884237781167\n",
      "------------------------------------ epoch 4797 (28776 steps) ------------------------------------\n",
      "Max loss: 0.009006107226014137\n",
      "Min loss: 0.005128190852701664\n",
      "Mean loss: 0.006751669105142355\n",
      "Std loss: 0.0013636947884636229\n",
      "Total Loss: 0.04051001463085413\n",
      "------------------------------------ epoch 4798 (28782 steps) ------------------------------------\n",
      "Max loss: 0.014325698837637901\n",
      "Min loss: 0.006269603036344051\n",
      "Mean loss: 0.008703195800383886\n",
      "Std loss: 0.002899658457278714\n",
      "Total Loss: 0.052219174802303314\n",
      "------------------------------------ epoch 4799 (28788 steps) ------------------------------------\n",
      "Max loss: 0.03429420292377472\n",
      "Min loss: 0.004340062849223614\n",
      "Mean loss: 0.011977358255535364\n",
      "Std loss: 0.010212896018928183\n",
      "Total Loss: 0.07186414953321218\n",
      "------------------------------------ epoch 4800 (28794 steps) ------------------------------------\n",
      "Max loss: 0.01695001870393753\n",
      "Min loss: 0.005049549508839846\n",
      "Mean loss: 0.008507016658162078\n",
      "Std loss: 0.003927238828461544\n",
      "Total Loss: 0.051042099948972464\n",
      "------------------------------------ epoch 4801 (28800 steps) ------------------------------------\n",
      "Max loss: 0.025554126128554344\n",
      "Min loss: 0.004580510314553976\n",
      "Mean loss: 0.011978030127162734\n",
      "Std loss: 0.007607056545373266\n",
      "Total Loss: 0.07186818076297641\n",
      "saved model at ./weights/model_4801.pth\n",
      "------------------------------------ epoch 4802 (28806 steps) ------------------------------------\n",
      "Max loss: 0.016439875587821007\n",
      "Min loss: 0.005931164138019085\n",
      "Mean loss: 0.012074409984052181\n",
      "Std loss: 0.0036530055576908045\n",
      "Total Loss: 0.07244645990431309\n",
      "------------------------------------ epoch 4803 (28812 steps) ------------------------------------\n",
      "Max loss: 0.012896470725536346\n",
      "Min loss: 0.005512466188520193\n",
      "Mean loss: 0.008622194640338421\n",
      "Std loss: 0.0025953103397301004\n",
      "Total Loss: 0.051733167842030525\n",
      "------------------------------------ epoch 4804 (28818 steps) ------------------------------------\n",
      "Max loss: 0.039667174220085144\n",
      "Min loss: 0.007197631057351828\n",
      "Mean loss: 0.015294693177565932\n",
      "Std loss: 0.011630961438782197\n",
      "Total Loss: 0.0917681590653956\n",
      "------------------------------------ epoch 4805 (28824 steps) ------------------------------------\n",
      "Max loss: 0.013434670865535736\n",
      "Min loss: 0.005839311983436346\n",
      "Mean loss: 0.009475806960836053\n",
      "Std loss: 0.0023904049101111584\n",
      "Total Loss: 0.05685484176501632\n",
      "------------------------------------ epoch 4806 (28830 steps) ------------------------------------\n",
      "Max loss: 0.011016450822353363\n",
      "Min loss: 0.006116755306720734\n",
      "Mean loss: 0.007680229997883241\n",
      "Std loss: 0.0017525488351788927\n",
      "Total Loss: 0.04608137998729944\n",
      "------------------------------------ epoch 4807 (28836 steps) ------------------------------------\n",
      "Max loss: 0.01826738566160202\n",
      "Min loss: 0.007900345139205456\n",
      "Mean loss: 0.013589673830817143\n",
      "Std loss: 0.004625647530024557\n",
      "Total Loss: 0.08153804298490286\n",
      "------------------------------------ epoch 4808 (28842 steps) ------------------------------------\n",
      "Max loss: 0.019238241016864777\n",
      "Min loss: 0.006588004529476166\n",
      "Mean loss: 0.011673170297096172\n",
      "Std loss: 0.004233862957499201\n",
      "Total Loss: 0.07003902178257704\n",
      "------------------------------------ epoch 4809 (28848 steps) ------------------------------------\n",
      "Max loss: 0.01855960488319397\n",
      "Min loss: 0.008155290968716145\n",
      "Mean loss: 0.013629742277165255\n",
      "Std loss: 0.004081553593186182\n",
      "Total Loss: 0.08177845366299152\n",
      "------------------------------------ epoch 4810 (28854 steps) ------------------------------------\n",
      "Max loss: 0.021732840687036514\n",
      "Min loss: 0.006671193055808544\n",
      "Mean loss: 0.011379564957072338\n",
      "Std loss: 0.005085980408042595\n",
      "Total Loss: 0.06827738974243402\n",
      "------------------------------------ epoch 4811 (28860 steps) ------------------------------------\n",
      "Max loss: 0.025673311203718185\n",
      "Min loss: 0.008057544007897377\n",
      "Mean loss: 0.014903429429978132\n",
      "Std loss: 0.00597841786524781\n",
      "Total Loss: 0.0894205765798688\n",
      "------------------------------------ epoch 4812 (28866 steps) ------------------------------------\n",
      "Max loss: 0.02412104420363903\n",
      "Min loss: 0.005559311714023352\n",
      "Mean loss: 0.011410142139842113\n",
      "Std loss: 0.006385654091803892\n",
      "Total Loss: 0.06846085283905268\n",
      "------------------------------------ epoch 4813 (28872 steps) ------------------------------------\n",
      "Max loss: 0.02022760733962059\n",
      "Min loss: 0.006327773444354534\n",
      "Mean loss: 0.012760172442843517\n",
      "Std loss: 0.005047092692480268\n",
      "Total Loss: 0.0765610346570611\n",
      "------------------------------------ epoch 4814 (28878 steps) ------------------------------------\n",
      "Max loss: 0.04266413673758507\n",
      "Min loss: 0.006647360976785421\n",
      "Mean loss: 0.016535382019355893\n",
      "Std loss: 0.01212381609286095\n",
      "Total Loss: 0.09921229211613536\n",
      "------------------------------------ epoch 4815 (28884 steps) ------------------------------------\n",
      "Max loss: 0.02973860315978527\n",
      "Min loss: 0.005676267668604851\n",
      "Mean loss: 0.01188213339385887\n",
      "Std loss: 0.008455719998887135\n",
      "Total Loss: 0.07129280036315322\n",
      "------------------------------------ epoch 4816 (28890 steps) ------------------------------------\n",
      "Max loss: 0.015433331951498985\n",
      "Min loss: 0.006163169629871845\n",
      "Mean loss: 0.010181731932486096\n",
      "Std loss: 0.0034026996137032544\n",
      "Total Loss: 0.06109039159491658\n",
      "------------------------------------ epoch 4817 (28896 steps) ------------------------------------\n",
      "Max loss: 0.025084957480430603\n",
      "Min loss: 0.006331232842057943\n",
      "Mean loss: 0.012162266609569391\n",
      "Std loss: 0.006290788225028757\n",
      "Total Loss: 0.07297359965741634\n",
      "------------------------------------ epoch 4818 (28902 steps) ------------------------------------\n",
      "Max loss: 0.013133645057678223\n",
      "Min loss: 0.005689255893230438\n",
      "Mean loss: 0.008484862667197982\n",
      "Std loss: 0.002682269224171832\n",
      "Total Loss: 0.050909176003187895\n",
      "------------------------------------ epoch 4819 (28908 steps) ------------------------------------\n",
      "Max loss: 0.020785648375749588\n",
      "Min loss: 0.00850084237754345\n",
      "Mean loss: 0.01241722609847784\n",
      "Std loss: 0.003883075849227704\n",
      "Total Loss: 0.07450335659086704\n",
      "------------------------------------ epoch 4820 (28914 steps) ------------------------------------\n",
      "Max loss: 0.027354329824447632\n",
      "Min loss: 0.008294741623103619\n",
      "Mean loss: 0.013629970140755177\n",
      "Std loss: 0.006662251922136803\n",
      "Total Loss: 0.08177982084453106\n",
      "------------------------------------ epoch 4821 (28920 steps) ------------------------------------\n",
      "Max loss: 0.017314374446868896\n",
      "Min loss: 0.004815198481082916\n",
      "Mean loss: 0.010131605435162783\n",
      "Std loss: 0.004212929459112391\n",
      "Total Loss: 0.060789632610976696\n",
      "------------------------------------ epoch 4822 (28926 steps) ------------------------------------\n",
      "Max loss: 0.02415471151471138\n",
      "Min loss: 0.007152134086936712\n",
      "Mean loss: 0.011781105228389302\n",
      "Std loss: 0.006339256458336296\n",
      "Total Loss: 0.07068663137033582\n",
      "------------------------------------ epoch 4823 (28932 steps) ------------------------------------\n",
      "Max loss: 0.012414740398526192\n",
      "Min loss: 0.005787137430161238\n",
      "Mean loss: 0.009144977744047841\n",
      "Std loss: 0.0021712643646789034\n",
      "Total Loss: 0.05486986646428704\n",
      "------------------------------------ epoch 4824 (28938 steps) ------------------------------------\n",
      "Max loss: 0.008991509675979614\n",
      "Min loss: 0.005235744174569845\n",
      "Mean loss: 0.0071336075974007445\n",
      "Std loss: 0.0011068823451899044\n",
      "Total Loss: 0.04280164558440447\n",
      "------------------------------------ epoch 4825 (28944 steps) ------------------------------------\n",
      "Max loss: 0.011412869207561016\n",
      "Min loss: 0.005294959992170334\n",
      "Mean loss: 0.007006739964708686\n",
      "Std loss: 0.002142715772770698\n",
      "Total Loss: 0.042040439788252115\n",
      "------------------------------------ epoch 4826 (28950 steps) ------------------------------------\n",
      "Max loss: 0.021600453183054924\n",
      "Min loss: 0.004992488771677017\n",
      "Mean loss: 0.011881520428384343\n",
      "Std loss: 0.006962464664177956\n",
      "Total Loss: 0.07128912257030606\n",
      "------------------------------------ epoch 4827 (28956 steps) ------------------------------------\n",
      "Max loss: 0.041155457496643066\n",
      "Min loss: 0.005652758292853832\n",
      "Mean loss: 0.014139165403321385\n",
      "Std loss: 0.012521220714516392\n",
      "Total Loss: 0.08483499241992831\n",
      "------------------------------------ epoch 4828 (28962 steps) ------------------------------------\n",
      "Max loss: 0.03460662066936493\n",
      "Min loss: 0.00668694544583559\n",
      "Mean loss: 0.012095756595954299\n",
      "Std loss: 0.010126017616811457\n",
      "Total Loss: 0.0725745395757258\n",
      "------------------------------------ epoch 4829 (28968 steps) ------------------------------------\n",
      "Max loss: 0.029846251010894775\n",
      "Min loss: 0.008557898923754692\n",
      "Mean loss: 0.013870038402577242\n",
      "Std loss: 0.0073118515655688976\n",
      "Total Loss: 0.08322023041546345\n",
      "------------------------------------ epoch 4830 (28974 steps) ------------------------------------\n",
      "Max loss: 0.01899034157395363\n",
      "Min loss: 0.005948928184807301\n",
      "Mean loss: 0.009664558029423157\n",
      "Std loss: 0.004344442790312536\n",
      "Total Loss: 0.057987348176538944\n",
      "------------------------------------ epoch 4831 (28980 steps) ------------------------------------\n",
      "Max loss: 0.0168803371489048\n",
      "Min loss: 0.0070174261927604675\n",
      "Mean loss: 0.011374042058984438\n",
      "Std loss: 0.003644294399092321\n",
      "Total Loss: 0.06824425235390663\n",
      "------------------------------------ epoch 4832 (28986 steps) ------------------------------------\n",
      "Max loss: 0.01584051363170147\n",
      "Min loss: 0.004883081652224064\n",
      "Mean loss: 0.009141767087082068\n",
      "Std loss: 0.0035793208370743126\n",
      "Total Loss: 0.05485060252249241\n",
      "------------------------------------ epoch 4833 (28992 steps) ------------------------------------\n",
      "Max loss: 0.01946420408785343\n",
      "Min loss: 0.007987345568835735\n",
      "Mean loss: 0.01249690850575765\n",
      "Std loss: 0.003823852248137039\n",
      "Total Loss: 0.0749814510345459\n",
      "------------------------------------ epoch 4834 (28998 steps) ------------------------------------\n",
      "Max loss: 0.016948625445365906\n",
      "Min loss: 0.005170058459043503\n",
      "Mean loss: 0.01047265351129075\n",
      "Std loss: 0.004507121112989239\n",
      "Total Loss: 0.0628359210677445\n",
      "------------------------------------ epoch 4835 (29004 steps) ------------------------------------\n",
      "Max loss: 0.026188060641288757\n",
      "Min loss: 0.006852148100733757\n",
      "Mean loss: 0.012355590937659144\n",
      "Std loss: 0.006673797261889705\n",
      "Total Loss: 0.07413354562595487\n",
      "------------------------------------ epoch 4836 (29010 steps) ------------------------------------\n",
      "Max loss: 0.028782671317458153\n",
      "Min loss: 0.0053613148629665375\n",
      "Mean loss: 0.015186910051852465\n",
      "Std loss: 0.008133549910044112\n",
      "Total Loss: 0.09112146031111479\n",
      "------------------------------------ epoch 4837 (29016 steps) ------------------------------------\n",
      "Max loss: 0.013245610520243645\n",
      "Min loss: 0.0055952235125005245\n",
      "Mean loss: 0.008105051626140872\n",
      "Std loss: 0.0027679251457834618\n",
      "Total Loss: 0.048630309756845236\n",
      "------------------------------------ epoch 4838 (29022 steps) ------------------------------------\n",
      "Max loss: 0.017674464732408524\n",
      "Min loss: 0.006248930003494024\n",
      "Mean loss: 0.010682244785130024\n",
      "Std loss: 0.003835235938486025\n",
      "Total Loss: 0.06409346871078014\n",
      "------------------------------------ epoch 4839 (29028 steps) ------------------------------------\n",
      "Max loss: 0.03476250171661377\n",
      "Min loss: 0.007811419665813446\n",
      "Mean loss: 0.015842532739043236\n",
      "Std loss: 0.009391191143234086\n",
      "Total Loss: 0.09505519643425941\n",
      "------------------------------------ epoch 4840 (29034 steps) ------------------------------------\n",
      "Max loss: 0.02071618288755417\n",
      "Min loss: 0.0073680393397808075\n",
      "Mean loss: 0.012591124512255192\n",
      "Std loss: 0.004525126955526577\n",
      "Total Loss: 0.07554674707353115\n",
      "------------------------------------ epoch 4841 (29040 steps) ------------------------------------\n",
      "Max loss: 0.020002558827400208\n",
      "Min loss: 0.006408681161701679\n",
      "Mean loss: 0.011956300431241592\n",
      "Std loss: 0.004439083278921273\n",
      "Total Loss: 0.07173780258744955\n",
      "------------------------------------ epoch 4842 (29046 steps) ------------------------------------\n",
      "Max loss: 0.02473513036966324\n",
      "Min loss: 0.0073023587465286255\n",
      "Mean loss: 0.012529936075831452\n",
      "Std loss: 0.006252649620522301\n",
      "Total Loss: 0.07517961645498872\n",
      "------------------------------------ epoch 4843 (29052 steps) ------------------------------------\n",
      "Max loss: 0.009185123257339\n",
      "Min loss: 0.005699596367776394\n",
      "Mean loss: 0.007487631247689326\n",
      "Std loss: 0.0010442791609025657\n",
      "Total Loss: 0.04492578748613596\n",
      "------------------------------------ epoch 4844 (29058 steps) ------------------------------------\n",
      "Max loss: 0.018827399238944054\n",
      "Min loss: 0.0069229481741786\n",
      "Mean loss: 0.01105848466977477\n",
      "Std loss: 0.003750262825057947\n",
      "Total Loss: 0.06635090801864862\n",
      "------------------------------------ epoch 4845 (29064 steps) ------------------------------------\n",
      "Max loss: 0.0191463865339756\n",
      "Min loss: 0.0060486565344035625\n",
      "Mean loss: 0.009434796636924148\n",
      "Std loss: 0.004494750808178884\n",
      "Total Loss: 0.056608779821544886\n",
      "------------------------------------ epoch 4846 (29070 steps) ------------------------------------\n",
      "Max loss: 0.024894800037145615\n",
      "Min loss: 0.007578202523291111\n",
      "Mean loss: 0.012754970230162144\n",
      "Std loss: 0.006061479282438687\n",
      "Total Loss: 0.07652982138097286\n",
      "------------------------------------ epoch 4847 (29076 steps) ------------------------------------\n",
      "Max loss: 0.00988900475203991\n",
      "Min loss: 0.005427553318440914\n",
      "Mean loss: 0.008176142970720926\n",
      "Std loss: 0.0018907134136016987\n",
      "Total Loss: 0.04905685782432556\n",
      "------------------------------------ epoch 4848 (29082 steps) ------------------------------------\n",
      "Max loss: 0.008572653867304325\n",
      "Min loss: 0.004648013040423393\n",
      "Mean loss: 0.006849266123026609\n",
      "Std loss: 0.0012280218516865094\n",
      "Total Loss: 0.041095596738159657\n",
      "------------------------------------ epoch 4849 (29088 steps) ------------------------------------\n",
      "Max loss: 0.03530006855726242\n",
      "Min loss: 0.004870356991887093\n",
      "Mean loss: 0.016656155542780954\n",
      "Std loss: 0.013358746029157105\n",
      "Total Loss: 0.09993693325668573\n",
      "------------------------------------ epoch 4850 (29094 steps) ------------------------------------\n",
      "Max loss: 0.021286262199282646\n",
      "Min loss: 0.005009490996599197\n",
      "Mean loss: 0.011673542205244303\n",
      "Std loss: 0.005561983485134793\n",
      "Total Loss: 0.07004125323146582\n",
      "------------------------------------ epoch 4851 (29100 steps) ------------------------------------\n",
      "Max loss: 0.03350089490413666\n",
      "Min loss: 0.007111544720828533\n",
      "Mean loss: 0.01644164075454076\n",
      "Std loss: 0.008650453639630825\n",
      "Total Loss: 0.09864984452724457\n",
      "------------------------------------ epoch 4852 (29106 steps) ------------------------------------\n",
      "Max loss: 0.020679712295532227\n",
      "Min loss: 0.008172743953764439\n",
      "Mean loss: 0.013905717059969902\n",
      "Std loss: 0.0048341343177524115\n",
      "Total Loss: 0.08343430235981941\n",
      "------------------------------------ epoch 4853 (29112 steps) ------------------------------------\n",
      "Max loss: 0.014960912987589836\n",
      "Min loss: 0.006688650697469711\n",
      "Mean loss: 0.010727799497544765\n",
      "Std loss: 0.0032204444412033055\n",
      "Total Loss: 0.06436679698526859\n",
      "------------------------------------ epoch 4854 (29118 steps) ------------------------------------\n",
      "Max loss: 0.011029228568077087\n",
      "Min loss: 0.006206282880157232\n",
      "Mean loss: 0.008218065369874239\n",
      "Std loss: 0.0015380030559748673\n",
      "Total Loss: 0.049308392219245434\n",
      "------------------------------------ epoch 4855 (29124 steps) ------------------------------------\n",
      "Max loss: 0.03115825168788433\n",
      "Min loss: 0.0056862011551856995\n",
      "Mean loss: 0.01336420412796239\n",
      "Std loss: 0.00855420631423923\n",
      "Total Loss: 0.08018522476777434\n",
      "------------------------------------ epoch 4856 (29130 steps) ------------------------------------\n",
      "Max loss: 0.013996323570609093\n",
      "Min loss: 0.005875441245734692\n",
      "Mean loss: 0.009195563538620869\n",
      "Std loss: 0.0028029626000769547\n",
      "Total Loss: 0.055173381231725216\n",
      "------------------------------------ epoch 4857 (29136 steps) ------------------------------------\n",
      "Max loss: 0.024089578539133072\n",
      "Min loss: 0.00617907103151083\n",
      "Mean loss: 0.01223709744711717\n",
      "Std loss: 0.005771327589763278\n",
      "Total Loss: 0.07342258468270302\n",
      "------------------------------------ epoch 4858 (29142 steps) ------------------------------------\n",
      "Max loss: 0.030434492975473404\n",
      "Min loss: 0.00926217995584011\n",
      "Mean loss: 0.016951135204484064\n",
      "Std loss: 0.007852705135948596\n",
      "Total Loss: 0.10170681122690439\n",
      "------------------------------------ epoch 4859 (29148 steps) ------------------------------------\n",
      "Max loss: 0.012685321271419525\n",
      "Min loss: 0.008827165700495243\n",
      "Mean loss: 0.010125829527775446\n",
      "Std loss: 0.0015724685829825967\n",
      "Total Loss: 0.06075497716665268\n",
      "------------------------------------ epoch 4860 (29154 steps) ------------------------------------\n",
      "Max loss: 0.039886217564344406\n",
      "Min loss: 0.005462100729346275\n",
      "Mean loss: 0.014880619710311294\n",
      "Std loss: 0.011977843635103054\n",
      "Total Loss: 0.08928371826186776\n",
      "------------------------------------ epoch 4861 (29160 steps) ------------------------------------\n",
      "Max loss: 0.019103877246379852\n",
      "Min loss: 0.008654985576868057\n",
      "Mean loss: 0.01310349659373363\n",
      "Std loss: 0.0037235524202391362\n",
      "Total Loss: 0.07862097956240177\n",
      "------------------------------------ epoch 4862 (29166 steps) ------------------------------------\n",
      "Max loss: 0.03476070612668991\n",
      "Min loss: 0.008269171230494976\n",
      "Mean loss: 0.01873899670317769\n",
      "Std loss: 0.00917482575959642\n",
      "Total Loss: 0.11243398021906614\n",
      "------------------------------------ epoch 4863 (29172 steps) ------------------------------------\n",
      "Max loss: 0.018999170511960983\n",
      "Min loss: 0.007368074264377356\n",
      "Mean loss: 0.011334423053388795\n",
      "Std loss: 0.0038318014312265013\n",
      "Total Loss: 0.06800653832033277\n",
      "------------------------------------ epoch 4864 (29178 steps) ------------------------------------\n",
      "Max loss: 0.010285425931215286\n",
      "Min loss: 0.006874581798911095\n",
      "Mean loss: 0.008580011781305075\n",
      "Std loss: 0.0010952441154335145\n",
      "Total Loss: 0.05148007068783045\n",
      "------------------------------------ epoch 4865 (29184 steps) ------------------------------------\n",
      "Max loss: 0.012179506942629814\n",
      "Min loss: 0.006256640888750553\n",
      "Mean loss: 0.009789390799899897\n",
      "Std loss: 0.0019395297808794938\n",
      "Total Loss: 0.058736344799399376\n",
      "------------------------------------ epoch 4866 (29190 steps) ------------------------------------\n",
      "Max loss: 0.013137072324752808\n",
      "Min loss: 0.006077423691749573\n",
      "Mean loss: 0.00991222153728207\n",
      "Std loss: 0.0025893239979122742\n",
      "Total Loss: 0.05947332922369242\n",
      "------------------------------------ epoch 4867 (29196 steps) ------------------------------------\n",
      "Max loss: 0.011751381680369377\n",
      "Min loss: 0.0052863070741295815\n",
      "Mean loss: 0.006902647592748205\n",
      "Std loss: 0.002204081322638332\n",
      "Total Loss: 0.04141588555648923\n",
      "------------------------------------ epoch 4868 (29202 steps) ------------------------------------\n",
      "Max loss: 0.01685001328587532\n",
      "Min loss: 0.006701081059873104\n",
      "Mean loss: 0.010736237280070782\n",
      "Std loss: 0.00341914469845104\n",
      "Total Loss: 0.06441742368042469\n",
      "------------------------------------ epoch 4869 (29208 steps) ------------------------------------\n",
      "Max loss: 0.03125116601586342\n",
      "Min loss: 0.005508404225111008\n",
      "Mean loss: 0.011565281543880701\n",
      "Std loss: 0.008926203431855798\n",
      "Total Loss: 0.0693916892632842\n",
      "------------------------------------ epoch 4870 (29214 steps) ------------------------------------\n",
      "Max loss: 0.018491186201572418\n",
      "Min loss: 0.006220579147338867\n",
      "Mean loss: 0.00980225756453971\n",
      "Std loss: 0.0040077864770587035\n",
      "Total Loss: 0.058813545387238264\n",
      "------------------------------------ epoch 4871 (29220 steps) ------------------------------------\n",
      "Max loss: 0.026767831295728683\n",
      "Min loss: 0.005332984030246735\n",
      "Mean loss: 0.013204607646912336\n",
      "Std loss: 0.007725790195674974\n",
      "Total Loss: 0.07922764588147402\n",
      "------------------------------------ epoch 4872 (29226 steps) ------------------------------------\n",
      "Max loss: 0.053488366305828094\n",
      "Min loss: 0.006386998109519482\n",
      "Mean loss: 0.017101471467564505\n",
      "Std loss: 0.016550334626053716\n",
      "Total Loss: 0.10260882880538702\n",
      "------------------------------------ epoch 4873 (29232 steps) ------------------------------------\n",
      "Max loss: 0.025473903864622116\n",
      "Min loss: 0.0074743544682860374\n",
      "Mean loss: 0.011697186467548212\n",
      "Std loss: 0.006253445358368704\n",
      "Total Loss: 0.07018311880528927\n",
      "------------------------------------ epoch 4874 (29238 steps) ------------------------------------\n",
      "Max loss: 0.014089927077293396\n",
      "Min loss: 0.0054253144189715385\n",
      "Mean loss: 0.009348352750142416\n",
      "Std loss: 0.0033992554374028595\n",
      "Total Loss: 0.05609011650085449\n",
      "------------------------------------ epoch 4875 (29244 steps) ------------------------------------\n",
      "Max loss: 0.04413546621799469\n",
      "Min loss: 0.006297468673437834\n",
      "Mean loss: 0.01585656777024269\n",
      "Std loss: 0.013023707239538726\n",
      "Total Loss: 0.09513940662145615\n",
      "------------------------------------ epoch 4876 (29250 steps) ------------------------------------\n",
      "Max loss: 0.041184425354003906\n",
      "Min loss: 0.005510265938937664\n",
      "Mean loss: 0.014933145216976603\n",
      "Std loss: 0.012768174152384236\n",
      "Total Loss: 0.08959887130185962\n",
      "------------------------------------ epoch 4877 (29256 steps) ------------------------------------\n",
      "Max loss: 0.018957331776618958\n",
      "Min loss: 0.00846836157143116\n",
      "Mean loss: 0.01313268564020594\n",
      "Std loss: 0.00423418094679234\n",
      "Total Loss: 0.07879611384123564\n",
      "------------------------------------ epoch 4878 (29262 steps) ------------------------------------\n",
      "Max loss: 0.023282017558813095\n",
      "Min loss: 0.01054324209690094\n",
      "Mean loss: 0.019045194300512474\n",
      "Std loss: 0.004293246289276793\n",
      "Total Loss: 0.11427116580307484\n",
      "------------------------------------ epoch 4879 (29268 steps) ------------------------------------\n",
      "Max loss: 0.04265868291258812\n",
      "Min loss: 0.015200400725007057\n",
      "Mean loss: 0.027037240875264008\n",
      "Std loss: 0.010733127871760551\n",
      "Total Loss: 0.16222344525158405\n",
      "------------------------------------ epoch 4880 (29274 steps) ------------------------------------\n",
      "Max loss: 0.030508529394865036\n",
      "Min loss: 0.007536380551755428\n",
      "Mean loss: 0.01514472501973311\n",
      "Std loss: 0.00891251049467645\n",
      "Total Loss: 0.09086835011839867\n",
      "------------------------------------ epoch 4881 (29280 steps) ------------------------------------\n",
      "Max loss: 0.023122813552618027\n",
      "Min loss: 0.007061265408992767\n",
      "Mean loss: 0.012376101144279042\n",
      "Std loss: 0.00552759643850051\n",
      "Total Loss: 0.07425660686567426\n",
      "------------------------------------ epoch 4882 (29286 steps) ------------------------------------\n",
      "Max loss: 0.02132577821612358\n",
      "Min loss: 0.006170875392854214\n",
      "Mean loss: 0.012054336024448276\n",
      "Std loss: 0.006147826481190784\n",
      "Total Loss: 0.07232601614668965\n",
      "------------------------------------ epoch 4883 (29292 steps) ------------------------------------\n",
      "Max loss: 0.02294146828353405\n",
      "Min loss: 0.0069716693833470345\n",
      "Mean loss: 0.012570828199386597\n",
      "Std loss: 0.0053889591311305825\n",
      "Total Loss: 0.07542496919631958\n",
      "------------------------------------ epoch 4884 (29298 steps) ------------------------------------\n",
      "Max loss: 0.013922659680247307\n",
      "Min loss: 0.0078116049990057945\n",
      "Mean loss: 0.010800355579704046\n",
      "Std loss: 0.00254396671819107\n",
      "Total Loss: 0.06480213347822428\n",
      "------------------------------------ epoch 4885 (29304 steps) ------------------------------------\n",
      "Max loss: 0.019083943217992783\n",
      "Min loss: 0.006048382725566626\n",
      "Mean loss: 0.01107123129380246\n",
      "Std loss: 0.005156838955959407\n",
      "Total Loss: 0.06642738776281476\n",
      "------------------------------------ epoch 4886 (29310 steps) ------------------------------------\n",
      "Max loss: 0.01997852511703968\n",
      "Min loss: 0.007306599523872137\n",
      "Mean loss: 0.012895203738783797\n",
      "Std loss: 0.004136223528719057\n",
      "Total Loss: 0.07737122243270278\n",
      "------------------------------------ epoch 4887 (29316 steps) ------------------------------------\n",
      "Max loss: 0.012847303412854671\n",
      "Min loss: 0.005802845116704702\n",
      "Mean loss: 0.009943972264106074\n",
      "Std loss: 0.0023580217739301128\n",
      "Total Loss: 0.05966383358463645\n",
      "------------------------------------ epoch 4888 (29322 steps) ------------------------------------\n",
      "Max loss: 0.01248937752097845\n",
      "Min loss: 0.005205090157687664\n",
      "Mean loss: 0.008404413238167763\n",
      "Std loss: 0.0025370722686458283\n",
      "Total Loss: 0.05042647942900658\n",
      "------------------------------------ epoch 4889 (29328 steps) ------------------------------------\n",
      "Max loss: 0.01646491512656212\n",
      "Min loss: 0.007442185655236244\n",
      "Mean loss: 0.011649393321325382\n",
      "Std loss: 0.0029274218028824735\n",
      "Total Loss: 0.06989635992795229\n",
      "------------------------------------ epoch 4890 (29334 steps) ------------------------------------\n",
      "Max loss: 0.02258303016424179\n",
      "Min loss: 0.0053621502593159676\n",
      "Mean loss: 0.010939568902055422\n",
      "Std loss: 0.005657660238322543\n",
      "Total Loss: 0.06563741341233253\n",
      "------------------------------------ epoch 4891 (29340 steps) ------------------------------------\n",
      "Max loss: 0.015963438898324966\n",
      "Min loss: 0.004356437362730503\n",
      "Mean loss: 0.00871593380967776\n",
      "Std loss: 0.0038542369386185267\n",
      "Total Loss: 0.05229560285806656\n",
      "------------------------------------ epoch 4892 (29346 steps) ------------------------------------\n",
      "Max loss: 0.0177045576274395\n",
      "Min loss: 0.005921734031289816\n",
      "Mean loss: 0.00952734681777656\n",
      "Std loss: 0.003867115524386072\n",
      "Total Loss: 0.057164080906659365\n",
      "------------------------------------ epoch 4893 (29352 steps) ------------------------------------\n",
      "Max loss: 0.01303813885897398\n",
      "Min loss: 0.007412984035909176\n",
      "Mean loss: 0.010036118483791748\n",
      "Std loss: 0.0017297132197271717\n",
      "Total Loss: 0.06021671090275049\n",
      "------------------------------------ epoch 4894 (29358 steps) ------------------------------------\n",
      "Max loss: 0.0243392251431942\n",
      "Min loss: 0.007418639492243528\n",
      "Mean loss: 0.012149721461658677\n",
      "Std loss: 0.005747592734594569\n",
      "Total Loss: 0.07289832876995206\n",
      "------------------------------------ epoch 4895 (29364 steps) ------------------------------------\n",
      "Max loss: 0.011189309880137444\n",
      "Min loss: 0.0060704960487782955\n",
      "Mean loss: 0.007862556803350648\n",
      "Std loss: 0.0017764596446127414\n",
      "Total Loss: 0.047175340820103884\n",
      "------------------------------------ epoch 4896 (29370 steps) ------------------------------------\n",
      "Max loss: 0.016631072387099266\n",
      "Min loss: 0.00752769410610199\n",
      "Mean loss: 0.012355235870927572\n",
      "Std loss: 0.0034439025256801058\n",
      "Total Loss: 0.07413141522556543\n",
      "------------------------------------ epoch 4897 (29376 steps) ------------------------------------\n",
      "Max loss: 0.04103963077068329\n",
      "Min loss: 0.007087416481226683\n",
      "Mean loss: 0.01610088759722809\n",
      "Std loss: 0.01296360414887662\n",
      "Total Loss: 0.09660532558336854\n",
      "------------------------------------ epoch 4898 (29382 steps) ------------------------------------\n",
      "Max loss: 0.023577198386192322\n",
      "Min loss: 0.007130607031285763\n",
      "Mean loss: 0.012293795123696327\n",
      "Std loss: 0.005309829334187161\n",
      "Total Loss: 0.07376277074217796\n",
      "------------------------------------ epoch 4899 (29388 steps) ------------------------------------\n",
      "Max loss: 0.01957593485713005\n",
      "Min loss: 0.006277169566601515\n",
      "Mean loss: 0.012735953942562142\n",
      "Std loss: 0.004391167593273287\n",
      "Total Loss: 0.07641572365537286\n",
      "------------------------------------ epoch 4900 (29394 steps) ------------------------------------\n",
      "Max loss: 0.01927708089351654\n",
      "Min loss: 0.006997888442128897\n",
      "Mean loss: 0.011251002550125122\n",
      "Std loss: 0.004466116963290653\n",
      "Total Loss: 0.06750601530075073\n",
      "------------------------------------ epoch 4901 (29400 steps) ------------------------------------\n",
      "Max loss: 0.02585175447165966\n",
      "Min loss: 0.005859166383743286\n",
      "Mean loss: 0.010977563603470722\n",
      "Std loss: 0.006829793472472959\n",
      "Total Loss: 0.06586538162082434\n",
      "saved model at ./weights/model_4901.pth\n",
      "------------------------------------ epoch 4902 (29406 steps) ------------------------------------\n",
      "Max loss: 0.03386351093649864\n",
      "Min loss: 0.010601349174976349\n",
      "Mean loss: 0.018566869820157688\n",
      "Std loss: 0.007894006954879025\n",
      "Total Loss: 0.11140121892094612\n",
      "------------------------------------ epoch 4903 (29412 steps) ------------------------------------\n",
      "Max loss: 0.19612953066825867\n",
      "Min loss: 0.02440747246146202\n",
      "Mean loss: 0.08532423836489518\n",
      "Std loss: 0.06570835834725448\n",
      "Total Loss: 0.5119454301893711\n",
      "------------------------------------ epoch 4904 (29418 steps) ------------------------------------\n",
      "Max loss: 0.6008133292198181\n",
      "Min loss: 0.28520938754081726\n",
      "Mean loss: 0.3958670496940613\n",
      "Std loss: 0.10562374394979752\n",
      "Total Loss: 2.3752022981643677\n",
      "------------------------------------ epoch 4905 (29424 steps) ------------------------------------\n",
      "Max loss: 0.5740407705307007\n",
      "Min loss: 0.28124818205833435\n",
      "Mean loss: 0.38137773672739667\n",
      "Std loss: 0.12232429631318047\n",
      "Total Loss: 2.28826642036438\n",
      "------------------------------------ epoch 4906 (29430 steps) ------------------------------------\n",
      "Max loss: 0.7421787977218628\n",
      "Min loss: 0.22252434492111206\n",
      "Mean loss: 0.34958890080451965\n",
      "Std loss: 0.17918841092272206\n",
      "Total Loss: 2.097533404827118\n",
      "------------------------------------ epoch 4907 (29436 steps) ------------------------------------\n",
      "Max loss: 0.2642134428024292\n",
      "Min loss: 0.17877018451690674\n",
      "Mean loss: 0.21592701971530914\n",
      "Std loss: 0.02829186243802353\n",
      "Total Loss: 1.2955621182918549\n",
      "------------------------------------ epoch 4908 (29442 steps) ------------------------------------\n",
      "Max loss: 0.19648759067058563\n",
      "Min loss: 0.12953439354896545\n",
      "Mean loss: 0.15965298314889273\n",
      "Std loss: 0.02257771387029761\n",
      "Total Loss: 0.9579178988933563\n",
      "------------------------------------ epoch 4909 (29448 steps) ------------------------------------\n",
      "Max loss: 0.15837186574935913\n",
      "Min loss: 0.08502049744129181\n",
      "Mean loss: 0.12098961944381396\n",
      "Std loss: 0.02195784296966018\n",
      "Total Loss: 0.7259377166628838\n",
      "------------------------------------ epoch 4910 (29454 steps) ------------------------------------\n",
      "Max loss: 0.12920407950878143\n",
      "Min loss: 0.06855058670043945\n",
      "Mean loss: 0.09125944351156552\n",
      "Std loss: 0.01885032867169746\n",
      "Total Loss: 0.5475566610693932\n",
      "------------------------------------ epoch 4911 (29460 steps) ------------------------------------\n",
      "Max loss: 0.08033891022205353\n",
      "Min loss: 0.059011489152908325\n",
      "Mean loss: 0.0662162322551012\n",
      "Std loss: 0.006965907294004216\n",
      "Total Loss: 0.3972973935306072\n",
      "------------------------------------ epoch 4912 (29466 steps) ------------------------------------\n",
      "Max loss: 0.06889133900403976\n",
      "Min loss: 0.03786331042647362\n",
      "Mean loss: 0.05478163125614325\n",
      "Std loss: 0.011243803943820166\n",
      "Total Loss: 0.3286897875368595\n",
      "------------------------------------ epoch 4913 (29472 steps) ------------------------------------\n",
      "Max loss: 0.052997320890426636\n",
      "Min loss: 0.03426284343004227\n",
      "Mean loss: 0.04055975750088692\n",
      "Std loss: 0.006369927781883202\n",
      "Total Loss: 0.2433585450053215\n",
      "------------------------------------ epoch 4914 (29478 steps) ------------------------------------\n",
      "Max loss: 0.0519414059817791\n",
      "Min loss: 0.02960871532559395\n",
      "Mean loss: 0.04362733600040277\n",
      "Std loss: 0.00796839892680996\n",
      "Total Loss: 0.2617640160024166\n",
      "------------------------------------ epoch 4915 (29484 steps) ------------------------------------\n",
      "Max loss: 0.04481928050518036\n",
      "Min loss: 0.021985111758112907\n",
      "Mean loss: 0.03480193049957355\n",
      "Std loss: 0.0077646533097545465\n",
      "Total Loss: 0.2088115829974413\n",
      "------------------------------------ epoch 4916 (29490 steps) ------------------------------------\n",
      "Max loss: 0.07260671257972717\n",
      "Min loss: 0.021161139011383057\n",
      "Mean loss: 0.03668992997457584\n",
      "Std loss: 0.017427460285962714\n",
      "Total Loss: 0.22013957984745502\n",
      "------------------------------------ epoch 4917 (29496 steps) ------------------------------------\n",
      "Max loss: 0.0613766610622406\n",
      "Min loss: 0.021834170445799828\n",
      "Mean loss: 0.03490214794874191\n",
      "Std loss: 0.013051972843710331\n",
      "Total Loss: 0.20941288769245148\n",
      "------------------------------------ epoch 4918 (29502 steps) ------------------------------------\n",
      "Max loss: 0.03047947585582733\n",
      "Min loss: 0.0205237939953804\n",
      "Mean loss: 0.0270091878871123\n",
      "Std loss: 0.0033396390604044394\n",
      "Total Loss: 0.1620551273226738\n",
      "------------------------------------ epoch 4919 (29508 steps) ------------------------------------\n",
      "Max loss: 0.03665880858898163\n",
      "Min loss: 0.017330080270767212\n",
      "Mean loss: 0.026076943303147953\n",
      "Std loss: 0.005947305301191364\n",
      "Total Loss: 0.1564616598188877\n",
      "------------------------------------ epoch 4920 (29514 steps) ------------------------------------\n",
      "Max loss: 0.046310003846883774\n",
      "Min loss: 0.016184013336896896\n",
      "Mean loss: 0.03066142772634824\n",
      "Std loss: 0.01030433044778707\n",
      "Total Loss: 0.18396856635808945\n",
      "------------------------------------ epoch 4921 (29520 steps) ------------------------------------\n",
      "Max loss: 0.021724965423345566\n",
      "Min loss: 0.013914061710238457\n",
      "Mean loss: 0.018477282486855984\n",
      "Std loss: 0.002776740514144995\n",
      "Total Loss: 0.1108636949211359\n",
      "------------------------------------ epoch 4922 (29526 steps) ------------------------------------\n",
      "Max loss: 0.0588783323764801\n",
      "Min loss: 0.017794229090213776\n",
      "Mean loss: 0.027754022118945915\n",
      "Std loss: 0.014351737145563313\n",
      "Total Loss: 0.1665241327136755\n",
      "------------------------------------ epoch 4923 (29532 steps) ------------------------------------\n",
      "Max loss: 0.029062509536743164\n",
      "Min loss: 0.017751462757587433\n",
      "Mean loss: 0.02494951554884513\n",
      "Std loss: 0.005078100663745396\n",
      "Total Loss: 0.1496970932930708\n",
      "------------------------------------ epoch 4924 (29538 steps) ------------------------------------\n",
      "Max loss: 0.0357184112071991\n",
      "Min loss: 0.013801145367324352\n",
      "Mean loss: 0.02115898656969269\n",
      "Std loss: 0.007494469411750655\n",
      "Total Loss: 0.12695391941815615\n",
      "------------------------------------ epoch 4925 (29544 steps) ------------------------------------\n",
      "Max loss: 0.03067271038889885\n",
      "Min loss: 0.01440468616783619\n",
      "Mean loss: 0.02490311022847891\n",
      "Std loss: 0.006993060255288522\n",
      "Total Loss: 0.14941866137087345\n",
      "------------------------------------ epoch 4926 (29550 steps) ------------------------------------\n",
      "Max loss: 0.03003120794892311\n",
      "Min loss: 0.011936495080590248\n",
      "Mean loss: 0.02335693531980117\n",
      "Std loss: 0.006519222443362683\n",
      "Total Loss: 0.14014161191880703\n",
      "------------------------------------ epoch 4927 (29556 steps) ------------------------------------\n",
      "Max loss: 0.02113807201385498\n",
      "Min loss: 0.01220052968710661\n",
      "Mean loss: 0.015967875719070435\n",
      "Std loss: 0.0031053844538456664\n",
      "Total Loss: 0.09580725431442261\n",
      "------------------------------------ epoch 4928 (29562 steps) ------------------------------------\n",
      "Max loss: 0.02484823763370514\n",
      "Min loss: 0.010617959313094616\n",
      "Mean loss: 0.018686942600955565\n",
      "Std loss: 0.004961533284630496\n",
      "Total Loss: 0.1121216556057334\n",
      "------------------------------------ epoch 4929 (29568 steps) ------------------------------------\n",
      "Max loss: 0.021292870864272118\n",
      "Min loss: 0.010182837955653667\n",
      "Mean loss: 0.014471135723094145\n",
      "Std loss: 0.003770701672680075\n",
      "Total Loss: 0.08682681433856487\n",
      "------------------------------------ epoch 4930 (29574 steps) ------------------------------------\n",
      "Max loss: 0.02054087445139885\n",
      "Min loss: 0.010186737403273582\n",
      "Mean loss: 0.013299637318899235\n",
      "Std loss: 0.0034383031483653835\n",
      "Total Loss: 0.0797978239133954\n",
      "------------------------------------ epoch 4931 (29580 steps) ------------------------------------\n",
      "Max loss: 0.02477513812482357\n",
      "Min loss: 0.010382834821939468\n",
      "Mean loss: 0.01585191022604704\n",
      "Std loss: 0.0047790897624793065\n",
      "Total Loss: 0.09511146135628223\n",
      "------------------------------------ epoch 4932 (29586 steps) ------------------------------------\n",
      "Max loss: 0.02181251347064972\n",
      "Min loss: 0.0103333480656147\n",
      "Mean loss: 0.015794199736167986\n",
      "Std loss: 0.004770554543059936\n",
      "Total Loss: 0.09476519841700792\n",
      "------------------------------------ epoch 4933 (29592 steps) ------------------------------------\n",
      "Max loss: 0.022965392097830772\n",
      "Min loss: 0.009807588532567024\n",
      "Mean loss: 0.01712360993648569\n",
      "Std loss: 0.005304467535676348\n",
      "Total Loss: 0.10274165961891413\n",
      "------------------------------------ epoch 4934 (29598 steps) ------------------------------------\n",
      "Max loss: 0.03977710008621216\n",
      "Min loss: 0.009963552467525005\n",
      "Mean loss: 0.021465374312053125\n",
      "Std loss: 0.010681185051071499\n",
      "Total Loss: 0.12879224587231874\n",
      "------------------------------------ epoch 4935 (29604 steps) ------------------------------------\n",
      "Max loss: 0.01765552908182144\n",
      "Min loss: 0.00882425345480442\n",
      "Mean loss: 0.011956769973039627\n",
      "Std loss: 0.0030085978675581295\n",
      "Total Loss: 0.07174061983823776\n",
      "------------------------------------ epoch 4936 (29610 steps) ------------------------------------\n",
      "Max loss: 0.036902446299791336\n",
      "Min loss: 0.01075549703091383\n",
      "Mean loss: 0.01923287659883499\n",
      "Std loss: 0.008544563531669951\n",
      "Total Loss: 0.11539725959300995\n",
      "------------------------------------ epoch 4937 (29616 steps) ------------------------------------\n",
      "Max loss: 0.026859482750296593\n",
      "Min loss: 0.01073683425784111\n",
      "Mean loss: 0.016603990147511166\n",
      "Std loss: 0.005015599322530304\n",
      "Total Loss: 0.09962394088506699\n",
      "------------------------------------ epoch 4938 (29622 steps) ------------------------------------\n",
      "Max loss: 0.016904419288039207\n",
      "Min loss: 0.009466145187616348\n",
      "Mean loss: 0.014059911016374826\n",
      "Std loss: 0.0023289609254434807\n",
      "Total Loss: 0.08435946609824896\n",
      "------------------------------------ epoch 4939 (29628 steps) ------------------------------------\n",
      "Max loss: 0.024809574708342552\n",
      "Min loss: 0.010689005255699158\n",
      "Mean loss: 0.015391036247213682\n",
      "Std loss: 0.004957107235872145\n",
      "Total Loss: 0.09234621748328209\n",
      "------------------------------------ epoch 4940 (29634 steps) ------------------------------------\n",
      "Max loss: 0.012662254273891449\n",
      "Min loss: 0.010127795860171318\n",
      "Mean loss: 0.01100221835076809\n",
      "Std loss: 0.0009317796090064827\n",
      "Total Loss: 0.06601331010460854\n",
      "------------------------------------ epoch 4941 (29640 steps) ------------------------------------\n",
      "Max loss: 0.04012395814061165\n",
      "Min loss: 0.010030314326286316\n",
      "Mean loss: 0.020551165100187063\n",
      "Std loss: 0.012170035888005666\n",
      "Total Loss: 0.12330699060112238\n",
      "------------------------------------ epoch 4942 (29646 steps) ------------------------------------\n",
      "Max loss: 0.030779343098402023\n",
      "Min loss: 0.010536355897784233\n",
      "Mean loss: 0.01888959047695001\n",
      "Std loss: 0.008564293415758985\n",
      "Total Loss: 0.11333754286170006\n",
      "------------------------------------ epoch 4943 (29652 steps) ------------------------------------\n",
      "Max loss: 0.04885193705558777\n",
      "Min loss: 0.011374546214938164\n",
      "Mean loss: 0.02231288158024351\n",
      "Std loss: 0.013103109690630842\n",
      "Total Loss: 0.13387728948146105\n",
      "------------------------------------ epoch 4944 (29658 steps) ------------------------------------\n",
      "Max loss: 0.0589187890291214\n",
      "Min loss: 0.009679554030299187\n",
      "Mean loss: 0.024684071075171232\n",
      "Std loss: 0.017182878674886236\n",
      "Total Loss: 0.1481044264510274\n",
      "------------------------------------ epoch 4945 (29664 steps) ------------------------------------\n",
      "Max loss: 0.04027676582336426\n",
      "Min loss: 0.01073906384408474\n",
      "Mean loss: 0.019501649619390566\n",
      "Std loss: 0.009901359634837454\n",
      "Total Loss: 0.1170098977163434\n",
      "------------------------------------ epoch 4946 (29670 steps) ------------------------------------\n",
      "Max loss: 0.02105918526649475\n",
      "Min loss: 0.010395164601504803\n",
      "Mean loss: 0.015220919313530127\n",
      "Std loss: 0.0034146732995450806\n",
      "Total Loss: 0.09132551588118076\n",
      "------------------------------------ epoch 4947 (29676 steps) ------------------------------------\n",
      "Max loss: 0.031149668619036674\n",
      "Min loss: 0.008504204452037811\n",
      "Mean loss: 0.01803125347942114\n",
      "Std loss: 0.007501602769689785\n",
      "Total Loss: 0.10818752087652683\n",
      "------------------------------------ epoch 4948 (29682 steps) ------------------------------------\n",
      "Max loss: 0.019803158938884735\n",
      "Min loss: 0.009803643450140953\n",
      "Mean loss: 0.013583114370703697\n",
      "Std loss: 0.0032030865057534933\n",
      "Total Loss: 0.08149868622422218\n",
      "------------------------------------ epoch 4949 (29688 steps) ------------------------------------\n",
      "Max loss: 0.030644115060567856\n",
      "Min loss: 0.009441707283258438\n",
      "Mean loss: 0.015289593332757553\n",
      "Std loss: 0.007812087477899872\n",
      "Total Loss: 0.09173755999654531\n",
      "------------------------------------ epoch 4950 (29694 steps) ------------------------------------\n",
      "Max loss: 0.019932663068175316\n",
      "Min loss: 0.007878015749156475\n",
      "Mean loss: 0.012801525959124168\n",
      "Std loss: 0.004886257364209583\n",
      "Total Loss: 0.076809155754745\n",
      "------------------------------------ epoch 4951 (29700 steps) ------------------------------------\n",
      "Max loss: 0.02197718434035778\n",
      "Min loss: 0.009243005886673927\n",
      "Mean loss: 0.0157052722449104\n",
      "Std loss: 0.004675110013558325\n",
      "Total Loss: 0.0942316334694624\n",
      "------------------------------------ epoch 4952 (29706 steps) ------------------------------------\n",
      "Max loss: 0.048202864825725555\n",
      "Min loss: 0.010242372751235962\n",
      "Mean loss: 0.028212142176926136\n",
      "Std loss: 0.015009014356419995\n",
      "Total Loss: 0.16927285306155682\n",
      "------------------------------------ epoch 4953 (29712 steps) ------------------------------------\n",
      "Max loss: 0.01683054491877556\n",
      "Min loss: 0.009396830573678017\n",
      "Mean loss: 0.013786976380894581\n",
      "Std loss: 0.0023357912790248746\n",
      "Total Loss: 0.08272185828536749\n",
      "------------------------------------ epoch 4954 (29718 steps) ------------------------------------\n",
      "Max loss: 0.03356359526515007\n",
      "Min loss: 0.010777881368994713\n",
      "Mean loss: 0.01647747416670124\n",
      "Std loss: 0.008131060987987989\n",
      "Total Loss: 0.09886484500020742\n",
      "------------------------------------ epoch 4955 (29724 steps) ------------------------------------\n",
      "Max loss: 0.05023274943232536\n",
      "Min loss: 0.009713450446724892\n",
      "Mean loss: 0.024118300372113783\n",
      "Std loss: 0.015818996771512973\n",
      "Total Loss: 0.1447098022326827\n",
      "------------------------------------ epoch 4956 (29730 steps) ------------------------------------\n",
      "Max loss: 0.035575754940509796\n",
      "Min loss: 0.011429175734519958\n",
      "Mean loss: 0.01785896811634302\n",
      "Std loss: 0.008186219142565612\n",
      "Total Loss: 0.10715380869805813\n",
      "------------------------------------ epoch 4957 (29736 steps) ------------------------------------\n",
      "Max loss: 0.048556409776210785\n",
      "Min loss: 0.011648759245872498\n",
      "Mean loss: 0.022812428108106058\n",
      "Std loss: 0.014548012872833653\n",
      "Total Loss: 0.13687456864863634\n",
      "------------------------------------ epoch 4958 (29742 steps) ------------------------------------\n",
      "Max loss: 0.037857796996831894\n",
      "Min loss: 0.008441360667347908\n",
      "Mean loss: 0.01665363957484563\n",
      "Std loss: 0.009982442600639104\n",
      "Total Loss: 0.09992183744907379\n",
      "------------------------------------ epoch 4959 (29748 steps) ------------------------------------\n",
      "Max loss: 0.034927330911159515\n",
      "Min loss: 0.011318265460431576\n",
      "Mean loss: 0.019365254168709118\n",
      "Std loss: 0.008398212333162113\n",
      "Total Loss: 0.11619152501225471\n",
      "------------------------------------ epoch 4960 (29754 steps) ------------------------------------\n",
      "Max loss: 0.023378973826766014\n",
      "Min loss: 0.008630338124930859\n",
      "Mean loss: 0.016276797590156395\n",
      "Std loss: 0.005712217112591509\n",
      "Total Loss: 0.09766078554093838\n",
      "------------------------------------ epoch 4961 (29760 steps) ------------------------------------\n",
      "Max loss: 0.019350839778780937\n",
      "Min loss: 0.007902462035417557\n",
      "Mean loss: 0.014026253173748652\n",
      "Std loss: 0.0033666344326894687\n",
      "Total Loss: 0.08415751904249191\n",
      "------------------------------------ epoch 4962 (29766 steps) ------------------------------------\n",
      "Max loss: 0.020064283162355423\n",
      "Min loss: 0.007385085336863995\n",
      "Mean loss: 0.011882901657372713\n",
      "Std loss: 0.004133364491333005\n",
      "Total Loss: 0.07129740994423628\n",
      "------------------------------------ epoch 4963 (29772 steps) ------------------------------------\n",
      "Max loss: 0.026162078604102135\n",
      "Min loss: 0.009127883240580559\n",
      "Mean loss: 0.014119308597097794\n",
      "Std loss: 0.005638530277025994\n",
      "Total Loss: 0.08471585158258677\n",
      "------------------------------------ epoch 4964 (29778 steps) ------------------------------------\n",
      "Max loss: 0.01355731301009655\n",
      "Min loss: 0.009335001930594444\n",
      "Mean loss: 0.01206031565864881\n",
      "Std loss: 0.0015884150083455644\n",
      "Total Loss: 0.07236189395189285\n",
      "------------------------------------ epoch 4965 (29784 steps) ------------------------------------\n",
      "Max loss: 0.014448586851358414\n",
      "Min loss: 0.008477982133626938\n",
      "Mean loss: 0.010822195714960495\n",
      "Std loss: 0.002072985625611743\n",
      "Total Loss: 0.06493317428976297\n",
      "------------------------------------ epoch 4966 (29790 steps) ------------------------------------\n",
      "Max loss: 0.036540862172842026\n",
      "Min loss: 0.0069831064902246\n",
      "Mean loss: 0.01387806151372691\n",
      "Std loss: 0.0102205298400364\n",
      "Total Loss: 0.08326836908236146\n",
      "------------------------------------ epoch 4967 (29796 steps) ------------------------------------\n",
      "Max loss: 0.012635370716452599\n",
      "Min loss: 0.006947997957468033\n",
      "Mean loss: 0.009526071914782127\n",
      "Std loss: 0.0018742933842912333\n",
      "Total Loss: 0.05715643148869276\n",
      "------------------------------------ epoch 4968 (29802 steps) ------------------------------------\n",
      "Max loss: 0.011208954267203808\n",
      "Min loss: 0.007073432207107544\n",
      "Mean loss: 0.008302080755432447\n",
      "Std loss: 0.001523034125150564\n",
      "Total Loss: 0.04981248453259468\n",
      "------------------------------------ epoch 4969 (29808 steps) ------------------------------------\n",
      "Max loss: 0.022554388269782066\n",
      "Min loss: 0.009323387406766415\n",
      "Mean loss: 0.01337139323974649\n",
      "Std loss: 0.004340455989967595\n",
      "Total Loss: 0.08022835943847895\n",
      "------------------------------------ epoch 4970 (29814 steps) ------------------------------------\n",
      "Max loss: 0.02704053744673729\n",
      "Min loss: 0.007292056456208229\n",
      "Mean loss: 0.01421146560460329\n",
      "Std loss: 0.007715216307751222\n",
      "Total Loss: 0.08526879362761974\n",
      "------------------------------------ epoch 4971 (29820 steps) ------------------------------------\n",
      "Max loss: 0.03680688142776489\n",
      "Min loss: 0.007434960454702377\n",
      "Mean loss: 0.01432515944664677\n",
      "Std loss: 0.010398003494449214\n",
      "Total Loss: 0.08595095667988062\n",
      "------------------------------------ epoch 4972 (29826 steps) ------------------------------------\n",
      "Max loss: 0.014598160982131958\n",
      "Min loss: 0.006947958841919899\n",
      "Mean loss: 0.009759678815801939\n",
      "Std loss: 0.0025464010778796595\n",
      "Total Loss: 0.05855807289481163\n",
      "------------------------------------ epoch 4973 (29832 steps) ------------------------------------\n",
      "Max loss: 0.02395131252706051\n",
      "Min loss: 0.007749262265861034\n",
      "Mean loss: 0.012428362232943376\n",
      "Std loss: 0.0054315798635354426\n",
      "Total Loss: 0.07457017339766026\n",
      "------------------------------------ epoch 4974 (29838 steps) ------------------------------------\n",
      "Max loss: 0.023633582517504692\n",
      "Min loss: 0.006724120583385229\n",
      "Mean loss: 0.012546874660377702\n",
      "Std loss: 0.005311212613224544\n",
      "Total Loss: 0.0752812479622662\n",
      "------------------------------------ epoch 4975 (29844 steps) ------------------------------------\n",
      "Max loss: 0.01864342764019966\n",
      "Min loss: 0.008178419433534145\n",
      "Mean loss: 0.01345525278399388\n",
      "Std loss: 0.003773402063594721\n",
      "Total Loss: 0.08073151670396328\n",
      "------------------------------------ epoch 4976 (29850 steps) ------------------------------------\n",
      "Max loss: 0.011876816861331463\n",
      "Min loss: 0.006592314690351486\n",
      "Mean loss: 0.008171603471661607\n",
      "Std loss: 0.0017752354723256005\n",
      "Total Loss: 0.049029620829969645\n",
      "------------------------------------ epoch 4977 (29856 steps) ------------------------------------\n",
      "Max loss: 0.021777406334877014\n",
      "Min loss: 0.00869082473218441\n",
      "Mean loss: 0.015495631222923597\n",
      "Std loss: 0.005390127800636454\n",
      "Total Loss: 0.09297378733754158\n",
      "------------------------------------ epoch 4978 (29862 steps) ------------------------------------\n",
      "Max loss: 0.026259463280439377\n",
      "Min loss: 0.006006866227835417\n",
      "Mean loss: 0.011795289969692627\n",
      "Std loss: 0.007133139491158145\n",
      "Total Loss: 0.07077173981815577\n",
      "------------------------------------ epoch 4979 (29868 steps) ------------------------------------\n",
      "Max loss: 0.020399710163474083\n",
      "Min loss: 0.006449113599956036\n",
      "Mean loss: 0.013272597454488277\n",
      "Std loss: 0.0049830195616963075\n",
      "Total Loss: 0.07963558472692966\n",
      "------------------------------------ epoch 4980 (29874 steps) ------------------------------------\n",
      "Max loss: 0.01862446777522564\n",
      "Min loss: 0.007969297468662262\n",
      "Mean loss: 0.011584716538588205\n",
      "Std loss: 0.003626957941553793\n",
      "Total Loss: 0.06950829923152924\n",
      "------------------------------------ epoch 4981 (29880 steps) ------------------------------------\n",
      "Max loss: 0.024013351649045944\n",
      "Min loss: 0.007502250373363495\n",
      "Mean loss: 0.015015648367504278\n",
      "Std loss: 0.006587378011626068\n",
      "Total Loss: 0.09009389020502567\n",
      "------------------------------------ epoch 4982 (29886 steps) ------------------------------------\n",
      "Max loss: 0.02221289835870266\n",
      "Min loss: 0.008393332362174988\n",
      "Mean loss: 0.014087277930229902\n",
      "Std loss: 0.005623639186726988\n",
      "Total Loss: 0.08452366758137941\n",
      "------------------------------------ epoch 4983 (29892 steps) ------------------------------------\n",
      "Max loss: 0.04956521838903427\n",
      "Min loss: 0.007350006606429815\n",
      "Mean loss: 0.016992106490458053\n",
      "Std loss: 0.014855312263890239\n",
      "Total Loss: 0.10195263894274831\n",
      "------------------------------------ epoch 4984 (29898 steps) ------------------------------------\n",
      "Max loss: 0.0269162580370903\n",
      "Min loss: 0.006965601351112127\n",
      "Mean loss: 0.01262981390270094\n",
      "Std loss: 0.007115770804758642\n",
      "Total Loss: 0.07577888341620564\n",
      "------------------------------------ epoch 4985 (29904 steps) ------------------------------------\n",
      "Max loss: 0.018632415682077408\n",
      "Min loss: 0.006336279213428497\n",
      "Mean loss: 0.009994736174121499\n",
      "Std loss: 0.004162028112553102\n",
      "Total Loss: 0.059968417044728994\n",
      "------------------------------------ epoch 4986 (29910 steps) ------------------------------------\n",
      "Max loss: 0.02629934251308441\n",
      "Min loss: 0.006922255735844374\n",
      "Mean loss: 0.012801149704804024\n",
      "Std loss: 0.007019412742735666\n",
      "Total Loss: 0.07680689822882414\n",
      "------------------------------------ epoch 4987 (29916 steps) ------------------------------------\n",
      "Max loss: 0.01492473017424345\n",
      "Min loss: 0.0067716725170612335\n",
      "Mean loss: 0.009990800792972246\n",
      "Std loss: 0.0034829652690713565\n",
      "Total Loss: 0.05994480475783348\n",
      "------------------------------------ epoch 4988 (29922 steps) ------------------------------------\n",
      "Max loss: 0.025201087817549706\n",
      "Min loss: 0.009203746914863586\n",
      "Mean loss: 0.01480751826117436\n",
      "Std loss: 0.0057624164333478466\n",
      "Total Loss: 0.08884510956704617\n",
      "------------------------------------ epoch 4989 (29928 steps) ------------------------------------\n",
      "Max loss: 0.01572568342089653\n",
      "Min loss: 0.006187828257679939\n",
      "Mean loss: 0.010911653439203898\n",
      "Std loss: 0.0035625940300599017\n",
      "Total Loss: 0.06546992063522339\n",
      "------------------------------------ epoch 4990 (29934 steps) ------------------------------------\n",
      "Max loss: 0.02324060909450054\n",
      "Min loss: 0.007220137864351273\n",
      "Mean loss: 0.014721431924651066\n",
      "Std loss: 0.005815165725632899\n",
      "Total Loss: 0.0883285915479064\n",
      "------------------------------------ epoch 4991 (29940 steps) ------------------------------------\n",
      "Max loss: 0.039794906973838806\n",
      "Min loss: 0.012471118941903114\n",
      "Mean loss: 0.02316358871757984\n",
      "Std loss: 0.00967234398759046\n",
      "Total Loss: 0.13898153230547905\n",
      "------------------------------------ epoch 4992 (29946 steps) ------------------------------------\n",
      "Max loss: 0.028567319735884666\n",
      "Min loss: 0.006414750125259161\n",
      "Mean loss: 0.013713627199952802\n",
      "Std loss: 0.007545644990597786\n",
      "Total Loss: 0.0822817631997168\n",
      "------------------------------------ epoch 4993 (29952 steps) ------------------------------------\n",
      "Max loss: 0.030160218477249146\n",
      "Min loss: 0.006643661763519049\n",
      "Mean loss: 0.013196529044459263\n",
      "Std loss: 0.008115384141964279\n",
      "Total Loss: 0.07917917426675558\n",
      "------------------------------------ epoch 4994 (29958 steps) ------------------------------------\n",
      "Max loss: 0.01818593591451645\n",
      "Min loss: 0.006610560696572065\n",
      "Mean loss: 0.011690384785955152\n",
      "Std loss: 0.0042613216878088925\n",
      "Total Loss: 0.0701423087157309\n",
      "------------------------------------ epoch 4995 (29964 steps) ------------------------------------\n",
      "Max loss: 0.0491182804107666\n",
      "Min loss: 0.006201904267072678\n",
      "Mean loss: 0.017546766670420766\n",
      "Std loss: 0.014566639224911254\n",
      "Total Loss: 0.1052806000225246\n",
      "------------------------------------ epoch 4996 (29970 steps) ------------------------------------\n",
      "Max loss: 0.03919166326522827\n",
      "Min loss: 0.007236613892018795\n",
      "Mean loss: 0.019309124598900478\n",
      "Std loss: 0.011647119968241004\n",
      "Total Loss: 0.11585474759340286\n",
      "------------------------------------ epoch 4997 (29976 steps) ------------------------------------\n",
      "Max loss: 0.03182963281869888\n",
      "Min loss: 0.008121060207486153\n",
      "Mean loss: 0.01658523470784227\n",
      "Std loss: 0.009055505854122105\n",
      "Total Loss: 0.09951140824705362\n",
      "------------------------------------ epoch 4998 (29982 steps) ------------------------------------\n",
      "Max loss: 0.024755552411079407\n",
      "Min loss: 0.006750183179974556\n",
      "Mean loss: 0.011539066287999352\n",
      "Std loss: 0.006352230392535665\n",
      "Total Loss: 0.06923439772799611\n",
      "------------------------------------ epoch 4999 (29988 steps) ------------------------------------\n",
      "Max loss: 0.012823302298784256\n",
      "Min loss: 0.007247917354106903\n",
      "Mean loss: 0.010218181647360325\n",
      "Std loss: 0.0020998450633820487\n",
      "Total Loss: 0.06130908988416195\n",
      "------------------------------------ epoch 5000 (29994 steps) ------------------------------------\n",
      "Max loss: 0.01357276737689972\n",
      "Min loss: 0.007381522096693516\n",
      "Mean loss: 0.010787061182782054\n",
      "Std loss: 0.0023985627412302824\n",
      "Total Loss: 0.06472236709669232\n",
      "------------------------------------ epoch 5001 (30000 steps) ------------------------------------\n",
      "Max loss: 0.013555931858718395\n",
      "Min loss: 0.007558173965662718\n",
      "Mean loss: 0.009953058247144023\n",
      "Std loss: 0.002053553354152979\n",
      "Total Loss: 0.05971834948286414\n",
      "saved model at ./weights/model_5001.pth\n",
      "------------------------------------ epoch 5002 (30006 steps) ------------------------------------\n",
      "Max loss: 0.024267982691526413\n",
      "Min loss: 0.00538295041769743\n",
      "Mean loss: 0.012337827744583288\n",
      "Std loss: 0.006558053492442805\n",
      "Total Loss: 0.07402696646749973\n",
      "------------------------------------ epoch 5003 (30012 steps) ------------------------------------\n",
      "Max loss: 0.04959443211555481\n",
      "Min loss: 0.0061294399201869965\n",
      "Mean loss: 0.018502151904006798\n",
      "Std loss: 0.0166119268199946\n",
      "Total Loss: 0.1110129114240408\n",
      "------------------------------------ epoch 5004 (30018 steps) ------------------------------------\n",
      "Max loss: 0.017646100372076035\n",
      "Min loss: 0.006825979333370924\n",
      "Mean loss: 0.011090853018686175\n",
      "Std loss: 0.004326419869949752\n",
      "Total Loss: 0.06654511811211705\n",
      "------------------------------------ epoch 5005 (30024 steps) ------------------------------------\n",
      "Max loss: 0.013235232792794704\n",
      "Min loss: 0.006178026553243399\n",
      "Mean loss: 0.010405737596253553\n",
      "Std loss: 0.0028937329036767875\n",
      "Total Loss: 0.062434425577521324\n",
      "------------------------------------ epoch 5006 (30030 steps) ------------------------------------\n",
      "Max loss: 0.03935623914003372\n",
      "Min loss: 0.008362237364053726\n",
      "Mean loss: 0.01899588170150916\n",
      "Std loss: 0.009595575655116524\n",
      "Total Loss: 0.11397529020905495\n",
      "------------------------------------ epoch 5007 (30036 steps) ------------------------------------\n",
      "Max loss: 0.02176085114479065\n",
      "Min loss: 0.008086921647191048\n",
      "Mean loss: 0.014378165050099293\n",
      "Std loss: 0.004236582226817879\n",
      "Total Loss: 0.08626899030059576\n",
      "------------------------------------ epoch 5008 (30042 steps) ------------------------------------\n",
      "Max loss: 0.01967531070113182\n",
      "Min loss: 0.0064188274554908276\n",
      "Mean loss: 0.011270154500380158\n",
      "Std loss: 0.0041328852839576085\n",
      "Total Loss: 0.06762092700228095\n",
      "------------------------------------ epoch 5009 (30048 steps) ------------------------------------\n",
      "Max loss: 0.06259942054748535\n",
      "Min loss: 0.007512235548347235\n",
      "Mean loss: 0.02250676982415219\n",
      "Std loss: 0.01991110945131855\n",
      "Total Loss: 0.13504061894491315\n",
      "------------------------------------ epoch 5010 (30054 steps) ------------------------------------\n",
      "Max loss: 0.016326649114489555\n",
      "Min loss: 0.006411758717149496\n",
      "Mean loss: 0.011282561734939614\n",
      "Std loss: 0.0035328383337094294\n",
      "Total Loss: 0.06769537040963769\n",
      "------------------------------------ epoch 5011 (30060 steps) ------------------------------------\n",
      "Max loss: 0.02967589721083641\n",
      "Min loss: 0.006945681758224964\n",
      "Mean loss: 0.013021710251147548\n",
      "Std loss: 0.007794008777222437\n",
      "Total Loss: 0.07813026150688529\n",
      "------------------------------------ epoch 5012 (30066 steps) ------------------------------------\n",
      "Max loss: 0.019690509885549545\n",
      "Min loss: 0.008868478238582611\n",
      "Mean loss: 0.01150515660022696\n",
      "Std loss: 0.0038937333772293575\n",
      "Total Loss: 0.06903093960136175\n",
      "------------------------------------ epoch 5013 (30072 steps) ------------------------------------\n",
      "Max loss: 0.0414106622338295\n",
      "Min loss: 0.005776450969278812\n",
      "Mean loss: 0.012708982064699134\n",
      "Std loss: 0.012862483859963468\n",
      "Total Loss: 0.0762538923881948\n",
      "------------------------------------ epoch 5014 (30078 steps) ------------------------------------\n",
      "Max loss: 0.015831533819437027\n",
      "Min loss: 0.006254270207136869\n",
      "Mean loss: 0.01133105104478697\n",
      "Std loss: 0.0037022946839044284\n",
      "Total Loss: 0.06798630626872182\n",
      "------------------------------------ epoch 5015 (30084 steps) ------------------------------------\n",
      "Max loss: 0.010982762090861797\n",
      "Min loss: 0.006277164444327354\n",
      "Mean loss: 0.008406635529051224\n",
      "Std loss: 0.0017168383086079782\n",
      "Total Loss: 0.050439813174307346\n",
      "------------------------------------ epoch 5016 (30090 steps) ------------------------------------\n",
      "Max loss: 0.015340765938162804\n",
      "Min loss: 0.005852024536579847\n",
      "Mean loss: 0.009662242528672019\n",
      "Std loss: 0.0037809925332727345\n",
      "Total Loss: 0.05797345517203212\n",
      "------------------------------------ epoch 5017 (30096 steps) ------------------------------------\n",
      "Max loss: 0.025979435071349144\n",
      "Min loss: 0.006072898395359516\n",
      "Mean loss: 0.011007827240973711\n",
      "Std loss: 0.006804758116390408\n",
      "Total Loss: 0.06604696344584227\n",
      "------------------------------------ epoch 5018 (30102 steps) ------------------------------------\n",
      "Max loss: 0.012579969130456448\n",
      "Min loss: 0.005490401294082403\n",
      "Mean loss: 0.009373942157253623\n",
      "Std loss: 0.002662135163326325\n",
      "Total Loss: 0.05624365294352174\n",
      "------------------------------------ epoch 5019 (30108 steps) ------------------------------------\n",
      "Max loss: 0.011645399034023285\n",
      "Min loss: 0.005907382350414991\n",
      "Mean loss: 0.009452175426607331\n",
      "Std loss: 0.002510627681935463\n",
      "Total Loss: 0.056713052559643984\n",
      "------------------------------------ epoch 5020 (30114 steps) ------------------------------------\n",
      "Max loss: 0.018539773300290108\n",
      "Min loss: 0.00596193503588438\n",
      "Mean loss: 0.00892746060465773\n",
      "Std loss: 0.004365285737203991\n",
      "Total Loss: 0.05356476362794638\n",
      "------------------------------------ epoch 5021 (30120 steps) ------------------------------------\n",
      "Max loss: 0.016183694824576378\n",
      "Min loss: 0.0059718238189816475\n",
      "Mean loss: 0.009301464771851897\n",
      "Std loss: 0.003693244676793089\n",
      "Total Loss: 0.05580878863111138\n",
      "------------------------------------ epoch 5022 (30126 steps) ------------------------------------\n",
      "Max loss: 0.013754329644143581\n",
      "Min loss: 0.00711280619725585\n",
      "Mean loss: 0.011152942587311069\n",
      "Std loss: 0.0022176439922114996\n",
      "Total Loss: 0.06691765552386642\n",
      "------------------------------------ epoch 5023 (30132 steps) ------------------------------------\n",
      "Max loss: 0.01966017857193947\n",
      "Min loss: 0.0081863384693861\n",
      "Mean loss: 0.012273189766953388\n",
      "Std loss: 0.0038338196924888168\n",
      "Total Loss: 0.07363913860172033\n",
      "------------------------------------ epoch 5024 (30138 steps) ------------------------------------\n",
      "Max loss: 0.010221928358078003\n",
      "Min loss: 0.007024997379630804\n",
      "Mean loss: 0.00881927638935546\n",
      "Std loss: 0.0010460178041585245\n",
      "Total Loss: 0.052915658336132765\n",
      "------------------------------------ epoch 5025 (30144 steps) ------------------------------------\n",
      "Max loss: 0.018546976149082184\n",
      "Min loss: 0.0065020229667425156\n",
      "Mean loss: 0.01100321083019177\n",
      "Std loss: 0.003996274867950747\n",
      "Total Loss: 0.06601926498115063\n",
      "------------------------------------ epoch 5026 (30150 steps) ------------------------------------\n",
      "Max loss: 0.017744198441505432\n",
      "Min loss: 0.005910724401473999\n",
      "Mean loss: 0.010976733329395453\n",
      "Std loss: 0.004211585071729846\n",
      "Total Loss: 0.06586039997637272\n",
      "------------------------------------ epoch 5027 (30156 steps) ------------------------------------\n",
      "Max loss: 0.01493244618177414\n",
      "Min loss: 0.005976988933980465\n",
      "Mean loss: 0.010107490389297405\n",
      "Std loss: 0.0034391812141530737\n",
      "Total Loss: 0.060644942335784435\n",
      "------------------------------------ epoch 5028 (30162 steps) ------------------------------------\n",
      "Max loss: 0.011829273775219917\n",
      "Min loss: 0.00627865782007575\n",
      "Mean loss: 0.00823028509815534\n",
      "Std loss: 0.0017560521620310786\n",
      "Total Loss: 0.04938171058893204\n",
      "------------------------------------ epoch 5029 (30168 steps) ------------------------------------\n",
      "Max loss: 0.01162778865545988\n",
      "Min loss: 0.007322987541556358\n",
      "Mean loss: 0.009327382935831944\n",
      "Std loss: 0.0014078061911473054\n",
      "Total Loss: 0.055964297614991665\n",
      "------------------------------------ epoch 5030 (30174 steps) ------------------------------------\n",
      "Max loss: 0.023877181112766266\n",
      "Min loss: 0.005574746988713741\n",
      "Mean loss: 0.01243282419939836\n",
      "Std loss: 0.0073262383655463125\n",
      "Total Loss: 0.07459694519639015\n",
      "------------------------------------ epoch 5031 (30180 steps) ------------------------------------\n",
      "Max loss: 0.02191757783293724\n",
      "Min loss: 0.0053822947666049\n",
      "Mean loss: 0.011682487713793913\n",
      "Std loss: 0.005155487591528314\n",
      "Total Loss: 0.07009492628276348\n",
      "------------------------------------ epoch 5032 (30186 steps) ------------------------------------\n",
      "Max loss: 0.025647126138210297\n",
      "Min loss: 0.005193380638957024\n",
      "Mean loss: 0.013166983922322592\n",
      "Std loss: 0.00859217195631477\n",
      "Total Loss: 0.07900190353393555\n",
      "------------------------------------ epoch 5033 (30192 steps) ------------------------------------\n",
      "Max loss: 0.03185027837753296\n",
      "Min loss: 0.007921937853097916\n",
      "Mean loss: 0.01496321273346742\n",
      "Std loss: 0.008314449172302382\n",
      "Total Loss: 0.08977927640080452\n",
      "------------------------------------ epoch 5034 (30198 steps) ------------------------------------\n",
      "Max loss: 0.009595352225005627\n",
      "Min loss: 0.006936145480722189\n",
      "Mean loss: 0.008195558950925866\n",
      "Std loss: 0.0008181637915979064\n",
      "Total Loss: 0.0491733537055552\n",
      "------------------------------------ epoch 5035 (30204 steps) ------------------------------------\n",
      "Max loss: 0.02005523070693016\n",
      "Min loss: 0.00861947052180767\n",
      "Mean loss: 0.013384842313826084\n",
      "Std loss: 0.004249863163110156\n",
      "Total Loss: 0.0803090538829565\n",
      "------------------------------------ epoch 5036 (30210 steps) ------------------------------------\n",
      "Max loss: 0.0263851135969162\n",
      "Min loss: 0.008008945733308792\n",
      "Mean loss: 0.01743407764782508\n",
      "Std loss: 0.006333217982061207\n",
      "Total Loss: 0.10460446588695049\n",
      "------------------------------------ epoch 5037 (30216 steps) ------------------------------------\n",
      "Max loss: 0.015179833397269249\n",
      "Min loss: 0.008304115384817123\n",
      "Mean loss: 0.011274367726097504\n",
      "Std loss: 0.002796824811768966\n",
      "Total Loss: 0.06764620635658503\n",
      "------------------------------------ epoch 5038 (30222 steps) ------------------------------------\n",
      "Max loss: 0.0336863212287426\n",
      "Min loss: 0.006443564780056477\n",
      "Mean loss: 0.01850948731104533\n",
      "Std loss: 0.010441616358258448\n",
      "Total Loss: 0.11105692386627197\n",
      "------------------------------------ epoch 5039 (30228 steps) ------------------------------------\n",
      "Max loss: 0.04129226878285408\n",
      "Min loss: 0.008568148128688335\n",
      "Mean loss: 0.019130497084309656\n",
      "Std loss: 0.010981378068689311\n",
      "Total Loss: 0.11478298250585794\n",
      "------------------------------------ epoch 5040 (30234 steps) ------------------------------------\n",
      "Max loss: 0.01810571923851967\n",
      "Min loss: 0.00877939723432064\n",
      "Mean loss: 0.011283988288293282\n",
      "Std loss: 0.0032376951583489577\n",
      "Total Loss: 0.0677039297297597\n",
      "------------------------------------ epoch 5041 (30240 steps) ------------------------------------\n",
      "Max loss: 0.01898975670337677\n",
      "Min loss: 0.005484666675329208\n",
      "Mean loss: 0.011283622356131673\n",
      "Std loss: 0.005137224305081057\n",
      "Total Loss: 0.06770173413679004\n",
      "------------------------------------ epoch 5042 (30246 steps) ------------------------------------\n",
      "Max loss: 0.02483399771153927\n",
      "Min loss: 0.0064688557758927345\n",
      "Mean loss: 0.011448592878878117\n",
      "Std loss: 0.0061789585684684165\n",
      "Total Loss: 0.0686915572732687\n",
      "------------------------------------ epoch 5043 (30252 steps) ------------------------------------\n",
      "Max loss: 0.017177654430270195\n",
      "Min loss: 0.005399525165557861\n",
      "Mean loss: 0.008450633070121208\n",
      "Std loss: 0.004051787552741066\n",
      "Total Loss: 0.05070379842072725\n",
      "------------------------------------ epoch 5044 (30258 steps) ------------------------------------\n",
      "Max loss: 0.017339473590254784\n",
      "Min loss: 0.005448922514915466\n",
      "Mean loss: 0.01064939113954703\n",
      "Std loss: 0.0037718457379301903\n",
      "Total Loss: 0.06389634683728218\n",
      "------------------------------------ epoch 5045 (30264 steps) ------------------------------------\n",
      "Max loss: 0.019839953631162643\n",
      "Min loss: 0.0073877787217497826\n",
      "Mean loss: 0.011733940492073694\n",
      "Std loss: 0.004025602546531824\n",
      "Total Loss: 0.07040364295244217\n",
      "------------------------------------ epoch 5046 (30270 steps) ------------------------------------\n",
      "Max loss: 0.012468169443309307\n",
      "Min loss: 0.005762431770563126\n",
      "Mean loss: 0.008131252291301886\n",
      "Std loss: 0.002332725888325907\n",
      "Total Loss: 0.04878751374781132\n",
      "------------------------------------ epoch 5047 (30276 steps) ------------------------------------\n",
      "Max loss: 0.013728592544794083\n",
      "Min loss: 0.004832779057323933\n",
      "Mean loss: 0.009502446899811426\n",
      "Std loss: 0.0032079192270181313\n",
      "Total Loss: 0.05701468139886856\n",
      "------------------------------------ epoch 5048 (30282 steps) ------------------------------------\n",
      "Max loss: 0.012638803571462631\n",
      "Min loss: 0.006208219565451145\n",
      "Mean loss: 0.009564673993736506\n",
      "Std loss: 0.002214668797739868\n",
      "Total Loss: 0.05738804396241903\n",
      "------------------------------------ epoch 5049 (30288 steps) ------------------------------------\n",
      "Max loss: 0.03130703419446945\n",
      "Min loss: 0.0062877885065972805\n",
      "Mean loss: 0.016079727637891967\n",
      "Std loss: 0.009702238017279543\n",
      "Total Loss: 0.09647836582735181\n",
      "------------------------------------ epoch 5050 (30294 steps) ------------------------------------\n",
      "Max loss: 0.017153222113847733\n",
      "Min loss: 0.005938937421888113\n",
      "Mean loss: 0.010608180969332656\n",
      "Std loss: 0.004458018878762914\n",
      "Total Loss: 0.06364908581599593\n",
      "------------------------------------ epoch 5051 (30300 steps) ------------------------------------\n",
      "Max loss: 0.014412405900657177\n",
      "Min loss: 0.005503495689481497\n",
      "Mean loss: 0.008541250213359794\n",
      "Std loss: 0.0028737354874724085\n",
      "Total Loss: 0.05124750128015876\n",
      "------------------------------------ epoch 5052 (30306 steps) ------------------------------------\n",
      "Max loss: 0.013750570826232433\n",
      "Min loss: 0.0057408614084124565\n",
      "Mean loss: 0.010012861186017593\n",
      "Std loss: 0.0030175393367019806\n",
      "Total Loss: 0.060077167116105556\n",
      "------------------------------------ epoch 5053 (30312 steps) ------------------------------------\n",
      "Max loss: 0.01410609669983387\n",
      "Min loss: 0.006494396831840277\n",
      "Mean loss: 0.008288801182061434\n",
      "Std loss: 0.0027006580351954792\n",
      "Total Loss: 0.0497328070923686\n",
      "------------------------------------ epoch 5054 (30318 steps) ------------------------------------\n",
      "Max loss: 0.010243261232972145\n",
      "Min loss: 0.005278120748698711\n",
      "Mean loss: 0.007443551827842991\n",
      "Std loss: 0.0015786048028363126\n",
      "Total Loss: 0.04466131096705794\n",
      "------------------------------------ epoch 5055 (30324 steps) ------------------------------------\n",
      "Max loss: 0.010561483912169933\n",
      "Min loss: 0.0058161113411188126\n",
      "Mean loss: 0.007836569721500078\n",
      "Std loss: 0.001623865132619047\n",
      "Total Loss: 0.04701941832900047\n",
      "------------------------------------ epoch 5056 (30330 steps) ------------------------------------\n",
      "Max loss: 0.013185973279178143\n",
      "Min loss: 0.004570317454636097\n",
      "Mean loss: 0.00799551730354627\n",
      "Std loss: 0.0028575461831209523\n",
      "Total Loss: 0.04797310382127762\n",
      "------------------------------------ epoch 5057 (30336 steps) ------------------------------------\n",
      "Max loss: 0.03135726973414421\n",
      "Min loss: 0.006422251462936401\n",
      "Mean loss: 0.01348245640595754\n",
      "Std loss: 0.008714025784712922\n",
      "Total Loss: 0.08089473843574524\n",
      "------------------------------------ epoch 5058 (30342 steps) ------------------------------------\n",
      "Max loss: 0.016143295913934708\n",
      "Min loss: 0.006323858164250851\n",
      "Mean loss: 0.010473757516592741\n",
      "Std loss: 0.0035007106163808554\n",
      "Total Loss: 0.06284254509955645\n",
      "------------------------------------ epoch 5059 (30348 steps) ------------------------------------\n",
      "Max loss: 0.012545629404485226\n",
      "Min loss: 0.005307940766215324\n",
      "Mean loss: 0.007938931696116924\n",
      "Std loss: 0.00230991128330702\n",
      "Total Loss: 0.047633590176701546\n",
      "------------------------------------ epoch 5060 (30354 steps) ------------------------------------\n",
      "Max loss: 0.033026013523340225\n",
      "Min loss: 0.005401519127190113\n",
      "Mean loss: 0.013149889341245094\n",
      "Std loss: 0.009068425453278977\n",
      "Total Loss: 0.07889933604747057\n",
      "------------------------------------ epoch 5061 (30360 steps) ------------------------------------\n",
      "Max loss: 0.02775048092007637\n",
      "Min loss: 0.006112971343100071\n",
      "Mean loss: 0.012854660550753275\n",
      "Std loss: 0.007032581685015862\n",
      "Total Loss: 0.07712796330451965\n",
      "------------------------------------ epoch 5062 (30366 steps) ------------------------------------\n",
      "Max loss: 0.023104041814804077\n",
      "Min loss: 0.005410840269178152\n",
      "Mean loss: 0.011484182672575116\n",
      "Std loss: 0.006027248811683179\n",
      "Total Loss: 0.0689050960354507\n",
      "------------------------------------ epoch 5063 (30372 steps) ------------------------------------\n",
      "Max loss: 0.026530463248491287\n",
      "Min loss: 0.0063123879954218864\n",
      "Mean loss: 0.013051379704847932\n",
      "Std loss: 0.008442725700856992\n",
      "Total Loss: 0.07830827822908759\n",
      "------------------------------------ epoch 5064 (30378 steps) ------------------------------------\n",
      "Max loss: 0.019711090251803398\n",
      "Min loss: 0.005869037471711636\n",
      "Mean loss: 0.010937268380075693\n",
      "Std loss: 0.004882464931011904\n",
      "Total Loss: 0.06562361028045416\n",
      "------------------------------------ epoch 5065 (30384 steps) ------------------------------------\n",
      "Max loss: 0.013632608577609062\n",
      "Min loss: 0.005673244595527649\n",
      "Mean loss: 0.009056476410478354\n",
      "Std loss: 0.003363002813063832\n",
      "Total Loss: 0.05433885846287012\n",
      "------------------------------------ epoch 5066 (30390 steps) ------------------------------------\n",
      "Max loss: 0.025149140506982803\n",
      "Min loss: 0.006410266272723675\n",
      "Mean loss: 0.011861658189445734\n",
      "Std loss: 0.006343304570638262\n",
      "Total Loss: 0.0711699491366744\n",
      "------------------------------------ epoch 5067 (30396 steps) ------------------------------------\n",
      "Max loss: 0.05585312098264694\n",
      "Min loss: 0.005981238093227148\n",
      "Mean loss: 0.021785221916312974\n",
      "Std loss: 0.018915465382029744\n",
      "Total Loss: 0.13071133149787784\n",
      "------------------------------------ epoch 5068 (30402 steps) ------------------------------------\n",
      "Max loss: 0.015450064092874527\n",
      "Min loss: 0.006626286543905735\n",
      "Mean loss: 0.010270571801811457\n",
      "Std loss: 0.003668195212588334\n",
      "Total Loss: 0.06162343081086874\n",
      "------------------------------------ epoch 5069 (30408 steps) ------------------------------------\n",
      "Max loss: 0.027823932468891144\n",
      "Min loss: 0.0057097263634204865\n",
      "Mean loss: 0.012340221476430694\n",
      "Std loss: 0.008859381488334142\n",
      "Total Loss: 0.07404132885858417\n",
      "------------------------------------ epoch 5070 (30414 steps) ------------------------------------\n",
      "Max loss: 0.013818979263305664\n",
      "Min loss: 0.00830929446965456\n",
      "Mean loss: 0.01056186001126965\n",
      "Std loss: 0.001754526334979974\n",
      "Total Loss: 0.0633711600676179\n",
      "------------------------------------ epoch 5071 (30420 steps) ------------------------------------\n",
      "Max loss: 0.027857331559062004\n",
      "Min loss: 0.005409945733845234\n",
      "Mean loss: 0.011934564759333929\n",
      "Std loss: 0.007644313403991569\n",
      "Total Loss: 0.07160738855600357\n",
      "------------------------------------ epoch 5072 (30426 steps) ------------------------------------\n",
      "Max loss: 0.01306026428937912\n",
      "Min loss: 0.00605330616235733\n",
      "Mean loss: 0.008973049310346445\n",
      "Std loss: 0.0022662082284340404\n",
      "Total Loss: 0.05383829586207867\n",
      "------------------------------------ epoch 5073 (30432 steps) ------------------------------------\n",
      "Max loss: 0.02690552920103073\n",
      "Min loss: 0.005264592356979847\n",
      "Mean loss: 0.011873046712328991\n",
      "Std loss: 0.007465732225771856\n",
      "Total Loss: 0.07123828027397394\n",
      "------------------------------------ epoch 5074 (30438 steps) ------------------------------------\n",
      "Max loss: 0.016325056552886963\n",
      "Min loss: 0.006153015419840813\n",
      "Mean loss: 0.011000328309213122\n",
      "Std loss: 0.004371445410333993\n",
      "Total Loss: 0.06600196985527873\n",
      "------------------------------------ epoch 5075 (30444 steps) ------------------------------------\n",
      "Max loss: 0.03221093863248825\n",
      "Min loss: 0.00711856409907341\n",
      "Mean loss: 0.013449568301439285\n",
      "Std loss: 0.008562263137929883\n",
      "Total Loss: 0.08069740980863571\n",
      "------------------------------------ epoch 5076 (30450 steps) ------------------------------------\n",
      "Max loss: 0.023530621081590652\n",
      "Min loss: 0.005979700945317745\n",
      "Mean loss: 0.01357012257600824\n",
      "Std loss: 0.006460522066155232\n",
      "Total Loss: 0.08142073545604944\n",
      "------------------------------------ epoch 5077 (30456 steps) ------------------------------------\n",
      "Max loss: 0.013260433450341225\n",
      "Min loss: 0.006503496319055557\n",
      "Mean loss: 0.008842195384204388\n",
      "Std loss: 0.002256046058028478\n",
      "Total Loss: 0.053053172305226326\n",
      "------------------------------------ epoch 5078 (30462 steps) ------------------------------------\n",
      "Max loss: 0.03263460099697113\n",
      "Min loss: 0.0064657386392354965\n",
      "Mean loss: 0.015235794397691885\n",
      "Std loss: 0.008624704389881763\n",
      "Total Loss: 0.09141476638615131\n",
      "------------------------------------ epoch 5079 (30468 steps) ------------------------------------\n",
      "Max loss: 0.0454472079873085\n",
      "Min loss: 0.007134570740163326\n",
      "Mean loss: 0.021046463244905073\n",
      "Std loss: 0.014773251199196256\n",
      "Total Loss: 0.12627877946943045\n",
      "------------------------------------ epoch 5080 (30474 steps) ------------------------------------\n",
      "Max loss: 0.025497030466794968\n",
      "Min loss: 0.006066122557967901\n",
      "Mean loss: 0.011673421521360675\n",
      "Std loss: 0.0068240167763026095\n",
      "Total Loss: 0.07004052912816405\n",
      "------------------------------------ epoch 5081 (30480 steps) ------------------------------------\n",
      "Max loss: 0.020311450585722923\n",
      "Min loss: 0.007117322646081448\n",
      "Mean loss: 0.01009647020449241\n",
      "Std loss: 0.0046527628255279745\n",
      "Total Loss: 0.06057882122695446\n",
      "------------------------------------ epoch 5082 (30486 steps) ------------------------------------\n",
      "Max loss: 0.02418358437716961\n",
      "Min loss: 0.005099811591207981\n",
      "Mean loss: 0.010556344175711274\n",
      "Std loss: 0.006421058564702779\n",
      "Total Loss: 0.06333806505426764\n",
      "------------------------------------ epoch 5083 (30492 steps) ------------------------------------\n",
      "Max loss: 0.017540529370307922\n",
      "Min loss: 0.007568784058094025\n",
      "Mean loss: 0.010645653742055098\n",
      "Std loss: 0.00378899736654407\n",
      "Total Loss: 0.06387392245233059\n",
      "------------------------------------ epoch 5084 (30498 steps) ------------------------------------\n",
      "Max loss: 0.009752808138728142\n",
      "Min loss: 0.006465231068432331\n",
      "Mean loss: 0.00851698654393355\n",
      "Std loss: 0.0010971515939047591\n",
      "Total Loss: 0.0511019192636013\n",
      "------------------------------------ epoch 5085 (30504 steps) ------------------------------------\n",
      "Max loss: 0.02381931245326996\n",
      "Min loss: 0.005814909003674984\n",
      "Mean loss: 0.01223787215227882\n",
      "Std loss: 0.005697601491308033\n",
      "Total Loss: 0.07342723291367292\n",
      "------------------------------------ epoch 5086 (30510 steps) ------------------------------------\n",
      "Max loss: 0.013178301975131035\n",
      "Min loss: 0.006337174214422703\n",
      "Mean loss: 0.008200587704777718\n",
      "Std loss: 0.002392835750880867\n",
      "Total Loss: 0.049203526228666306\n",
      "------------------------------------ epoch 5087 (30516 steps) ------------------------------------\n",
      "Max loss: 0.014876259490847588\n",
      "Min loss: 0.005694584455341101\n",
      "Mean loss: 0.009677202673628926\n",
      "Std loss: 0.003487387310463311\n",
      "Total Loss: 0.05806321604177356\n",
      "------------------------------------ epoch 5088 (30522 steps) ------------------------------------\n",
      "Max loss: 0.01539949607104063\n",
      "Min loss: 0.00505721103399992\n",
      "Mean loss: 0.00854949412556986\n",
      "Std loss: 0.003385639319688562\n",
      "Total Loss: 0.05129696475341916\n",
      "------------------------------------ epoch 5089 (30528 steps) ------------------------------------\n",
      "Max loss: 0.015023846179246902\n",
      "Min loss: 0.00601833825930953\n",
      "Mean loss: 0.009253973218922814\n",
      "Std loss: 0.002877667671664914\n",
      "Total Loss: 0.05552383931353688\n",
      "------------------------------------ epoch 5090 (30534 steps) ------------------------------------\n",
      "Max loss: 0.022415485233068466\n",
      "Min loss: 0.005340214818716049\n",
      "Mean loss: 0.01077152982664605\n",
      "Std loss: 0.0058250896156573605\n",
      "Total Loss: 0.0646291789598763\n",
      "------------------------------------ epoch 5091 (30540 steps) ------------------------------------\n",
      "Max loss: 0.03601350635290146\n",
      "Min loss: 0.00489348080009222\n",
      "Mean loss: 0.013268570493285855\n",
      "Std loss: 0.010604316687447438\n",
      "Total Loss: 0.07961142295971513\n",
      "------------------------------------ epoch 5092 (30546 steps) ------------------------------------\n",
      "Max loss: 0.01709602400660515\n",
      "Min loss: 0.006028655916452408\n",
      "Mean loss: 0.010182841370503107\n",
      "Std loss: 0.004445556315167219\n",
      "Total Loss: 0.061097048223018646\n",
      "------------------------------------ epoch 5093 (30552 steps) ------------------------------------\n",
      "Max loss: 0.015736376866698265\n",
      "Min loss: 0.0064760721288621426\n",
      "Mean loss: 0.01020149583928287\n",
      "Std loss: 0.0030287644008373765\n",
      "Total Loss: 0.06120897503569722\n",
      "------------------------------------ epoch 5094 (30558 steps) ------------------------------------\n",
      "Max loss: 0.02404586784541607\n",
      "Min loss: 0.00683943647891283\n",
      "Mean loss: 0.012094363880654177\n",
      "Std loss: 0.006115157971422419\n",
      "Total Loss: 0.07256618328392506\n",
      "------------------------------------ epoch 5095 (30564 steps) ------------------------------------\n",
      "Max loss: 0.03531680628657341\n",
      "Min loss: 0.006005842238664627\n",
      "Mean loss: 0.012751528527587652\n",
      "Std loss: 0.010184932923015916\n",
      "Total Loss: 0.07650917116552591\n",
      "------------------------------------ epoch 5096 (30570 steps) ------------------------------------\n",
      "Max loss: 0.02322433516383171\n",
      "Min loss: 0.008004676550626755\n",
      "Mean loss: 0.013131331962843737\n",
      "Std loss: 0.005155578862067214\n",
      "Total Loss: 0.07878799177706242\n",
      "------------------------------------ epoch 5097 (30576 steps) ------------------------------------\n",
      "Max loss: 0.02842392399907112\n",
      "Min loss: 0.006712599657475948\n",
      "Mean loss: 0.012084682316829761\n",
      "Std loss: 0.007997016505189584\n",
      "Total Loss: 0.07250809390097857\n",
      "------------------------------------ epoch 5098 (30582 steps) ------------------------------------\n",
      "Max loss: 0.02057727426290512\n",
      "Min loss: 0.007421666290611029\n",
      "Mean loss: 0.012499909459923705\n",
      "Std loss: 0.004031182203539212\n",
      "Total Loss: 0.07499945675954223\n",
      "------------------------------------ epoch 5099 (30588 steps) ------------------------------------\n",
      "Max loss: 0.015930524095892906\n",
      "Min loss: 0.005918721668422222\n",
      "Mean loss: 0.00889145831267039\n",
      "Std loss: 0.0034876918810704738\n",
      "Total Loss: 0.05334874987602234\n",
      "------------------------------------ epoch 5100 (30594 steps) ------------------------------------\n",
      "Max loss: 0.017981821671128273\n",
      "Min loss: 0.006533478852361441\n",
      "Mean loss: 0.010632144597669443\n",
      "Std loss: 0.00417687145846654\n",
      "Total Loss: 0.06379286758601665\n",
      "------------------------------------ epoch 5101 (30600 steps) ------------------------------------\n",
      "Max loss: 0.01349298469722271\n",
      "Min loss: 0.007061192765831947\n",
      "Mean loss: 0.009097070588419834\n",
      "Std loss: 0.0020756271899281943\n",
      "Total Loss: 0.05458242353051901\n",
      "saved model at ./weights/model_5101.pth\n",
      "------------------------------------ epoch 5102 (30606 steps) ------------------------------------\n",
      "Max loss: 0.02372361533343792\n",
      "Min loss: 0.00637341383844614\n",
      "Mean loss: 0.013117183775951466\n",
      "Std loss: 0.006892416655755669\n",
      "Total Loss: 0.07870310265570879\n",
      "------------------------------------ epoch 5103 (30612 steps) ------------------------------------\n",
      "Max loss: 0.020987017080187798\n",
      "Min loss: 0.0058573707938194275\n",
      "Mean loss: 0.014391994414230188\n",
      "Std loss: 0.004943762483622048\n",
      "Total Loss: 0.08635196648538113\n",
      "------------------------------------ epoch 5104 (30618 steps) ------------------------------------\n",
      "Max loss: 0.03202011436223984\n",
      "Min loss: 0.006686264183372259\n",
      "Mean loss: 0.01411718176677823\n",
      "Std loss: 0.009080901069150745\n",
      "Total Loss: 0.08470309060066938\n",
      "------------------------------------ epoch 5105 (30624 steps) ------------------------------------\n",
      "Max loss: 0.020849063992500305\n",
      "Min loss: 0.007946278899908066\n",
      "Mean loss: 0.014147231665750345\n",
      "Std loss: 0.004239400947969751\n",
      "Total Loss: 0.08488338999450207\n",
      "------------------------------------ epoch 5106 (30630 steps) ------------------------------------\n",
      "Max loss: 0.021438470110297203\n",
      "Min loss: 0.007817916572093964\n",
      "Mean loss: 0.012968067235002914\n",
      "Std loss: 0.004734941657552093\n",
      "Total Loss: 0.07780840341001749\n",
      "------------------------------------ epoch 5107 (30636 steps) ------------------------------------\n",
      "Max loss: 0.02663378044962883\n",
      "Min loss: 0.006259340792894363\n",
      "Mean loss: 0.012849193687240282\n",
      "Std loss: 0.006562015000195667\n",
      "Total Loss: 0.0770951621234417\n",
      "------------------------------------ epoch 5108 (30642 steps) ------------------------------------\n",
      "Max loss: 0.0245180856436491\n",
      "Min loss: 0.005436865147203207\n",
      "Mean loss: 0.010127211067204675\n",
      "Std loss: 0.006669026923947674\n",
      "Total Loss: 0.060763266403228045\n",
      "------------------------------------ epoch 5109 (30648 steps) ------------------------------------\n",
      "Max loss: 0.012201439589262009\n",
      "Min loss: 0.005750218406319618\n",
      "Mean loss: 0.009705291129648685\n",
      "Std loss: 0.0020916484850414317\n",
      "Total Loss: 0.05823174677789211\n",
      "------------------------------------ epoch 5110 (30654 steps) ------------------------------------\n",
      "Max loss: 0.02619769051671028\n",
      "Min loss: 0.006663565989583731\n",
      "Mean loss: 0.011582259476805726\n",
      "Std loss: 0.006676009107145912\n",
      "Total Loss: 0.06949355686083436\n",
      "------------------------------------ epoch 5111 (30660 steps) ------------------------------------\n",
      "Max loss: 0.026169609278440475\n",
      "Min loss: 0.006831447593867779\n",
      "Mean loss: 0.013431796959290901\n",
      "Std loss: 0.0061220188479503635\n",
      "Total Loss: 0.08059078175574541\n",
      "------------------------------------ epoch 5112 (30666 steps) ------------------------------------\n",
      "Max loss: 0.01120038703083992\n",
      "Min loss: 0.0063780201599001884\n",
      "Mean loss: 0.008434872763852278\n",
      "Std loss: 0.0014586696053459187\n",
      "Total Loss: 0.05060923658311367\n",
      "------------------------------------ epoch 5113 (30672 steps) ------------------------------------\n",
      "Max loss: 0.013803021982312202\n",
      "Min loss: 0.006800881586968899\n",
      "Mean loss: 0.010440418341507515\n",
      "Std loss: 0.0023630993389576744\n",
      "Total Loss: 0.06264251004904509\n",
      "------------------------------------ epoch 5114 (30678 steps) ------------------------------------\n",
      "Max loss: 0.03188684582710266\n",
      "Min loss: 0.0063536763191223145\n",
      "Mean loss: 0.013849799986928701\n",
      "Std loss: 0.008431106993650635\n",
      "Total Loss: 0.08309879992157221\n",
      "------------------------------------ epoch 5115 (30684 steps) ------------------------------------\n",
      "Max loss: 0.02666652575135231\n",
      "Min loss: 0.00700936047360301\n",
      "Mean loss: 0.013060351756090919\n",
      "Std loss: 0.007018582988500954\n",
      "Total Loss: 0.07836211053654552\n",
      "------------------------------------ epoch 5116 (30690 steps) ------------------------------------\n",
      "Max loss: 0.016054727137088776\n",
      "Min loss: 0.005734102334827185\n",
      "Mean loss: 0.010447658520812789\n",
      "Std loss: 0.0034824491821586596\n",
      "Total Loss: 0.06268595112487674\n",
      "------------------------------------ epoch 5117 (30696 steps) ------------------------------------\n",
      "Max loss: 0.021742811426520348\n",
      "Min loss: 0.005552760325372219\n",
      "Mean loss: 0.012805071038504442\n",
      "Std loss: 0.005805410428553165\n",
      "Total Loss: 0.07683042623102665\n",
      "------------------------------------ epoch 5118 (30702 steps) ------------------------------------\n",
      "Max loss: 0.025615615770220757\n",
      "Min loss: 0.006953897885978222\n",
      "Mean loss: 0.014837992222358784\n",
      "Std loss: 0.006769122993415576\n",
      "Total Loss: 0.0890279533341527\n",
      "------------------------------------ epoch 5119 (30708 steps) ------------------------------------\n",
      "Max loss: 0.06841392070055008\n",
      "Min loss: 0.005140556953847408\n",
      "Mean loss: 0.0174785905983299\n",
      "Std loss: 0.02285547843982479\n",
      "Total Loss: 0.10487154358997941\n",
      "------------------------------------ epoch 5120 (30714 steps) ------------------------------------\n",
      "Max loss: 0.027640368789434433\n",
      "Min loss: 0.007253460586071014\n",
      "Mean loss: 0.013978976135452589\n",
      "Std loss: 0.007881552939120089\n",
      "Total Loss: 0.08387385681271553\n",
      "------------------------------------ epoch 5121 (30720 steps) ------------------------------------\n",
      "Max loss: 0.01735251396894455\n",
      "Min loss: 0.006942405365407467\n",
      "Mean loss: 0.01060299233843883\n",
      "Std loss: 0.003712487912638532\n",
      "Total Loss: 0.06361795403063297\n",
      "------------------------------------ epoch 5122 (30726 steps) ------------------------------------\n",
      "Max loss: 0.015737488865852356\n",
      "Min loss: 0.007318188901990652\n",
      "Mean loss: 0.011745121718073884\n",
      "Std loss: 0.0027635543161927674\n",
      "Total Loss: 0.07047073030844331\n",
      "------------------------------------ epoch 5123 (30732 steps) ------------------------------------\n",
      "Max loss: 0.02100040391087532\n",
      "Min loss: 0.005771256517618895\n",
      "Mean loss: 0.011768961480508247\n",
      "Std loss: 0.005518231873736265\n",
      "Total Loss: 0.07061376888304949\n",
      "------------------------------------ epoch 5124 (30738 steps) ------------------------------------\n",
      "Max loss: 0.013725800439715385\n",
      "Min loss: 0.005546148866415024\n",
      "Mean loss: 0.00908360087002317\n",
      "Std loss: 0.0033190895630026813\n",
      "Total Loss: 0.05450160522013903\n",
      "------------------------------------ epoch 5125 (30744 steps) ------------------------------------\n",
      "Max loss: 0.028938403353095055\n",
      "Min loss: 0.0077943517826497555\n",
      "Mean loss: 0.019485129431510966\n",
      "Std loss: 0.007226501001871723\n",
      "Total Loss: 0.11691077658906579\n",
      "------------------------------------ epoch 5126 (30750 steps) ------------------------------------\n",
      "Max loss: 0.01654941588640213\n",
      "Min loss: 0.006828776560723782\n",
      "Mean loss: 0.012629599931339422\n",
      "Std loss: 0.0033550062211981247\n",
      "Total Loss: 0.07577759958803654\n",
      "------------------------------------ epoch 5127 (30756 steps) ------------------------------------\n",
      "Max loss: 0.020011384040117264\n",
      "Min loss: 0.0065114120952785015\n",
      "Mean loss: 0.014677062087381879\n",
      "Std loss: 0.004244564568348443\n",
      "Total Loss: 0.08806237252429128\n",
      "------------------------------------ epoch 5128 (30762 steps) ------------------------------------\n",
      "Max loss: 0.014467846602201462\n",
      "Min loss: 0.005833775736391544\n",
      "Mean loss: 0.010501878956953684\n",
      "Std loss: 0.003013637267876512\n",
      "Total Loss: 0.0630112737417221\n",
      "------------------------------------ epoch 5129 (30768 steps) ------------------------------------\n",
      "Max loss: 0.020731016993522644\n",
      "Min loss: 0.005169095005840063\n",
      "Mean loss: 0.010573211203639707\n",
      "Std loss: 0.005659594510714128\n",
      "Total Loss: 0.06343926722183824\n",
      "------------------------------------ epoch 5130 (30774 steps) ------------------------------------\n",
      "Max loss: 0.020798582583665848\n",
      "Min loss: 0.0057538952678442\n",
      "Mean loss: 0.009535077804078659\n",
      "Std loss: 0.0053580031440300805\n",
      "Total Loss: 0.05721046682447195\n",
      "------------------------------------ epoch 5131 (30780 steps) ------------------------------------\n",
      "Max loss: 0.017461178824305534\n",
      "Min loss: 0.005595396272838116\n",
      "Mean loss: 0.010186664682502547\n",
      "Std loss: 0.003858652799198785\n",
      "Total Loss: 0.06111998809501529\n",
      "------------------------------------ epoch 5132 (30786 steps) ------------------------------------\n",
      "Max loss: 0.03521111235022545\n",
      "Min loss: 0.007526311092078686\n",
      "Mean loss: 0.02295064429442088\n",
      "Std loss: 0.011134295758091063\n",
      "Total Loss: 0.13770386576652527\n",
      "------------------------------------ epoch 5133 (30792 steps) ------------------------------------\n",
      "Max loss: 0.024144869297742844\n",
      "Min loss: 0.006154359318315983\n",
      "Mean loss: 0.011186495112876097\n",
      "Std loss: 0.006176468124079433\n",
      "Total Loss: 0.06711897067725658\n",
      "------------------------------------ epoch 5134 (30798 steps) ------------------------------------\n",
      "Max loss: 0.017163323238492012\n",
      "Min loss: 0.006078800186514854\n",
      "Mean loss: 0.011042325447003046\n",
      "Std loss: 0.0041329988266400235\n",
      "Total Loss: 0.06625395268201828\n",
      "------------------------------------ epoch 5135 (30804 steps) ------------------------------------\n",
      "Max loss: 0.022338030859827995\n",
      "Min loss: 0.008670639246702194\n",
      "Mean loss: 0.013460009514043728\n",
      "Std loss: 0.004635083465239184\n",
      "Total Loss: 0.08076005708426237\n",
      "------------------------------------ epoch 5136 (30810 steps) ------------------------------------\n",
      "Max loss: 0.028600944206118584\n",
      "Min loss: 0.00503483135253191\n",
      "Mean loss: 0.012765074924876293\n",
      "Std loss: 0.0075020631927891455\n",
      "Total Loss: 0.07659044954925776\n",
      "------------------------------------ epoch 5137 (30816 steps) ------------------------------------\n",
      "Max loss: 0.015816036611795425\n",
      "Min loss: 0.006266871001571417\n",
      "Mean loss: 0.009574031922966242\n",
      "Std loss: 0.003624942388546738\n",
      "Total Loss: 0.05744419153779745\n",
      "------------------------------------ epoch 5138 (30822 steps) ------------------------------------\n",
      "Max loss: 0.014885634183883667\n",
      "Min loss: 0.00633635651320219\n",
      "Mean loss: 0.009145071031525731\n",
      "Std loss: 0.002960576208453722\n",
      "Total Loss: 0.054870426189154387\n",
      "------------------------------------ epoch 5139 (30828 steps) ------------------------------------\n",
      "Max loss: 0.020075619220733643\n",
      "Min loss: 0.005551799666136503\n",
      "Mean loss: 0.011395490650708476\n",
      "Std loss: 0.005688819336114249\n",
      "Total Loss: 0.06837294390425086\n",
      "------------------------------------ epoch 5140 (30834 steps) ------------------------------------\n",
      "Max loss: 0.02008793316781521\n",
      "Min loss: 0.0063073476776480675\n",
      "Mean loss: 0.010797562931353847\n",
      "Std loss: 0.004508558121566887\n",
      "Total Loss: 0.06478537758812308\n",
      "------------------------------------ epoch 5141 (30840 steps) ------------------------------------\n",
      "Max loss: 0.028841495513916016\n",
      "Min loss: 0.004942914471030235\n",
      "Mean loss: 0.01519800148283442\n",
      "Std loss: 0.007934878309687836\n",
      "Total Loss: 0.09118800889700651\n",
      "------------------------------------ epoch 5142 (30846 steps) ------------------------------------\n",
      "Max loss: 0.015580947510898113\n",
      "Min loss: 0.005039457231760025\n",
      "Mean loss: 0.011297021992504597\n",
      "Std loss: 0.003708384584179809\n",
      "Total Loss: 0.06778213195502758\n",
      "------------------------------------ epoch 5143 (30852 steps) ------------------------------------\n",
      "Max loss: 0.0241678636521101\n",
      "Min loss: 0.006671627517789602\n",
      "Mean loss: 0.01268816584100326\n",
      "Std loss: 0.005789203720101092\n",
      "Total Loss: 0.07612899504601955\n",
      "------------------------------------ epoch 5144 (30858 steps) ------------------------------------\n",
      "Max loss: 0.015516906976699829\n",
      "Min loss: 0.006490262225270271\n",
      "Mean loss: 0.010697530039275685\n",
      "Std loss: 0.0031664869989981807\n",
      "Total Loss: 0.06418518023565412\n",
      "------------------------------------ epoch 5145 (30864 steps) ------------------------------------\n",
      "Max loss: 0.020272430032491684\n",
      "Min loss: 0.005702567752450705\n",
      "Mean loss: 0.011956411646679044\n",
      "Std loss: 0.00506345362476143\n",
      "Total Loss: 0.07173846988007426\n",
      "------------------------------------ epoch 5146 (30870 steps) ------------------------------------\n",
      "Max loss: 0.011291920207440853\n",
      "Min loss: 0.0060289157554507256\n",
      "Mean loss: 0.009576061895738045\n",
      "Std loss: 0.0019002670231844923\n",
      "Total Loss: 0.05745637137442827\n",
      "------------------------------------ epoch 5147 (30876 steps) ------------------------------------\n",
      "Max loss: 0.02779596857726574\n",
      "Min loss: 0.00477929413318634\n",
      "Mean loss: 0.012816388004769882\n",
      "Std loss: 0.008177305680609422\n",
      "Total Loss: 0.07689832802861929\n",
      "------------------------------------ epoch 5148 (30882 steps) ------------------------------------\n",
      "Max loss: 0.03922773897647858\n",
      "Min loss: 0.0053890664130449295\n",
      "Mean loss: 0.01337223251660665\n",
      "Std loss: 0.011754728891825647\n",
      "Total Loss: 0.08023339509963989\n",
      "------------------------------------ epoch 5149 (30888 steps) ------------------------------------\n",
      "Max loss: 0.016251470893621445\n",
      "Min loss: 0.007112554274499416\n",
      "Mean loss: 0.012166545415918032\n",
      "Std loss: 0.0028599838528344745\n",
      "Total Loss: 0.0729992724955082\n",
      "------------------------------------ epoch 5150 (30894 steps) ------------------------------------\n",
      "Max loss: 0.015782486647367477\n",
      "Min loss: 0.008136622607707977\n",
      "Mean loss: 0.011494058960427841\n",
      "Std loss: 0.0026222835273550108\n",
      "Total Loss: 0.06896435376256704\n",
      "------------------------------------ epoch 5151 (30900 steps) ------------------------------------\n",
      "Max loss: 0.012412717565894127\n",
      "Min loss: 0.00589461624622345\n",
      "Mean loss: 0.0074843425924579305\n",
      "Std loss: 0.00228345052129683\n",
      "Total Loss: 0.04490605555474758\n",
      "------------------------------------ epoch 5152 (30906 steps) ------------------------------------\n",
      "Max loss: 0.020089086145162582\n",
      "Min loss: 0.008126478642225266\n",
      "Mean loss: 0.011795092063645521\n",
      "Std loss: 0.004027316121368463\n",
      "Total Loss: 0.07077055238187313\n",
      "------------------------------------ epoch 5153 (30912 steps) ------------------------------------\n",
      "Max loss: 0.03614896535873413\n",
      "Min loss: 0.0058196596801280975\n",
      "Mean loss: 0.014021863462403417\n",
      "Std loss: 0.010591820592382215\n",
      "Total Loss: 0.0841311807744205\n",
      "------------------------------------ epoch 5154 (30918 steps) ------------------------------------\n",
      "Max loss: 0.014401922933757305\n",
      "Min loss: 0.008622584864497185\n",
      "Mean loss: 0.010858605615794659\n",
      "Std loss: 0.0022231807206760214\n",
      "Total Loss: 0.06515163369476795\n",
      "------------------------------------ epoch 5155 (30924 steps) ------------------------------------\n",
      "Max loss: 0.010907161049544811\n",
      "Min loss: 0.006348600145429373\n",
      "Mean loss: 0.008319539794077476\n",
      "Std loss: 0.0018264285616162162\n",
      "Total Loss: 0.049917238764464855\n",
      "------------------------------------ epoch 5156 (30930 steps) ------------------------------------\n",
      "Max loss: 0.014270400628447533\n",
      "Min loss: 0.006649303250014782\n",
      "Mean loss: 0.009103935134286681\n",
      "Std loss: 0.0026279185753149653\n",
      "Total Loss: 0.05462361080572009\n",
      "------------------------------------ epoch 5157 (30936 steps) ------------------------------------\n",
      "Max loss: 0.04497528076171875\n",
      "Min loss: 0.007432415150105953\n",
      "Mean loss: 0.016486399962256353\n",
      "Std loss: 0.01291400464725153\n",
      "Total Loss: 0.09891839977353811\n",
      "------------------------------------ epoch 5158 (30942 steps) ------------------------------------\n",
      "Max loss: 0.022732561454176903\n",
      "Min loss: 0.005950722843408585\n",
      "Mean loss: 0.010331264231353998\n",
      "Std loss: 0.005736767569459258\n",
      "Total Loss: 0.06198758538812399\n",
      "------------------------------------ epoch 5159 (30948 steps) ------------------------------------\n",
      "Max loss: 0.013152826577425003\n",
      "Min loss: 0.006539423018693924\n",
      "Mean loss: 0.009884421713650227\n",
      "Std loss: 0.0021444461917229803\n",
      "Total Loss: 0.05930653028190136\n",
      "------------------------------------ epoch 5160 (30954 steps) ------------------------------------\n",
      "Max loss: 0.019040394574403763\n",
      "Min loss: 0.006451660767197609\n",
      "Mean loss: 0.010999310451249281\n",
      "Std loss: 0.004443921243616117\n",
      "Total Loss: 0.06599586270749569\n",
      "------------------------------------ epoch 5161 (30960 steps) ------------------------------------\n",
      "Max loss: 0.014100014232099056\n",
      "Min loss: 0.006353300996124744\n",
      "Mean loss: 0.009734774163613716\n",
      "Std loss: 0.0026993352012353784\n",
      "Total Loss: 0.0584086449816823\n",
      "------------------------------------ epoch 5162 (30966 steps) ------------------------------------\n",
      "Max loss: 0.023640869185328484\n",
      "Min loss: 0.0053183091804385185\n",
      "Mean loss: 0.010350693405295411\n",
      "Std loss: 0.006189065842825634\n",
      "Total Loss: 0.06210416043177247\n",
      "------------------------------------ epoch 5163 (30972 steps) ------------------------------------\n",
      "Max loss: 0.009335779584944248\n",
      "Min loss: 0.005965458229184151\n",
      "Mean loss: 0.007307647339378794\n",
      "Std loss: 0.0013021474886256161\n",
      "Total Loss: 0.043845884036272764\n",
      "------------------------------------ epoch 5164 (30978 steps) ------------------------------------\n",
      "Max loss: 0.015824943780899048\n",
      "Min loss: 0.008463457226753235\n",
      "Mean loss: 0.010868254893769821\n",
      "Std loss: 0.0024957135756686695\n",
      "Total Loss: 0.06520952936261892\n",
      "------------------------------------ epoch 5165 (30984 steps) ------------------------------------\n",
      "Max loss: 0.010528584942221642\n",
      "Min loss: 0.004756030160933733\n",
      "Mean loss: 0.007748439675197005\n",
      "Std loss: 0.0019613620712616623\n",
      "Total Loss: 0.04649063805118203\n",
      "------------------------------------ epoch 5166 (30990 steps) ------------------------------------\n",
      "Max loss: 0.03816230967640877\n",
      "Min loss: 0.0057957712560892105\n",
      "Mean loss: 0.014026892061034838\n",
      "Std loss: 0.011120717186833422\n",
      "Total Loss: 0.08416135236620903\n",
      "------------------------------------ epoch 5167 (30996 steps) ------------------------------------\n",
      "Max loss: 0.01631530001759529\n",
      "Min loss: 0.005892305634915829\n",
      "Mean loss: 0.009029599217077097\n",
      "Std loss: 0.003820826875473199\n",
      "Total Loss: 0.05417759530246258\n",
      "------------------------------------ epoch 5168 (31002 steps) ------------------------------------\n",
      "Max loss: 0.02946285717189312\n",
      "Min loss: 0.007292566355317831\n",
      "Mean loss: 0.015211001271381974\n",
      "Std loss: 0.007501275693257961\n",
      "Total Loss: 0.09126600762829185\n",
      "------------------------------------ epoch 5169 (31008 steps) ------------------------------------\n",
      "Max loss: 0.013694233261048794\n",
      "Min loss: 0.005209929309785366\n",
      "Mean loss: 0.00900388058895866\n",
      "Std loss: 0.0032324112127280085\n",
      "Total Loss: 0.054023283533751965\n",
      "------------------------------------ epoch 5170 (31014 steps) ------------------------------------\n",
      "Max loss: 0.01074972189962864\n",
      "Min loss: 0.00605314876884222\n",
      "Mean loss: 0.009155091208716234\n",
      "Std loss: 0.0015508411012048497\n",
      "Total Loss: 0.0549305472522974\n",
      "------------------------------------ epoch 5171 (31020 steps) ------------------------------------\n",
      "Max loss: 0.05444026738405228\n",
      "Min loss: 0.006349352188408375\n",
      "Mean loss: 0.017820951684067648\n",
      "Std loss: 0.01681834144827861\n",
      "Total Loss: 0.10692571010440588\n",
      "------------------------------------ epoch 5172 (31026 steps) ------------------------------------\n",
      "Max loss: 0.03302384167909622\n",
      "Min loss: 0.006254003848880529\n",
      "Mean loss: 0.016243417048826814\n",
      "Std loss: 0.010077537808523152\n",
      "Total Loss: 0.09746050229296088\n",
      "------------------------------------ epoch 5173 (31032 steps) ------------------------------------\n",
      "Max loss: 0.038304101675748825\n",
      "Min loss: 0.0056830598041415215\n",
      "Mean loss: 0.013055380356187621\n",
      "Std loss: 0.011507410006205348\n",
      "Total Loss: 0.07833228213712573\n",
      "------------------------------------ epoch 5174 (31038 steps) ------------------------------------\n",
      "Max loss: 0.013377012684941292\n",
      "Min loss: 0.005558639299124479\n",
      "Mean loss: 0.010011322951565186\n",
      "Std loss: 0.00295970276784729\n",
      "Total Loss: 0.06006793770939112\n",
      "------------------------------------ epoch 5175 (31044 steps) ------------------------------------\n",
      "Max loss: 0.024177897721529007\n",
      "Min loss: 0.006055654492229223\n",
      "Mean loss: 0.010784496630852422\n",
      "Std loss: 0.00624787029967478\n",
      "Total Loss: 0.06470697978511453\n",
      "------------------------------------ epoch 5176 (31050 steps) ------------------------------------\n",
      "Max loss: 0.008341706357896328\n",
      "Min loss: 0.00634279428049922\n",
      "Mean loss: 0.007149405699844162\n",
      "Std loss: 0.0007002938026725843\n",
      "Total Loss: 0.04289643419906497\n",
      "------------------------------------ epoch 5177 (31056 steps) ------------------------------------\n",
      "Max loss: 0.04407587647438049\n",
      "Min loss: 0.007145627401769161\n",
      "Mean loss: 0.016139404848217964\n",
      "Std loss: 0.012959489847236586\n",
      "Total Loss: 0.09683642908930779\n",
      "------------------------------------ epoch 5178 (31062 steps) ------------------------------------\n",
      "Max loss: 0.019753839820623398\n",
      "Min loss: 0.008448919281363487\n",
      "Mean loss: 0.011561399015287558\n",
      "Std loss: 0.0038192251755996163\n",
      "Total Loss: 0.06936839409172535\n",
      "------------------------------------ epoch 5179 (31068 steps) ------------------------------------\n",
      "Max loss: 0.014027968049049377\n",
      "Min loss: 0.008836108259856701\n",
      "Mean loss: 0.010324338761468729\n",
      "Std loss: 0.0018761483139919568\n",
      "Total Loss: 0.06194603256881237\n",
      "------------------------------------ epoch 5180 (31074 steps) ------------------------------------\n",
      "Max loss: 0.031859152019023895\n",
      "Min loss: 0.006001302972435951\n",
      "Mean loss: 0.016477196011692286\n",
      "Std loss: 0.00930346947231428\n",
      "Total Loss: 0.09886317607015371\n",
      "------------------------------------ epoch 5181 (31080 steps) ------------------------------------\n",
      "Max loss: 0.020921703428030014\n",
      "Min loss: 0.00542787741869688\n",
      "Mean loss: 0.011464118647078672\n",
      "Std loss: 0.005013813626392213\n",
      "Total Loss: 0.06878471188247204\n",
      "------------------------------------ epoch 5182 (31086 steps) ------------------------------------\n",
      "Max loss: 0.013638613745570183\n",
      "Min loss: 0.008907140232622623\n",
      "Mean loss: 0.01124024981011947\n",
      "Std loss: 0.0017121069780152605\n",
      "Total Loss: 0.06744149886071682\n",
      "------------------------------------ epoch 5183 (31092 steps) ------------------------------------\n",
      "Max loss: 0.01805035211145878\n",
      "Min loss: 0.004876298829913139\n",
      "Mean loss: 0.011003222549334168\n",
      "Std loss: 0.004217579790249773\n",
      "Total Loss: 0.06601933529600501\n",
      "------------------------------------ epoch 5184 (31098 steps) ------------------------------------\n",
      "Max loss: 0.01937262713909149\n",
      "Min loss: 0.004896371625363827\n",
      "Mean loss: 0.010266438747445742\n",
      "Std loss: 0.004661456910870862\n",
      "Total Loss: 0.061598632484674454\n",
      "------------------------------------ epoch 5185 (31104 steps) ------------------------------------\n",
      "Max loss: 0.05107785761356354\n",
      "Min loss: 0.007881175726652145\n",
      "Mean loss: 0.020785798629124958\n",
      "Std loss: 0.015773988456460653\n",
      "Total Loss: 0.12471479177474976\n",
      "------------------------------------ epoch 5186 (31110 steps) ------------------------------------\n",
      "Max loss: 0.029376547783613205\n",
      "Min loss: 0.006639473140239716\n",
      "Mean loss: 0.012462118252490958\n",
      "Std loss: 0.007977380291805607\n",
      "Total Loss: 0.07477270951494575\n",
      "------------------------------------ epoch 5187 (31116 steps) ------------------------------------\n",
      "Max loss: 0.01868607848882675\n",
      "Min loss: 0.0070487093180418015\n",
      "Mean loss: 0.012614233729739984\n",
      "Std loss: 0.004141313152910999\n",
      "Total Loss: 0.0756854023784399\n",
      "------------------------------------ epoch 5188 (31122 steps) ------------------------------------\n",
      "Max loss: 0.01291310228407383\n",
      "Min loss: 0.005436001345515251\n",
      "Mean loss: 0.00902153691276908\n",
      "Std loss: 0.0027291574075269964\n",
      "Total Loss: 0.054129221476614475\n",
      "------------------------------------ epoch 5189 (31128 steps) ------------------------------------\n",
      "Max loss: 0.02230939269065857\n",
      "Min loss: 0.005300590302795172\n",
      "Mean loss: 0.01273331162519753\n",
      "Std loss: 0.006421149578316493\n",
      "Total Loss: 0.07639986975118518\n",
      "------------------------------------ epoch 5190 (31134 steps) ------------------------------------\n",
      "Max loss: 0.012056446634232998\n",
      "Min loss: 0.00626299437135458\n",
      "Mean loss: 0.007671635675554474\n",
      "Std loss: 0.0019808824674688442\n",
      "Total Loss: 0.046029814053326845\n",
      "------------------------------------ epoch 5191 (31140 steps) ------------------------------------\n",
      "Max loss: 0.012026309967041016\n",
      "Min loss: 0.0046230582520365715\n",
      "Mean loss: 0.007225528592243791\n",
      "Std loss: 0.0023666379218122155\n",
      "Total Loss: 0.043353171553462744\n",
      "------------------------------------ epoch 5192 (31146 steps) ------------------------------------\n",
      "Max loss: 0.013740578666329384\n",
      "Min loss: 0.005212498363107443\n",
      "Mean loss: 0.00944306383219858\n",
      "Std loss: 0.0034067844230979637\n",
      "Total Loss: 0.05665838299319148\n",
      "------------------------------------ epoch 5193 (31152 steps) ------------------------------------\n",
      "Max loss: 0.023681698366999626\n",
      "Min loss: 0.005687966011464596\n",
      "Mean loss: 0.013607549481093884\n",
      "Std loss: 0.006882114858329587\n",
      "Total Loss: 0.0816452968865633\n",
      "------------------------------------ epoch 5194 (31158 steps) ------------------------------------\n",
      "Max loss: 0.03462936729192734\n",
      "Min loss: 0.008662385866045952\n",
      "Mean loss: 0.019051212041328352\n",
      "Std loss: 0.008542135237729941\n",
      "Total Loss: 0.1143072722479701\n",
      "------------------------------------ epoch 5195 (31164 steps) ------------------------------------\n",
      "Max loss: 0.015459137968719006\n",
      "Min loss: 0.005155979655683041\n",
      "Mean loss: 0.011623278570671877\n",
      "Std loss: 0.0035106531525903183\n",
      "Total Loss: 0.06973967142403126\n",
      "------------------------------------ epoch 5196 (31170 steps) ------------------------------------\n",
      "Max loss: 0.014190597459673882\n",
      "Min loss: 0.004836095962673426\n",
      "Mean loss: 0.010561339479560653\n",
      "Std loss: 0.0033170918813759716\n",
      "Total Loss: 0.06336803687736392\n",
      "------------------------------------ epoch 5197 (31176 steps) ------------------------------------\n",
      "Max loss: 0.015133222565054893\n",
      "Min loss: 0.005869182758033276\n",
      "Mean loss: 0.01133840112015605\n",
      "Std loss: 0.002956106100920034\n",
      "Total Loss: 0.0680304067209363\n",
      "------------------------------------ epoch 5198 (31182 steps) ------------------------------------\n",
      "Max loss: 0.01623574271798134\n",
      "Min loss: 0.006061044987291098\n",
      "Mean loss: 0.009374037384986877\n",
      "Std loss: 0.003677496577103742\n",
      "Total Loss: 0.056244224309921265\n",
      "------------------------------------ epoch 5199 (31188 steps) ------------------------------------\n",
      "Max loss: 0.018099017441272736\n",
      "Min loss: 0.005901110824197531\n",
      "Mean loss: 0.009490329461793104\n",
      "Std loss: 0.00456437164273461\n",
      "Total Loss: 0.05694197677075863\n",
      "------------------------------------ epoch 5200 (31194 steps) ------------------------------------\n",
      "Max loss: 0.014913210645318031\n",
      "Min loss: 0.007183577865362167\n",
      "Mean loss: 0.008983156022926172\n",
      "Std loss: 0.002672485813717322\n",
      "Total Loss: 0.05389893613755703\n",
      "------------------------------------ epoch 5201 (31200 steps) ------------------------------------\n",
      "Max loss: 0.011480111628770828\n",
      "Min loss: 0.0045136744156479836\n",
      "Mean loss: 0.006503916733587782\n",
      "Std loss: 0.0024089640385209095\n",
      "Total Loss: 0.03902350040152669\n",
      "saved model at ./weights/model_5201.pth\n",
      "------------------------------------ epoch 5202 (31206 steps) ------------------------------------\n",
      "Max loss: 0.02149045094847679\n",
      "Min loss: 0.006356450729072094\n",
      "Mean loss: 0.010694881901144981\n",
      "Std loss: 0.005535956138030981\n",
      "Total Loss: 0.06416929140686989\n",
      "------------------------------------ epoch 5203 (31212 steps) ------------------------------------\n",
      "Max loss: 0.02265854738652706\n",
      "Min loss: 0.005599099211394787\n",
      "Mean loss: 0.010138755043347677\n",
      "Std loss: 0.005766499270804353\n",
      "Total Loss: 0.06083253026008606\n",
      "------------------------------------ epoch 5204 (31218 steps) ------------------------------------\n",
      "Max loss: 0.024028023704886436\n",
      "Min loss: 0.005300015211105347\n",
      "Mean loss: 0.011380339972674847\n",
      "Std loss: 0.008349938514282581\n",
      "Total Loss: 0.06828203983604908\n",
      "------------------------------------ epoch 5205 (31224 steps) ------------------------------------\n",
      "Max loss: 0.01867774687707424\n",
      "Min loss: 0.005248121917247772\n",
      "Mean loss: 0.01251968244711558\n",
      "Std loss: 0.004036223184237106\n",
      "Total Loss: 0.07511809468269348\n",
      "------------------------------------ epoch 5206 (31230 steps) ------------------------------------\n",
      "Max loss: 0.029123660176992416\n",
      "Min loss: 0.0049035875126719475\n",
      "Mean loss: 0.012431190814822912\n",
      "Std loss: 0.007881860083555781\n",
      "Total Loss: 0.07458714488893747\n",
      "------------------------------------ epoch 5207 (31236 steps) ------------------------------------\n",
      "Max loss: 0.019048158079385757\n",
      "Min loss: 0.006193378008902073\n",
      "Mean loss: 0.009748850017786026\n",
      "Std loss: 0.004367138286036998\n",
      "Total Loss: 0.058493100106716156\n",
      "------------------------------------ epoch 5208 (31242 steps) ------------------------------------\n",
      "Max loss: 0.014495976269245148\n",
      "Min loss: 0.005013563670217991\n",
      "Mean loss: 0.007510318187996745\n",
      "Std loss: 0.0033719839225692366\n",
      "Total Loss: 0.04506190912798047\n",
      "------------------------------------ epoch 5209 (31248 steps) ------------------------------------\n",
      "Max loss: 0.011607840657234192\n",
      "Min loss: 0.0048741623759269714\n",
      "Mean loss: 0.00709333224222064\n",
      "Std loss: 0.0023761980336606957\n",
      "Total Loss: 0.04255999345332384\n",
      "------------------------------------ epoch 5210 (31254 steps) ------------------------------------\n",
      "Max loss: 0.012698760256171227\n",
      "Min loss: 0.0049880500882864\n",
      "Mean loss: 0.008886833985646566\n",
      "Std loss: 0.0024936984805388413\n",
      "Total Loss: 0.053321003913879395\n",
      "------------------------------------ epoch 5211 (31260 steps) ------------------------------------\n",
      "Max loss: 0.012922748923301697\n",
      "Min loss: 0.005240699276328087\n",
      "Mean loss: 0.009013648610562086\n",
      "Std loss: 0.0029163933834796727\n",
      "Total Loss: 0.05408189166337252\n",
      "------------------------------------ epoch 5212 (31266 steps) ------------------------------------\n",
      "Max loss: 0.019975844770669937\n",
      "Min loss: 0.005193380638957024\n",
      "Mean loss: 0.009299366036430001\n",
      "Std loss: 0.004899631170731853\n",
      "Total Loss: 0.05579619621858001\n",
      "------------------------------------ epoch 5213 (31272 steps) ------------------------------------\n",
      "Max loss: 0.022155247628688812\n",
      "Min loss: 0.005029103718698025\n",
      "Mean loss: 0.009541821278010806\n",
      "Std loss: 0.005947960466510262\n",
      "Total Loss: 0.05725092766806483\n",
      "------------------------------------ epoch 5214 (31278 steps) ------------------------------------\n",
      "Max loss: 0.023575913161039352\n",
      "Min loss: 0.007629114203155041\n",
      "Mean loss: 0.015602263156324625\n",
      "Std loss: 0.007428489550872406\n",
      "Total Loss: 0.09361357893794775\n",
      "------------------------------------ epoch 5215 (31284 steps) ------------------------------------\n",
      "Max loss: 0.02334175631403923\n",
      "Min loss: 0.006194043438881636\n",
      "Mean loss: 0.012555740385626754\n",
      "Std loss: 0.0060004530278717555\n",
      "Total Loss: 0.07533444231376052\n",
      "------------------------------------ epoch 5216 (31290 steps) ------------------------------------\n",
      "Max loss: 0.013451958075165749\n",
      "Min loss: 0.005881324410438538\n",
      "Mean loss: 0.008675424459700784\n",
      "Std loss: 0.0023748872852827722\n",
      "Total Loss: 0.0520525467582047\n",
      "------------------------------------ epoch 5217 (31296 steps) ------------------------------------\n",
      "Max loss: 0.014643172733485699\n",
      "Min loss: 0.005370141472667456\n",
      "Mean loss: 0.008674266592909893\n",
      "Std loss: 0.0032740231438808582\n",
      "Total Loss: 0.052045599557459354\n",
      "------------------------------------ epoch 5218 (31302 steps) ------------------------------------\n",
      "Max loss: 0.018272534012794495\n",
      "Min loss: 0.005052964668720961\n",
      "Mean loss: 0.01129304951367279\n",
      "Std loss: 0.004815831115237651\n",
      "Total Loss: 0.06775829708203673\n",
      "------------------------------------ epoch 5219 (31308 steps) ------------------------------------\n",
      "Max loss: 0.0329248383641243\n",
      "Min loss: 0.006308234296739101\n",
      "Mean loss: 0.012824759042511383\n",
      "Std loss: 0.009187894986056266\n",
      "Total Loss: 0.0769485542550683\n",
      "------------------------------------ epoch 5220 (31314 steps) ------------------------------------\n",
      "Max loss: 0.021241378039121628\n",
      "Min loss: 0.0068869381211698055\n",
      "Mean loss: 0.013436225708574057\n",
      "Std loss: 0.0054811883328909974\n",
      "Total Loss: 0.08061735425144434\n",
      "------------------------------------ epoch 5221 (31320 steps) ------------------------------------\n",
      "Max loss: 0.017638644203543663\n",
      "Min loss: 0.0070049092173576355\n",
      "Mean loss: 0.010809299458439151\n",
      "Std loss: 0.004219714147974807\n",
      "Total Loss: 0.06485579675063491\n",
      "------------------------------------ epoch 5222 (31326 steps) ------------------------------------\n",
      "Max loss: 0.01728343963623047\n",
      "Min loss: 0.005349751561880112\n",
      "Mean loss: 0.00808488104181985\n",
      "Std loss: 0.004184130339739396\n",
      "Total Loss: 0.048509286250919104\n",
      "------------------------------------ epoch 5223 (31332 steps) ------------------------------------\n",
      "Max loss: 0.011496028862893581\n",
      "Min loss: 0.005313725210726261\n",
      "Mean loss: 0.008264034986495972\n",
      "Std loss: 0.002329226406200155\n",
      "Total Loss: 0.04958420991897583\n",
      "------------------------------------ epoch 5224 (31338 steps) ------------------------------------\n",
      "Max loss: 0.01662066951394081\n",
      "Min loss: 0.004899467341601849\n",
      "Mean loss: 0.00995230907574296\n",
      "Std loss: 0.004441260979938448\n",
      "Total Loss: 0.05971385445445776\n",
      "------------------------------------ epoch 5225 (31344 steps) ------------------------------------\n",
      "Max loss: 0.016353242099285126\n",
      "Min loss: 0.005661841481924057\n",
      "Mean loss: 0.010004855459555984\n",
      "Std loss: 0.003929002346932573\n",
      "Total Loss: 0.0600291327573359\n",
      "------------------------------------ epoch 5226 (31350 steps) ------------------------------------\n",
      "Max loss: 0.009939760901033878\n",
      "Min loss: 0.004932280629873276\n",
      "Mean loss: 0.00787703925743699\n",
      "Std loss: 0.0019985824012069223\n",
      "Total Loss: 0.047262235544621944\n",
      "------------------------------------ epoch 5227 (31356 steps) ------------------------------------\n",
      "Max loss: 0.016422338783740997\n",
      "Min loss: 0.00495475810021162\n",
      "Mean loss: 0.00967487646266818\n",
      "Std loss: 0.003762398432993738\n",
      "Total Loss: 0.05804925877600908\n",
      "------------------------------------ epoch 5228 (31362 steps) ------------------------------------\n",
      "Max loss: 0.013000729493796825\n",
      "Min loss: 0.005186839960515499\n",
      "Mean loss: 0.007412513640398781\n",
      "Std loss: 0.002592483892573501\n",
      "Total Loss: 0.04447508184239268\n",
      "------------------------------------ epoch 5229 (31368 steps) ------------------------------------\n",
      "Max loss: 0.007960318587720394\n",
      "Min loss: 0.004489849787205458\n",
      "Mean loss: 0.00636164207632343\n",
      "Std loss: 0.0012083431572381036\n",
      "Total Loss: 0.03816985245794058\n",
      "------------------------------------ epoch 5230 (31374 steps) ------------------------------------\n",
      "Max loss: 0.04573392868041992\n",
      "Min loss: 0.007854484021663666\n",
      "Mean loss: 0.022653330583125353\n",
      "Std loss: 0.01392351496074283\n",
      "Total Loss: 0.13591998349875212\n",
      "------------------------------------ epoch 5231 (31380 steps) ------------------------------------\n",
      "Max loss: 0.01660294272005558\n",
      "Min loss: 0.005684635601937771\n",
      "Mean loss: 0.010102962686990699\n",
      "Std loss: 0.0037204108900834665\n",
      "Total Loss: 0.06061777612194419\n",
      "------------------------------------ epoch 5232 (31386 steps) ------------------------------------\n",
      "Max loss: 0.013586703687906265\n",
      "Min loss: 0.006442047655582428\n",
      "Mean loss: 0.008793163967008391\n",
      "Std loss: 0.002334750847254284\n",
      "Total Loss: 0.05275898380205035\n",
      "------------------------------------ epoch 5233 (31392 steps) ------------------------------------\n",
      "Max loss: 0.019676104187965393\n",
      "Min loss: 0.0062681762501597404\n",
      "Mean loss: 0.010575828142464161\n",
      "Std loss: 0.004413411692354267\n",
      "Total Loss: 0.06345496885478497\n",
      "------------------------------------ epoch 5234 (31398 steps) ------------------------------------\n",
      "Max loss: 0.027659550309181213\n",
      "Min loss: 0.006321453955024481\n",
      "Mean loss: 0.01600076483252148\n",
      "Std loss: 0.009192080857884734\n",
      "Total Loss: 0.09600458899512887\n",
      "------------------------------------ epoch 5235 (31404 steps) ------------------------------------\n",
      "Max loss: 0.017249224707484245\n",
      "Min loss: 0.00574130704626441\n",
      "Mean loss: 0.010127453521514932\n",
      "Std loss: 0.004373206049623536\n",
      "Total Loss: 0.060764721129089594\n",
      "------------------------------------ epoch 5236 (31410 steps) ------------------------------------\n",
      "Max loss: 0.011261768639087677\n",
      "Min loss: 0.0053840335458517075\n",
      "Mean loss: 0.007328419325252374\n",
      "Std loss: 0.002083846892930282\n",
      "Total Loss: 0.043970515951514244\n",
      "------------------------------------ epoch 5237 (31416 steps) ------------------------------------\n",
      "Max loss: 0.017229843884706497\n",
      "Min loss: 0.005968866404145956\n",
      "Mean loss: 0.00951005588285625\n",
      "Std loss: 0.004095258407823457\n",
      "Total Loss: 0.0570603352971375\n",
      "------------------------------------ epoch 5238 (31422 steps) ------------------------------------\n",
      "Max loss: 0.017860688269138336\n",
      "Min loss: 0.006753875873982906\n",
      "Mean loss: 0.011083243104318777\n",
      "Std loss: 0.003494379771988705\n",
      "Total Loss: 0.06649945862591267\n",
      "------------------------------------ epoch 5239 (31428 steps) ------------------------------------\n",
      "Max loss: 0.027245251461863518\n",
      "Min loss: 0.005793245509266853\n",
      "Mean loss: 0.012699518507967392\n",
      "Std loss: 0.00732002046818508\n",
      "Total Loss: 0.07619711104780436\n",
      "------------------------------------ epoch 5240 (31434 steps) ------------------------------------\n",
      "Max loss: 0.02417142689228058\n",
      "Min loss: 0.006661391817033291\n",
      "Mean loss: 0.013652900078644356\n",
      "Std loss: 0.006708426435557036\n",
      "Total Loss: 0.08191740047186613\n",
      "------------------------------------ epoch 5241 (31440 steps) ------------------------------------\n",
      "Max loss: 0.02753235027194023\n",
      "Min loss: 0.005944914650171995\n",
      "Mean loss: 0.012038542190566659\n",
      "Std loss: 0.007359577563002461\n",
      "Total Loss: 0.07223125314339995\n",
      "------------------------------------ epoch 5242 (31446 steps) ------------------------------------\n",
      "Max loss: 0.010836555622518063\n",
      "Min loss: 0.004907015711069107\n",
      "Mean loss: 0.00704234279692173\n",
      "Std loss: 0.002307095971804999\n",
      "Total Loss: 0.04225405678153038\n",
      "------------------------------------ epoch 5243 (31452 steps) ------------------------------------\n",
      "Max loss: 0.017782878130674362\n",
      "Min loss: 0.005568632390350103\n",
      "Mean loss: 0.01038614590652287\n",
      "Std loss: 0.003970496261860268\n",
      "Total Loss: 0.06231687543913722\n",
      "------------------------------------ epoch 5244 (31458 steps) ------------------------------------\n",
      "Max loss: 0.03540014103055\n",
      "Min loss: 0.007945752702653408\n",
      "Mean loss: 0.015246165140221516\n",
      "Std loss: 0.009220577265025122\n",
      "Total Loss: 0.0914769908413291\n",
      "------------------------------------ epoch 5245 (31464 steps) ------------------------------------\n",
      "Max loss: 0.02699936553835869\n",
      "Min loss: 0.004594980739057064\n",
      "Mean loss: 0.012416612822562456\n",
      "Std loss: 0.0076780760858575\n",
      "Total Loss: 0.07449967693537474\n",
      "------------------------------------ epoch 5246 (31470 steps) ------------------------------------\n",
      "Max loss: 0.010416911914944649\n",
      "Min loss: 0.0051191421225667\n",
      "Mean loss: 0.0074039419026424485\n",
      "Std loss: 0.0015732292611137324\n",
      "Total Loss: 0.04442365141585469\n",
      "------------------------------------ epoch 5247 (31476 steps) ------------------------------------\n",
      "Max loss: 0.024334674701094627\n",
      "Min loss: 0.007956585846841335\n",
      "Mean loss: 0.014051398417601982\n",
      "Std loss: 0.006248428501417641\n",
      "Total Loss: 0.0843083905056119\n",
      "------------------------------------ epoch 5248 (31482 steps) ------------------------------------\n",
      "Max loss: 0.014306357130408287\n",
      "Min loss: 0.005401410162448883\n",
      "Mean loss: 0.008374303268889586\n",
      "Std loss: 0.0033757063324866975\n",
      "Total Loss: 0.05024581961333752\n",
      "------------------------------------ epoch 5249 (31488 steps) ------------------------------------\n",
      "Max loss: 0.013123754411935806\n",
      "Min loss: 0.006046021357178688\n",
      "Mean loss: 0.00851871445775032\n",
      "Std loss: 0.0024427218126143414\n",
      "Total Loss: 0.05111228674650192\n",
      "------------------------------------ epoch 5250 (31494 steps) ------------------------------------\n",
      "Max loss: 0.037318117916584015\n",
      "Min loss: 0.010898164473474026\n",
      "Mean loss: 0.025270809574673574\n",
      "Std loss: 0.008723606111523306\n",
      "Total Loss: 0.15162485744804144\n",
      "------------------------------------ epoch 5251 (31500 steps) ------------------------------------\n",
      "Max loss: 0.01910099945962429\n",
      "Min loss: 0.006347285117954016\n",
      "Mean loss: 0.012193937708313266\n",
      "Std loss: 0.004002263631659277\n",
      "Total Loss: 0.0731636262498796\n",
      "------------------------------------ epoch 5252 (31506 steps) ------------------------------------\n",
      "Max loss: 0.015345366671681404\n",
      "Min loss: 0.007237120531499386\n",
      "Mean loss: 0.011982756666839123\n",
      "Std loss: 0.002783157755435147\n",
      "Total Loss: 0.07189654000103474\n",
      "------------------------------------ epoch 5253 (31512 steps) ------------------------------------\n",
      "Max loss: 0.011209500953555107\n",
      "Min loss: 0.005409643054008484\n",
      "Mean loss: 0.007576761534437537\n",
      "Std loss: 0.001921046260043645\n",
      "Total Loss: 0.04546056920662522\n",
      "------------------------------------ epoch 5254 (31518 steps) ------------------------------------\n",
      "Max loss: 0.027228638529777527\n",
      "Min loss: 0.005760099273175001\n",
      "Mean loss: 0.012759856258829435\n",
      "Std loss: 0.007846402218091679\n",
      "Total Loss: 0.07655913755297661\n",
      "------------------------------------ epoch 5255 (31524 steps) ------------------------------------\n",
      "Max loss: 0.019852569326758385\n",
      "Min loss: 0.006307873874902725\n",
      "Mean loss: 0.010119327732051412\n",
      "Std loss: 0.005075961009999806\n",
      "Total Loss: 0.060715966392308474\n",
      "------------------------------------ epoch 5256 (31530 steps) ------------------------------------\n",
      "Max loss: 0.022726980969309807\n",
      "Min loss: 0.0057576741091907024\n",
      "Mean loss: 0.01028230576775968\n",
      "Std loss: 0.005875188093769278\n",
      "Total Loss: 0.061693834606558084\n",
      "------------------------------------ epoch 5257 (31536 steps) ------------------------------------\n",
      "Max loss: 0.0410643070936203\n",
      "Min loss: 0.0055800797417759895\n",
      "Mean loss: 0.013226755273838839\n",
      "Std loss: 0.012543356471408385\n",
      "Total Loss: 0.07936053164303303\n",
      "------------------------------------ epoch 5258 (31542 steps) ------------------------------------\n",
      "Max loss: 0.01846882700920105\n",
      "Min loss: 0.0060238465666770935\n",
      "Mean loss: 0.009714567878594002\n",
      "Std loss: 0.004430153955261691\n",
      "Total Loss: 0.05828740727156401\n",
      "------------------------------------ epoch 5259 (31548 steps) ------------------------------------\n",
      "Max loss: 0.03958781063556671\n",
      "Min loss: 0.006577789317816496\n",
      "Mean loss: 0.013134757212052742\n",
      "Std loss: 0.011896784733876235\n",
      "Total Loss: 0.07880854327231646\n",
      "------------------------------------ epoch 5260 (31554 steps) ------------------------------------\n",
      "Max loss: 0.01748625375330448\n",
      "Min loss: 0.005756310652941465\n",
      "Mean loss: 0.010973191587254405\n",
      "Std loss: 0.003646848783236142\n",
      "Total Loss: 0.06583914952352643\n",
      "------------------------------------ epoch 5261 (31560 steps) ------------------------------------\n",
      "Max loss: 0.0221731998026371\n",
      "Min loss: 0.008813942782580853\n",
      "Mean loss: 0.014177807761977116\n",
      "Std loss: 0.004727728729799181\n",
      "Total Loss: 0.0850668465718627\n",
      "------------------------------------ epoch 5262 (31566 steps) ------------------------------------\n",
      "Max loss: 0.03810004144906998\n",
      "Min loss: 0.01019194908440113\n",
      "Mean loss: 0.016978766148289044\n",
      "Std loss: 0.009731703736131015\n",
      "Total Loss: 0.10187259688973427\n",
      "------------------------------------ epoch 5263 (31572 steps) ------------------------------------\n",
      "Max loss: 0.019465995952486992\n",
      "Min loss: 0.006223085802048445\n",
      "Mean loss: 0.010263425686086217\n",
      "Std loss: 0.004631549402359544\n",
      "Total Loss: 0.061580554116517305\n",
      "------------------------------------ epoch 5264 (31578 steps) ------------------------------------\n",
      "Max loss: 0.03215457499027252\n",
      "Min loss: 0.006184105761349201\n",
      "Mean loss: 0.012193457611526052\n",
      "Std loss: 0.009161305628177059\n",
      "Total Loss: 0.07316074566915631\n",
      "------------------------------------ epoch 5265 (31584 steps) ------------------------------------\n",
      "Max loss: 0.026099205017089844\n",
      "Min loss: 0.005141079891473055\n",
      "Mean loss: 0.012474716796229282\n",
      "Std loss: 0.008518781565153108\n",
      "Total Loss: 0.0748483007773757\n",
      "------------------------------------ epoch 5266 (31590 steps) ------------------------------------\n",
      "Max loss: 0.014461631886661053\n",
      "Min loss: 0.006451703608036041\n",
      "Mean loss: 0.00957055917630593\n",
      "Std loss: 0.0027484124754950864\n",
      "Total Loss: 0.05742335505783558\n",
      "------------------------------------ epoch 5267 (31596 steps) ------------------------------------\n",
      "Max loss: 0.01331360638141632\n",
      "Min loss: 0.005794783588498831\n",
      "Mean loss: 0.008852158129836122\n",
      "Std loss: 0.002669880092275684\n",
      "Total Loss: 0.05311294877901673\n",
      "------------------------------------ epoch 5268 (31602 steps) ------------------------------------\n",
      "Max loss: 0.012496136128902435\n",
      "Min loss: 0.0060611083172261715\n",
      "Mean loss: 0.008703364835431179\n",
      "Std loss: 0.00236317790996016\n",
      "Total Loss: 0.05222018901258707\n",
      "------------------------------------ epoch 5269 (31608 steps) ------------------------------------\n",
      "Max loss: 0.013858569785952568\n",
      "Min loss: 0.005495691206306219\n",
      "Mean loss: 0.009115093620494008\n",
      "Std loss: 0.003044902774421892\n",
      "Total Loss: 0.05469056172296405\n",
      "------------------------------------ epoch 5270 (31614 steps) ------------------------------------\n",
      "Max loss: 0.04179657995700836\n",
      "Min loss: 0.005241214297711849\n",
      "Mean loss: 0.012772550728792945\n",
      "Std loss: 0.013051900286046178\n",
      "Total Loss: 0.07663530437275767\n",
      "------------------------------------ epoch 5271 (31620 steps) ------------------------------------\n",
      "Max loss: 0.012893102131783962\n",
      "Min loss: 0.006545144133269787\n",
      "Mean loss: 0.008721852907910943\n",
      "Std loss: 0.0022434936581472373\n",
      "Total Loss: 0.05233111744746566\n",
      "------------------------------------ epoch 5272 (31626 steps) ------------------------------------\n",
      "Max loss: 0.013694125227630138\n",
      "Min loss: 0.005293217953294516\n",
      "Mean loss: 0.007631026053180297\n",
      "Std loss: 0.002823455972290053\n",
      "Total Loss: 0.04578615631908178\n",
      "------------------------------------ epoch 5273 (31632 steps) ------------------------------------\n",
      "Max loss: 0.016745245084166527\n",
      "Min loss: 0.004419795703142881\n",
      "Mean loss: 0.008109489378208915\n",
      "Std loss: 0.0041774671180924506\n",
      "Total Loss: 0.04865693626925349\n",
      "------------------------------------ epoch 5274 (31638 steps) ------------------------------------\n",
      "Max loss: 0.03000248596072197\n",
      "Min loss: 0.006500771269202232\n",
      "Mean loss: 0.011478088020036617\n",
      "Std loss: 0.008356011369619987\n",
      "Total Loss: 0.06886852812021971\n",
      "------------------------------------ epoch 5275 (31644 steps) ------------------------------------\n",
      "Max loss: 0.021100077778100967\n",
      "Min loss: 0.005147955380380154\n",
      "Mean loss: 0.013258954820533594\n",
      "Std loss: 0.004925742017235357\n",
      "Total Loss: 0.07955372892320156\n",
      "------------------------------------ epoch 5276 (31650 steps) ------------------------------------\n",
      "Max loss: 0.029100244864821434\n",
      "Min loss: 0.006200913339853287\n",
      "Mean loss: 0.014944083678225676\n",
      "Std loss: 0.007682636912264073\n",
      "Total Loss: 0.08966450206935406\n",
      "------------------------------------ epoch 5277 (31656 steps) ------------------------------------\n",
      "Max loss: 0.013292583636939526\n",
      "Min loss: 0.006207601632922888\n",
      "Mean loss: 0.009467885441457232\n",
      "Std loss: 0.0028853453532759078\n",
      "Total Loss: 0.05680731264874339\n",
      "------------------------------------ epoch 5278 (31662 steps) ------------------------------------\n",
      "Max loss: 0.02005278691649437\n",
      "Min loss: 0.005087226629257202\n",
      "Mean loss: 0.009664800638953844\n",
      "Std loss: 0.0049602238548807406\n",
      "Total Loss: 0.05798880383372307\n",
      "------------------------------------ epoch 5279 (31668 steps) ------------------------------------\n",
      "Max loss: 0.0349656343460083\n",
      "Min loss: 0.0056475321762263775\n",
      "Mean loss: 0.01545685863432785\n",
      "Std loss: 0.010984669298764684\n",
      "Total Loss: 0.09274115180596709\n",
      "------------------------------------ epoch 5280 (31674 steps) ------------------------------------\n",
      "Max loss: 0.010065901093184948\n",
      "Min loss: 0.005991200916469097\n",
      "Mean loss: 0.008418108647068342\n",
      "Std loss: 0.0017479394701460091\n",
      "Total Loss: 0.05050865188241005\n",
      "------------------------------------ epoch 5281 (31680 steps) ------------------------------------\n",
      "Max loss: 0.02583416923880577\n",
      "Min loss: 0.005181677173823118\n",
      "Mean loss: 0.013512584303195277\n",
      "Std loss: 0.0067272879028639505\n",
      "Total Loss: 0.08107550581917167\n",
      "------------------------------------ epoch 5282 (31686 steps) ------------------------------------\n",
      "Max loss: 0.009684765711426735\n",
      "Min loss: 0.005795108154416084\n",
      "Mean loss: 0.008329234163587293\n",
      "Std loss: 0.001436643176140965\n",
      "Total Loss: 0.04997540498152375\n",
      "------------------------------------ epoch 5283 (31692 steps) ------------------------------------\n",
      "Max loss: 0.027751024812459946\n",
      "Min loss: 0.004862319678068161\n",
      "Mean loss: 0.010924127806598941\n",
      "Std loss: 0.007798793112002812\n",
      "Total Loss: 0.06554476683959365\n",
      "------------------------------------ epoch 5284 (31698 steps) ------------------------------------\n",
      "Max loss: 0.016763150691986084\n",
      "Min loss: 0.007577832788228989\n",
      "Mean loss: 0.01218184626971682\n",
      "Std loss: 0.003451453925067167\n",
      "Total Loss: 0.07309107761830091\n",
      "------------------------------------ epoch 5285 (31704 steps) ------------------------------------\n",
      "Max loss: 0.012736525386571884\n",
      "Min loss: 0.006303745321929455\n",
      "Mean loss: 0.008883360928545395\n",
      "Std loss: 0.0024138413422685376\n",
      "Total Loss: 0.05330016557127237\n",
      "------------------------------------ epoch 5286 (31710 steps) ------------------------------------\n",
      "Max loss: 0.025444291532039642\n",
      "Min loss: 0.005018091294914484\n",
      "Mean loss: 0.011141602570811907\n",
      "Std loss: 0.006667502914295471\n",
      "Total Loss: 0.06684961542487144\n",
      "------------------------------------ epoch 5287 (31716 steps) ------------------------------------\n",
      "Max loss: 0.01628899574279785\n",
      "Min loss: 0.005743609741330147\n",
      "Mean loss: 0.008287055340285102\n",
      "Std loss: 0.003740078916802604\n",
      "Total Loss: 0.049722332041710615\n",
      "------------------------------------ epoch 5288 (31722 steps) ------------------------------------\n",
      "Max loss: 0.020110301673412323\n",
      "Min loss: 0.006189936306327581\n",
      "Mean loss: 0.011519413519029817\n",
      "Std loss: 0.004677415004959071\n",
      "Total Loss: 0.0691164811141789\n",
      "------------------------------------ epoch 5289 (31728 steps) ------------------------------------\n",
      "Max loss: 0.057867731899023056\n",
      "Min loss: 0.0051836781203746796\n",
      "Mean loss: 0.016661641808847587\n",
      "Std loss: 0.018510488416704062\n",
      "Total Loss: 0.09996985085308552\n",
      "------------------------------------ epoch 5290 (31734 steps) ------------------------------------\n",
      "Max loss: 0.009097477421164513\n",
      "Min loss: 0.006549853831529617\n",
      "Mean loss: 0.007929945597425103\n",
      "Std loss: 0.0008089208777861316\n",
      "Total Loss: 0.04757967358455062\n",
      "------------------------------------ epoch 5291 (31740 steps) ------------------------------------\n",
      "Max loss: 0.06037270277738571\n",
      "Min loss: 0.004552966915071011\n",
      "Mean loss: 0.02221713758384188\n",
      "Std loss: 0.018379885548627045\n",
      "Total Loss: 0.13330282550305128\n",
      "------------------------------------ epoch 5292 (31746 steps) ------------------------------------\n",
      "Max loss: 0.01232588104903698\n",
      "Min loss: 0.0057267858646810055\n",
      "Mean loss: 0.008123148077478012\n",
      "Std loss: 0.002264758076492437\n",
      "Total Loss: 0.04873888846486807\n",
      "------------------------------------ epoch 5293 (31752 steps) ------------------------------------\n",
      "Max loss: 0.015345307998359203\n",
      "Min loss: 0.006402893923223019\n",
      "Mean loss: 0.010947749484330416\n",
      "Std loss: 0.003208685784506294\n",
      "Total Loss: 0.0656864969059825\n",
      "------------------------------------ epoch 5294 (31758 steps) ------------------------------------\n",
      "Max loss: 0.038442179560661316\n",
      "Min loss: 0.005461906548589468\n",
      "Mean loss: 0.012452661525458097\n",
      "Std loss: 0.011770676356758849\n",
      "Total Loss: 0.07471596915274858\n",
      "------------------------------------ epoch 5295 (31764 steps) ------------------------------------\n",
      "Max loss: 0.018919657915830612\n",
      "Min loss: 0.0065278224647045135\n",
      "Mean loss: 0.012446875063081583\n",
      "Std loss: 0.004973016005647891\n",
      "Total Loss: 0.0746812503784895\n",
      "------------------------------------ epoch 5296 (31770 steps) ------------------------------------\n",
      "Max loss: 0.03045037016272545\n",
      "Min loss: 0.0052277385257184505\n",
      "Mean loss: 0.012429449163998166\n",
      "Std loss: 0.00838478833556599\n",
      "Total Loss: 0.074576694983989\n",
      "------------------------------------ epoch 5297 (31776 steps) ------------------------------------\n",
      "Max loss: 0.015519818291068077\n",
      "Min loss: 0.005634762346744537\n",
      "Mean loss: 0.009599830722436309\n",
      "Std loss: 0.00359952740065631\n",
      "Total Loss: 0.05759898433461785\n",
      "------------------------------------ epoch 5298 (31782 steps) ------------------------------------\n",
      "Max loss: 0.015267981216311455\n",
      "Min loss: 0.005325817968696356\n",
      "Mean loss: 0.009523448767140508\n",
      "Std loss: 0.0035016073632950367\n",
      "Total Loss: 0.057140692602843046\n",
      "------------------------------------ epoch 5299 (31788 steps) ------------------------------------\n",
      "Max loss: 0.018644612282514572\n",
      "Min loss: 0.0059659406542778015\n",
      "Mean loss: 0.010895009618252516\n",
      "Std loss: 0.004288741885744081\n",
      "Total Loss: 0.0653700577095151\n",
      "------------------------------------ epoch 5300 (31794 steps) ------------------------------------\n",
      "Max loss: 0.02227950096130371\n",
      "Min loss: 0.005430019460618496\n",
      "Mean loss: 0.013510581261167923\n",
      "Std loss: 0.005171447381274489\n",
      "Total Loss: 0.08106348756700754\n",
      "------------------------------------ epoch 5301 (31800 steps) ------------------------------------\n",
      "Max loss: 0.01017916202545166\n",
      "Min loss: 0.006020146422088146\n",
      "Mean loss: 0.008349802034596602\n",
      "Std loss: 0.001364380997947551\n",
      "Total Loss: 0.05009881220757961\n",
      "saved model at ./weights/model_5301.pth\n",
      "------------------------------------ epoch 5302 (31806 steps) ------------------------------------\n",
      "Max loss: 0.020582165569067\n",
      "Min loss: 0.0049920761957764626\n",
      "Mean loss: 0.01139586434389154\n",
      "Std loss: 0.00593333216013467\n",
      "Total Loss: 0.06837518606334925\n",
      "------------------------------------ epoch 5303 (31812 steps) ------------------------------------\n",
      "Max loss: 0.023871852084994316\n",
      "Min loss: 0.005788004957139492\n",
      "Mean loss: 0.010372125776484609\n",
      "Std loss: 0.006146629838434465\n",
      "Total Loss: 0.06223275465890765\n",
      "------------------------------------ epoch 5304 (31818 steps) ------------------------------------\n",
      "Max loss: 0.03745464235544205\n",
      "Min loss: 0.0053601390682160854\n",
      "Mean loss: 0.013183854830761751\n",
      "Std loss: 0.011038345129129951\n",
      "Total Loss: 0.0791031289845705\n",
      "------------------------------------ epoch 5305 (31824 steps) ------------------------------------\n",
      "Max loss: 0.018333962187170982\n",
      "Min loss: 0.005577194504439831\n",
      "Mean loss: 0.011524088137472669\n",
      "Std loss: 0.004419711098133673\n",
      "Total Loss: 0.06914452882483602\n",
      "------------------------------------ epoch 5306 (31830 steps) ------------------------------------\n",
      "Max loss: 0.03359859436750412\n",
      "Min loss: 0.0076064299792051315\n",
      "Mean loss: 0.013958043418824673\n",
      "Std loss: 0.009038754467109526\n",
      "Total Loss: 0.08374826051294804\n",
      "------------------------------------ epoch 5307 (31836 steps) ------------------------------------\n",
      "Max loss: 0.012934697791934013\n",
      "Min loss: 0.005456616170704365\n",
      "Mean loss: 0.008435469275961319\n",
      "Std loss: 0.0025173444059267513\n",
      "Total Loss: 0.05061281565576792\n",
      "------------------------------------ epoch 5308 (31842 steps) ------------------------------------\n",
      "Max loss: 0.01016993261873722\n",
      "Min loss: 0.005903158336877823\n",
      "Mean loss: 0.00739605111690859\n",
      "Std loss: 0.001567864884284427\n",
      "Total Loss: 0.04437630670145154\n",
      "------------------------------------ epoch 5309 (31848 steps) ------------------------------------\n",
      "Max loss: 0.042889006435871124\n",
      "Min loss: 0.005932181142270565\n",
      "Mean loss: 0.017652844699720543\n",
      "Std loss: 0.012849598616736967\n",
      "Total Loss: 0.10591706819832325\n",
      "------------------------------------ epoch 5310 (31854 steps) ------------------------------------\n",
      "Max loss: 0.018075134605169296\n",
      "Min loss: 0.005066467449069023\n",
      "Mean loss: 0.010652696713805199\n",
      "Std loss: 0.004613347221121772\n",
      "Total Loss: 0.06391618028283119\n",
      "------------------------------------ epoch 5311 (31860 steps) ------------------------------------\n",
      "Max loss: 0.01562393456697464\n",
      "Min loss: 0.00667682196944952\n",
      "Mean loss: 0.010854500501106182\n",
      "Std loss: 0.0032501707841958724\n",
      "Total Loss: 0.0651270030066371\n",
      "------------------------------------ epoch 5312 (31866 steps) ------------------------------------\n",
      "Max loss: 0.0245374646037817\n",
      "Min loss: 0.005495267920196056\n",
      "Mean loss: 0.012882490021487078\n",
      "Std loss: 0.006144415264576899\n",
      "Total Loss: 0.07729494012892246\n",
      "------------------------------------ epoch 5313 (31872 steps) ------------------------------------\n",
      "Max loss: 0.01682649552822113\n",
      "Min loss: 0.005520370788872242\n",
      "Mean loss: 0.009614226485912999\n",
      "Std loss: 0.0036292721149822973\n",
      "Total Loss: 0.05768535891547799\n",
      "------------------------------------ epoch 5314 (31878 steps) ------------------------------------\n",
      "Max loss: 0.011381166987121105\n",
      "Min loss: 0.0071302661672234535\n",
      "Mean loss: 0.009547952717791\n",
      "Std loss: 0.001678683643805497\n",
      "Total Loss: 0.057287716306746006\n",
      "------------------------------------ epoch 5315 (31884 steps) ------------------------------------\n",
      "Max loss: 0.01350008137524128\n",
      "Min loss: 0.007007136009633541\n",
      "Mean loss: 0.010914367468406757\n",
      "Std loss: 0.0027524414401555214\n",
      "Total Loss: 0.06548620481044054\n",
      "------------------------------------ epoch 5316 (31890 steps) ------------------------------------\n",
      "Max loss: 0.04011140391230583\n",
      "Min loss: 0.005957703106105328\n",
      "Mean loss: 0.01582810018832485\n",
      "Std loss: 0.011746121286718399\n",
      "Total Loss: 0.09496860112994909\n",
      "------------------------------------ epoch 5317 (31896 steps) ------------------------------------\n",
      "Max loss: 0.02029462531208992\n",
      "Min loss: 0.006111171096563339\n",
      "Mean loss: 0.011005641426891088\n",
      "Std loss: 0.004808339323189686\n",
      "Total Loss: 0.06603384856134653\n",
      "------------------------------------ epoch 5318 (31902 steps) ------------------------------------\n",
      "Max loss: 0.01928728073835373\n",
      "Min loss: 0.005729782395064831\n",
      "Mean loss: 0.011630591315527758\n",
      "Std loss: 0.004715646657975342\n",
      "Total Loss: 0.06978354789316654\n",
      "------------------------------------ epoch 5319 (31908 steps) ------------------------------------\n",
      "Max loss: 0.019516851752996445\n",
      "Min loss: 0.008582163602113724\n",
      "Mean loss: 0.013743961229920387\n",
      "Std loss: 0.0038125181416589706\n",
      "Total Loss: 0.08246376737952232\n",
      "------------------------------------ epoch 5320 (31914 steps) ------------------------------------\n",
      "Max loss: 0.023967823013663292\n",
      "Min loss: 0.007856445387005806\n",
      "Mean loss: 0.011358289203296104\n",
      "Std loss: 0.005730733117444553\n",
      "Total Loss: 0.06814973521977663\n",
      "------------------------------------ epoch 5321 (31920 steps) ------------------------------------\n",
      "Max loss: 0.022114386782050133\n",
      "Min loss: 0.005341804586350918\n",
      "Mean loss: 0.011006210387373963\n",
      "Std loss: 0.005301046658056414\n",
      "Total Loss: 0.06603726232424378\n",
      "------------------------------------ epoch 5322 (31926 steps) ------------------------------------\n",
      "Max loss: 0.019166817888617516\n",
      "Min loss: 0.007655089255422354\n",
      "Mean loss: 0.011329444979007045\n",
      "Std loss: 0.003947158537836016\n",
      "Total Loss: 0.06797666987404227\n",
      "------------------------------------ epoch 5323 (31932 steps) ------------------------------------\n",
      "Max loss: 0.0188140869140625\n",
      "Min loss: 0.007272777613252401\n",
      "Mean loss: 0.009977676983301839\n",
      "Std loss: 0.004014754673688285\n",
      "Total Loss: 0.05986606189981103\n",
      "------------------------------------ epoch 5324 (31938 steps) ------------------------------------\n",
      "Max loss: 0.017233990132808685\n",
      "Min loss: 0.008265204727649689\n",
      "Mean loss: 0.011444485901544491\n",
      "Std loss: 0.002863624801953641\n",
      "Total Loss: 0.06866691540926695\n",
      "------------------------------------ epoch 5325 (31944 steps) ------------------------------------\n",
      "Max loss: 0.019277622923254967\n",
      "Min loss: 0.008779369294643402\n",
      "Mean loss: 0.013239748775959015\n",
      "Std loss: 0.004151032636948466\n",
      "Total Loss: 0.07943849265575409\n",
      "------------------------------------ epoch 5326 (31950 steps) ------------------------------------\n",
      "Max loss: 0.026306262239813805\n",
      "Min loss: 0.006273859646171331\n",
      "Mean loss: 0.010515659814700484\n",
      "Std loss: 0.007169063890442236\n",
      "Total Loss: 0.0630939588882029\n",
      "------------------------------------ epoch 5327 (31956 steps) ------------------------------------\n",
      "Max loss: 0.01584874838590622\n",
      "Min loss: 0.0058120968751609325\n",
      "Mean loss: 0.00900218146853149\n",
      "Std loss: 0.0032320498035885363\n",
      "Total Loss: 0.054013088811188936\n",
      "------------------------------------ epoch 5328 (31962 steps) ------------------------------------\n",
      "Max loss: 0.012325774878263474\n",
      "Min loss: 0.0060072047635912895\n",
      "Mean loss: 0.007905518636107445\n",
      "Std loss: 0.0020477523312711823\n",
      "Total Loss: 0.04743311181664467\n",
      "------------------------------------ epoch 5329 (31968 steps) ------------------------------------\n",
      "Max loss: 0.02035803347826004\n",
      "Min loss: 0.005892681889235973\n",
      "Mean loss: 0.010052661333854\n",
      "Std loss: 0.0055081362317888375\n",
      "Total Loss: 0.060315968003124\n",
      "------------------------------------ epoch 5330 (31974 steps) ------------------------------------\n",
      "Max loss: 0.04063193127512932\n",
      "Min loss: 0.0068445513024926186\n",
      "Mean loss: 0.01987014788513382\n",
      "Std loss: 0.013438775650188204\n",
      "Total Loss: 0.11922088731080294\n",
      "------------------------------------ epoch 5331 (31980 steps) ------------------------------------\n",
      "Max loss: 0.02244747243821621\n",
      "Min loss: 0.004653081763535738\n",
      "Mean loss: 0.011199951016654571\n",
      "Std loss: 0.007219584530528692\n",
      "Total Loss: 0.06719970609992743\n",
      "------------------------------------ epoch 5332 (31986 steps) ------------------------------------\n",
      "Max loss: 0.015471689403057098\n",
      "Min loss: 0.005211331881582737\n",
      "Mean loss: 0.010005883562068144\n",
      "Std loss: 0.003196582564124982\n",
      "Total Loss: 0.06003530137240887\n",
      "------------------------------------ epoch 5333 (31992 steps) ------------------------------------\n",
      "Max loss: 0.0220344215631485\n",
      "Min loss: 0.006120249163359404\n",
      "Mean loss: 0.013059490593150258\n",
      "Std loss: 0.006498845774600493\n",
      "Total Loss: 0.07835694355890155\n",
      "------------------------------------ epoch 5334 (31998 steps) ------------------------------------\n",
      "Max loss: 0.02571663074195385\n",
      "Min loss: 0.0061656697653234005\n",
      "Mean loss: 0.0148392995664229\n",
      "Std loss: 0.007155235500144963\n",
      "Total Loss: 0.0890357973985374\n",
      "------------------------------------ epoch 5335 (32004 steps) ------------------------------------\n",
      "Max loss: 0.017759982496500015\n",
      "Min loss: 0.0065683890134096146\n",
      "Mean loss: 0.011594094491253296\n",
      "Std loss: 0.004008322593968481\n",
      "Total Loss: 0.06956456694751978\n",
      "------------------------------------ epoch 5336 (32010 steps) ------------------------------------\n",
      "Max loss: 0.03752945736050606\n",
      "Min loss: 0.00775435846298933\n",
      "Mean loss: 0.01634069283803304\n",
      "Std loss: 0.010533643422694366\n",
      "Total Loss: 0.09804415702819824\n",
      "------------------------------------ epoch 5337 (32016 steps) ------------------------------------\n",
      "Max loss: 0.020816195756196976\n",
      "Min loss: 0.005217365920543671\n",
      "Mean loss: 0.010341548671325048\n",
      "Std loss: 0.00526353305967133\n",
      "Total Loss: 0.06204929202795029\n",
      "------------------------------------ epoch 5338 (32022 steps) ------------------------------------\n",
      "Max loss: 0.024846404790878296\n",
      "Min loss: 0.006544724106788635\n",
      "Mean loss: 0.01230716387120386\n",
      "Std loss: 0.006369142687734075\n",
      "Total Loss: 0.07384298322722316\n",
      "------------------------------------ epoch 5339 (32028 steps) ------------------------------------\n",
      "Max loss: 0.01322020124644041\n",
      "Min loss: 0.006104863248765469\n",
      "Mean loss: 0.010291981045156717\n",
      "Std loss: 0.0025202391205010207\n",
      "Total Loss: 0.061751886270940304\n",
      "------------------------------------ epoch 5340 (32034 steps) ------------------------------------\n",
      "Max loss: 0.020750336349010468\n",
      "Min loss: 0.008427336812019348\n",
      "Mean loss: 0.011375968189289173\n",
      "Std loss: 0.004410785698820673\n",
      "Total Loss: 0.06825580913573503\n",
      "------------------------------------ epoch 5341 (32040 steps) ------------------------------------\n",
      "Max loss: 0.0332685224711895\n",
      "Min loss: 0.004676922224462032\n",
      "Mean loss: 0.013122270892684659\n",
      "Std loss: 0.009977621402518469\n",
      "Total Loss: 0.07873362535610795\n",
      "------------------------------------ epoch 5342 (32046 steps) ------------------------------------\n",
      "Max loss: 0.018496423959732056\n",
      "Min loss: 0.006667193025350571\n",
      "Mean loss: 0.013927238527685404\n",
      "Std loss: 0.004042696144905164\n",
      "Total Loss: 0.08356343116611242\n",
      "------------------------------------ epoch 5343 (32052 steps) ------------------------------------\n",
      "Max loss: 0.010486958548426628\n",
      "Min loss: 0.00776170939207077\n",
      "Mean loss: 0.008663089325030645\n",
      "Std loss: 0.0008557521080437085\n",
      "Total Loss: 0.05197853595018387\n",
      "------------------------------------ epoch 5344 (32058 steps) ------------------------------------\n",
      "Max loss: 0.017669174820184708\n",
      "Min loss: 0.005417252890765667\n",
      "Mean loss: 0.008951726369559765\n",
      "Std loss: 0.004247597160625676\n",
      "Total Loss: 0.05371035821735859\n",
      "------------------------------------ epoch 5345 (32064 steps) ------------------------------------\n",
      "Max loss: 0.034354738891124725\n",
      "Min loss: 0.005030478350818157\n",
      "Mean loss: 0.011587977098921934\n",
      "Std loss: 0.01046162982600341\n",
      "Total Loss: 0.06952786259353161\n",
      "------------------------------------ epoch 5346 (32070 steps) ------------------------------------\n",
      "Max loss: 0.02339305728673935\n",
      "Min loss: 0.005833671893924475\n",
      "Mean loss: 0.01364929974079132\n",
      "Std loss: 0.006170616561419862\n",
      "Total Loss: 0.08189579844474792\n",
      "------------------------------------ epoch 5347 (32076 steps) ------------------------------------\n",
      "Max loss: 0.014971619471907616\n",
      "Min loss: 0.007005649618804455\n",
      "Mean loss: 0.010317007390161356\n",
      "Std loss: 0.0031380665498518243\n",
      "Total Loss: 0.06190204434096813\n",
      "------------------------------------ epoch 5348 (32082 steps) ------------------------------------\n",
      "Max loss: 0.01115505676716566\n",
      "Min loss: 0.004457724280655384\n",
      "Mean loss: 0.007901639211922884\n",
      "Std loss: 0.0020811230499215043\n",
      "Total Loss: 0.047409835271537304\n",
      "------------------------------------ epoch 5349 (32088 steps) ------------------------------------\n",
      "Max loss: 0.027397580444812775\n",
      "Min loss: 0.00478305434808135\n",
      "Mean loss: 0.011956930393353105\n",
      "Std loss: 0.007731935814050612\n",
      "Total Loss: 0.07174158236011863\n",
      "------------------------------------ epoch 5350 (32094 steps) ------------------------------------\n",
      "Max loss: 0.017217278480529785\n",
      "Min loss: 0.007377422880381346\n",
      "Mean loss: 0.012446705174321929\n",
      "Std loss: 0.004000481500237284\n",
      "Total Loss: 0.07468023104593158\n",
      "------------------------------------ epoch 5351 (32100 steps) ------------------------------------\n",
      "Max loss: 0.06289610266685486\n",
      "Min loss: 0.010124940425157547\n",
      "Mean loss: 0.024112652987241745\n",
      "Std loss: 0.01864445832074897\n",
      "Total Loss: 0.14467591792345047\n",
      "------------------------------------ epoch 5352 (32106 steps) ------------------------------------\n",
      "Max loss: 0.01408302690833807\n",
      "Min loss: 0.006068721413612366\n",
      "Mean loss: 0.0089986736420542\n",
      "Std loss: 0.002866378981943196\n",
      "Total Loss: 0.0539920418523252\n",
      "------------------------------------ epoch 5353 (32112 steps) ------------------------------------\n",
      "Max loss: 0.010535618290305138\n",
      "Min loss: 0.006577238440513611\n",
      "Mean loss: 0.007843058478708068\n",
      "Std loss: 0.0012955906139409316\n",
      "Total Loss: 0.04705835087224841\n",
      "------------------------------------ epoch 5354 (32118 steps) ------------------------------------\n",
      "Max loss: 0.02587796375155449\n",
      "Min loss: 0.007907293736934662\n",
      "Mean loss: 0.01248988090083003\n",
      "Std loss: 0.006165364364947135\n",
      "Total Loss: 0.07493928540498018\n",
      "------------------------------------ epoch 5355 (32124 steps) ------------------------------------\n",
      "Max loss: 0.010187320411205292\n",
      "Min loss: 0.0052679553627967834\n",
      "Mean loss: 0.006545077155654629\n",
      "Std loss: 0.0016857781670305287\n",
      "Total Loss: 0.039270462933927774\n",
      "------------------------------------ epoch 5356 (32130 steps) ------------------------------------\n",
      "Max loss: 0.051584772765636444\n",
      "Min loss: 0.0057706511579453945\n",
      "Mean loss: 0.015294779480124513\n",
      "Std loss: 0.016331783961458162\n",
      "Total Loss: 0.09176867688074708\n",
      "------------------------------------ epoch 5357 (32136 steps) ------------------------------------\n",
      "Max loss: 0.05217451602220535\n",
      "Min loss: 0.008053557947278023\n",
      "Mean loss: 0.017851494407902162\n",
      "Std loss: 0.015529550873805618\n",
      "Total Loss: 0.10710896644741297\n",
      "------------------------------------ epoch 5358 (32142 steps) ------------------------------------\n",
      "Max loss: 0.016107317060232162\n",
      "Min loss: 0.0066795628517866135\n",
      "Mean loss: 0.010109621410568556\n",
      "Std loss: 0.003214419206247802\n",
      "Total Loss: 0.06065772846341133\n",
      "------------------------------------ epoch 5359 (32148 steps) ------------------------------------\n",
      "Max loss: 0.026741918176412582\n",
      "Min loss: 0.007063975092023611\n",
      "Mean loss: 0.012419059251745542\n",
      "Std loss: 0.007002226442401701\n",
      "Total Loss: 0.07451435551047325\n",
      "------------------------------------ epoch 5360 (32154 steps) ------------------------------------\n",
      "Max loss: 0.03235889971256256\n",
      "Min loss: 0.005956943146884441\n",
      "Mean loss: 0.01148404561293622\n",
      "Std loss: 0.00936923319205347\n",
      "Total Loss: 0.06890427367761731\n",
      "------------------------------------ epoch 5361 (32160 steps) ------------------------------------\n",
      "Max loss: 0.0553385429084301\n",
      "Min loss: 0.006563116796314716\n",
      "Mean loss: 0.016496025258675218\n",
      "Std loss: 0.017487308822766275\n",
      "Total Loss: 0.0989761515520513\n",
      "------------------------------------ epoch 5362 (32166 steps) ------------------------------------\n",
      "Max loss: 0.04035371541976929\n",
      "Min loss: 0.006187904626131058\n",
      "Mean loss: 0.012901299788306156\n",
      "Std loss: 0.012365779119492593\n",
      "Total Loss: 0.07740779872983694\n",
      "------------------------------------ epoch 5363 (32172 steps) ------------------------------------\n",
      "Max loss: 0.03179391846060753\n",
      "Min loss: 0.006423229351639748\n",
      "Mean loss: 0.014915700846662125\n",
      "Std loss: 0.008338836982960032\n",
      "Total Loss: 0.08949420507997274\n",
      "------------------------------------ epoch 5364 (32178 steps) ------------------------------------\n",
      "Max loss: 0.01782594621181488\n",
      "Min loss: 0.006899961270391941\n",
      "Mean loss: 0.012067397125065327\n",
      "Std loss: 0.0036386405943970306\n",
      "Total Loss: 0.07240438275039196\n",
      "------------------------------------ epoch 5365 (32184 steps) ------------------------------------\n",
      "Max loss: 0.032942406833171844\n",
      "Min loss: 0.005933581851422787\n",
      "Mean loss: 0.012393320988242825\n",
      "Std loss: 0.009402458741597272\n",
      "Total Loss: 0.07435992592945695\n",
      "------------------------------------ epoch 5366 (32190 steps) ------------------------------------\n",
      "Max loss: 0.014709757640957832\n",
      "Min loss: 0.006810867693275213\n",
      "Mean loss: 0.009729482388744751\n",
      "Std loss: 0.002513386803042115\n",
      "Total Loss: 0.05837689433246851\n",
      "------------------------------------ epoch 5367 (32196 steps) ------------------------------------\n",
      "Max loss: 0.031375326216220856\n",
      "Min loss: 0.005619313567876816\n",
      "Mean loss: 0.012802289022753635\n",
      "Std loss: 0.008863342317336213\n",
      "Total Loss: 0.07681373413652182\n",
      "------------------------------------ epoch 5368 (32202 steps) ------------------------------------\n",
      "Max loss: 0.014785774052143097\n",
      "Min loss: 0.006331110373139381\n",
      "Mean loss: 0.008289433705310026\n",
      "Std loss: 0.0029491956430307787\n",
      "Total Loss: 0.04973660223186016\n",
      "------------------------------------ epoch 5369 (32208 steps) ------------------------------------\n",
      "Max loss: 0.019080806523561478\n",
      "Min loss: 0.006384415086358786\n",
      "Mean loss: 0.010319680208340287\n",
      "Std loss: 0.004469752201957627\n",
      "Total Loss: 0.06191808125004172\n",
      "------------------------------------ epoch 5370 (32214 steps) ------------------------------------\n",
      "Max loss: 0.01107335090637207\n",
      "Min loss: 0.005701380781829357\n",
      "Mean loss: 0.00829083596666654\n",
      "Std loss: 0.002420840126618901\n",
      "Total Loss: 0.04974501579999924\n",
      "------------------------------------ epoch 5371 (32220 steps) ------------------------------------\n",
      "Max loss: 0.016203369945287704\n",
      "Min loss: 0.004819892346858978\n",
      "Mean loss: 0.008842086885124445\n",
      "Std loss: 0.003800177398481696\n",
      "Total Loss: 0.05305252131074667\n",
      "------------------------------------ epoch 5372 (32226 steps) ------------------------------------\n",
      "Max loss: 0.019295277073979378\n",
      "Min loss: 0.005344633013010025\n",
      "Mean loss: 0.00987077426786224\n",
      "Std loss: 0.0048065708837884205\n",
      "Total Loss: 0.05922464560717344\n",
      "------------------------------------ epoch 5373 (32232 steps) ------------------------------------\n",
      "Max loss: 0.01969713158905506\n",
      "Min loss: 0.006497861817479134\n",
      "Mean loss: 0.009652887393410007\n",
      "Std loss: 0.004742714701660042\n",
      "Total Loss: 0.05791732436046004\n",
      "------------------------------------ epoch 5374 (32238 steps) ------------------------------------\n",
      "Max loss: 0.01698928326368332\n",
      "Min loss: 0.004309229552745819\n",
      "Mean loss: 0.007974126220991215\n",
      "Std loss: 0.004343113367647134\n",
      "Total Loss: 0.047844757325947285\n",
      "------------------------------------ epoch 5375 (32244 steps) ------------------------------------\n",
      "Max loss: 0.008370166644454002\n",
      "Min loss: 0.005228697322309017\n",
      "Mean loss: 0.006505498119319479\n",
      "Std loss: 0.0010428105464385005\n",
      "Total Loss: 0.03903298871591687\n",
      "------------------------------------ epoch 5376 (32250 steps) ------------------------------------\n",
      "Max loss: 0.009490197524428368\n",
      "Min loss: 0.005054813344031572\n",
      "Mean loss: 0.007116740336641669\n",
      "Std loss: 0.0013943058965390157\n",
      "Total Loss: 0.042700442019850016\n",
      "------------------------------------ epoch 5377 (32256 steps) ------------------------------------\n",
      "Max loss: 0.022108322009444237\n",
      "Min loss: 0.00568004883825779\n",
      "Mean loss: 0.012722923808420697\n",
      "Std loss: 0.006827229423579029\n",
      "Total Loss: 0.07633754285052419\n",
      "------------------------------------ epoch 5378 (32262 steps) ------------------------------------\n",
      "Max loss: 0.013173786923289299\n",
      "Min loss: 0.005684270523488522\n",
      "Mean loss: 0.00863213751775523\n",
      "Std loss: 0.002438321982537768\n",
      "Total Loss: 0.05179282510653138\n",
      "------------------------------------ epoch 5379 (32268 steps) ------------------------------------\n",
      "Max loss: 0.01312183402478695\n",
      "Min loss: 0.004133162088692188\n",
      "Mean loss: 0.007499989044542114\n",
      "Std loss: 0.0030036747924903494\n",
      "Total Loss: 0.044999934267252684\n",
      "------------------------------------ epoch 5380 (32274 steps) ------------------------------------\n",
      "Max loss: 0.008653087541460991\n",
      "Min loss: 0.005306609906256199\n",
      "Mean loss: 0.006813046522438526\n",
      "Std loss: 0.0011306663622852793\n",
      "Total Loss: 0.04087827913463116\n",
      "------------------------------------ epoch 5381 (32280 steps) ------------------------------------\n",
      "Max loss: 0.014511790126562119\n",
      "Min loss: 0.005261027719825506\n",
      "Mean loss: 0.008075836192195615\n",
      "Std loss: 0.0032507518982363283\n",
      "Total Loss: 0.048455017153173685\n",
      "------------------------------------ epoch 5382 (32286 steps) ------------------------------------\n",
      "Max loss: 0.04144606739282608\n",
      "Min loss: 0.005098999477922916\n",
      "Mean loss: 0.015566825633868575\n",
      "Std loss: 0.012970679477392071\n",
      "Total Loss: 0.09340095380321145\n",
      "------------------------------------ epoch 5383 (32292 steps) ------------------------------------\n",
      "Max loss: 0.033998921513557434\n",
      "Min loss: 0.0076050772331655025\n",
      "Mean loss: 0.01656864342900614\n",
      "Std loss: 0.008943613619667794\n",
      "Total Loss: 0.09941186057403684\n",
      "------------------------------------ epoch 5384 (32298 steps) ------------------------------------\n",
      "Max loss: 0.01319742389023304\n",
      "Min loss: 0.005091272760182619\n",
      "Mean loss: 0.009837887482717633\n",
      "Std loss: 0.0027577758026177683\n",
      "Total Loss: 0.0590273248963058\n",
      "------------------------------------ epoch 5385 (32304 steps) ------------------------------------\n",
      "Max loss: 0.03887753561139107\n",
      "Min loss: 0.0061445413157343864\n",
      "Mean loss: 0.01274210944150885\n",
      "Std loss: 0.011776880537113723\n",
      "Total Loss: 0.0764526566490531\n",
      "------------------------------------ epoch 5386 (32310 steps) ------------------------------------\n",
      "Max loss: 0.019414573907852173\n",
      "Min loss: 0.004844651557505131\n",
      "Mean loss: 0.010955336581294736\n",
      "Std loss: 0.0054202630297321645\n",
      "Total Loss: 0.06573201948776841\n",
      "------------------------------------ epoch 5387 (32316 steps) ------------------------------------\n",
      "Max loss: 0.01170305348932743\n",
      "Min loss: 0.004878993611782789\n",
      "Mean loss: 0.0068516907437394066\n",
      "Std loss: 0.002253558507084012\n",
      "Total Loss: 0.04111014446243644\n",
      "------------------------------------ epoch 5388 (32322 steps) ------------------------------------\n",
      "Max loss: 0.02315245196223259\n",
      "Min loss: 0.005495849531143904\n",
      "Mean loss: 0.011735941981896758\n",
      "Std loss: 0.006418535237834627\n",
      "Total Loss: 0.07041565189138055\n",
      "------------------------------------ epoch 5389 (32328 steps) ------------------------------------\n",
      "Max loss: 0.012646026909351349\n",
      "Min loss: 0.005271988920867443\n",
      "Mean loss: 0.007332234177738428\n",
      "Std loss: 0.0024611307112432077\n",
      "Total Loss: 0.04399340506643057\n",
      "------------------------------------ epoch 5390 (32334 steps) ------------------------------------\n",
      "Max loss: 0.028049221262335777\n",
      "Min loss: 0.005950489081442356\n",
      "Mean loss: 0.010609854633609453\n",
      "Std loss: 0.007848071938460852\n",
      "Total Loss: 0.06365912780165672\n",
      "------------------------------------ epoch 5391 (32340 steps) ------------------------------------\n",
      "Max loss: 0.009950383566319942\n",
      "Min loss: 0.0047975326888263226\n",
      "Mean loss: 0.007351925441374381\n",
      "Std loss: 0.0018628372086250356\n",
      "Total Loss: 0.04411155264824629\n",
      "------------------------------------ epoch 5392 (32346 steps) ------------------------------------\n",
      "Max loss: 0.020174238830804825\n",
      "Min loss: 0.004811566788703203\n",
      "Mean loss: 0.01126071453715364\n",
      "Std loss: 0.006456888810424823\n",
      "Total Loss: 0.06756428722292185\n",
      "------------------------------------ epoch 5393 (32352 steps) ------------------------------------\n",
      "Max loss: 0.01789752207696438\n",
      "Min loss: 0.0043688369914889336\n",
      "Mean loss: 0.010160365995640555\n",
      "Std loss: 0.005146708679912717\n",
      "Total Loss: 0.060962195973843336\n",
      "------------------------------------ epoch 5394 (32358 steps) ------------------------------------\n",
      "Max loss: 0.024110345169901848\n",
      "Min loss: 0.005019533913582563\n",
      "Mean loss: 0.011655426972235242\n",
      "Std loss: 0.00787494078541389\n",
      "Total Loss: 0.06993256183341146\n",
      "------------------------------------ epoch 5395 (32364 steps) ------------------------------------\n",
      "Max loss: 0.013957984745502472\n",
      "Min loss: 0.005762563552707434\n",
      "Mean loss: 0.007872353618343672\n",
      "Std loss: 0.0028429008911334875\n",
      "Total Loss: 0.04723412171006203\n",
      "------------------------------------ epoch 5396 (32370 steps) ------------------------------------\n",
      "Max loss: 0.019477179273962975\n",
      "Min loss: 0.009511361829936504\n",
      "Mean loss: 0.014119973250975212\n",
      "Std loss: 0.004264641302208457\n",
      "Total Loss: 0.08471983950585127\n",
      "------------------------------------ epoch 5397 (32376 steps) ------------------------------------\n",
      "Max loss: 0.016583595424890518\n",
      "Min loss: 0.005234269425272942\n",
      "Mean loss: 0.010865070081005493\n",
      "Std loss: 0.003861088519455187\n",
      "Total Loss: 0.06519042048603296\n",
      "------------------------------------ epoch 5398 (32382 steps) ------------------------------------\n",
      "Max loss: 0.01000127475708723\n",
      "Min loss: 0.005946390330791473\n",
      "Mean loss: 0.00812002643942833\n",
      "Std loss: 0.0015769806342949093\n",
      "Total Loss: 0.04872015863656998\n",
      "------------------------------------ epoch 5399 (32388 steps) ------------------------------------\n",
      "Max loss: 0.014091907069087029\n",
      "Min loss: 0.005222532898187637\n",
      "Mean loss: 0.008894360546643535\n",
      "Std loss: 0.003526226600597076\n",
      "Total Loss: 0.05336616327986121\n",
      "------------------------------------ epoch 5400 (32394 steps) ------------------------------------\n",
      "Max loss: 0.013290062546730042\n",
      "Min loss: 0.005550882779061794\n",
      "Mean loss: 0.0081665872130543\n",
      "Std loss: 0.0033359206698200376\n",
      "Total Loss: 0.048999523278325796\n",
      "------------------------------------ epoch 5401 (32400 steps) ------------------------------------\n",
      "Max loss: 0.055501487106084824\n",
      "Min loss: 0.005917779169976711\n",
      "Mean loss: 0.019592476543039083\n",
      "Std loss: 0.017997212900460587\n",
      "Total Loss: 0.1175548592582345\n",
      "saved model at ./weights/model_5401.pth\n",
      "------------------------------------ epoch 5402 (32406 steps) ------------------------------------\n",
      "Max loss: 0.028554117307066917\n",
      "Min loss: 0.009276237338781357\n",
      "Mean loss: 0.01545510021969676\n",
      "Std loss: 0.006792331155658441\n",
      "Total Loss: 0.09273060131818056\n",
      "------------------------------------ epoch 5403 (32412 steps) ------------------------------------\n",
      "Max loss: 0.02254318818449974\n",
      "Min loss: 0.0071802763268351555\n",
      "Mean loss: 0.015084161888808012\n",
      "Std loss: 0.0059606402871081035\n",
      "Total Loss: 0.09050497133284807\n",
      "------------------------------------ epoch 5404 (32418 steps) ------------------------------------\n",
      "Max loss: 0.02319657802581787\n",
      "Min loss: 0.0053261397406458855\n",
      "Mean loss: 0.010238698062797388\n",
      "Std loss: 0.006066484159302885\n",
      "Total Loss: 0.061432188376784325\n",
      "------------------------------------ epoch 5405 (32424 steps) ------------------------------------\n",
      "Max loss: 0.017697755247354507\n",
      "Min loss: 0.006327492650598288\n",
      "Mean loss: 0.009254375550275048\n",
      "Std loss: 0.00393453985246719\n",
      "Total Loss: 0.055526253301650286\n",
      "------------------------------------ epoch 5406 (32430 steps) ------------------------------------\n",
      "Max loss: 0.017599917948246002\n",
      "Min loss: 0.005678042769432068\n",
      "Mean loss: 0.012110061477869749\n",
      "Std loss: 0.003876207910681014\n",
      "Total Loss: 0.0726603688672185\n",
      "------------------------------------ epoch 5407 (32436 steps) ------------------------------------\n",
      "Max loss: 0.021116267889738083\n",
      "Min loss: 0.007612267043441534\n",
      "Mean loss: 0.01433225356352826\n",
      "Std loss: 0.005154827182569424\n",
      "Total Loss: 0.08599352138116956\n",
      "------------------------------------ epoch 5408 (32442 steps) ------------------------------------\n",
      "Max loss: 0.04414086043834686\n",
      "Min loss: 0.005415758118033409\n",
      "Mean loss: 0.013176224504907927\n",
      "Std loss: 0.01390874200905988\n",
      "Total Loss: 0.07905734702944756\n",
      "------------------------------------ epoch 5409 (32448 steps) ------------------------------------\n",
      "Max loss: 0.014576105400919914\n",
      "Min loss: 0.006308571435511112\n",
      "Mean loss: 0.009442490991204977\n",
      "Std loss: 0.0029047104882863072\n",
      "Total Loss: 0.05665494594722986\n",
      "------------------------------------ epoch 5410 (32454 steps) ------------------------------------\n",
      "Max loss: 0.017929542809724808\n",
      "Min loss: 0.005514423828572035\n",
      "Mean loss: 0.010226032075782618\n",
      "Std loss: 0.004089271180650623\n",
      "Total Loss: 0.0613561924546957\n",
      "------------------------------------ epoch 5411 (32460 steps) ------------------------------------\n",
      "Max loss: 0.016116950660943985\n",
      "Min loss: 0.005302037112414837\n",
      "Mean loss: 0.007958404952660203\n",
      "Std loss: 0.0038020052553963163\n",
      "Total Loss: 0.04775042971596122\n",
      "------------------------------------ epoch 5412 (32466 steps) ------------------------------------\n",
      "Max loss: 0.009334614500403404\n",
      "Min loss: 0.005283377133309841\n",
      "Mean loss: 0.0067220251852025585\n",
      "Std loss: 0.0014257145639603992\n",
      "Total Loss: 0.04033215111121535\n",
      "------------------------------------ epoch 5413 (32472 steps) ------------------------------------\n",
      "Max loss: 0.012119988910853863\n",
      "Min loss: 0.004275846295058727\n",
      "Mean loss: 0.008218259628241261\n",
      "Std loss: 0.0026238864641474936\n",
      "Total Loss: 0.049309557769447565\n",
      "------------------------------------ epoch 5414 (32478 steps) ------------------------------------\n",
      "Max loss: 0.015379372052848339\n",
      "Min loss: 0.005310251377522945\n",
      "Mean loss: 0.008549368940293789\n",
      "Std loss: 0.003302950650798319\n",
      "Total Loss: 0.05129621364176273\n",
      "------------------------------------ epoch 5415 (32484 steps) ------------------------------------\n",
      "Max loss: 0.016275513917207718\n",
      "Min loss: 0.0047821463085711\n",
      "Mean loss: 0.009521452244371176\n",
      "Std loss: 0.0038126494446682037\n",
      "Total Loss: 0.057128713466227055\n",
      "------------------------------------ epoch 5416 (32490 steps) ------------------------------------\n",
      "Max loss: 0.012070843949913979\n",
      "Min loss: 0.0058609237894415855\n",
      "Mean loss: 0.00798904080875218\n",
      "Std loss: 0.00211878189224444\n",
      "Total Loss: 0.047934244852513075\n",
      "------------------------------------ epoch 5417 (32496 steps) ------------------------------------\n",
      "Max loss: 0.014080330729484558\n",
      "Min loss: 0.006333156488835812\n",
      "Mean loss: 0.009555990885322293\n",
      "Std loss: 0.0027901208616852914\n",
      "Total Loss: 0.057335945311933756\n",
      "------------------------------------ epoch 5418 (32502 steps) ------------------------------------\n",
      "Max loss: 0.02633070945739746\n",
      "Min loss: 0.005626671016216278\n",
      "Mean loss: 0.011965562744687\n",
      "Std loss: 0.00691546403909139\n",
      "Total Loss: 0.071793376468122\n",
      "------------------------------------ epoch 5419 (32508 steps) ------------------------------------\n",
      "Max loss: 0.011042218655347824\n",
      "Min loss: 0.0058730244636535645\n",
      "Mean loss: 0.007667755785708626\n",
      "Std loss: 0.0016120060277409487\n",
      "Total Loss: 0.04600653471425176\n",
      "------------------------------------ epoch 5420 (32514 steps) ------------------------------------\n",
      "Max loss: 0.02350991778075695\n",
      "Min loss: 0.006007231306284666\n",
      "Mean loss: 0.011666418674091497\n",
      "Std loss: 0.0062375116297816\n",
      "Total Loss: 0.06999851204454899\n",
      "------------------------------------ epoch 5421 (32520 steps) ------------------------------------\n",
      "Max loss: 0.028227075934410095\n",
      "Min loss: 0.006178183481097221\n",
      "Mean loss: 0.014662142222126326\n",
      "Std loss: 0.0074160117393632895\n",
      "Total Loss: 0.08797285333275795\n",
      "------------------------------------ epoch 5422 (32526 steps) ------------------------------------\n",
      "Max loss: 0.018552206456661224\n",
      "Min loss: 0.006751595064997673\n",
      "Mean loss: 0.00988453448129197\n",
      "Std loss: 0.004070389649550699\n",
      "Total Loss: 0.05930720688775182\n",
      "------------------------------------ epoch 5423 (32532 steps) ------------------------------------\n",
      "Max loss: 0.023947102949023247\n",
      "Min loss: 0.005136994179338217\n",
      "Mean loss: 0.009027793693045775\n",
      "Std loss: 0.006700364559219012\n",
      "Total Loss: 0.05416676215827465\n",
      "------------------------------------ epoch 5424 (32538 steps) ------------------------------------\n",
      "Max loss: 0.023791320621967316\n",
      "Min loss: 0.00405145063996315\n",
      "Mean loss: 0.011018727595607439\n",
      "Std loss: 0.007956348726995915\n",
      "Total Loss: 0.06611236557364464\n",
      "------------------------------------ epoch 5425 (32544 steps) ------------------------------------\n",
      "Max loss: 0.02595404163002968\n",
      "Min loss: 0.0052087451331317425\n",
      "Mean loss: 0.010343635842824975\n",
      "Std loss: 0.007257114755280497\n",
      "Total Loss: 0.062061815056949854\n",
      "------------------------------------ epoch 5426 (32550 steps) ------------------------------------\n",
      "Max loss: 0.016091078519821167\n",
      "Min loss: 0.006765899248421192\n",
      "Mean loss: 0.009154174942523241\n",
      "Std loss: 0.003171895477960953\n",
      "Total Loss: 0.054925049655139446\n",
      "------------------------------------ epoch 5427 (32556 steps) ------------------------------------\n",
      "Max loss: 0.012925602495670319\n",
      "Min loss: 0.004749140236526728\n",
      "Mean loss: 0.008442089039211472\n",
      "Std loss: 0.003257250451310418\n",
      "Total Loss: 0.05065253423526883\n",
      "------------------------------------ epoch 5428 (32562 steps) ------------------------------------\n",
      "Max loss: 0.008136016316711903\n",
      "Min loss: 0.004937188234180212\n",
      "Mean loss: 0.006473914487287402\n",
      "Std loss: 0.0012479245415015548\n",
      "Total Loss: 0.03884348692372441\n",
      "------------------------------------ epoch 5429 (32568 steps) ------------------------------------\n",
      "Max loss: 0.013016445562243462\n",
      "Min loss: 0.005838415585458279\n",
      "Mean loss: 0.008867912382508317\n",
      "Std loss: 0.0024775587507966245\n",
      "Total Loss: 0.053207474295049906\n",
      "------------------------------------ epoch 5430 (32574 steps) ------------------------------------\n",
      "Max loss: 0.025611117482185364\n",
      "Min loss: 0.005556029267609119\n",
      "Mean loss: 0.010858888815467557\n",
      "Std loss: 0.007185618099600964\n",
      "Total Loss: 0.06515333289280534\n",
      "------------------------------------ epoch 5431 (32580 steps) ------------------------------------\n",
      "Max loss: 0.018102742731571198\n",
      "Min loss: 0.006738079246133566\n",
      "Mean loss: 0.011007574464504918\n",
      "Std loss: 0.004417466825940023\n",
      "Total Loss: 0.0660454467870295\n",
      "------------------------------------ epoch 5432 (32586 steps) ------------------------------------\n",
      "Max loss: 0.029257243499159813\n",
      "Min loss: 0.0056549739092588425\n",
      "Mean loss: 0.01317686114149789\n",
      "Std loss: 0.00826870786991942\n",
      "Total Loss: 0.07906116684898734\n",
      "------------------------------------ epoch 5433 (32592 steps) ------------------------------------\n",
      "Max loss: 0.017438000068068504\n",
      "Min loss: 0.008861360140144825\n",
      "Mean loss: 0.012505718351652225\n",
      "Std loss: 0.003532339765960051\n",
      "Total Loss: 0.07503431010991335\n",
      "------------------------------------ epoch 5434 (32598 steps) ------------------------------------\n",
      "Max loss: 0.018246103078126907\n",
      "Min loss: 0.006713949143886566\n",
      "Mean loss: 0.011035746894776821\n",
      "Std loss: 0.0037344725538447344\n",
      "Total Loss: 0.06621448136866093\n",
      "------------------------------------ epoch 5435 (32604 steps) ------------------------------------\n",
      "Max loss: 0.008239435032010078\n",
      "Min loss: 0.005092829465866089\n",
      "Mean loss: 0.0061000568481783075\n",
      "Std loss: 0.0011256290548413458\n",
      "Total Loss: 0.03660034108906984\n",
      "------------------------------------ epoch 5436 (32610 steps) ------------------------------------\n",
      "Max loss: 0.030656486749649048\n",
      "Min loss: 0.004640772007405758\n",
      "Mean loss: 0.010584526617700854\n",
      "Std loss: 0.0090588265649623\n",
      "Total Loss: 0.06350715970620513\n",
      "------------------------------------ epoch 5437 (32616 steps) ------------------------------------\n",
      "Max loss: 0.022952191531658173\n",
      "Min loss: 0.006139443721622229\n",
      "Mean loss: 0.01195210738418003\n",
      "Std loss: 0.006157475936904952\n",
      "Total Loss: 0.07171264430508018\n",
      "------------------------------------ epoch 5438 (32622 steps) ------------------------------------\n",
      "Max loss: 0.01743931695818901\n",
      "Min loss: 0.005219915881752968\n",
      "Mean loss: 0.01055365443850557\n",
      "Std loss: 0.004207940426488383\n",
      "Total Loss: 0.06332192663103342\n",
      "------------------------------------ epoch 5439 (32628 steps) ------------------------------------\n",
      "Max loss: 0.04258590564131737\n",
      "Min loss: 0.006348452530801296\n",
      "Mean loss: 0.013684905755023161\n",
      "Std loss: 0.012991284794095037\n",
      "Total Loss: 0.08210943453013897\n",
      "------------------------------------ epoch 5440 (32634 steps) ------------------------------------\n",
      "Max loss: 0.04802292585372925\n",
      "Min loss: 0.005398246459662914\n",
      "Mean loss: 0.0161062713402013\n",
      "Std loss: 0.014679181692594792\n",
      "Total Loss: 0.09663762804120779\n",
      "------------------------------------ epoch 5441 (32640 steps) ------------------------------------\n",
      "Max loss: 0.018389254808425903\n",
      "Min loss: 0.008332399651408195\n",
      "Mean loss: 0.01210135609532396\n",
      "Std loss: 0.0036559578838675005\n",
      "Total Loss: 0.07260813657194376\n",
      "------------------------------------ epoch 5442 (32646 steps) ------------------------------------\n",
      "Max loss: 0.03208785504102707\n",
      "Min loss: 0.006740853190422058\n",
      "Mean loss: 0.015665970044210553\n",
      "Std loss: 0.010037516744883358\n",
      "Total Loss: 0.09399582026526332\n",
      "------------------------------------ epoch 5443 (32652 steps) ------------------------------------\n",
      "Max loss: 0.014366346411406994\n",
      "Min loss: 0.007184937130659819\n",
      "Mean loss: 0.010619115162019929\n",
      "Std loss: 0.0029813000087600522\n",
      "Total Loss: 0.06371469097211957\n",
      "------------------------------------ epoch 5444 (32658 steps) ------------------------------------\n",
      "Max loss: 0.021099073812365532\n",
      "Min loss: 0.010881338268518448\n",
      "Mean loss: 0.016314067256947357\n",
      "Std loss: 0.003673370039739273\n",
      "Total Loss: 0.09788440354168415\n",
      "------------------------------------ epoch 5445 (32664 steps) ------------------------------------\n",
      "Max loss: 0.021566063165664673\n",
      "Min loss: 0.006608030758798122\n",
      "Mean loss: 0.01347161162023743\n",
      "Std loss: 0.004771522659018523\n",
      "Total Loss: 0.08082966972142458\n",
      "------------------------------------ epoch 5446 (32670 steps) ------------------------------------\n",
      "Max loss: 0.017602484673261642\n",
      "Min loss: 0.007349520921707153\n",
      "Mean loss: 0.011312326105932394\n",
      "Std loss: 0.003502883100673268\n",
      "Total Loss: 0.06787395663559437\n",
      "------------------------------------ epoch 5447 (32676 steps) ------------------------------------\n",
      "Max loss: 0.027872547507286072\n",
      "Min loss: 0.007286557462066412\n",
      "Mean loss: 0.015341176418587565\n",
      "Std loss: 0.006933868286051341\n",
      "Total Loss: 0.09204705851152539\n",
      "------------------------------------ epoch 5448 (32682 steps) ------------------------------------\n",
      "Max loss: 0.027710599824786186\n",
      "Min loss: 0.006518179550766945\n",
      "Mean loss: 0.013275036200260123\n",
      "Std loss: 0.00829553613997997\n",
      "Total Loss: 0.07965021720156074\n",
      "------------------------------------ epoch 5449 (32688 steps) ------------------------------------\n",
      "Max loss: 0.0202945563942194\n",
      "Min loss: 0.005004058592021465\n",
      "Mean loss: 0.01119644179319342\n",
      "Std loss: 0.0050543624617323075\n",
      "Total Loss: 0.06717865075916052\n",
      "------------------------------------ epoch 5450 (32694 steps) ------------------------------------\n",
      "Max loss: 0.012126332148909569\n",
      "Min loss: 0.0061591616831719875\n",
      "Mean loss: 0.00839752370181183\n",
      "Std loss: 0.002594261142091181\n",
      "Total Loss: 0.05038514221087098\n",
      "------------------------------------ epoch 5451 (32700 steps) ------------------------------------\n",
      "Max loss: 0.012319270521402359\n",
      "Min loss: 0.004837804939597845\n",
      "Mean loss: 0.008038481852660576\n",
      "Std loss: 0.0027230576967048536\n",
      "Total Loss: 0.04823089111596346\n",
      "------------------------------------ epoch 5452 (32706 steps) ------------------------------------\n",
      "Max loss: 0.01195630244910717\n",
      "Min loss: 0.004980262368917465\n",
      "Mean loss: 0.008554258616641164\n",
      "Std loss: 0.0029847561599392605\n",
      "Total Loss: 0.05132555169984698\n",
      "------------------------------------ epoch 5453 (32712 steps) ------------------------------------\n",
      "Max loss: 0.019766423851251602\n",
      "Min loss: 0.005792539566755295\n",
      "Mean loss: 0.013136922847479582\n",
      "Std loss: 0.004455507381542539\n",
      "Total Loss: 0.07882153708487749\n",
      "------------------------------------ epoch 5454 (32718 steps) ------------------------------------\n",
      "Max loss: 0.020158657804131508\n",
      "Min loss: 0.006411938928067684\n",
      "Mean loss: 0.01069072742636005\n",
      "Std loss: 0.0045556712317106245\n",
      "Total Loss: 0.0641443645581603\n",
      "------------------------------------ epoch 5455 (32724 steps) ------------------------------------\n",
      "Max loss: 0.020012252032756805\n",
      "Min loss: 0.006157984491437674\n",
      "Mean loss: 0.010106200817972422\n",
      "Std loss: 0.004788620219926247\n",
      "Total Loss: 0.06063720490783453\n",
      "------------------------------------ epoch 5456 (32730 steps) ------------------------------------\n",
      "Max loss: 0.023035887628793716\n",
      "Min loss: 0.004725025035440922\n",
      "Mean loss: 0.009563162922859192\n",
      "Std loss: 0.006236469824242611\n",
      "Total Loss: 0.05737897753715515\n",
      "------------------------------------ epoch 5457 (32736 steps) ------------------------------------\n",
      "Max loss: 0.009445522911846638\n",
      "Min loss: 0.0062167830765247345\n",
      "Mean loss: 0.008065574414407214\n",
      "Std loss: 0.0013466242355132088\n",
      "Total Loss: 0.04839344648644328\n",
      "------------------------------------ epoch 5458 (32742 steps) ------------------------------------\n",
      "Max loss: 0.018204517662525177\n",
      "Min loss: 0.004434550646692514\n",
      "Mean loss: 0.009473338800792893\n",
      "Std loss: 0.0047093447305536695\n",
      "Total Loss: 0.05684003280475736\n",
      "------------------------------------ epoch 5459 (32748 steps) ------------------------------------\n",
      "Max loss: 0.017748411744832993\n",
      "Min loss: 0.00428374856710434\n",
      "Mean loss: 0.009308846512188515\n",
      "Std loss: 0.004328987589344417\n",
      "Total Loss: 0.055853079073131084\n",
      "------------------------------------ epoch 5460 (32754 steps) ------------------------------------\n",
      "Max loss: 0.023044269531965256\n",
      "Min loss: 0.005471624433994293\n",
      "Mean loss: 0.012385023990646005\n",
      "Std loss: 0.006142549053165979\n",
      "Total Loss: 0.07431014394387603\n",
      "------------------------------------ epoch 5461 (32760 steps) ------------------------------------\n",
      "Max loss: 0.03197956830263138\n",
      "Min loss: 0.0052271089516580105\n",
      "Mean loss: 0.0112740199547261\n",
      "Std loss: 0.009383814802264174\n",
      "Total Loss: 0.0676441197283566\n",
      "------------------------------------ epoch 5462 (32766 steps) ------------------------------------\n",
      "Max loss: 0.027411840856075287\n",
      "Min loss: 0.006314202211797237\n",
      "Mean loss: 0.01107442930030326\n",
      "Std loss: 0.007415619906430501\n",
      "Total Loss: 0.06644657580181956\n",
      "------------------------------------ epoch 5463 (32772 steps) ------------------------------------\n",
      "Max loss: 0.035450249910354614\n",
      "Min loss: 0.005843512713909149\n",
      "Mean loss: 0.015110506831357876\n",
      "Std loss: 0.010122263259863904\n",
      "Total Loss: 0.09066304098814726\n",
      "------------------------------------ epoch 5464 (32778 steps) ------------------------------------\n",
      "Max loss: 0.019743066281080246\n",
      "Min loss: 0.007434528321027756\n",
      "Mean loss: 0.011846065521240234\n",
      "Std loss: 0.004036054080649986\n",
      "Total Loss: 0.0710763931274414\n",
      "------------------------------------ epoch 5465 (32784 steps) ------------------------------------\n",
      "Max loss: 0.011894787661731243\n",
      "Min loss: 0.005713965743780136\n",
      "Mean loss: 0.00859991709391276\n",
      "Std loss: 0.002217181944670122\n",
      "Total Loss: 0.05159950256347656\n",
      "------------------------------------ epoch 5466 (32790 steps) ------------------------------------\n",
      "Max loss: 0.02781519666314125\n",
      "Min loss: 0.00637846952304244\n",
      "Mean loss: 0.013113220765565833\n",
      "Std loss: 0.007104816095374406\n",
      "Total Loss: 0.078679324593395\n",
      "------------------------------------ epoch 5467 (32796 steps) ------------------------------------\n",
      "Max loss: 0.0530414804816246\n",
      "Min loss: 0.006829723250120878\n",
      "Mean loss: 0.015805091786508758\n",
      "Std loss: 0.016672187191762486\n",
      "Total Loss: 0.09483055071905255\n",
      "------------------------------------ epoch 5468 (32802 steps) ------------------------------------\n",
      "Max loss: 0.034936752170324326\n",
      "Min loss: 0.008118227124214172\n",
      "Mean loss: 0.014349387182543675\n",
      "Std loss: 0.009272994218715906\n",
      "Total Loss: 0.08609632309526205\n",
      "------------------------------------ epoch 5469 (32808 steps) ------------------------------------\n",
      "Max loss: 0.01525479182600975\n",
      "Min loss: 0.0075257825665175915\n",
      "Mean loss: 0.012148785327250758\n",
      "Std loss: 0.002802117092181423\n",
      "Total Loss: 0.07289271196350455\n",
      "------------------------------------ epoch 5470 (32814 steps) ------------------------------------\n",
      "Max loss: 0.03216824680566788\n",
      "Min loss: 0.006895693950355053\n",
      "Mean loss: 0.01414489334759613\n",
      "Std loss: 0.00892088727024786\n",
      "Total Loss: 0.08486936008557677\n",
      "------------------------------------ epoch 5471 (32820 steps) ------------------------------------\n",
      "Max loss: 0.019382553175091743\n",
      "Min loss: 0.006658478640019894\n",
      "Mean loss: 0.011743312468752265\n",
      "Std loss: 0.004326335541962476\n",
      "Total Loss: 0.07045987481251359\n",
      "------------------------------------ epoch 5472 (32826 steps) ------------------------------------\n",
      "Max loss: 0.018710151314735413\n",
      "Min loss: 0.0064263781532645226\n",
      "Mean loss: 0.013414887711405754\n",
      "Std loss: 0.004269519419915\n",
      "Total Loss: 0.08048932626843452\n",
      "------------------------------------ epoch 5473 (32832 steps) ------------------------------------\n",
      "Max loss: 0.03372865915298462\n",
      "Min loss: 0.00531147513538599\n",
      "Mean loss: 0.010840610911448797\n",
      "Std loss: 0.010261648900236016\n",
      "Total Loss: 0.06504366546869278\n",
      "------------------------------------ epoch 5474 (32838 steps) ------------------------------------\n",
      "Max loss: 0.0283245500177145\n",
      "Min loss: 0.007656654343008995\n",
      "Mean loss: 0.015334381256252527\n",
      "Std loss: 0.008994478807702492\n",
      "Total Loss: 0.09200628753751516\n",
      "------------------------------------ epoch 5475 (32844 steps) ------------------------------------\n",
      "Max loss: 0.01554857101291418\n",
      "Min loss: 0.005933044478297234\n",
      "Mean loss: 0.009488479234278202\n",
      "Std loss: 0.0033707733386256575\n",
      "Total Loss: 0.05693087540566921\n",
      "------------------------------------ epoch 5476 (32850 steps) ------------------------------------\n",
      "Max loss: 0.021237816661596298\n",
      "Min loss: 0.004802507348358631\n",
      "Mean loss: 0.009510745372002324\n",
      "Std loss: 0.005396474254516224\n",
      "Total Loss: 0.05706447223201394\n",
      "------------------------------------ epoch 5477 (32856 steps) ------------------------------------\n",
      "Max loss: 0.013428292237222195\n",
      "Min loss: 0.004657860845327377\n",
      "Mean loss: 0.008919210095579425\n",
      "Std loss: 0.0030776011493603454\n",
      "Total Loss: 0.05351526057347655\n",
      "------------------------------------ epoch 5478 (32862 steps) ------------------------------------\n",
      "Max loss: 0.013579238206148148\n",
      "Min loss: 0.006623791065067053\n",
      "Mean loss: 0.009581341641023755\n",
      "Std loss: 0.002449060254114811\n",
      "Total Loss: 0.05748804984614253\n",
      "------------------------------------ epoch 5479 (32868 steps) ------------------------------------\n",
      "Max loss: 0.01653752103447914\n",
      "Min loss: 0.005148600786924362\n",
      "Mean loss: 0.008014900609850883\n",
      "Std loss: 0.004073974584571595\n",
      "Total Loss: 0.0480894036591053\n",
      "------------------------------------ epoch 5480 (32874 steps) ------------------------------------\n",
      "Max loss: 0.016158511862158775\n",
      "Min loss: 0.0049836901016533375\n",
      "Mean loss: 0.008907343881825605\n",
      "Std loss: 0.003968724820718441\n",
      "Total Loss: 0.053444063290953636\n",
      "------------------------------------ epoch 5481 (32880 steps) ------------------------------------\n",
      "Max loss: 0.0307078268378973\n",
      "Min loss: 0.005299112759530544\n",
      "Mean loss: 0.013623059649641315\n",
      "Std loss: 0.00870161980997967\n",
      "Total Loss: 0.08173835789784789\n",
      "------------------------------------ epoch 5482 (32886 steps) ------------------------------------\n",
      "Max loss: 0.01921827904880047\n",
      "Min loss: 0.006083815358579159\n",
      "Mean loss: 0.011825815619279942\n",
      "Std loss: 0.004935396765325598\n",
      "Total Loss: 0.07095489371567965\n",
      "------------------------------------ epoch 5483 (32892 steps) ------------------------------------\n",
      "Max loss: 0.014821032993495464\n",
      "Min loss: 0.006887264549732208\n",
      "Mean loss: 0.010724254262944063\n",
      "Std loss: 0.002788930119624951\n",
      "Total Loss: 0.06434552557766438\n",
      "------------------------------------ epoch 5484 (32898 steps) ------------------------------------\n",
      "Max loss: 0.04336228594183922\n",
      "Min loss: 0.009745225310325623\n",
      "Mean loss: 0.016993972783287365\n",
      "Std loss: 0.011855779523510572\n",
      "Total Loss: 0.1019638366997242\n",
      "------------------------------------ epoch 5485 (32904 steps) ------------------------------------\n",
      "Max loss: 0.033826958388090134\n",
      "Min loss: 0.005750216543674469\n",
      "Mean loss: 0.014153085183352232\n",
      "Std loss: 0.00966627490508791\n",
      "Total Loss: 0.08491851110011339\n",
      "------------------------------------ epoch 5486 (32910 steps) ------------------------------------\n",
      "Max loss: 0.048489272594451904\n",
      "Min loss: 0.005832440219819546\n",
      "Mean loss: 0.024341208084175985\n",
      "Std loss: 0.013300530138025382\n",
      "Total Loss: 0.1460472485050559\n",
      "------------------------------------ epoch 5487 (32916 steps) ------------------------------------\n",
      "Max loss: 0.021774008870124817\n",
      "Min loss: 0.007006741128861904\n",
      "Mean loss: 0.01244102775429686\n",
      "Std loss: 0.004732625663496874\n",
      "Total Loss: 0.07464616652578115\n",
      "------------------------------------ epoch 5488 (32922 steps) ------------------------------------\n",
      "Max loss: 0.02129589021205902\n",
      "Min loss: 0.0057594068348407745\n",
      "Mean loss: 0.011704479654630026\n",
      "Std loss: 0.005655291818815249\n",
      "Total Loss: 0.07022687792778015\n",
      "------------------------------------ epoch 5489 (32928 steps) ------------------------------------\n",
      "Max loss: 0.00968070700764656\n",
      "Min loss: 0.0065477155148983\n",
      "Mean loss: 0.008178000959257284\n",
      "Std loss: 0.0011903760148586514\n",
      "Total Loss: 0.04906800575554371\n",
      "------------------------------------ epoch 5490 (32934 steps) ------------------------------------\n",
      "Max loss: 0.02154618315398693\n",
      "Min loss: 0.00585995614528656\n",
      "Mean loss: 0.01056590179602305\n",
      "Std loss: 0.005178241026880057\n",
      "Total Loss: 0.0633954107761383\n",
      "------------------------------------ epoch 5491 (32940 steps) ------------------------------------\n",
      "Max loss: 0.013783736154437065\n",
      "Min loss: 0.0047045741230249405\n",
      "Mean loss: 0.009031448746100068\n",
      "Std loss: 0.0032318810767547985\n",
      "Total Loss: 0.05418869247660041\n",
      "------------------------------------ epoch 5492 (32946 steps) ------------------------------------\n",
      "Max loss: 0.043041061609983444\n",
      "Min loss: 0.004908386617898941\n",
      "Mean loss: 0.015826189735283453\n",
      "Std loss: 0.013154667325598607\n",
      "Total Loss: 0.09495713841170073\n",
      "------------------------------------ epoch 5493 (32952 steps) ------------------------------------\n",
      "Max loss: 0.024385739117860794\n",
      "Min loss: 0.006832540035247803\n",
      "Mean loss: 0.013206516082088152\n",
      "Std loss: 0.005919260978294834\n",
      "Total Loss: 0.07923909649252892\n",
      "------------------------------------ epoch 5494 (32958 steps) ------------------------------------\n",
      "Max loss: 0.023135002702474594\n",
      "Min loss: 0.007008385844528675\n",
      "Mean loss: 0.013146622416873774\n",
      "Std loss: 0.006772865832141804\n",
      "Total Loss: 0.07887973450124264\n",
      "------------------------------------ epoch 5495 (32964 steps) ------------------------------------\n",
      "Max loss: 0.022902537137269974\n",
      "Min loss: 0.008016250096261501\n",
      "Mean loss: 0.013791657518595457\n",
      "Std loss: 0.005763628410379084\n",
      "Total Loss: 0.08274994511157274\n",
      "------------------------------------ epoch 5496 (32970 steps) ------------------------------------\n",
      "Max loss: 0.02688251994550228\n",
      "Min loss: 0.0058942995965480804\n",
      "Mean loss: 0.011852578648055593\n",
      "Std loss: 0.007377901129384\n",
      "Total Loss: 0.07111547188833356\n",
      "------------------------------------ epoch 5497 (32976 steps) ------------------------------------\n",
      "Max loss: 0.030167382210493088\n",
      "Min loss: 0.0051250578835606575\n",
      "Mean loss: 0.01304032769985497\n",
      "Std loss: 0.008251763009093499\n",
      "Total Loss: 0.07824196619912982\n",
      "------------------------------------ epoch 5498 (32982 steps) ------------------------------------\n",
      "Max loss: 0.026915300637483597\n",
      "Min loss: 0.008165921084582806\n",
      "Mean loss: 0.012889700010418892\n",
      "Std loss: 0.0063852251400878905\n",
      "Total Loss: 0.07733820006251335\n",
      "------------------------------------ epoch 5499 (32988 steps) ------------------------------------\n",
      "Max loss: 0.01774493232369423\n",
      "Min loss: 0.006535188294947147\n",
      "Mean loss: 0.01282530122747024\n",
      "Std loss: 0.003979243755319732\n",
      "Total Loss: 0.07695180736482143\n",
      "------------------------------------ epoch 5500 (32994 steps) ------------------------------------\n",
      "Max loss: 0.0257167499512434\n",
      "Min loss: 0.006661504507064819\n",
      "Mean loss: 0.01376116539662083\n",
      "Std loss: 0.007859683212454276\n",
      "Total Loss: 0.08256699237972498\n",
      "------------------------------------ epoch 5501 (33000 steps) ------------------------------------\n",
      "Max loss: 0.011188476346433163\n",
      "Min loss: 0.005459047853946686\n",
      "Mean loss: 0.008177291912337145\n",
      "Std loss: 0.0020120313158727586\n",
      "Total Loss: 0.049063751474022865\n",
      "saved model at ./weights/model_5501.pth\n",
      "------------------------------------ epoch 5502 (33006 steps) ------------------------------------\n",
      "Max loss: 0.01891438663005829\n",
      "Min loss: 0.007631087210029364\n",
      "Mean loss: 0.012444186567639312\n",
      "Std loss: 0.004391139080016199\n",
      "Total Loss: 0.07466511940583587\n",
      "------------------------------------ epoch 5503 (33012 steps) ------------------------------------\n",
      "Max loss: 0.015776408836245537\n",
      "Min loss: 0.006054053083062172\n",
      "Mean loss: 0.010208521892006198\n",
      "Std loss: 0.0035001525327239087\n",
      "Total Loss: 0.06125113135203719\n",
      "------------------------------------ epoch 5504 (33018 steps) ------------------------------------\n",
      "Max loss: 0.01755351573228836\n",
      "Min loss: 0.005152832251042128\n",
      "Mean loss: 0.008503905885542432\n",
      "Std loss: 0.004243378268177004\n",
      "Total Loss: 0.051023435313254595\n",
      "------------------------------------ epoch 5505 (33024 steps) ------------------------------------\n",
      "Max loss: 0.01617717742919922\n",
      "Min loss: 0.005370493978261948\n",
      "Mean loss: 0.011181508811811606\n",
      "Std loss: 0.003525186884661645\n",
      "Total Loss: 0.06708905287086964\n",
      "------------------------------------ epoch 5506 (33030 steps) ------------------------------------\n",
      "Max loss: 0.018195880576968193\n",
      "Min loss: 0.005108072888106108\n",
      "Mean loss: 0.012613987162088355\n",
      "Std loss: 0.004593213279762217\n",
      "Total Loss: 0.07568392297253013\n",
      "------------------------------------ epoch 5507 (33036 steps) ------------------------------------\n",
      "Max loss: 0.01601819321513176\n",
      "Min loss: 0.006702379323542118\n",
      "Mean loss: 0.010025681151698032\n",
      "Std loss: 0.003002643606458946\n",
      "Total Loss: 0.0601540869101882\n",
      "------------------------------------ epoch 5508 (33042 steps) ------------------------------------\n",
      "Max loss: 0.008924493566155434\n",
      "Min loss: 0.006101622246205807\n",
      "Mean loss: 0.0071589311119169\n",
      "Std loss: 0.0011554240170351796\n",
      "Total Loss: 0.0429535866715014\n",
      "------------------------------------ epoch 5509 (33048 steps) ------------------------------------\n",
      "Max loss: 0.02240912616252899\n",
      "Min loss: 0.004821679554879665\n",
      "Mean loss: 0.011531559439996878\n",
      "Std loss: 0.00629947347572034\n",
      "Total Loss: 0.06918935663998127\n",
      "------------------------------------ epoch 5510 (33054 steps) ------------------------------------\n",
      "Max loss: 0.011736191809177399\n",
      "Min loss: 0.005540727637708187\n",
      "Mean loss: 0.0077104658509294195\n",
      "Std loss: 0.0021798649806198814\n",
      "Total Loss: 0.046262795105576515\n",
      "------------------------------------ epoch 5511 (33060 steps) ------------------------------------\n",
      "Max loss: 0.009881786070764065\n",
      "Min loss: 0.004916180856525898\n",
      "Mean loss: 0.007222694422428806\n",
      "Std loss: 0.0016684249791441919\n",
      "Total Loss: 0.04333616653457284\n",
      "------------------------------------ epoch 5512 (33066 steps) ------------------------------------\n",
      "Max loss: 0.02334655076265335\n",
      "Min loss: 0.005631137639284134\n",
      "Mean loss: 0.0126844416372478\n",
      "Std loss: 0.0061346872115741\n",
      "Total Loss: 0.0761066498234868\n",
      "------------------------------------ epoch 5513 (33072 steps) ------------------------------------\n",
      "Max loss: 0.02602459490299225\n",
      "Min loss: 0.003956516273319721\n",
      "Mean loss: 0.010273460376386842\n",
      "Std loss: 0.007604899506940205\n",
      "Total Loss: 0.06164076225832105\n",
      "------------------------------------ epoch 5514 (33078 steps) ------------------------------------\n",
      "Max loss: 0.021644098684191704\n",
      "Min loss: 0.004458674695342779\n",
      "Mean loss: 0.009198558361579975\n",
      "Std loss: 0.0059583035848894805\n",
      "Total Loss: 0.05519135016947985\n",
      "------------------------------------ epoch 5515 (33084 steps) ------------------------------------\n",
      "Max loss: 0.020762791857123375\n",
      "Min loss: 0.005662149749696255\n",
      "Mean loss: 0.011411207572867474\n",
      "Std loss: 0.005437645214634421\n",
      "Total Loss: 0.06846724543720484\n",
      "------------------------------------ epoch 5516 (33090 steps) ------------------------------------\n",
      "Max loss: 0.019530214369297028\n",
      "Min loss: 0.006055775098502636\n",
      "Mean loss: 0.011640024526665608\n",
      "Std loss: 0.005194462832911399\n",
      "Total Loss: 0.06984014715999365\n",
      "------------------------------------ epoch 5517 (33096 steps) ------------------------------------\n",
      "Max loss: 0.010057314299046993\n",
      "Min loss: 0.004985816776752472\n",
      "Mean loss: 0.007499733396495382\n",
      "Std loss: 0.0020981229445349724\n",
      "Total Loss: 0.04499840037897229\n",
      "------------------------------------ epoch 5518 (33102 steps) ------------------------------------\n",
      "Max loss: 0.011291079223155975\n",
      "Min loss: 0.005337459966540337\n",
      "Mean loss: 0.008084983642523488\n",
      "Std loss: 0.0022841233663752347\n",
      "Total Loss: 0.048509901855140924\n",
      "------------------------------------ epoch 5519 (33108 steps) ------------------------------------\n",
      "Max loss: 0.02442382648587227\n",
      "Min loss: 0.006317148916423321\n",
      "Mean loss: 0.012687749384591976\n",
      "Std loss: 0.007413217164891931\n",
      "Total Loss: 0.07612649630755186\n",
      "------------------------------------ epoch 5520 (33114 steps) ------------------------------------\n",
      "Max loss: 0.018718784675002098\n",
      "Min loss: 0.0087797362357378\n",
      "Mean loss: 0.012151793111115694\n",
      "Std loss: 0.0038084414783783526\n",
      "Total Loss: 0.07291075866669416\n",
      "------------------------------------ epoch 5521 (33120 steps) ------------------------------------\n",
      "Max loss: 0.013251423835754395\n",
      "Min loss: 0.007474492769688368\n",
      "Mean loss: 0.009990192639331022\n",
      "Std loss: 0.002100564253285455\n",
      "Total Loss: 0.05994115583598614\n",
      "------------------------------------ epoch 5522 (33126 steps) ------------------------------------\n",
      "Max loss: 0.014338395558297634\n",
      "Min loss: 0.005310756154358387\n",
      "Mean loss: 0.00841243495233357\n",
      "Std loss: 0.0030050416345304204\n",
      "Total Loss: 0.05047460971400142\n",
      "------------------------------------ epoch 5523 (33132 steps) ------------------------------------\n",
      "Max loss: 0.01698870025575161\n",
      "Min loss: 0.006342391949146986\n",
      "Mean loss: 0.009639563038945198\n",
      "Std loss: 0.003517406276685974\n",
      "Total Loss: 0.05783737823367119\n",
      "------------------------------------ epoch 5524 (33138 steps) ------------------------------------\n",
      "Max loss: 0.01308958139270544\n",
      "Min loss: 0.005859746597707272\n",
      "Mean loss: 0.008608525541300574\n",
      "Std loss: 0.002494649787259741\n",
      "Total Loss: 0.05165115324780345\n",
      "------------------------------------ epoch 5525 (33144 steps) ------------------------------------\n",
      "Max loss: 0.016869431361556053\n",
      "Min loss: 0.004697597585618496\n",
      "Mean loss: 0.010597108242412409\n",
      "Std loss: 0.004622771166704556\n",
      "Total Loss: 0.06358264945447445\n",
      "------------------------------------ epoch 5526 (33150 steps) ------------------------------------\n",
      "Max loss: 0.012837032787501812\n",
      "Min loss: 0.004216118715703487\n",
      "Mean loss: 0.008759394753724337\n",
      "Std loss: 0.0031777389925699263\n",
      "Total Loss: 0.05255636852234602\n",
      "------------------------------------ epoch 5527 (33156 steps) ------------------------------------\n",
      "Max loss: 0.014202674850821495\n",
      "Min loss: 0.0046904864721000195\n",
      "Mean loss: 0.007957288064062595\n",
      "Std loss: 0.003415197824938617\n",
      "Total Loss: 0.04774372838437557\n",
      "------------------------------------ epoch 5528 (33162 steps) ------------------------------------\n",
      "Max loss: 0.019243905320763588\n",
      "Min loss: 0.004542154259979725\n",
      "Mean loss: 0.010686244970808426\n",
      "Std loss: 0.005406385530640265\n",
      "Total Loss: 0.06411746982485056\n",
      "------------------------------------ epoch 5529 (33168 steps) ------------------------------------\n",
      "Max loss: 0.028144460171461105\n",
      "Min loss: 0.004874126054346561\n",
      "Mean loss: 0.011670663875217238\n",
      "Std loss: 0.00826377892880298\n",
      "Total Loss: 0.07002398325130343\n",
      "------------------------------------ epoch 5530 (33174 steps) ------------------------------------\n",
      "Max loss: 0.01521830540150404\n",
      "Min loss: 0.005126729141920805\n",
      "Mean loss: 0.010830982510621348\n",
      "Std loss: 0.0037718832397256683\n",
      "Total Loss: 0.0649858950637281\n",
      "------------------------------------ epoch 5531 (33180 steps) ------------------------------------\n",
      "Max loss: 0.010830281302332878\n",
      "Min loss: 0.005061741918325424\n",
      "Mean loss: 0.007499826528752844\n",
      "Std loss: 0.0025349429596763823\n",
      "Total Loss: 0.04499895917251706\n",
      "------------------------------------ epoch 5532 (33186 steps) ------------------------------------\n",
      "Max loss: 0.038203924894332886\n",
      "Min loss: 0.007530088536441326\n",
      "Mean loss: 0.016844061358521383\n",
      "Std loss: 0.01083016256055103\n",
      "Total Loss: 0.10106436815112829\n",
      "------------------------------------ epoch 5533 (33192 steps) ------------------------------------\n",
      "Max loss: 0.0377853624522686\n",
      "Min loss: 0.005045758560299873\n",
      "Mean loss: 0.011976317269727588\n",
      "Std loss: 0.011788385131584377\n",
      "Total Loss: 0.07185790361836553\n",
      "------------------------------------ epoch 5534 (33198 steps) ------------------------------------\n",
      "Max loss: 0.023902516812086105\n",
      "Min loss: 0.006066538393497467\n",
      "Mean loss: 0.010249692170570293\n",
      "Std loss: 0.0062707859976169015\n",
      "Total Loss: 0.061498153023421764\n",
      "------------------------------------ epoch 5535 (33204 steps) ------------------------------------\n",
      "Max loss: 0.017152976244688034\n",
      "Min loss: 0.006079667713493109\n",
      "Mean loss: 0.010606788486863175\n",
      "Std loss: 0.003614631777514294\n",
      "Total Loss: 0.06364073092117906\n",
      "------------------------------------ epoch 5536 (33210 steps) ------------------------------------\n",
      "Max loss: 0.024031003937125206\n",
      "Min loss: 0.006942687556147575\n",
      "Mean loss: 0.014195395012696585\n",
      "Std loss: 0.006091332123027137\n",
      "Total Loss: 0.0851723700761795\n",
      "------------------------------------ epoch 5537 (33216 steps) ------------------------------------\n",
      "Max loss: 0.025080136954784393\n",
      "Min loss: 0.007491298019886017\n",
      "Mean loss: 0.01446127767364184\n",
      "Std loss: 0.006657237489502564\n",
      "Total Loss: 0.08676766604185104\n",
      "------------------------------------ epoch 5538 (33222 steps) ------------------------------------\n",
      "Max loss: 0.017156032845377922\n",
      "Min loss: 0.00620592525228858\n",
      "Mean loss: 0.010252234137927493\n",
      "Std loss: 0.0033906111968857707\n",
      "Total Loss: 0.061513404827564955\n",
      "------------------------------------ epoch 5539 (33228 steps) ------------------------------------\n",
      "Max loss: 0.009429950267076492\n",
      "Min loss: 0.006275653373450041\n",
      "Mean loss: 0.007700706133618951\n",
      "Std loss: 0.0010577453816307764\n",
      "Total Loss: 0.046204236801713705\n",
      "------------------------------------ epoch 5540 (33234 steps) ------------------------------------\n",
      "Max loss: 0.017077049240469933\n",
      "Min loss: 0.005307396408170462\n",
      "Mean loss: 0.009332481383656463\n",
      "Std loss: 0.004011886177884187\n",
      "Total Loss: 0.05599488830193877\n",
      "------------------------------------ epoch 5541 (33240 steps) ------------------------------------\n",
      "Max loss: 0.02166104130446911\n",
      "Min loss: 0.005169282201677561\n",
      "Mean loss: 0.009701205495124062\n",
      "Std loss: 0.005562858987252654\n",
      "Total Loss: 0.05820723297074437\n",
      "------------------------------------ epoch 5542 (33246 steps) ------------------------------------\n",
      "Max loss: 0.008148310706019402\n",
      "Min loss: 0.0056925006210803986\n",
      "Mean loss: 0.006711758052309354\n",
      "Std loss: 0.0008267406286171731\n",
      "Total Loss: 0.040270548313856125\n",
      "------------------------------------ epoch 5543 (33252 steps) ------------------------------------\n",
      "Max loss: 0.011854872107505798\n",
      "Min loss: 0.00519299041479826\n",
      "Mean loss: 0.007896510961775979\n",
      "Std loss: 0.0026559723651216373\n",
      "Total Loss: 0.04737906577065587\n",
      "------------------------------------ epoch 5544 (33258 steps) ------------------------------------\n",
      "Max loss: 0.010525258257985115\n",
      "Min loss: 0.004844575189054012\n",
      "Mean loss: 0.008102958323433995\n",
      "Std loss: 0.0019653524710824253\n",
      "Total Loss: 0.04861774994060397\n",
      "------------------------------------ epoch 5545 (33264 steps) ------------------------------------\n",
      "Max loss: 0.01573382504284382\n",
      "Min loss: 0.004584143869578838\n",
      "Mean loss: 0.008553246657053629\n",
      "Std loss: 0.003581378249754075\n",
      "Total Loss: 0.05131947994232178\n",
      "------------------------------------ epoch 5546 (33270 steps) ------------------------------------\n",
      "Max loss: 0.014240091666579247\n",
      "Min loss: 0.007106132805347443\n",
      "Mean loss: 0.011134168443580469\n",
      "Std loss: 0.00247967885840944\n",
      "Total Loss: 0.06680501066148281\n",
      "------------------------------------ epoch 5547 (33276 steps) ------------------------------------\n",
      "Max loss: 0.027153169736266136\n",
      "Min loss: 0.0053462982177734375\n",
      "Mean loss: 0.016167160278807085\n",
      "Std loss: 0.007254539640115\n",
      "Total Loss: 0.0970029616728425\n",
      "------------------------------------ epoch 5548 (33282 steps) ------------------------------------\n",
      "Max loss: 0.022011972963809967\n",
      "Min loss: 0.007273612078279257\n",
      "Mean loss: 0.013036952121183276\n",
      "Std loss: 0.005644369737424557\n",
      "Total Loss: 0.07822171272709966\n",
      "------------------------------------ epoch 5549 (33288 steps) ------------------------------------\n",
      "Max loss: 0.023030072450637817\n",
      "Min loss: 0.00566205894574523\n",
      "Mean loss: 0.011188420544688901\n",
      "Std loss: 0.0059425015566420235\n",
      "Total Loss: 0.0671305232681334\n",
      "------------------------------------ epoch 5550 (33294 steps) ------------------------------------\n",
      "Max loss: 0.017195463180541992\n",
      "Min loss: 0.005318272393196821\n",
      "Mean loss: 0.008334603471060595\n",
      "Std loss: 0.004065256329765169\n",
      "Total Loss: 0.050007620826363564\n",
      "------------------------------------ epoch 5551 (33300 steps) ------------------------------------\n",
      "Max loss: 0.02167537435889244\n",
      "Min loss: 0.005303480662405491\n",
      "Mean loss: 0.011084128015985092\n",
      "Std loss: 0.005513067429451002\n",
      "Total Loss: 0.06650476809591055\n",
      "------------------------------------ epoch 5552 (33306 steps) ------------------------------------\n",
      "Max loss: 0.02279670350253582\n",
      "Min loss: 0.007130246609449387\n",
      "Mean loss: 0.015023684284339348\n",
      "Std loss: 0.004655296311005839\n",
      "Total Loss: 0.09014210570603609\n",
      "------------------------------------ epoch 5553 (33312 steps) ------------------------------------\n",
      "Max loss: 0.02577497810125351\n",
      "Min loss: 0.0067280372604727745\n",
      "Mean loss: 0.013335304722810784\n",
      "Std loss: 0.007509030982806924\n",
      "Total Loss: 0.08001182833686471\n",
      "------------------------------------ epoch 5554 (33318 steps) ------------------------------------\n",
      "Max loss: 0.027758507058024406\n",
      "Min loss: 0.006218963768333197\n",
      "Mean loss: 0.012086037003124753\n",
      "Std loss: 0.00742818769807368\n",
      "Total Loss: 0.07251622201874852\n",
      "------------------------------------ epoch 5555 (33324 steps) ------------------------------------\n",
      "Max loss: 0.008862048387527466\n",
      "Min loss: 0.005078618880361319\n",
      "Mean loss: 0.0068887341767549515\n",
      "Std loss: 0.0013974785152333982\n",
      "Total Loss: 0.04133240506052971\n",
      "------------------------------------ epoch 5556 (33330 steps) ------------------------------------\n",
      "Max loss: 0.010023994371294975\n",
      "Min loss: 0.005176227074116468\n",
      "Mean loss: 0.0069663018609086675\n",
      "Std loss: 0.00175626668485622\n",
      "Total Loss: 0.041797811165452003\n",
      "------------------------------------ epoch 5557 (33336 steps) ------------------------------------\n",
      "Max loss: 0.01598503440618515\n",
      "Min loss: 0.00462301867082715\n",
      "Mean loss: 0.010133258144681653\n",
      "Std loss: 0.0035523105641081906\n",
      "Total Loss: 0.060799548868089914\n",
      "------------------------------------ epoch 5558 (33342 steps) ------------------------------------\n",
      "Max loss: 0.03119794651865959\n",
      "Min loss: 0.0048919497057795525\n",
      "Mean loss: 0.011199048409859339\n",
      "Std loss: 0.009175923791072908\n",
      "Total Loss: 0.06719429045915604\n",
      "------------------------------------ epoch 5559 (33348 steps) ------------------------------------\n",
      "Max loss: 0.017832733690738678\n",
      "Min loss: 0.007413730025291443\n",
      "Mean loss: 0.011926723799357811\n",
      "Std loss: 0.003197727448982633\n",
      "Total Loss: 0.07156034279614687\n",
      "------------------------------------ epoch 5560 (33354 steps) ------------------------------------\n",
      "Max loss: 0.021442240104079247\n",
      "Min loss: 0.004229725804179907\n",
      "Mean loss: 0.010097659348199764\n",
      "Std loss: 0.005910861722637635\n",
      "Total Loss: 0.06058595608919859\n",
      "------------------------------------ epoch 5561 (33360 steps) ------------------------------------\n",
      "Max loss: 0.031439900398254395\n",
      "Min loss: 0.004865817725658417\n",
      "Mean loss: 0.010670974384993315\n",
      "Std loss: 0.009369532248050448\n",
      "Total Loss: 0.06402584630995989\n",
      "------------------------------------ epoch 5562 (33366 steps) ------------------------------------\n",
      "Max loss: 0.012263323180377483\n",
      "Min loss: 0.004860063083469868\n",
      "Mean loss: 0.006981783701727788\n",
      "Std loss: 0.002546659134998978\n",
      "Total Loss: 0.041890702210366726\n",
      "------------------------------------ epoch 5563 (33372 steps) ------------------------------------\n",
      "Max loss: 0.02290942333638668\n",
      "Min loss: 0.006628838833421469\n",
      "Mean loss: 0.011469509452581406\n",
      "Std loss: 0.0055128014865733635\n",
      "Total Loss: 0.06881705671548843\n",
      "------------------------------------ epoch 5564 (33378 steps) ------------------------------------\n",
      "Max loss: 0.016099048778414726\n",
      "Min loss: 0.0052075572311878204\n",
      "Mean loss: 0.009205310450245937\n",
      "Std loss: 0.0035971253356996244\n",
      "Total Loss: 0.05523186270147562\n",
      "------------------------------------ epoch 5565 (33384 steps) ------------------------------------\n",
      "Max loss: 0.009406762197613716\n",
      "Min loss: 0.007160404697060585\n",
      "Mean loss: 0.008018583292141557\n",
      "Std loss: 0.0007762815761290771\n",
      "Total Loss: 0.04811149975284934\n",
      "------------------------------------ epoch 5566 (33390 steps) ------------------------------------\n",
      "Max loss: 0.020322270691394806\n",
      "Min loss: 0.0052981069311499596\n",
      "Mean loss: 0.00878326982880632\n",
      "Std loss: 0.005258327882833837\n",
      "Total Loss: 0.052699618972837925\n",
      "------------------------------------ epoch 5567 (33396 steps) ------------------------------------\n",
      "Max loss: 0.018334142863750458\n",
      "Min loss: 0.006255341228097677\n",
      "Mean loss: 0.011816331806282202\n",
      "Std loss: 0.004231796275907266\n",
      "Total Loss: 0.07089799083769321\n",
      "------------------------------------ epoch 5568 (33402 steps) ------------------------------------\n",
      "Max loss: 0.012656922452151775\n",
      "Min loss: 0.0055641895160079\n",
      "Mean loss: 0.009695690590888262\n",
      "Std loss: 0.002709442373583322\n",
      "Total Loss: 0.05817414354532957\n",
      "------------------------------------ epoch 5569 (33408 steps) ------------------------------------\n",
      "Max loss: 0.008148185908794403\n",
      "Min loss: 0.005271765403449535\n",
      "Mean loss: 0.0069454949504385395\n",
      "Std loss: 0.0010056049760374037\n",
      "Total Loss: 0.041672969702631235\n",
      "------------------------------------ epoch 5570 (33414 steps) ------------------------------------\n",
      "Max loss: 0.029409822076559067\n",
      "Min loss: 0.004282969981431961\n",
      "Mean loss: 0.011241013494630655\n",
      "Std loss: 0.008768733847550999\n",
      "Total Loss: 0.06744608096778393\n",
      "------------------------------------ epoch 5571 (33420 steps) ------------------------------------\n",
      "Max loss: 0.012861134484410286\n",
      "Min loss: 0.006625186651945114\n",
      "Mean loss: 0.00889138737693429\n",
      "Std loss: 0.002210773254429641\n",
      "Total Loss: 0.05334832426160574\n",
      "------------------------------------ epoch 5572 (33426 steps) ------------------------------------\n",
      "Max loss: 0.014728628098964691\n",
      "Min loss: 0.00643577566370368\n",
      "Mean loss: 0.009138540675242742\n",
      "Std loss: 0.002760948004185799\n",
      "Total Loss: 0.05483124405145645\n",
      "------------------------------------ epoch 5573 (33432 steps) ------------------------------------\n",
      "Max loss: 0.016570398584008217\n",
      "Min loss: 0.005242313724011183\n",
      "Mean loss: 0.009569454782952866\n",
      "Std loss: 0.004648923712800484\n",
      "Total Loss: 0.05741672869771719\n",
      "------------------------------------ epoch 5574 (33438 steps) ------------------------------------\n",
      "Max loss: 0.021471096202731133\n",
      "Min loss: 0.005205997731536627\n",
      "Mean loss: 0.009553997466961542\n",
      "Std loss: 0.005445561536163548\n",
      "Total Loss: 0.05732398480176926\n",
      "------------------------------------ epoch 5575 (33444 steps) ------------------------------------\n",
      "Max loss: 0.023701239377260208\n",
      "Min loss: 0.004864337854087353\n",
      "Mean loss: 0.009891644585877657\n",
      "Std loss: 0.006590652408092527\n",
      "Total Loss: 0.05934986751526594\n",
      "------------------------------------ epoch 5576 (33450 steps) ------------------------------------\n",
      "Max loss: 0.017295479774475098\n",
      "Min loss: 0.006334263365715742\n",
      "Mean loss: 0.009923029458150268\n",
      "Std loss: 0.003503590168258634\n",
      "Total Loss: 0.059538176748901606\n",
      "------------------------------------ epoch 5577 (33456 steps) ------------------------------------\n",
      "Max loss: 0.013257305137813091\n",
      "Min loss: 0.004869924392551184\n",
      "Mean loss: 0.00890060894501706\n",
      "Std loss: 0.00305016920502426\n",
      "Total Loss: 0.05340365367010236\n",
      "------------------------------------ epoch 5578 (33462 steps) ------------------------------------\n",
      "Max loss: 0.022299256175756454\n",
      "Min loss: 0.004416944459080696\n",
      "Mean loss: 0.011686559844141206\n",
      "Std loss: 0.006073750027580175\n",
      "Total Loss: 0.07011935906484723\n",
      "------------------------------------ epoch 5579 (33468 steps) ------------------------------------\n",
      "Max loss: 0.02849460393190384\n",
      "Min loss: 0.004304376896470785\n",
      "Mean loss: 0.01090939676699539\n",
      "Std loss: 0.008046900235681536\n",
      "Total Loss: 0.06545638060197234\n",
      "------------------------------------ epoch 5580 (33474 steps) ------------------------------------\n",
      "Max loss: 0.010775658302009106\n",
      "Min loss: 0.004968141671270132\n",
      "Mean loss: 0.007594874051089088\n",
      "Std loss: 0.002259570440653017\n",
      "Total Loss: 0.04556924430653453\n",
      "------------------------------------ epoch 5581 (33480 steps) ------------------------------------\n",
      "Max loss: 0.0177004411816597\n",
      "Min loss: 0.007755562663078308\n",
      "Mean loss: 0.012102541668961445\n",
      "Std loss: 0.002975534783745541\n",
      "Total Loss: 0.07261525001376867\n",
      "------------------------------------ epoch 5582 (33486 steps) ------------------------------------\n",
      "Max loss: 0.014339810237288475\n",
      "Min loss: 0.0051687913946807384\n",
      "Mean loss: 0.008372091532995304\n",
      "Std loss: 0.0031538731894492963\n",
      "Total Loss: 0.05023254919797182\n",
      "------------------------------------ epoch 5583 (33492 steps) ------------------------------------\n",
      "Max loss: 0.0235738568007946\n",
      "Min loss: 0.005927001126110554\n",
      "Mean loss: 0.010933219765623411\n",
      "Std loss: 0.006875860284794047\n",
      "Total Loss: 0.06559931859374046\n",
      "------------------------------------ epoch 5584 (33498 steps) ------------------------------------\n",
      "Max loss: 0.017191089689731598\n",
      "Min loss: 0.006122206337749958\n",
      "Mean loss: 0.010582577902823687\n",
      "Std loss: 0.0045922972441621345\n",
      "Total Loss: 0.06349546741694212\n",
      "------------------------------------ epoch 5585 (33504 steps) ------------------------------------\n",
      "Max loss: 0.021269500255584717\n",
      "Min loss: 0.005135043524205685\n",
      "Mean loss: 0.010945970347772041\n",
      "Std loss: 0.005941253969482185\n",
      "Total Loss: 0.06567582208663225\n",
      "------------------------------------ epoch 5586 (33510 steps) ------------------------------------\n",
      "Max loss: 0.017231782898306847\n",
      "Min loss: 0.0058114915154874325\n",
      "Mean loss: 0.009774677688255906\n",
      "Std loss: 0.0038114692491102563\n",
      "Total Loss: 0.05864806612953544\n",
      "------------------------------------ epoch 5587 (33516 steps) ------------------------------------\n",
      "Max loss: 0.033887214958667755\n",
      "Min loss: 0.004983509425073862\n",
      "Mean loss: 0.01178164539548258\n",
      "Std loss: 0.010109119395371593\n",
      "Total Loss: 0.07068987237289548\n",
      "------------------------------------ epoch 5588 (33522 steps) ------------------------------------\n",
      "Max loss: 0.03509732335805893\n",
      "Min loss: 0.005002579186111689\n",
      "Mean loss: 0.01488199271261692\n",
      "Std loss: 0.012330677086960408\n",
      "Total Loss: 0.08929195627570152\n",
      "------------------------------------ epoch 5589 (33528 steps) ------------------------------------\n",
      "Max loss: 0.02457687258720398\n",
      "Min loss: 0.005656796507537365\n",
      "Mean loss: 0.012074669823050499\n",
      "Std loss: 0.006629363111301689\n",
      "Total Loss: 0.072448018938303\n",
      "------------------------------------ epoch 5590 (33534 steps) ------------------------------------\n",
      "Max loss: 0.036809153854846954\n",
      "Min loss: 0.005595075897872448\n",
      "Mean loss: 0.016850143826256197\n",
      "Std loss: 0.01018520797458136\n",
      "Total Loss: 0.10110086295753717\n",
      "------------------------------------ epoch 5591 (33540 steps) ------------------------------------\n",
      "Max loss: 0.015971167013049126\n",
      "Min loss: 0.006978700868785381\n",
      "Mean loss: 0.011055047313372294\n",
      "Std loss: 0.0027503556932504704\n",
      "Total Loss: 0.06633028388023376\n",
      "------------------------------------ epoch 5592 (33546 steps) ------------------------------------\n",
      "Max loss: 0.03080226480960846\n",
      "Min loss: 0.006146909203380346\n",
      "Mean loss: 0.012218301572526494\n",
      "Std loss: 0.008443547378970918\n",
      "Total Loss: 0.07330980943515897\n",
      "------------------------------------ epoch 5593 (33552 steps) ------------------------------------\n",
      "Max loss: 0.07863935828208923\n",
      "Min loss: 0.007225135341286659\n",
      "Mean loss: 0.02256002277135849\n",
      "Std loss: 0.02538102770943836\n",
      "Total Loss: 0.13536013662815094\n",
      "------------------------------------ epoch 5594 (33558 steps) ------------------------------------\n",
      "Max loss: 0.04813583195209503\n",
      "Min loss: 0.00774042122066021\n",
      "Mean loss: 0.01730231133600076\n",
      "Std loss: 0.014615005041972015\n",
      "Total Loss: 0.10381386801600456\n",
      "------------------------------------ epoch 5595 (33564 steps) ------------------------------------\n",
      "Max loss: 0.027370601892471313\n",
      "Min loss: 0.009579738602042198\n",
      "Mean loss: 0.016609779403855402\n",
      "Std loss: 0.006931456650812412\n",
      "Total Loss: 0.09965867642313242\n",
      "------------------------------------ epoch 5596 (33570 steps) ------------------------------------\n",
      "Max loss: 0.022036157548427582\n",
      "Min loss: 0.006519315764307976\n",
      "Mean loss: 0.012769767548888922\n",
      "Std loss: 0.00478910178440064\n",
      "Total Loss: 0.07661860529333353\n",
      "------------------------------------ epoch 5597 (33576 steps) ------------------------------------\n",
      "Max loss: 0.028009383007884026\n",
      "Min loss: 0.009708613157272339\n",
      "Mean loss: 0.016085765479753416\n",
      "Std loss: 0.006852413538423564\n",
      "Total Loss: 0.09651459287852049\n",
      "------------------------------------ epoch 5598 (33582 steps) ------------------------------------\n",
      "Max loss: 0.015292266383767128\n",
      "Min loss: 0.008589142933487892\n",
      "Mean loss: 0.011746634729206562\n",
      "Std loss: 0.0020985235824555895\n",
      "Total Loss: 0.07047980837523937\n",
      "------------------------------------ epoch 5599 (33588 steps) ------------------------------------\n",
      "Max loss: 0.01882467046380043\n",
      "Min loss: 0.005829883273690939\n",
      "Mean loss: 0.011143878723184267\n",
      "Std loss: 0.004450078612343803\n",
      "Total Loss: 0.0668632723391056\n",
      "------------------------------------ epoch 5600 (33594 steps) ------------------------------------\n",
      "Max loss: 0.01853817328810692\n",
      "Min loss: 0.005092652514576912\n",
      "Mean loss: 0.01169211456241707\n",
      "Std loss: 0.005275225481646996\n",
      "Total Loss: 0.07015268737450242\n",
      "------------------------------------ epoch 5601 (33600 steps) ------------------------------------\n",
      "Max loss: 0.01510228868573904\n",
      "Min loss: 0.0054673245176672935\n",
      "Mean loss: 0.008606142364442348\n",
      "Std loss: 0.0031279105365557315\n",
      "Total Loss: 0.05163685418665409\n",
      "saved model at ./weights/model_5601.pth\n",
      "------------------------------------ epoch 5602 (33606 steps) ------------------------------------\n",
      "Max loss: 0.025630047544836998\n",
      "Min loss: 0.005818579345941544\n",
      "Mean loss: 0.013270500116050243\n",
      "Std loss: 0.006956194975751938\n",
      "Total Loss: 0.07962300069630146\n",
      "------------------------------------ epoch 5603 (33612 steps) ------------------------------------\n",
      "Max loss: 0.013194961473345757\n",
      "Min loss: 0.005244439467787743\n",
      "Mean loss: 0.007835105837633213\n",
      "Std loss: 0.002845341817234158\n",
      "Total Loss: 0.047010635025799274\n",
      "------------------------------------ epoch 5604 (33618 steps) ------------------------------------\n",
      "Max loss: 0.01382412575185299\n",
      "Min loss: 0.005839842837303877\n",
      "Mean loss: 0.009467821257809797\n",
      "Std loss: 0.0030272387503414125\n",
      "Total Loss: 0.05680692754685879\n",
      "------------------------------------ epoch 5605 (33624 steps) ------------------------------------\n",
      "Max loss: 0.04373445361852646\n",
      "Min loss: 0.005766850896179676\n",
      "Mean loss: 0.01717561824868123\n",
      "Std loss: 0.012850282595314658\n",
      "Total Loss: 0.10305370949208736\n",
      "------------------------------------ epoch 5606 (33630 steps) ------------------------------------\n",
      "Max loss: 0.0481022410094738\n",
      "Min loss: 0.004815536551177502\n",
      "Mean loss: 0.01506835587012271\n",
      "Std loss: 0.015229458284108438\n",
      "Total Loss: 0.09041013522073627\n",
      "------------------------------------ epoch 5607 (33636 steps) ------------------------------------\n",
      "Max loss: 0.029165145009756088\n",
      "Min loss: 0.008130042813718319\n",
      "Mean loss: 0.012750192079693079\n",
      "Std loss: 0.0074866650995352\n",
      "Total Loss: 0.07650115247815847\n",
      "------------------------------------ epoch 5608 (33642 steps) ------------------------------------\n",
      "Max loss: 0.018238935619592667\n",
      "Min loss: 0.008098950609564781\n",
      "Mean loss: 0.012728980431954065\n",
      "Std loss: 0.003977825974198178\n",
      "Total Loss: 0.0763738825917244\n",
      "------------------------------------ epoch 5609 (33648 steps) ------------------------------------\n",
      "Max loss: 0.017946718260645866\n",
      "Min loss: 0.005969170480966568\n",
      "Mean loss: 0.008653120758632818\n",
      "Std loss: 0.00419750636007117\n",
      "Total Loss: 0.05191872455179691\n",
      "------------------------------------ epoch 5610 (33654 steps) ------------------------------------\n",
      "Max loss: 0.017973914742469788\n",
      "Min loss: 0.0054810307919979095\n",
      "Mean loss: 0.009283104368175069\n",
      "Std loss: 0.004305745439076368\n",
      "Total Loss: 0.05569862620905042\n",
      "------------------------------------ epoch 5611 (33660 steps) ------------------------------------\n",
      "Max loss: 0.01714123785495758\n",
      "Min loss: 0.006093952804803848\n",
      "Mean loss: 0.01000705462259551\n",
      "Std loss: 0.0043654593855510905\n",
      "Total Loss: 0.06004232773557305\n",
      "------------------------------------ epoch 5612 (33666 steps) ------------------------------------\n",
      "Max loss: 0.020458268001675606\n",
      "Min loss: 0.008907961659133434\n",
      "Mean loss: 0.01414259352410833\n",
      "Std loss: 0.0040610933667250245\n",
      "Total Loss: 0.08485556114464998\n",
      "------------------------------------ epoch 5613 (33672 steps) ------------------------------------\n",
      "Max loss: 0.00968700647354126\n",
      "Min loss: 0.004724775440990925\n",
      "Mean loss: 0.006502802716568112\n",
      "Std loss: 0.0015915033542460472\n",
      "Total Loss: 0.039016816299408674\n",
      "------------------------------------ epoch 5614 (33678 steps) ------------------------------------\n",
      "Max loss: 0.023110797628760338\n",
      "Min loss: 0.00504380464553833\n",
      "Mean loss: 0.010651385644450784\n",
      "Std loss: 0.0061378514497689805\n",
      "Total Loss: 0.0639083138667047\n",
      "------------------------------------ epoch 5615 (33684 steps) ------------------------------------\n",
      "Max loss: 0.00880440603941679\n",
      "Min loss: 0.005964822601526976\n",
      "Mean loss: 0.007506844975675146\n",
      "Std loss: 0.0008293756584344635\n",
      "Total Loss: 0.045041069854050875\n",
      "------------------------------------ epoch 5616 (33690 steps) ------------------------------------\n",
      "Max loss: 0.019442271441221237\n",
      "Min loss: 0.005103908479213715\n",
      "Mean loss: 0.009138147036234537\n",
      "Std loss: 0.004922968660513908\n",
      "Total Loss: 0.05482888221740723\n",
      "------------------------------------ epoch 5617 (33696 steps) ------------------------------------\n",
      "Max loss: 0.008786242455244064\n",
      "Min loss: 0.0052528358064591885\n",
      "Mean loss: 0.006364360374088089\n",
      "Std loss: 0.0013237258426230841\n",
      "Total Loss: 0.03818616224452853\n",
      "------------------------------------ epoch 5618 (33702 steps) ------------------------------------\n",
      "Max loss: 0.008343052119016647\n",
      "Min loss: 0.0057500191032886505\n",
      "Mean loss: 0.0067535526274393005\n",
      "Std loss: 0.001025549797089347\n",
      "Total Loss: 0.0405213157646358\n",
      "------------------------------------ epoch 5619 (33708 steps) ------------------------------------\n",
      "Max loss: 0.009934665635228157\n",
      "Min loss: 0.0051905494183301926\n",
      "Mean loss: 0.006833644506211082\n",
      "Std loss: 0.0019022841804011649\n",
      "Total Loss: 0.04100186703726649\n",
      "------------------------------------ epoch 5620 (33714 steps) ------------------------------------\n",
      "Max loss: 0.016410093754529953\n",
      "Min loss: 0.0044927136041224\n",
      "Mean loss: 0.008329951437190175\n",
      "Std loss: 0.0038039871858503603\n",
      "Total Loss: 0.04997970862314105\n",
      "------------------------------------ epoch 5621 (33720 steps) ------------------------------------\n",
      "Max loss: 0.027067426592111588\n",
      "Min loss: 0.004101125057786703\n",
      "Mean loss: 0.010078304136792818\n",
      "Std loss: 0.008141496545769265\n",
      "Total Loss: 0.06046982482075691\n",
      "------------------------------------ epoch 5622 (33726 steps) ------------------------------------\n",
      "Max loss: 0.016342096030712128\n",
      "Min loss: 0.005790849216282368\n",
      "Mean loss: 0.009555943310260773\n",
      "Std loss: 0.003261853123675492\n",
      "Total Loss: 0.057335659861564636\n",
      "------------------------------------ epoch 5623 (33732 steps) ------------------------------------\n",
      "Max loss: 0.014534858986735344\n",
      "Min loss: 0.006030600517988205\n",
      "Mean loss: 0.009768173874666294\n",
      "Std loss: 0.0026222102443820843\n",
      "Total Loss: 0.05860904324799776\n",
      "------------------------------------ epoch 5624 (33738 steps) ------------------------------------\n",
      "Max loss: 0.013502281159162521\n",
      "Min loss: 0.004624770488590002\n",
      "Mean loss: 0.00822568490790824\n",
      "Std loss: 0.0031541560830802545\n",
      "Total Loss: 0.049354109447449446\n",
      "------------------------------------ epoch 5625 (33744 steps) ------------------------------------\n",
      "Max loss: 0.02079726941883564\n",
      "Min loss: 0.006450779736042023\n",
      "Mean loss: 0.010914551870276531\n",
      "Std loss: 0.004948343272575857\n",
      "Total Loss: 0.06548731122165918\n",
      "------------------------------------ epoch 5626 (33750 steps) ------------------------------------\n",
      "Max loss: 0.026255538687109947\n",
      "Min loss: 0.0047008260153234005\n",
      "Mean loss: 0.010005511809140444\n",
      "Std loss: 0.007559038407393257\n",
      "Total Loss: 0.06003307085484266\n",
      "------------------------------------ epoch 5627 (33756 steps) ------------------------------------\n",
      "Max loss: 0.029627490788698196\n",
      "Min loss: 0.010545100085437298\n",
      "Mean loss: 0.016930280874172848\n",
      "Std loss: 0.007958192657460799\n",
      "Total Loss: 0.10158168524503708\n",
      "------------------------------------ epoch 5628 (33762 steps) ------------------------------------\n",
      "Max loss: 0.02649712935090065\n",
      "Min loss: 0.006077518220990896\n",
      "Mean loss: 0.015011254465207458\n",
      "Std loss: 0.006667037101708672\n",
      "Total Loss: 0.09006752679124475\n",
      "------------------------------------ epoch 5629 (33768 steps) ------------------------------------\n",
      "Max loss: 0.014090456068515778\n",
      "Min loss: 0.006509138271212578\n",
      "Mean loss: 0.009485729349156221\n",
      "Std loss: 0.002710889462091364\n",
      "Total Loss: 0.056914376094937325\n",
      "------------------------------------ epoch 5630 (33774 steps) ------------------------------------\n",
      "Max loss: 0.08920097351074219\n",
      "Min loss: 0.006015662103891373\n",
      "Mean loss: 0.026318080723285675\n",
      "Std loss: 0.029964215130584792\n",
      "Total Loss: 0.15790848433971405\n",
      "------------------------------------ epoch 5631 (33780 steps) ------------------------------------\n",
      "Max loss: 0.04990819841623306\n",
      "Min loss: 0.0075332704000175\n",
      "Mean loss: 0.017722027221073706\n",
      "Std loss: 0.01466674622887654\n",
      "Total Loss: 0.10633216332644224\n",
      "------------------------------------ epoch 5632 (33786 steps) ------------------------------------\n",
      "Max loss: 0.020063748583197594\n",
      "Min loss: 0.007714058738201857\n",
      "Mean loss: 0.012674501709019145\n",
      "Std loss: 0.004072803953528257\n",
      "Total Loss: 0.07604701025411487\n",
      "------------------------------------ epoch 5633 (33792 steps) ------------------------------------\n",
      "Max loss: 0.02227793261408806\n",
      "Min loss: 0.009190835058689117\n",
      "Mean loss: 0.013347300545622906\n",
      "Std loss: 0.004679924161573707\n",
      "Total Loss: 0.08008380327373743\n",
      "------------------------------------ epoch 5634 (33798 steps) ------------------------------------\n",
      "Max loss: 0.02198890410363674\n",
      "Min loss: 0.006393548101186752\n",
      "Mean loss: 0.009984093718230724\n",
      "Std loss: 0.005501999414855735\n",
      "Total Loss: 0.059904562309384346\n",
      "------------------------------------ epoch 5635 (33804 steps) ------------------------------------\n",
      "Max loss: 0.04661850258708\n",
      "Min loss: 0.006318775471299887\n",
      "Mean loss: 0.016449448419734836\n",
      "Std loss: 0.01384082513990213\n",
      "Total Loss: 0.09869669051840901\n",
      "------------------------------------ epoch 5636 (33810 steps) ------------------------------------\n",
      "Max loss: 0.034698035567998886\n",
      "Min loss: 0.006213670130819082\n",
      "Mean loss: 0.014633990125730634\n",
      "Std loss: 0.010413640183334594\n",
      "Total Loss: 0.0878039407543838\n",
      "------------------------------------ epoch 5637 (33816 steps) ------------------------------------\n",
      "Max loss: 0.02910497784614563\n",
      "Min loss: 0.005284246988594532\n",
      "Mean loss: 0.012808858727415403\n",
      "Std loss: 0.007900794552897149\n",
      "Total Loss: 0.07685315236449242\n",
      "------------------------------------ epoch 5638 (33822 steps) ------------------------------------\n",
      "Max loss: 0.016827648505568504\n",
      "Min loss: 0.007569852285087109\n",
      "Mean loss: 0.011033586692065\n",
      "Std loss: 0.0037650522144815537\n",
      "Total Loss: 0.06620152015239\n",
      "------------------------------------ epoch 5639 (33828 steps) ------------------------------------\n",
      "Max loss: 0.015466663055121899\n",
      "Min loss: 0.008358217775821686\n",
      "Mean loss: 0.010744496869544188\n",
      "Std loss: 0.0022601490987032265\n",
      "Total Loss: 0.06446698121726513\n",
      "------------------------------------ epoch 5640 (33834 steps) ------------------------------------\n",
      "Max loss: 0.024651970714330673\n",
      "Min loss: 0.010995840653777122\n",
      "Mean loss: 0.015315753407776356\n",
      "Std loss: 0.00484571855427476\n",
      "Total Loss: 0.09189452044665813\n",
      "------------------------------------ epoch 5641 (33840 steps) ------------------------------------\n",
      "Max loss: 0.017388146370649338\n",
      "Min loss: 0.005363951437175274\n",
      "Mean loss: 0.009685411350801587\n",
      "Std loss: 0.004295840517759234\n",
      "Total Loss: 0.05811246810480952\n",
      "------------------------------------ epoch 5642 (33846 steps) ------------------------------------\n",
      "Max loss: 0.00809294544160366\n",
      "Min loss: 0.005940768402069807\n",
      "Mean loss: 0.006825152939806382\n",
      "Std loss: 0.0007902118190807321\n",
      "Total Loss: 0.04095091763883829\n",
      "------------------------------------ epoch 5643 (33852 steps) ------------------------------------\n",
      "Max loss: 0.02685416303575039\n",
      "Min loss: 0.004934642463922501\n",
      "Mean loss: 0.01014057508048912\n",
      "Std loss: 0.007544178159077295\n",
      "Total Loss: 0.06084345048293471\n",
      "------------------------------------ epoch 5644 (33858 steps) ------------------------------------\n",
      "Max loss: 0.00933925062417984\n",
      "Min loss: 0.005387168377637863\n",
      "Mean loss: 0.00799586127201716\n",
      "Std loss: 0.0013773717405125212\n",
      "Total Loss: 0.047975167632102966\n",
      "------------------------------------ epoch 5645 (33864 steps) ------------------------------------\n",
      "Max loss: 0.032142363488674164\n",
      "Min loss: 0.006899518426507711\n",
      "Mean loss: 0.01576281595043838\n",
      "Std loss: 0.008974545063101325\n",
      "Total Loss: 0.09457689570263028\n",
      "------------------------------------ epoch 5646 (33870 steps) ------------------------------------\n",
      "Max loss: 0.011156395077705383\n",
      "Min loss: 0.0067262123338878155\n",
      "Mean loss: 0.009463202751552066\n",
      "Std loss: 0.0013734676150837664\n",
      "Total Loss: 0.05677921650931239\n",
      "------------------------------------ epoch 5647 (33876 steps) ------------------------------------\n",
      "Max loss: 0.024065718054771423\n",
      "Min loss: 0.005202615167945623\n",
      "Mean loss: 0.011017543263733387\n",
      "Std loss: 0.007036665757624922\n",
      "Total Loss: 0.06610525958240032\n",
      "------------------------------------ epoch 5648 (33882 steps) ------------------------------------\n",
      "Max loss: 0.013512453995645046\n",
      "Min loss: 0.0069245449267327785\n",
      "Mean loss: 0.008825495218237242\n",
      "Std loss: 0.002293701753042183\n",
      "Total Loss: 0.05295297130942345\n",
      "------------------------------------ epoch 5649 (33888 steps) ------------------------------------\n",
      "Max loss: 0.029021352529525757\n",
      "Min loss: 0.006724880542606115\n",
      "Mean loss: 0.014589492930099368\n",
      "Std loss: 0.0074737067615670925\n",
      "Total Loss: 0.08753695758059621\n",
      "------------------------------------ epoch 5650 (33894 steps) ------------------------------------\n",
      "Max loss: 0.033040206879377365\n",
      "Min loss: 0.00494029838591814\n",
      "Mean loss: 0.014208734966814518\n",
      "Std loss: 0.009150922466423158\n",
      "Total Loss: 0.08525240980088711\n",
      "------------------------------------ epoch 5651 (33900 steps) ------------------------------------\n",
      "Max loss: 0.025567403063178062\n",
      "Min loss: 0.007277352269738913\n",
      "Mean loss: 0.013459812306488553\n",
      "Std loss: 0.006498522555314833\n",
      "Total Loss: 0.08075887383893132\n",
      "------------------------------------ epoch 5652 (33906 steps) ------------------------------------\n",
      "Max loss: 0.013047541491687298\n",
      "Min loss: 0.0050578792579472065\n",
      "Mean loss: 0.008574783569201827\n",
      "Std loss: 0.002952532453097889\n",
      "Total Loss: 0.05144870141521096\n",
      "------------------------------------ epoch 5653 (33912 steps) ------------------------------------\n",
      "Max loss: 0.014813023619353771\n",
      "Min loss: 0.006326145492494106\n",
      "Mean loss: 0.010206903330981731\n",
      "Std loss: 0.0030683071398289034\n",
      "Total Loss: 0.06124141998589039\n",
      "------------------------------------ epoch 5654 (33918 steps) ------------------------------------\n",
      "Max loss: 0.04588145762681961\n",
      "Min loss: 0.00499846413731575\n",
      "Mean loss: 0.013839013098428646\n",
      "Std loss: 0.014563950761088617\n",
      "Total Loss: 0.08303407859057188\n",
      "------------------------------------ epoch 5655 (33924 steps) ------------------------------------\n",
      "Max loss: 0.017071997746825218\n",
      "Min loss: 0.007518361788243055\n",
      "Mean loss: 0.010819917311891913\n",
      "Std loss: 0.0031487442866490476\n",
      "Total Loss: 0.06491950387135148\n",
      "------------------------------------ epoch 5656 (33930 steps) ------------------------------------\n",
      "Max loss: 0.017316093668341637\n",
      "Min loss: 0.0064299399964511395\n",
      "Mean loss: 0.011160942182565728\n",
      "Std loss: 0.004599733815943437\n",
      "Total Loss: 0.06696565309539437\n",
      "------------------------------------ epoch 5657 (33936 steps) ------------------------------------\n",
      "Max loss: 0.03614170104265213\n",
      "Min loss: 0.005340658128261566\n",
      "Mean loss: 0.013271763843173781\n",
      "Std loss: 0.010615595374342627\n",
      "Total Loss: 0.07963058305904269\n",
      "------------------------------------ epoch 5658 (33942 steps) ------------------------------------\n",
      "Max loss: 0.046698328107595444\n",
      "Min loss: 0.00533329276368022\n",
      "Mean loss: 0.01950666254075865\n",
      "Std loss: 0.01585646539627933\n",
      "Total Loss: 0.1170399752445519\n",
      "------------------------------------ epoch 5659 (33948 steps) ------------------------------------\n",
      "Max loss: 0.015526949428021908\n",
      "Min loss: 0.00694207614287734\n",
      "Mean loss: 0.010412382815654079\n",
      "Std loss: 0.0027887286598460457\n",
      "Total Loss: 0.062474296893924475\n",
      "------------------------------------ epoch 5660 (33954 steps) ------------------------------------\n",
      "Max loss: 0.015648841857910156\n",
      "Min loss: 0.007766725029796362\n",
      "Mean loss: 0.011299796169623733\n",
      "Std loss: 0.003106576769728309\n",
      "Total Loss: 0.0677987770177424\n",
      "------------------------------------ epoch 5661 (33960 steps) ------------------------------------\n",
      "Max loss: 0.008051903918385506\n",
      "Min loss: 0.005062371492385864\n",
      "Mean loss: 0.006614473648369312\n",
      "Std loss: 0.0009816195113104378\n",
      "Total Loss: 0.039686841890215874\n",
      "------------------------------------ epoch 5662 (33966 steps) ------------------------------------\n",
      "Max loss: 0.016346942633390427\n",
      "Min loss: 0.004715099930763245\n",
      "Mean loss: 0.01021778560243547\n",
      "Std loss: 0.0045064542561377915\n",
      "Total Loss: 0.06130671361461282\n",
      "------------------------------------ epoch 5663 (33972 steps) ------------------------------------\n",
      "Max loss: 0.03421282768249512\n",
      "Min loss: 0.006084105931222439\n",
      "Mean loss: 0.014954230360065898\n",
      "Std loss: 0.009396500199936996\n",
      "Total Loss: 0.08972538216039538\n",
      "------------------------------------ epoch 5664 (33978 steps) ------------------------------------\n",
      "Max loss: 0.009794943034648895\n",
      "Min loss: 0.005313688423484564\n",
      "Mean loss: 0.007032699106882016\n",
      "Std loss: 0.0015190021056891952\n",
      "Total Loss: 0.042196194641292095\n",
      "------------------------------------ epoch 5665 (33984 steps) ------------------------------------\n",
      "Max loss: 0.01238219439983368\n",
      "Min loss: 0.0057701594196259975\n",
      "Mean loss: 0.00909537038144966\n",
      "Std loss: 0.002598831063208864\n",
      "Total Loss: 0.05457222228869796\n",
      "------------------------------------ epoch 5666 (33990 steps) ------------------------------------\n",
      "Max loss: 0.009542395360767841\n",
      "Min loss: 0.0051977140828967094\n",
      "Mean loss: 0.0074697314606358605\n",
      "Std loss: 0.0016919005913443239\n",
      "Total Loss: 0.044818388763815165\n",
      "------------------------------------ epoch 5667 (33996 steps) ------------------------------------\n",
      "Max loss: 0.03413383662700653\n",
      "Min loss: 0.011612228117883205\n",
      "Mean loss: 0.024122478285183508\n",
      "Std loss: 0.008549986674635223\n",
      "Total Loss: 0.14473486971110106\n",
      "------------------------------------ epoch 5668 (34002 steps) ------------------------------------\n",
      "Max loss: 0.02763703092932701\n",
      "Min loss: 0.005077003501355648\n",
      "Mean loss: 0.012071530101820827\n",
      "Std loss: 0.00740750964789597\n",
      "Total Loss: 0.07242918061092496\n",
      "------------------------------------ epoch 5669 (34008 steps) ------------------------------------\n",
      "Max loss: 0.038253478705883026\n",
      "Min loss: 0.006767045706510544\n",
      "Mean loss: 0.015391061082482338\n",
      "Std loss: 0.010520826332561184\n",
      "Total Loss: 0.09234636649489403\n",
      "------------------------------------ epoch 5670 (34014 steps) ------------------------------------\n",
      "Max loss: 0.022382721304893494\n",
      "Min loss: 0.008415049873292446\n",
      "Mean loss: 0.013505400624126196\n",
      "Std loss: 0.005524359016297396\n",
      "Total Loss: 0.08103240374475718\n",
      "------------------------------------ epoch 5671 (34020 steps) ------------------------------------\n",
      "Max loss: 0.02279391698539257\n",
      "Min loss: 0.00560134369879961\n",
      "Mean loss: 0.01115072766939799\n",
      "Std loss: 0.005540786372631613\n",
      "Total Loss: 0.06690436601638794\n",
      "------------------------------------ epoch 5672 (34026 steps) ------------------------------------\n",
      "Max loss: 0.01237941812723875\n",
      "Min loss: 0.006451064255088568\n",
      "Mean loss: 0.008580165294309458\n",
      "Std loss: 0.001857910485725315\n",
      "Total Loss: 0.05148099176585674\n",
      "------------------------------------ epoch 5673 (34032 steps) ------------------------------------\n",
      "Max loss: 0.0503595694899559\n",
      "Min loss: 0.005237638019025326\n",
      "Mean loss: 0.016160741914063692\n",
      "Std loss: 0.015530912619485774\n",
      "Total Loss: 0.09696445148438215\n",
      "------------------------------------ epoch 5674 (34038 steps) ------------------------------------\n",
      "Max loss: 0.03092535398900509\n",
      "Min loss: 0.00648850342258811\n",
      "Mean loss: 0.012739556686331829\n",
      "Std loss: 0.008458420593304294\n",
      "Total Loss: 0.07643734011799097\n",
      "------------------------------------ epoch 5675 (34044 steps) ------------------------------------\n",
      "Max loss: 0.013097110204398632\n",
      "Min loss: 0.0057091559283435345\n",
      "Mean loss: 0.008387494987497726\n",
      "Std loss: 0.0028786427730338477\n",
      "Total Loss: 0.05032496992498636\n",
      "------------------------------------ epoch 5676 (34050 steps) ------------------------------------\n",
      "Max loss: 0.02031383290886879\n",
      "Min loss: 0.007130706217139959\n",
      "Mean loss: 0.012184672678510347\n",
      "Std loss: 0.005435221904747788\n",
      "Total Loss: 0.07310803607106209\n",
      "------------------------------------ epoch 5677 (34056 steps) ------------------------------------\n",
      "Max loss: 0.013604718260467052\n",
      "Min loss: 0.005482068285346031\n",
      "Mean loss: 0.009066231859227022\n",
      "Std loss: 0.003040584037226389\n",
      "Total Loss: 0.05439739115536213\n",
      "------------------------------------ epoch 5678 (34062 steps) ------------------------------------\n",
      "Max loss: 0.025578122586011887\n",
      "Min loss: 0.0077912462875247\n",
      "Mean loss: 0.014573107628772656\n",
      "Std loss: 0.005908910727811824\n",
      "Total Loss: 0.08743864577263594\n",
      "------------------------------------ epoch 5679 (34068 steps) ------------------------------------\n",
      "Max loss: 0.0221996046602726\n",
      "Min loss: 0.00566056277602911\n",
      "Mean loss: 0.016200455992172163\n",
      "Std loss: 0.006252071856999404\n",
      "Total Loss: 0.09720273595303297\n",
      "------------------------------------ epoch 5680 (34074 steps) ------------------------------------\n",
      "Max loss: 0.024257879704236984\n",
      "Min loss: 0.005686669610440731\n",
      "Mean loss: 0.012114131512741247\n",
      "Std loss: 0.006216242478235592\n",
      "Total Loss: 0.07268478907644749\n",
      "------------------------------------ epoch 5681 (34080 steps) ------------------------------------\n",
      "Max loss: 0.02314998209476471\n",
      "Min loss: 0.005738681182265282\n",
      "Mean loss: 0.01220247863481442\n",
      "Std loss: 0.006267293987745642\n",
      "Total Loss: 0.07321487180888653\n",
      "------------------------------------ epoch 5682 (34086 steps) ------------------------------------\n",
      "Max loss: 0.020217835903167725\n",
      "Min loss: 0.007612012326717377\n",
      "Mean loss: 0.012252014440794786\n",
      "Std loss: 0.004436319307163884\n",
      "Total Loss: 0.07351208664476871\n",
      "------------------------------------ epoch 5683 (34092 steps) ------------------------------------\n",
      "Max loss: 0.04052617400884628\n",
      "Min loss: 0.006300265900790691\n",
      "Mean loss: 0.015410496853291988\n",
      "Std loss: 0.011471733101872333\n",
      "Total Loss: 0.09246298111975193\n",
      "------------------------------------ epoch 5684 (34098 steps) ------------------------------------\n",
      "Max loss: 0.0336146280169487\n",
      "Min loss: 0.004968263208866119\n",
      "Mean loss: 0.011881615112846097\n",
      "Std loss: 0.010162687766548101\n",
      "Total Loss: 0.07128969067707658\n",
      "------------------------------------ epoch 5685 (34104 steps) ------------------------------------\n",
      "Max loss: 0.015234234742820263\n",
      "Min loss: 0.006403486244380474\n",
      "Mean loss: 0.011387986286232868\n",
      "Std loss: 0.003260921079464336\n",
      "Total Loss: 0.06832791771739721\n",
      "------------------------------------ epoch 5686 (34110 steps) ------------------------------------\n",
      "Max loss: 0.02326296642422676\n",
      "Min loss: 0.0070597208105027676\n",
      "Mean loss: 0.014090509231512746\n",
      "Std loss: 0.0063380165025479955\n",
      "Total Loss: 0.08454305538907647\n",
      "------------------------------------ epoch 5687 (34116 steps) ------------------------------------\n",
      "Max loss: 0.01147882454097271\n",
      "Min loss: 0.005951730068773031\n",
      "Mean loss: 0.008237597765401006\n",
      "Std loss: 0.0023047563577603255\n",
      "Total Loss: 0.049425586592406034\n",
      "------------------------------------ epoch 5688 (34122 steps) ------------------------------------\n",
      "Max loss: 0.01609116792678833\n",
      "Min loss: 0.004723352380096912\n",
      "Mean loss: 0.008879212429746985\n",
      "Std loss: 0.004912801742159326\n",
      "Total Loss: 0.05327527457848191\n",
      "------------------------------------ epoch 5689 (34128 steps) ------------------------------------\n",
      "Max loss: 0.031166715547442436\n",
      "Min loss: 0.006266535725444555\n",
      "Mean loss: 0.013768572437887391\n",
      "Std loss: 0.008100591615181053\n",
      "Total Loss: 0.08261143462732434\n",
      "------------------------------------ epoch 5690 (34134 steps) ------------------------------------\n",
      "Max loss: 0.023364156484603882\n",
      "Min loss: 0.006267311982810497\n",
      "Mean loss: 0.010550486079106728\n",
      "Std loss: 0.005904252843111375\n",
      "Total Loss: 0.06330291647464037\n",
      "------------------------------------ epoch 5691 (34140 steps) ------------------------------------\n",
      "Max loss: 0.012460853904485703\n",
      "Min loss: 0.004612446762621403\n",
      "Mean loss: 0.008050129748880863\n",
      "Std loss: 0.0027971067858855677\n",
      "Total Loss: 0.04830077849328518\n",
      "------------------------------------ epoch 5692 (34146 steps) ------------------------------------\n",
      "Max loss: 0.01721116714179516\n",
      "Min loss: 0.005625372286885977\n",
      "Mean loss: 0.008836372988298535\n",
      "Std loss: 0.003925860859058368\n",
      "Total Loss: 0.05301823792979121\n",
      "------------------------------------ epoch 5693 (34152 steps) ------------------------------------\n",
      "Max loss: 0.012538179755210876\n",
      "Min loss: 0.005437153857201338\n",
      "Mean loss: 0.008760460574800769\n",
      "Std loss: 0.0024784279121921765\n",
      "Total Loss: 0.05256276344880462\n",
      "------------------------------------ epoch 5694 (34158 steps) ------------------------------------\n",
      "Max loss: 0.008846787735819817\n",
      "Min loss: 0.0057855467312037945\n",
      "Mean loss: 0.007194358156993985\n",
      "Std loss: 0.0011530680984221425\n",
      "Total Loss: 0.04316614894196391\n",
      "------------------------------------ epoch 5695 (34164 steps) ------------------------------------\n",
      "Max loss: 0.017039921134710312\n",
      "Min loss: 0.004948917776346207\n",
      "Mean loss: 0.0113872440221409\n",
      "Std loss: 0.004246126787621448\n",
      "Total Loss: 0.0683234641328454\n",
      "------------------------------------ epoch 5696 (34170 steps) ------------------------------------\n",
      "Max loss: 0.036755286157131195\n",
      "Min loss: 0.005030319094657898\n",
      "Mean loss: 0.013135089461381236\n",
      "Std loss: 0.01087600328934616\n",
      "Total Loss: 0.07881053676828742\n",
      "------------------------------------ epoch 5697 (34176 steps) ------------------------------------\n",
      "Max loss: 0.022009938955307007\n",
      "Min loss: 0.00528779998421669\n",
      "Mean loss: 0.013257861059779922\n",
      "Std loss: 0.005882942940555899\n",
      "Total Loss: 0.07954716635867953\n",
      "------------------------------------ epoch 5698 (34182 steps) ------------------------------------\n",
      "Max loss: 0.012640481814742088\n",
      "Min loss: 0.005647354759275913\n",
      "Mean loss: 0.00875342171639204\n",
      "Std loss: 0.00281819969151255\n",
      "Total Loss: 0.05252053029835224\n",
      "------------------------------------ epoch 5699 (34188 steps) ------------------------------------\n",
      "Max loss: 0.011670118197798729\n",
      "Min loss: 0.005200366955250502\n",
      "Mean loss: 0.008241209589565793\n",
      "Std loss: 0.0025105858063619027\n",
      "Total Loss: 0.04944725753739476\n",
      "------------------------------------ epoch 5700 (34194 steps) ------------------------------------\n",
      "Max loss: 0.012588199228048325\n",
      "Min loss: 0.0045183030888438225\n",
      "Mean loss: 0.008838378669073185\n",
      "Std loss: 0.002927653471114955\n",
      "Total Loss: 0.053030272014439106\n",
      "------------------------------------ epoch 5701 (34200 steps) ------------------------------------\n",
      "Max loss: 0.009044421836733818\n",
      "Min loss: 0.006112126167863607\n",
      "Mean loss: 0.00706129001143078\n",
      "Std loss: 0.0010648743311027996\n",
      "Total Loss: 0.04236774006858468\n",
      "saved model at ./weights/model_5701.pth\n",
      "------------------------------------ epoch 5702 (34206 steps) ------------------------------------\n",
      "Max loss: 0.00864843837916851\n",
      "Min loss: 0.005756286438554525\n",
      "Mean loss: 0.006631617977594336\n",
      "Std loss: 0.0009608191217686104\n",
      "Total Loss: 0.039789707865566015\n",
      "------------------------------------ epoch 5703 (34212 steps) ------------------------------------\n",
      "Max loss: 0.01188628189265728\n",
      "Min loss: 0.007750330027192831\n",
      "Mean loss: 0.009230046765878797\n",
      "Std loss: 0.0013935955111241403\n",
      "Total Loss: 0.05538028059527278\n",
      "------------------------------------ epoch 5704 (34218 steps) ------------------------------------\n",
      "Max loss: 0.010888656601309776\n",
      "Min loss: 0.0055224355310201645\n",
      "Mean loss: 0.007943260095392665\n",
      "Std loss: 0.0019202703933033037\n",
      "Total Loss: 0.047659560572355986\n",
      "------------------------------------ epoch 5705 (34224 steps) ------------------------------------\n",
      "Max loss: 0.011777061969041824\n",
      "Min loss: 0.004950196947902441\n",
      "Mean loss: 0.007510283729061484\n",
      "Std loss: 0.0022091981033153865\n",
      "Total Loss: 0.045061702374368906\n",
      "------------------------------------ epoch 5706 (34230 steps) ------------------------------------\n",
      "Max loss: 0.012948254123330116\n",
      "Min loss: 0.004311207681894302\n",
      "Mean loss: 0.00787661550566554\n",
      "Std loss: 0.0027409415900200335\n",
      "Total Loss: 0.047259693033993244\n",
      "------------------------------------ epoch 5707 (34236 steps) ------------------------------------\n",
      "Max loss: 0.017352966591715813\n",
      "Min loss: 0.004107445944100618\n",
      "Mean loss: 0.009209663762400547\n",
      "Std loss: 0.004722090893853918\n",
      "Total Loss: 0.055257982574403286\n",
      "------------------------------------ epoch 5708 (34242 steps) ------------------------------------\n",
      "Max loss: 0.009616720490157604\n",
      "Min loss: 0.00520273158326745\n",
      "Mean loss: 0.0073992185449848575\n",
      "Std loss: 0.0017678827998564763\n",
      "Total Loss: 0.04439531126990914\n",
      "------------------------------------ epoch 5709 (34248 steps) ------------------------------------\n",
      "Max loss: 0.01712416112422943\n",
      "Min loss: 0.004293186590075493\n",
      "Mean loss: 0.007633075040454666\n",
      "Std loss: 0.004363276689515879\n",
      "Total Loss: 0.045798450242727995\n",
      "------------------------------------ epoch 5710 (34254 steps) ------------------------------------\n",
      "Max loss: 0.009778979234397411\n",
      "Min loss: 0.004167558625340462\n",
      "Mean loss: 0.006049551535397768\n",
      "Std loss: 0.0018641055366772144\n",
      "Total Loss: 0.03629730921238661\n",
      "------------------------------------ epoch 5711 (34260 steps) ------------------------------------\n",
      "Max loss: 0.023170672357082367\n",
      "Min loss: 0.004488036036491394\n",
      "Mean loss: 0.011803324101492763\n",
      "Std loss: 0.007100098641264923\n",
      "Total Loss: 0.07081994460895658\n",
      "------------------------------------ epoch 5712 (34266 steps) ------------------------------------\n",
      "Max loss: 0.013430225662887096\n",
      "Min loss: 0.005638548638671637\n",
      "Mean loss: 0.007949626073241234\n",
      "Std loss: 0.0026121110883405695\n",
      "Total Loss: 0.0476977564394474\n",
      "------------------------------------ epoch 5713 (34272 steps) ------------------------------------\n",
      "Max loss: 0.008471528068184853\n",
      "Min loss: 0.005182799883186817\n",
      "Mean loss: 0.0062358583478877945\n",
      "Std loss: 0.0012486448443732323\n",
      "Total Loss: 0.037415150087326765\n",
      "------------------------------------ epoch 5714 (34278 steps) ------------------------------------\n",
      "Max loss: 0.012429317459464073\n",
      "Min loss: 0.004582823719829321\n",
      "Mean loss: 0.007442577897260587\n",
      "Std loss: 0.0025936003825853204\n",
      "Total Loss: 0.04465546738356352\n",
      "------------------------------------ epoch 5715 (34284 steps) ------------------------------------\n",
      "Max loss: 0.01080003660172224\n",
      "Min loss: 0.006047335918992758\n",
      "Mean loss: 0.00810578892317911\n",
      "Std loss: 0.0020477543337492545\n",
      "Total Loss: 0.04863473353907466\n",
      "------------------------------------ epoch 5716 (34290 steps) ------------------------------------\n",
      "Max loss: 0.01063535362482071\n",
      "Min loss: 0.004471411928534508\n",
      "Mean loss: 0.008495519869029522\n",
      "Std loss: 0.0022017924156814256\n",
      "Total Loss: 0.05097311921417713\n",
      "------------------------------------ epoch 5717 (34296 steps) ------------------------------------\n",
      "Max loss: 0.026131443679332733\n",
      "Min loss: 0.004807524848729372\n",
      "Mean loss: 0.01197774133955439\n",
      "Std loss: 0.00869429966570496\n",
      "Total Loss: 0.07186644803732634\n",
      "------------------------------------ epoch 5718 (34302 steps) ------------------------------------\n",
      "Max loss: 0.008579946123063564\n",
      "Min loss: 0.005316648632287979\n",
      "Mean loss: 0.007297273104389508\n",
      "Std loss: 0.0013600845119358877\n",
      "Total Loss: 0.04378363862633705\n",
      "------------------------------------ epoch 5719 (34308 steps) ------------------------------------\n",
      "Max loss: 0.011938100680708885\n",
      "Min loss: 0.005352209322154522\n",
      "Mean loss: 0.008965956357618174\n",
      "Std loss: 0.002414327501778645\n",
      "Total Loss: 0.05379573814570904\n",
      "------------------------------------ epoch 5720 (34314 steps) ------------------------------------\n",
      "Max loss: 0.009708566591143608\n",
      "Min loss: 0.005070009268820286\n",
      "Mean loss: 0.007805946593483289\n",
      "Std loss: 0.0016581715567663036\n",
      "Total Loss: 0.046835679560899734\n",
      "------------------------------------ epoch 5721 (34320 steps) ------------------------------------\n",
      "Max loss: 0.025613700971007347\n",
      "Min loss: 0.006031710654497147\n",
      "Mean loss: 0.013504834845662117\n",
      "Std loss: 0.007979470957452491\n",
      "Total Loss: 0.0810290090739727\n",
      "------------------------------------ epoch 5722 (34326 steps) ------------------------------------\n",
      "Max loss: 0.013168907724320889\n",
      "Min loss: 0.00643379520624876\n",
      "Mean loss: 0.008621985480810205\n",
      "Std loss: 0.002301390850368805\n",
      "Total Loss: 0.05173191288486123\n",
      "------------------------------------ epoch 5723 (34332 steps) ------------------------------------\n",
      "Max loss: 0.022581130266189575\n",
      "Min loss: 0.004877304192632437\n",
      "Mean loss: 0.009164714952930808\n",
      "Std loss: 0.006140741254801612\n",
      "Total Loss: 0.05498828971758485\n",
      "------------------------------------ epoch 5724 (34338 steps) ------------------------------------\n",
      "Max loss: 0.018639154732227325\n",
      "Min loss: 0.00693934690207243\n",
      "Mean loss: 0.011758428377409777\n",
      "Std loss: 0.003964829757971086\n",
      "Total Loss: 0.07055057026445866\n",
      "------------------------------------ epoch 5725 (34344 steps) ------------------------------------\n",
      "Max loss: 0.022580420598387718\n",
      "Min loss: 0.006850973702967167\n",
      "Mean loss: 0.013297994543487826\n",
      "Std loss: 0.006360242079579347\n",
      "Total Loss: 0.07978796726092696\n",
      "------------------------------------ epoch 5726 (34350 steps) ------------------------------------\n",
      "Max loss: 0.010523373261094093\n",
      "Min loss: 0.00558449886739254\n",
      "Mean loss: 0.0071504253428429365\n",
      "Std loss: 0.0016207406955927152\n",
      "Total Loss: 0.04290255205705762\n",
      "------------------------------------ epoch 5727 (34356 steps) ------------------------------------\n",
      "Max loss: 0.013582177460193634\n",
      "Min loss: 0.0042145634070038795\n",
      "Mean loss: 0.008358072644720474\n",
      "Std loss: 0.0033254148951783165\n",
      "Total Loss: 0.05014843586832285\n",
      "------------------------------------ epoch 5728 (34362 steps) ------------------------------------\n",
      "Max loss: 0.01702232100069523\n",
      "Min loss: 0.005256257951259613\n",
      "Mean loss: 0.009058927651494741\n",
      "Std loss: 0.003929918940977761\n",
      "Total Loss: 0.05435356590896845\n",
      "------------------------------------ epoch 5729 (34368 steps) ------------------------------------\n",
      "Max loss: 0.019260868430137634\n",
      "Min loss: 0.004733889363706112\n",
      "Mean loss: 0.009889519540593028\n",
      "Std loss: 0.004970152768457781\n",
      "Total Loss: 0.05933711724355817\n",
      "------------------------------------ epoch 5730 (34374 steps) ------------------------------------\n",
      "Max loss: 0.04768875986337662\n",
      "Min loss: 0.004387581255286932\n",
      "Mean loss: 0.014287550933659077\n",
      "Std loss: 0.015159394460789533\n",
      "Total Loss: 0.08572530560195446\n",
      "------------------------------------ epoch 5731 (34380 steps) ------------------------------------\n",
      "Max loss: 0.014553124085068703\n",
      "Min loss: 0.005970960017293692\n",
      "Mean loss: 0.008824403242518505\n",
      "Std loss: 0.003461215444419037\n",
      "Total Loss: 0.05294641945511103\n",
      "------------------------------------ epoch 5732 (34386 steps) ------------------------------------\n",
      "Max loss: 0.02959052473306656\n",
      "Min loss: 0.005353979300707579\n",
      "Mean loss: 0.01225042319856584\n",
      "Std loss: 0.007956572604136614\n",
      "Total Loss: 0.07350253919139504\n",
      "------------------------------------ epoch 5733 (34392 steps) ------------------------------------\n",
      "Max loss: 0.020504411309957504\n",
      "Min loss: 0.004367870278656483\n",
      "Mean loss: 0.012188347211728493\n",
      "Std loss: 0.0066360600331646985\n",
      "Total Loss: 0.07313008327037096\n",
      "------------------------------------ epoch 5734 (34398 steps) ------------------------------------\n",
      "Max loss: 0.012511510401964188\n",
      "Min loss: 0.0044735632836818695\n",
      "Mean loss: 0.007552745829646786\n",
      "Std loss: 0.0030242962616614496\n",
      "Total Loss: 0.045316474977880716\n",
      "------------------------------------ epoch 5735 (34404 steps) ------------------------------------\n",
      "Max loss: 0.022347278892993927\n",
      "Min loss: 0.005126430653035641\n",
      "Mean loss: 0.010713735284904638\n",
      "Std loss: 0.005809247208531564\n",
      "Total Loss: 0.06428241170942783\n",
      "------------------------------------ epoch 5736 (34410 steps) ------------------------------------\n",
      "Max loss: 0.02395724132657051\n",
      "Min loss: 0.005666745360940695\n",
      "Mean loss: 0.011261333245784044\n",
      "Std loss: 0.006234077028199584\n",
      "Total Loss: 0.06756799947470427\n",
      "------------------------------------ epoch 5737 (34416 steps) ------------------------------------\n",
      "Max loss: 0.018124882131814957\n",
      "Min loss: 0.00686775054782629\n",
      "Mean loss: 0.01103232785438498\n",
      "Std loss: 0.004935968094419175\n",
      "Total Loss: 0.06619396712630987\n",
      "------------------------------------ epoch 5738 (34422 steps) ------------------------------------\n",
      "Max loss: 0.0134681835770607\n",
      "Min loss: 0.004860330373048782\n",
      "Mean loss: 0.007436320961763461\n",
      "Std loss: 0.0028682683506165933\n",
      "Total Loss: 0.04461792577058077\n",
      "------------------------------------ epoch 5739 (34428 steps) ------------------------------------\n",
      "Max loss: 0.008935232646763325\n",
      "Min loss: 0.004936084151268005\n",
      "Mean loss: 0.006616676536699136\n",
      "Std loss: 0.001674501434957951\n",
      "Total Loss: 0.03970005922019482\n",
      "------------------------------------ epoch 5740 (34434 steps) ------------------------------------\n",
      "Max loss: 0.01440538838505745\n",
      "Min loss: 0.0048571424558758736\n",
      "Mean loss: 0.008399304390574494\n",
      "Std loss: 0.0033543525638485053\n",
      "Total Loss: 0.05039582634344697\n",
      "------------------------------------ epoch 5741 (34440 steps) ------------------------------------\n",
      "Max loss: 0.010658014565706253\n",
      "Min loss: 0.004942429251968861\n",
      "Mean loss: 0.008023430593311787\n",
      "Std loss: 0.002083672369661545\n",
      "Total Loss: 0.04814058355987072\n",
      "------------------------------------ epoch 5742 (34446 steps) ------------------------------------\n",
      "Max loss: 0.017083942890167236\n",
      "Min loss: 0.00474764546379447\n",
      "Mean loss: 0.00735705664070944\n",
      "Std loss: 0.004371700532081707\n",
      "Total Loss: 0.04414233984425664\n",
      "------------------------------------ epoch 5743 (34452 steps) ------------------------------------\n",
      "Max loss: 0.020565535873174667\n",
      "Min loss: 0.005221194587647915\n",
      "Mean loss: 0.011703377589583397\n",
      "Std loss: 0.0055798704112176994\n",
      "Total Loss: 0.07022026553750038\n",
      "------------------------------------ epoch 5744 (34458 steps) ------------------------------------\n",
      "Max loss: 0.011388231068849564\n",
      "Min loss: 0.006013333331793547\n",
      "Mean loss: 0.008285763207823038\n",
      "Std loss: 0.0018070437341653923\n",
      "Total Loss: 0.04971457924693823\n",
      "------------------------------------ epoch 5745 (34464 steps) ------------------------------------\n",
      "Max loss: 0.007820723578333855\n",
      "Min loss: 0.005073383916169405\n",
      "Mean loss: 0.00618161183471481\n",
      "Std loss: 0.0008573685280998259\n",
      "Total Loss: 0.03708967100828886\n",
      "------------------------------------ epoch 5746 (34470 steps) ------------------------------------\n",
      "Max loss: 0.021700961515307426\n",
      "Min loss: 0.004023425281047821\n",
      "Mean loss: 0.010349770775064826\n",
      "Std loss: 0.005961684236681196\n",
      "Total Loss: 0.062098624650388956\n",
      "------------------------------------ epoch 5747 (34476 steps) ------------------------------------\n",
      "Max loss: 0.027848122641444206\n",
      "Min loss: 0.006565266754478216\n",
      "Mean loss: 0.014906829533477625\n",
      "Std loss: 0.0076035007761079175\n",
      "Total Loss: 0.08944097720086575\n",
      "------------------------------------ epoch 5748 (34482 steps) ------------------------------------\n",
      "Max loss: 0.025992002338171005\n",
      "Min loss: 0.008218805305659771\n",
      "Mean loss: 0.011665668648978075\n",
      "Std loss: 0.006431074751620029\n",
      "Total Loss: 0.06999401189386845\n",
      "------------------------------------ epoch 5749 (34488 steps) ------------------------------------\n",
      "Max loss: 0.025918450206518173\n",
      "Min loss: 0.006797091569751501\n",
      "Mean loss: 0.012070215695227185\n",
      "Std loss: 0.006492695289258939\n",
      "Total Loss: 0.07242129417136312\n",
      "------------------------------------ epoch 5750 (34494 steps) ------------------------------------\n",
      "Max loss: 0.021508779376745224\n",
      "Min loss: 0.005093271844089031\n",
      "Mean loss: 0.012555418458456794\n",
      "Std loss: 0.005500629059690568\n",
      "Total Loss: 0.07533251075074077\n",
      "------------------------------------ epoch 5751 (34500 steps) ------------------------------------\n",
      "Max loss: 0.015048879198729992\n",
      "Min loss: 0.006179254502058029\n",
      "Mean loss: 0.00955186178907752\n",
      "Std loss: 0.0030767200037003783\n",
      "Total Loss: 0.05731117073446512\n",
      "------------------------------------ epoch 5752 (34506 steps) ------------------------------------\n",
      "Max loss: 0.013714052736759186\n",
      "Min loss: 0.006876322440803051\n",
      "Mean loss: 0.01040903851389885\n",
      "Std loss: 0.0020618477924188882\n",
      "Total Loss: 0.0624542310833931\n",
      "------------------------------------ epoch 5753 (34512 steps) ------------------------------------\n",
      "Max loss: 0.03568969666957855\n",
      "Min loss: 0.006225378718227148\n",
      "Mean loss: 0.013100887881591916\n",
      "Std loss: 0.0102268932350187\n",
      "Total Loss: 0.0786053272895515\n",
      "------------------------------------ epoch 5754 (34518 steps) ------------------------------------\n",
      "Max loss: 0.013859642669558525\n",
      "Min loss: 0.0061076912097632885\n",
      "Mean loss: 0.010171340700859824\n",
      "Std loss: 0.0032482534947441584\n",
      "Total Loss: 0.06102804420515895\n",
      "------------------------------------ epoch 5755 (34524 steps) ------------------------------------\n",
      "Max loss: 0.039059828966856\n",
      "Min loss: 0.008651826530694962\n",
      "Mean loss: 0.018025943854202826\n",
      "Std loss: 0.010450017592278869\n",
      "Total Loss: 0.10815566312521696\n",
      "------------------------------------ epoch 5756 (34530 steps) ------------------------------------\n",
      "Max loss: 0.015424123033881187\n",
      "Min loss: 0.00609414279460907\n",
      "Mean loss: 0.009875338369359573\n",
      "Std loss: 0.003072560070701188\n",
      "Total Loss: 0.059252030216157436\n",
      "------------------------------------ epoch 5757 (34536 steps) ------------------------------------\n",
      "Max loss: 0.04112424701452255\n",
      "Min loss: 0.005599874071776867\n",
      "Mean loss: 0.019180626996482413\n",
      "Std loss: 0.014800288732232587\n",
      "Total Loss: 0.11508376197889447\n",
      "------------------------------------ epoch 5758 (34542 steps) ------------------------------------\n",
      "Max loss: 0.026760315522551537\n",
      "Min loss: 0.007078410591930151\n",
      "Mean loss: 0.013267157521719733\n",
      "Std loss: 0.00669071606459625\n",
      "Total Loss: 0.0796029451303184\n",
      "------------------------------------ epoch 5759 (34548 steps) ------------------------------------\n",
      "Max loss: 0.03201668709516525\n",
      "Min loss: 0.00714574521407485\n",
      "Mean loss: 0.018240486504510045\n",
      "Std loss: 0.008957254064816915\n",
      "Total Loss: 0.10944291902706027\n",
      "------------------------------------ epoch 5760 (34554 steps) ------------------------------------\n",
      "Max loss: 0.019923096522688866\n",
      "Min loss: 0.006287156604230404\n",
      "Mean loss: 0.012900771417965492\n",
      "Std loss: 0.00488698446781616\n",
      "Total Loss: 0.07740462850779295\n",
      "------------------------------------ epoch 5761 (34560 steps) ------------------------------------\n",
      "Max loss: 0.014567125588655472\n",
      "Min loss: 0.005218702368438244\n",
      "Mean loss: 0.00821489041360716\n",
      "Std loss: 0.0032162143380247326\n",
      "Total Loss: 0.04928934248164296\n",
      "------------------------------------ epoch 5762 (34566 steps) ------------------------------------\n",
      "Max loss: 0.02451060339808464\n",
      "Min loss: 0.007602208759635687\n",
      "Mean loss: 0.013300914550200105\n",
      "Std loss: 0.005670034039504481\n",
      "Total Loss: 0.07980548730120063\n",
      "------------------------------------ epoch 5763 (34572 steps) ------------------------------------\n",
      "Max loss: 0.046344660222530365\n",
      "Min loss: 0.007209139876067638\n",
      "Mean loss: 0.018988794026275475\n",
      "Std loss: 0.012959467187008841\n",
      "Total Loss: 0.11393276415765285\n",
      "------------------------------------ epoch 5764 (34578 steps) ------------------------------------\n",
      "Max loss: 0.015737397596240044\n",
      "Min loss: 0.0059982845559716225\n",
      "Mean loss: 0.00984199756445984\n",
      "Std loss: 0.003417066785872499\n",
      "Total Loss: 0.05905198538675904\n",
      "------------------------------------ epoch 5765 (34584 steps) ------------------------------------\n",
      "Max loss: 0.01856624148786068\n",
      "Min loss: 0.008742810226976871\n",
      "Mean loss: 0.014798132702708244\n",
      "Std loss: 0.0035190489610552643\n",
      "Total Loss: 0.08878879621624947\n",
      "------------------------------------ epoch 5766 (34590 steps) ------------------------------------\n",
      "Max loss: 0.05414917320013046\n",
      "Min loss: 0.013762268237769604\n",
      "Mean loss: 0.02732224032903711\n",
      "Std loss: 0.013091007137055048\n",
      "Total Loss: 0.16393344197422266\n",
      "------------------------------------ epoch 5767 (34596 steps) ------------------------------------\n",
      "Max loss: 0.07097908109426498\n",
      "Min loss: 0.013862568885087967\n",
      "Mean loss: 0.025680558135112126\n",
      "Std loss: 0.020483841324573433\n",
      "Total Loss: 0.15408334881067276\n",
      "------------------------------------ epoch 5768 (34602 steps) ------------------------------------\n",
      "Max loss: 0.025765791535377502\n",
      "Min loss: 0.013868710026144981\n",
      "Mean loss: 0.01849863041813175\n",
      "Std loss: 0.0043300628563381115\n",
      "Total Loss: 0.11099178250879049\n",
      "------------------------------------ epoch 5769 (34608 steps) ------------------------------------\n",
      "Max loss: 0.017671123147010803\n",
      "Min loss: 0.01000536885112524\n",
      "Mean loss: 0.013420856402566036\n",
      "Std loss: 0.002610679877330376\n",
      "Total Loss: 0.08052513841539621\n",
      "------------------------------------ epoch 5770 (34614 steps) ------------------------------------\n",
      "Max loss: 0.026061665266752243\n",
      "Min loss: 0.008645877242088318\n",
      "Mean loss: 0.014289704151451588\n",
      "Std loss: 0.005857895419989821\n",
      "Total Loss: 0.08573822490870953\n",
      "------------------------------------ epoch 5771 (34620 steps) ------------------------------------\n",
      "Max loss: 0.013422492891550064\n",
      "Min loss: 0.00858713872730732\n",
      "Mean loss: 0.011030548096944889\n",
      "Std loss: 0.001874400959822154\n",
      "Total Loss: 0.06618328858166933\n",
      "------------------------------------ epoch 5772 (34626 steps) ------------------------------------\n",
      "Max loss: 0.03403409197926521\n",
      "Min loss: 0.007186524569988251\n",
      "Mean loss: 0.012879358604550362\n",
      "Std loss: 0.009503501785992415\n",
      "Total Loss: 0.07727615162730217\n",
      "------------------------------------ epoch 5773 (34632 steps) ------------------------------------\n",
      "Max loss: 0.018903616815805435\n",
      "Min loss: 0.009522459469735622\n",
      "Mean loss: 0.013614374989022812\n",
      "Std loss: 0.0034219160903510297\n",
      "Total Loss: 0.08168624993413687\n",
      "------------------------------------ epoch 5774 (34638 steps) ------------------------------------\n",
      "Max loss: 0.05691883713006973\n",
      "Min loss: 0.008266083896160126\n",
      "Mean loss: 0.020101418097813923\n",
      "Std loss: 0.016878755953771214\n",
      "Total Loss: 0.12060850858688354\n",
      "------------------------------------ epoch 5775 (34644 steps) ------------------------------------\n",
      "Max loss: 0.01994318515062332\n",
      "Min loss: 0.008913643658161163\n",
      "Mean loss: 0.014272015231351057\n",
      "Std loss: 0.004375178323687462\n",
      "Total Loss: 0.08563209138810635\n",
      "------------------------------------ epoch 5776 (34650 steps) ------------------------------------\n",
      "Max loss: 0.04615600034594536\n",
      "Min loss: 0.007962644100189209\n",
      "Mean loss: 0.01990632340312004\n",
      "Std loss: 0.013149176995641396\n",
      "Total Loss: 0.11943794041872025\n",
      "------------------------------------ epoch 5777 (34656 steps) ------------------------------------\n",
      "Max loss: 0.03842165693640709\n",
      "Min loss: 0.0097507294267416\n",
      "Mean loss: 0.018658215335259836\n",
      "Std loss: 0.009529749196138176\n",
      "Total Loss: 0.11194929201155901\n",
      "------------------------------------ epoch 5778 (34662 steps) ------------------------------------\n",
      "Max loss: 0.029007013887166977\n",
      "Min loss: 0.007463305257260799\n",
      "Mean loss: 0.017105714573214453\n",
      "Std loss: 0.007407390737776471\n",
      "Total Loss: 0.10263428743928671\n",
      "------------------------------------ epoch 5779 (34668 steps) ------------------------------------\n",
      "Max loss: 0.045193806290626526\n",
      "Min loss: 0.0068460009060800076\n",
      "Mean loss: 0.021551292467241485\n",
      "Std loss: 0.015754345273383454\n",
      "Total Loss: 0.12930775480344892\n",
      "------------------------------------ epoch 5780 (34674 steps) ------------------------------------\n",
      "Max loss: 0.02712443470954895\n",
      "Min loss: 0.006155394017696381\n",
      "Mean loss: 0.015850063258161146\n",
      "Std loss: 0.0072699847939308665\n",
      "Total Loss: 0.09510037954896688\n",
      "------------------------------------ epoch 5781 (34680 steps) ------------------------------------\n",
      "Max loss: 0.012236621230840683\n",
      "Min loss: 0.007750825025141239\n",
      "Mean loss: 0.009796537769337496\n",
      "Std loss: 0.0014381788411587514\n",
      "Total Loss: 0.05877922661602497\n",
      "------------------------------------ epoch 5782 (34686 steps) ------------------------------------\n",
      "Max loss: 0.044026535004377365\n",
      "Min loss: 0.007982254959642887\n",
      "Mean loss: 0.01607751629004876\n",
      "Std loss: 0.012649371192346168\n",
      "Total Loss: 0.09646509774029255\n",
      "------------------------------------ epoch 5783 (34692 steps) ------------------------------------\n",
      "Max loss: 0.0166771300137043\n",
      "Min loss: 0.005769173614680767\n",
      "Mean loss: 0.01129676029086113\n",
      "Std loss: 0.003641696087066235\n",
      "Total Loss: 0.06778056174516678\n",
      "------------------------------------ epoch 5784 (34698 steps) ------------------------------------\n",
      "Max loss: 0.039919108152389526\n",
      "Min loss: 0.010366600006818771\n",
      "Mean loss: 0.019807842870553333\n",
      "Std loss: 0.011496918449179442\n",
      "Total Loss: 0.11884705722332001\n",
      "------------------------------------ epoch 5785 (34704 steps) ------------------------------------\n",
      "Max loss: 0.025542400777339935\n",
      "Min loss: 0.009032422676682472\n",
      "Mean loss: 0.015398692184438309\n",
      "Std loss: 0.0058685622752319465\n",
      "Total Loss: 0.09239215310662985\n",
      "------------------------------------ epoch 5786 (34710 steps) ------------------------------------\n",
      "Max loss: 0.04915022850036621\n",
      "Min loss: 0.00753236748278141\n",
      "Mean loss: 0.023576791243006785\n",
      "Std loss: 0.016579978008821147\n",
      "Total Loss: 0.14146074745804071\n",
      "------------------------------------ epoch 5787 (34716 steps) ------------------------------------\n",
      "Max loss: 0.040483199059963226\n",
      "Min loss: 0.006600316613912582\n",
      "Mean loss: 0.02200083713978529\n",
      "Std loss: 0.01257883461846278\n",
      "Total Loss: 0.13200502283871174\n",
      "------------------------------------ epoch 5788 (34722 steps) ------------------------------------\n",
      "Max loss: 0.025486696511507034\n",
      "Min loss: 0.01002351101487875\n",
      "Mean loss: 0.019156603918721277\n",
      "Std loss: 0.004821425674187642\n",
      "Total Loss: 0.11493962351232767\n",
      "------------------------------------ epoch 5789 (34728 steps) ------------------------------------\n",
      "Max loss: 0.026372026652097702\n",
      "Min loss: 0.00887405127286911\n",
      "Mean loss: 0.015801181085407734\n",
      "Std loss: 0.005728839495631517\n",
      "Total Loss: 0.0948070865124464\n",
      "------------------------------------ epoch 5790 (34734 steps) ------------------------------------\n",
      "Max loss: 0.030389446765184402\n",
      "Min loss: 0.006931637413799763\n",
      "Mean loss: 0.013460238929837942\n",
      "Std loss: 0.007949574802127256\n",
      "Total Loss: 0.08076143357902765\n",
      "------------------------------------ epoch 5791 (34740 steps) ------------------------------------\n",
      "Max loss: 0.02283213846385479\n",
      "Min loss: 0.006472072564065456\n",
      "Mean loss: 0.010297936542580525\n",
      "Std loss: 0.005719777842117155\n",
      "Total Loss: 0.06178761925548315\n",
      "------------------------------------ epoch 5792 (34746 steps) ------------------------------------\n",
      "Max loss: 0.01027220394462347\n",
      "Min loss: 0.005616658832877874\n",
      "Mean loss: 0.008126666924605766\n",
      "Std loss: 0.001421850046753743\n",
      "Total Loss: 0.0487600015476346\n",
      "------------------------------------ epoch 5793 (34752 steps) ------------------------------------\n",
      "Max loss: 0.018773185089230537\n",
      "Min loss: 0.007768324576318264\n",
      "Mean loss: 0.011343060992658138\n",
      "Std loss: 0.00405964374589175\n",
      "Total Loss: 0.06805836595594883\n",
      "------------------------------------ epoch 5794 (34758 steps) ------------------------------------\n",
      "Max loss: 0.017974115908145905\n",
      "Min loss: 0.006682080682367086\n",
      "Mean loss: 0.012273571842039624\n",
      "Std loss: 0.004194576375432913\n",
      "Total Loss: 0.07364143105223775\n",
      "------------------------------------ epoch 5795 (34764 steps) ------------------------------------\n",
      "Max loss: 0.020408576354384422\n",
      "Min loss: 0.0056421696208417416\n",
      "Mean loss: 0.010522037278860807\n",
      "Std loss: 0.004875141853962813\n",
      "Total Loss: 0.06313222367316484\n",
      "------------------------------------ epoch 5796 (34770 steps) ------------------------------------\n",
      "Max loss: 0.009848970919847488\n",
      "Min loss: 0.005081320181488991\n",
      "Mean loss: 0.007261834728221099\n",
      "Std loss: 0.0017714624597525374\n",
      "Total Loss: 0.04357100836932659\n",
      "------------------------------------ epoch 5797 (34776 steps) ------------------------------------\n",
      "Max loss: 0.03160763531923294\n",
      "Min loss: 0.006139520090073347\n",
      "Mean loss: 0.011654882303749522\n",
      "Std loss: 0.00899074378512532\n",
      "Total Loss: 0.06992929382249713\n",
      "------------------------------------ epoch 5798 (34782 steps) ------------------------------------\n",
      "Max loss: 0.0459359809756279\n",
      "Min loss: 0.0057318853214383125\n",
      "Mean loss: 0.01616385447171827\n",
      "Std loss: 0.013814814203035736\n",
      "Total Loss: 0.09698312683030963\n",
      "------------------------------------ epoch 5799 (34788 steps) ------------------------------------\n",
      "Max loss: 0.011655626818537712\n",
      "Min loss: 0.00520311389118433\n",
      "Mean loss: 0.007871314805621902\n",
      "Std loss: 0.0020181093149701504\n",
      "Total Loss: 0.04722788883373141\n",
      "------------------------------------ epoch 5800 (34794 steps) ------------------------------------\n",
      "Max loss: 0.014346806332468987\n",
      "Min loss: 0.005495497491210699\n",
      "Mean loss: 0.010642905641968051\n",
      "Std loss: 0.00317307142416419\n",
      "Total Loss: 0.06385743385180831\n",
      "------------------------------------ epoch 5801 (34800 steps) ------------------------------------\n",
      "Max loss: 0.012317128479480743\n",
      "Min loss: 0.007891228422522545\n",
      "Mean loss: 0.00918780422459046\n",
      "Std loss: 0.001456246491308747\n",
      "Total Loss: 0.05512682534754276\n",
      "saved model at ./weights/model_5801.pth\n",
      "------------------------------------ epoch 5802 (34806 steps) ------------------------------------\n",
      "Max loss: 0.010999640449881554\n",
      "Min loss: 0.005424877163022757\n",
      "Mean loss: 0.0075391722687830525\n",
      "Std loss: 0.0023584646407020598\n",
      "Total Loss: 0.04523503361269832\n",
      "------------------------------------ epoch 5803 (34812 steps) ------------------------------------\n",
      "Max loss: 0.02745947241783142\n",
      "Min loss: 0.004603780340403318\n",
      "Mean loss: 0.010897085769101977\n",
      "Std loss: 0.007662070944368348\n",
      "Total Loss: 0.06538251461461186\n",
      "------------------------------------ epoch 5804 (34818 steps) ------------------------------------\n",
      "Max loss: 0.009975082240998745\n",
      "Min loss: 0.0048952484503388405\n",
      "Mean loss: 0.00774036495325466\n",
      "Std loss: 0.0018578556146698752\n",
      "Total Loss: 0.04644218971952796\n",
      "------------------------------------ epoch 5805 (34824 steps) ------------------------------------\n",
      "Max loss: 0.047307491302490234\n",
      "Min loss: 0.009215538389980793\n",
      "Mean loss: 0.018377458211034536\n",
      "Std loss: 0.013251353402549331\n",
      "Total Loss: 0.11026474926620722\n",
      "------------------------------------ epoch 5806 (34830 steps) ------------------------------------\n",
      "Max loss: 0.014784028753638268\n",
      "Min loss: 0.006616150960326195\n",
      "Mean loss: 0.010630450832347075\n",
      "Std loss: 0.0031478586233480875\n",
      "Total Loss: 0.06378270499408245\n",
      "------------------------------------ epoch 5807 (34836 steps) ------------------------------------\n",
      "Max loss: 0.023577120155096054\n",
      "Min loss: 0.006380738224834204\n",
      "Mean loss: 0.011557386334364613\n",
      "Std loss: 0.005827730678215668\n",
      "Total Loss: 0.06934431800618768\n",
      "------------------------------------ epoch 5808 (34842 steps) ------------------------------------\n",
      "Max loss: 0.031039675697684288\n",
      "Min loss: 0.006040891632437706\n",
      "Mean loss: 0.01655305844421188\n",
      "Std loss: 0.009278332062947535\n",
      "Total Loss: 0.09931835066527128\n",
      "------------------------------------ epoch 5809 (34848 steps) ------------------------------------\n",
      "Max loss: 0.025931604206562042\n",
      "Min loss: 0.0055758352391421795\n",
      "Mean loss: 0.01284224345969657\n",
      "Std loss: 0.007184846930161958\n",
      "Total Loss: 0.07705346075817943\n",
      "------------------------------------ epoch 5810 (34854 steps) ------------------------------------\n",
      "Max loss: 0.01465089526027441\n",
      "Min loss: 0.004704702645540237\n",
      "Mean loss: 0.01124978174145023\n",
      "Std loss: 0.0031597852239448367\n",
      "Total Loss: 0.06749869044870138\n",
      "------------------------------------ epoch 5811 (34860 steps) ------------------------------------\n",
      "Max loss: 0.023795001208782196\n",
      "Min loss: 0.005554329138249159\n",
      "Mean loss: 0.011305562065293392\n",
      "Std loss: 0.0059614596377501345\n",
      "Total Loss: 0.06783337239176035\n",
      "------------------------------------ epoch 5812 (34866 steps) ------------------------------------\n",
      "Max loss: 0.022739827632904053\n",
      "Min loss: 0.0050961850211024284\n",
      "Mean loss: 0.010098658657322327\n",
      "Std loss: 0.006094030428765289\n",
      "Total Loss: 0.060591951943933964\n",
      "------------------------------------ epoch 5813 (34872 steps) ------------------------------------\n",
      "Max loss: 0.013675659894943237\n",
      "Min loss: 0.007624714635312557\n",
      "Mean loss: 0.009856162825599313\n",
      "Std loss: 0.0021466960075473556\n",
      "Total Loss: 0.05913697695359588\n",
      "------------------------------------ epoch 5814 (34878 steps) ------------------------------------\n",
      "Max loss: 0.010967823676764965\n",
      "Min loss: 0.0056710257194936275\n",
      "Mean loss: 0.00846625592870017\n",
      "Std loss: 0.001647140320261463\n",
      "Total Loss: 0.050797535572201014\n",
      "------------------------------------ epoch 5815 (34884 steps) ------------------------------------\n",
      "Max loss: 0.022489875555038452\n",
      "Min loss: 0.005826420150697231\n",
      "Mean loss: 0.011587329829732576\n",
      "Std loss: 0.005531276979977162\n",
      "Total Loss: 0.06952397897839546\n",
      "------------------------------------ epoch 5816 (34890 steps) ------------------------------------\n",
      "Max loss: 0.014151367358863354\n",
      "Min loss: 0.007257722318172455\n",
      "Mean loss: 0.009763399294267098\n",
      "Std loss: 0.0027675816588173707\n",
      "Total Loss: 0.05858039576560259\n",
      "------------------------------------ epoch 5817 (34896 steps) ------------------------------------\n",
      "Max loss: 0.015111908316612244\n",
      "Min loss: 0.006695752963423729\n",
      "Mean loss: 0.009146293935676416\n",
      "Std loss: 0.002833265326913058\n",
      "Total Loss: 0.054877763614058495\n",
      "------------------------------------ epoch 5818 (34902 steps) ------------------------------------\n",
      "Max loss: 0.013138527981936932\n",
      "Min loss: 0.005126343574374914\n",
      "Mean loss: 0.009136831775928536\n",
      "Std loss: 0.00315042799887072\n",
      "Total Loss: 0.05482099065557122\n",
      "------------------------------------ epoch 5819 (34908 steps) ------------------------------------\n",
      "Max loss: 0.022713256999850273\n",
      "Min loss: 0.004689574241638184\n",
      "Mean loss: 0.01017910505955418\n",
      "Std loss: 0.006129392024829137\n",
      "Total Loss: 0.06107463035732508\n",
      "------------------------------------ epoch 5820 (34914 steps) ------------------------------------\n",
      "Max loss: 0.013960178941488266\n",
      "Min loss: 0.005182985216379166\n",
      "Mean loss: 0.00953209555397431\n",
      "Std loss: 0.0032644609328423948\n",
      "Total Loss: 0.05719257332384586\n",
      "------------------------------------ epoch 5821 (34920 steps) ------------------------------------\n",
      "Max loss: 0.02183172106742859\n",
      "Min loss: 0.00635894387960434\n",
      "Mean loss: 0.01252254502226909\n",
      "Std loss: 0.005876896646723391\n",
      "Total Loss: 0.07513527013361454\n",
      "------------------------------------ epoch 5822 (34926 steps) ------------------------------------\n",
      "Max loss: 0.020261483266949654\n",
      "Min loss: 0.005519845988601446\n",
      "Mean loss: 0.010650377332543334\n",
      "Std loss: 0.00511353415626859\n",
      "Total Loss: 0.06390226399526\n",
      "------------------------------------ epoch 5823 (34932 steps) ------------------------------------\n",
      "Max loss: 0.017454691231250763\n",
      "Min loss: 0.005637120455503464\n",
      "Mean loss: 0.008895122058068713\n",
      "Std loss: 0.004028803816541973\n",
      "Total Loss: 0.053370732348412275\n",
      "------------------------------------ epoch 5824 (34938 steps) ------------------------------------\n",
      "Max loss: 0.007169400807470083\n",
      "Min loss: 0.004467088729143143\n",
      "Mean loss: 0.0063863567387064295\n",
      "Std loss: 0.0009314528636325618\n",
      "Total Loss: 0.03831814043223858\n",
      "------------------------------------ epoch 5825 (34944 steps) ------------------------------------\n",
      "Max loss: 0.018149718642234802\n",
      "Min loss: 0.006075263023376465\n",
      "Mean loss: 0.010532456450164318\n",
      "Std loss: 0.0038342543463841506\n",
      "Total Loss: 0.06319473870098591\n",
      "------------------------------------ epoch 5826 (34950 steps) ------------------------------------\n",
      "Max loss: 0.0251280777156353\n",
      "Min loss: 0.005500517785549164\n",
      "Mean loss: 0.012219941553970179\n",
      "Std loss: 0.006248323275439209\n",
      "Total Loss: 0.07331964932382107\n",
      "------------------------------------ epoch 5827 (34956 steps) ------------------------------------\n",
      "Max loss: 0.02146075665950775\n",
      "Min loss: 0.006013189442455769\n",
      "Mean loss: 0.010804375633597374\n",
      "Std loss: 0.005109721944431583\n",
      "Total Loss: 0.06482625380158424\n",
      "------------------------------------ epoch 5828 (34962 steps) ------------------------------------\n",
      "Max loss: 0.019098520278930664\n",
      "Min loss: 0.006832029204815626\n",
      "Mean loss: 0.010574853591000041\n",
      "Std loss: 0.004467811707845151\n",
      "Total Loss: 0.06344912154600024\n",
      "------------------------------------ epoch 5829 (34968 steps) ------------------------------------\n",
      "Max loss: 0.018062368035316467\n",
      "Min loss: 0.004863135050982237\n",
      "Mean loss: 0.009315696001673738\n",
      "Std loss: 0.004398875867622829\n",
      "Total Loss: 0.05589417601004243\n",
      "------------------------------------ epoch 5830 (34974 steps) ------------------------------------\n",
      "Max loss: 0.01678699627518654\n",
      "Min loss: 0.0057835085317492485\n",
      "Mean loss: 0.008988612331449986\n",
      "Std loss: 0.0038035921559843996\n",
      "Total Loss: 0.05393167398869991\n",
      "------------------------------------ epoch 5831 (34980 steps) ------------------------------------\n",
      "Max loss: 0.05914686247706413\n",
      "Min loss: 0.00529149454087019\n",
      "Mean loss: 0.02110502744714419\n",
      "Std loss: 0.01800945360910013\n",
      "Total Loss: 0.12663016468286514\n",
      "------------------------------------ epoch 5832 (34986 steps) ------------------------------------\n",
      "Max loss: 0.018188562244176865\n",
      "Min loss: 0.005266726948320866\n",
      "Mean loss: 0.00999858028565844\n",
      "Std loss: 0.004525068112228134\n",
      "Total Loss: 0.059991481713950634\n",
      "------------------------------------ epoch 5833 (34992 steps) ------------------------------------\n",
      "Max loss: 0.018999239429831505\n",
      "Min loss: 0.005963926203548908\n",
      "Mean loss: 0.011952330203106007\n",
      "Std loss: 0.00466882808298804\n",
      "Total Loss: 0.07171398121863604\n",
      "------------------------------------ epoch 5834 (34998 steps) ------------------------------------\n",
      "Max loss: 0.013193640857934952\n",
      "Min loss: 0.005850602872669697\n",
      "Mean loss: 0.009766453100989262\n",
      "Std loss: 0.0025803449194220776\n",
      "Total Loss: 0.058598718605935574\n",
      "------------------------------------ epoch 5835 (35004 steps) ------------------------------------\n",
      "Max loss: 0.013001780956983566\n",
      "Min loss: 0.006235620006918907\n",
      "Mean loss: 0.009455083248515924\n",
      "Std loss: 0.002419685824389216\n",
      "Total Loss: 0.05673049949109554\n",
      "------------------------------------ epoch 5836 (35010 steps) ------------------------------------\n",
      "Max loss: 0.010354236699640751\n",
      "Min loss: 0.005542472004890442\n",
      "Mean loss: 0.008443919708952308\n",
      "Std loss: 0.0016884723974389696\n",
      "Total Loss: 0.050663518253713846\n",
      "------------------------------------ epoch 5837 (35016 steps) ------------------------------------\n",
      "Max loss: 0.03593813255429268\n",
      "Min loss: 0.00537840835750103\n",
      "Mean loss: 0.014448484017824134\n",
      "Std loss: 0.010270278459935242\n",
      "Total Loss: 0.0866909041069448\n",
      "------------------------------------ epoch 5838 (35022 steps) ------------------------------------\n",
      "Max loss: 0.031051184982061386\n",
      "Min loss: 0.007793080992996693\n",
      "Mean loss: 0.014559756498783827\n",
      "Std loss: 0.007955948608922724\n",
      "Total Loss: 0.08735853899270296\n",
      "------------------------------------ epoch 5839 (35028 steps) ------------------------------------\n",
      "Max loss: 0.035756029188632965\n",
      "Min loss: 0.007341187447309494\n",
      "Mean loss: 0.017513145847866934\n",
      "Std loss: 0.009974742315385218\n",
      "Total Loss: 0.1050788750872016\n",
      "------------------------------------ epoch 5840 (35034 steps) ------------------------------------\n",
      "Max loss: 0.012314625084400177\n",
      "Min loss: 0.0059214867651462555\n",
      "Mean loss: 0.008843327173963189\n",
      "Std loss: 0.0022942597860528594\n",
      "Total Loss: 0.053059963043779135\n",
      "------------------------------------ epoch 5841 (35040 steps) ------------------------------------\n",
      "Max loss: 0.021714700385928154\n",
      "Min loss: 0.006226859986782074\n",
      "Mean loss: 0.011089764147376021\n",
      "Std loss: 0.005621471665140459\n",
      "Total Loss: 0.06653858488425612\n",
      "------------------------------------ epoch 5842 (35046 steps) ------------------------------------\n",
      "Max loss: 0.015043999999761581\n",
      "Min loss: 0.00667943712323904\n",
      "Mean loss: 0.009427634533494711\n",
      "Std loss: 0.003118349187366152\n",
      "Total Loss: 0.056565807200968266\n",
      "------------------------------------ epoch 5843 (35052 steps) ------------------------------------\n",
      "Max loss: 0.013248970732092857\n",
      "Min loss: 0.005068633239716291\n",
      "Mean loss: 0.007869783556088805\n",
      "Std loss: 0.002627548789826203\n",
      "Total Loss: 0.04721870133653283\n",
      "------------------------------------ epoch 5844 (35058 steps) ------------------------------------\n",
      "Max loss: 0.01760334149003029\n",
      "Min loss: 0.0072102672420442104\n",
      "Mean loss: 0.010638162571315965\n",
      "Std loss: 0.003446735653054608\n",
      "Total Loss: 0.06382897542789578\n",
      "------------------------------------ epoch 5845 (35064 steps) ------------------------------------\n",
      "Max loss: 0.016990644857287407\n",
      "Min loss: 0.005163219757378101\n",
      "Mean loss: 0.008538035986324152\n",
      "Std loss: 0.004085230739172934\n",
      "Total Loss: 0.05122821591794491\n",
      "------------------------------------ epoch 5846 (35070 steps) ------------------------------------\n",
      "Max loss: 0.04400187358260155\n",
      "Min loss: 0.006189487874507904\n",
      "Mean loss: 0.014354560058563948\n",
      "Std loss: 0.013418982606451826\n",
      "Total Loss: 0.08612736035138369\n",
      "------------------------------------ epoch 5847 (35076 steps) ------------------------------------\n",
      "Max loss: 0.022511307150125504\n",
      "Min loss: 0.00608482351526618\n",
      "Mean loss: 0.012679847190156579\n",
      "Std loss: 0.005202825634425838\n",
      "Total Loss: 0.07607908314093947\n",
      "------------------------------------ epoch 5848 (35082 steps) ------------------------------------\n",
      "Max loss: 0.00977015309035778\n",
      "Min loss: 0.00494126183912158\n",
      "Mean loss: 0.007159692235291004\n",
      "Std loss: 0.001732250482177936\n",
      "Total Loss: 0.042958153411746025\n",
      "------------------------------------ epoch 5849 (35088 steps) ------------------------------------\n",
      "Max loss: 0.010117081925272942\n",
      "Min loss: 0.006129632703959942\n",
      "Mean loss: 0.00798135328417023\n",
      "Std loss: 0.001620301365481438\n",
      "Total Loss: 0.04788811970502138\n",
      "------------------------------------ epoch 5850 (35094 steps) ------------------------------------\n",
      "Max loss: 0.01046308409422636\n",
      "Min loss: 0.005277058109641075\n",
      "Mean loss: 0.008059983995432654\n",
      "Std loss: 0.0020746787676024984\n",
      "Total Loss: 0.04835990397259593\n",
      "------------------------------------ epoch 5851 (35100 steps) ------------------------------------\n",
      "Max loss: 0.01954636536538601\n",
      "Min loss: 0.005384974181652069\n",
      "Mean loss: 0.009289754942680398\n",
      "Std loss: 0.004875559242195217\n",
      "Total Loss: 0.05573852965608239\n",
      "------------------------------------ epoch 5852 (35106 steps) ------------------------------------\n",
      "Max loss: 0.01656244695186615\n",
      "Min loss: 0.00496349111199379\n",
      "Mean loss: 0.008349071179206172\n",
      "Std loss: 0.0037839052690030535\n",
      "Total Loss: 0.050094427075237036\n",
      "------------------------------------ epoch 5853 (35112 steps) ------------------------------------\n",
      "Max loss: 0.015702273696660995\n",
      "Min loss: 0.005712335463613272\n",
      "Mean loss: 0.009273788814122478\n",
      "Std loss: 0.00402486599789047\n",
      "Total Loss: 0.05564273288473487\n",
      "------------------------------------ epoch 5854 (35118 steps) ------------------------------------\n",
      "Max loss: 0.009159444831311703\n",
      "Min loss: 0.00468794722110033\n",
      "Mean loss: 0.006650371787448724\n",
      "Std loss: 0.0016557882333130749\n",
      "Total Loss: 0.039902230724692345\n",
      "------------------------------------ epoch 5855 (35124 steps) ------------------------------------\n",
      "Max loss: 0.011966072022914886\n",
      "Min loss: 0.007051737979054451\n",
      "Mean loss: 0.009734815452247858\n",
      "Std loss: 0.001680980631544442\n",
      "Total Loss: 0.05840889271348715\n",
      "------------------------------------ epoch 5856 (35130 steps) ------------------------------------\n",
      "Max loss: 0.014578130096197128\n",
      "Min loss: 0.00567689910531044\n",
      "Mean loss: 0.009680946047107378\n",
      "Std loss: 0.003692075212384834\n",
      "Total Loss: 0.05808567628264427\n",
      "------------------------------------ epoch 5857 (35136 steps) ------------------------------------\n",
      "Max loss: 0.019306126981973648\n",
      "Min loss: 0.005272505339235067\n",
      "Mean loss: 0.00957576270836095\n",
      "Std loss: 0.004906316104034112\n",
      "Total Loss: 0.0574545762501657\n",
      "------------------------------------ epoch 5858 (35142 steps) ------------------------------------\n",
      "Max loss: 0.011249661445617676\n",
      "Min loss: 0.005470538046211004\n",
      "Mean loss: 0.0076998556808878975\n",
      "Std loss: 0.0019076162981679328\n",
      "Total Loss: 0.04619913408532739\n",
      "------------------------------------ epoch 5859 (35148 steps) ------------------------------------\n",
      "Max loss: 0.041382066905498505\n",
      "Min loss: 0.005039566196501255\n",
      "Mean loss: 0.01626562241775294\n",
      "Std loss: 0.012436887822308845\n",
      "Total Loss: 0.09759373450651765\n",
      "------------------------------------ epoch 5860 (35154 steps) ------------------------------------\n",
      "Max loss: 0.03710003197193146\n",
      "Min loss: 0.005993562750518322\n",
      "Mean loss: 0.013394795280570785\n",
      "Std loss: 0.010780926707625435\n",
      "Total Loss: 0.08036877168342471\n",
      "------------------------------------ epoch 5861 (35160 steps) ------------------------------------\n",
      "Max loss: 0.021577775478363037\n",
      "Min loss: 0.0070270998403429985\n",
      "Mean loss: 0.011317834413299957\n",
      "Std loss: 0.004838996197311533\n",
      "Total Loss: 0.06790700647979975\n",
      "------------------------------------ epoch 5862 (35166 steps) ------------------------------------\n",
      "Max loss: 0.017898235470056534\n",
      "Min loss: 0.005215104669332504\n",
      "Mean loss: 0.011455691109100977\n",
      "Std loss: 0.004823769189188156\n",
      "Total Loss: 0.06873414665460587\n",
      "------------------------------------ epoch 5863 (35172 steps) ------------------------------------\n",
      "Max loss: 0.01893787831068039\n",
      "Min loss: 0.005320007912814617\n",
      "Mean loss: 0.010533956500391165\n",
      "Std loss: 0.004556027891966124\n",
      "Total Loss: 0.06320373900234699\n",
      "------------------------------------ epoch 5864 (35178 steps) ------------------------------------\n",
      "Max loss: 0.044686298817396164\n",
      "Min loss: 0.0054201832972466946\n",
      "Mean loss: 0.014793774656330546\n",
      "Std loss: 0.013824936789612047\n",
      "Total Loss: 0.08876264793798327\n",
      "------------------------------------ epoch 5865 (35184 steps) ------------------------------------\n",
      "Max loss: 0.03452055901288986\n",
      "Min loss: 0.010193033143877983\n",
      "Mean loss: 0.0191343380138278\n",
      "Std loss: 0.007906855190874746\n",
      "Total Loss: 0.1148060280829668\n",
      "------------------------------------ epoch 5866 (35190 steps) ------------------------------------\n",
      "Max loss: 0.02332625351846218\n",
      "Min loss: 0.008235491812229156\n",
      "Mean loss: 0.013410468275348345\n",
      "Std loss: 0.0049818598792282275\n",
      "Total Loss: 0.08046280965209007\n",
      "------------------------------------ epoch 5867 (35196 steps) ------------------------------------\n",
      "Max loss: 0.022168010473251343\n",
      "Min loss: 0.006896892562508583\n",
      "Mean loss: 0.011580163069690267\n",
      "Std loss: 0.005086733550076565\n",
      "Total Loss: 0.0694809784181416\n",
      "------------------------------------ epoch 5868 (35202 steps) ------------------------------------\n",
      "Max loss: 0.02808642014861107\n",
      "Min loss: 0.006667288951575756\n",
      "Mean loss: 0.01325029352058967\n",
      "Std loss: 0.006866954034718692\n",
      "Total Loss: 0.07950176112353802\n",
      "------------------------------------ epoch 5869 (35208 steps) ------------------------------------\n",
      "Max loss: 0.020914798602461815\n",
      "Min loss: 0.006777390372008085\n",
      "Mean loss: 0.012526459759101272\n",
      "Std loss: 0.004566710662613909\n",
      "Total Loss: 0.07515875855460763\n",
      "------------------------------------ epoch 5870 (35214 steps) ------------------------------------\n",
      "Max loss: 0.058378666639328\n",
      "Min loss: 0.007518175058066845\n",
      "Mean loss: 0.021101639450838167\n",
      "Std loss: 0.01707678411108224\n",
      "Total Loss: 0.126609836705029\n",
      "------------------------------------ epoch 5871 (35220 steps) ------------------------------------\n",
      "Max loss: 0.01519010215997696\n",
      "Min loss: 0.007422570139169693\n",
      "Mean loss: 0.010604861502846083\n",
      "Std loss: 0.0026559292166088134\n",
      "Total Loss: 0.06362916901707649\n",
      "------------------------------------ epoch 5872 (35226 steps) ------------------------------------\n",
      "Max loss: 0.017698116600513458\n",
      "Min loss: 0.006917332299053669\n",
      "Mean loss: 0.010369949353237947\n",
      "Std loss: 0.003657062536066481\n",
      "Total Loss: 0.06221969611942768\n",
      "------------------------------------ epoch 5873 (35232 steps) ------------------------------------\n",
      "Max loss: 0.0322347953915596\n",
      "Min loss: 0.006627075374126434\n",
      "Mean loss: 0.014636983629316092\n",
      "Std loss: 0.008588532646028603\n",
      "Total Loss: 0.08782190177589655\n",
      "------------------------------------ epoch 5874 (35238 steps) ------------------------------------\n",
      "Max loss: 0.031216418370604515\n",
      "Min loss: 0.0060197049751877785\n",
      "Mean loss: 0.011731990690653523\n",
      "Std loss: 0.008852450780311046\n",
      "Total Loss: 0.07039194414392114\n",
      "------------------------------------ epoch 5875 (35244 steps) ------------------------------------\n",
      "Max loss: 0.023799769580364227\n",
      "Min loss: 0.005603262688964605\n",
      "Mean loss: 0.013037201715633273\n",
      "Std loss: 0.006373372962385677\n",
      "Total Loss: 0.07822321029379964\n",
      "------------------------------------ epoch 5876 (35250 steps) ------------------------------------\n",
      "Max loss: 0.01787712238729\n",
      "Min loss: 0.006472047418355942\n",
      "Mean loss: 0.010547725483775139\n",
      "Std loss: 0.00417906146835975\n",
      "Total Loss: 0.06328635290265083\n",
      "------------------------------------ epoch 5877 (35256 steps) ------------------------------------\n",
      "Max loss: 0.03152626007795334\n",
      "Min loss: 0.007373382803052664\n",
      "Mean loss: 0.01613189171378811\n",
      "Std loss: 0.00805713219169666\n",
      "Total Loss: 0.09679135028272867\n",
      "------------------------------------ epoch 5878 (35262 steps) ------------------------------------\n",
      "Max loss: 0.02788517437875271\n",
      "Min loss: 0.007956411689519882\n",
      "Mean loss: 0.01605976341913144\n",
      "Std loss: 0.008013339384835033\n",
      "Total Loss: 0.09635858051478863\n",
      "------------------------------------ epoch 5879 (35268 steps) ------------------------------------\n",
      "Max loss: 0.017771095037460327\n",
      "Min loss: 0.006754090078175068\n",
      "Mean loss: 0.010938760824501514\n",
      "Std loss: 0.003556236394966324\n",
      "Total Loss: 0.06563256494700909\n",
      "------------------------------------ epoch 5880 (35274 steps) ------------------------------------\n",
      "Max loss: 0.019656170159578323\n",
      "Min loss: 0.005496017634868622\n",
      "Mean loss: 0.009995171567425132\n",
      "Std loss: 0.005124369746751733\n",
      "Total Loss: 0.05997102940455079\n",
      "------------------------------------ epoch 5881 (35280 steps) ------------------------------------\n",
      "Max loss: 0.011211860924959183\n",
      "Min loss: 0.0055039990693330765\n",
      "Mean loss: 0.008242842896531025\n",
      "Std loss: 0.0024121823835565426\n",
      "Total Loss: 0.04945705737918615\n",
      "------------------------------------ epoch 5882 (35286 steps) ------------------------------------\n",
      "Max loss: 0.012041139416396618\n",
      "Min loss: 0.004542231559753418\n",
      "Mean loss: 0.007794807897880673\n",
      "Std loss: 0.0030214829639979885\n",
      "Total Loss: 0.04676884738728404\n",
      "------------------------------------ epoch 5883 (35292 steps) ------------------------------------\n",
      "Max loss: 0.020589381456375122\n",
      "Min loss: 0.005481514148414135\n",
      "Mean loss: 0.010053071814278761\n",
      "Std loss: 0.005219177503936308\n",
      "Total Loss: 0.06031843088567257\n",
      "------------------------------------ epoch 5884 (35298 steps) ------------------------------------\n",
      "Max loss: 0.01741168089210987\n",
      "Min loss: 0.006425797939300537\n",
      "Mean loss: 0.013343199777106443\n",
      "Std loss: 0.004895511810435936\n",
      "Total Loss: 0.08005919866263866\n",
      "------------------------------------ epoch 5885 (35304 steps) ------------------------------------\n",
      "Max loss: 0.010748978704214096\n",
      "Min loss: 0.005610156338661909\n",
      "Mean loss: 0.008207576271767417\n",
      "Std loss: 0.0020383419634679976\n",
      "Total Loss: 0.049245457630604506\n",
      "------------------------------------ epoch 5886 (35310 steps) ------------------------------------\n",
      "Max loss: 0.015473878011107445\n",
      "Min loss: 0.004748361650854349\n",
      "Mean loss: 0.009265866285810867\n",
      "Std loss: 0.0038316109127681367\n",
      "Total Loss: 0.05559519771486521\n",
      "------------------------------------ epoch 5887 (35316 steps) ------------------------------------\n",
      "Max loss: 0.012177231721580029\n",
      "Min loss: 0.004534326493740082\n",
      "Mean loss: 0.007176176101590197\n",
      "Std loss: 0.0025193989928469127\n",
      "Total Loss: 0.04305705660954118\n",
      "------------------------------------ epoch 5888 (35322 steps) ------------------------------------\n",
      "Max loss: 0.014331268146634102\n",
      "Min loss: 0.00511389272287488\n",
      "Mean loss: 0.00937750997642676\n",
      "Std loss: 0.003149522590627769\n",
      "Total Loss: 0.05626505985856056\n",
      "------------------------------------ epoch 5889 (35328 steps) ------------------------------------\n",
      "Max loss: 0.014456185512244701\n",
      "Min loss: 0.005906397011131048\n",
      "Mean loss: 0.010628614807501435\n",
      "Std loss: 0.002730636487025405\n",
      "Total Loss: 0.06377168884500861\n",
      "------------------------------------ epoch 5890 (35334 steps) ------------------------------------\n",
      "Max loss: 0.00881732814013958\n",
      "Min loss: 0.0055717783980071545\n",
      "Mean loss: 0.0070186393180241185\n",
      "Std loss: 0.0011677496004557269\n",
      "Total Loss: 0.04211183590814471\n",
      "------------------------------------ epoch 5891 (35340 steps) ------------------------------------\n",
      "Max loss: 0.014129336923360825\n",
      "Min loss: 0.006227309815585613\n",
      "Mean loss: 0.009880084777250886\n",
      "Std loss: 0.0031515434367495295\n",
      "Total Loss: 0.059280508663505316\n",
      "------------------------------------ epoch 5892 (35346 steps) ------------------------------------\n",
      "Max loss: 0.02013370208442211\n",
      "Min loss: 0.004290716256946325\n",
      "Mean loss: 0.008869469165802002\n",
      "Std loss: 0.0053482400798304216\n",
      "Total Loss: 0.05321681499481201\n",
      "------------------------------------ epoch 5893 (35352 steps) ------------------------------------\n",
      "Max loss: 0.02977338805794716\n",
      "Min loss: 0.005897327326238155\n",
      "Mean loss: 0.010888353843862811\n",
      "Std loss: 0.008494204368546228\n",
      "Total Loss: 0.06533012306317687\n",
      "------------------------------------ epoch 5894 (35358 steps) ------------------------------------\n",
      "Max loss: 0.015499934554100037\n",
      "Min loss: 0.00522844772785902\n",
      "Mean loss: 0.011416853405535221\n",
      "Std loss: 0.003328807947561175\n",
      "Total Loss: 0.06850112043321133\n",
      "------------------------------------ epoch 5895 (35364 steps) ------------------------------------\n",
      "Max loss: 0.035584092140197754\n",
      "Min loss: 0.005467971786856651\n",
      "Mean loss: 0.013233425716559092\n",
      "Std loss: 0.010436053702188324\n",
      "Total Loss: 0.07940055429935455\n",
      "------------------------------------ epoch 5896 (35370 steps) ------------------------------------\n",
      "Max loss: 0.014965981245040894\n",
      "Min loss: 0.00551889231428504\n",
      "Mean loss: 0.008730905751387278\n",
      "Std loss: 0.0030477230682815727\n",
      "Total Loss: 0.05238543450832367\n",
      "------------------------------------ epoch 5897 (35376 steps) ------------------------------------\n",
      "Max loss: 0.027588579803705215\n",
      "Min loss: 0.00659441202878952\n",
      "Mean loss: 0.013837396167218685\n",
      "Std loss: 0.007243022785127058\n",
      "Total Loss: 0.08302437700331211\n",
      "------------------------------------ epoch 5898 (35382 steps) ------------------------------------\n",
      "Max loss: 0.015106391161680222\n",
      "Min loss: 0.004995273891836405\n",
      "Mean loss: 0.00936977689464887\n",
      "Std loss: 0.003635616133778673\n",
      "Total Loss: 0.05621866136789322\n",
      "------------------------------------ epoch 5899 (35388 steps) ------------------------------------\n",
      "Max loss: 0.01840246468782425\n",
      "Min loss: 0.00537856575101614\n",
      "Mean loss: 0.012686441807697216\n",
      "Std loss: 0.005010816666471617\n",
      "Total Loss: 0.0761186508461833\n",
      "------------------------------------ epoch 5900 (35394 steps) ------------------------------------\n",
      "Max loss: 0.021454233676195145\n",
      "Min loss: 0.0046822745352983475\n",
      "Mean loss: 0.012152904489388069\n",
      "Std loss: 0.006319709766103538\n",
      "Total Loss: 0.07291742693632841\n",
      "------------------------------------ epoch 5901 (35400 steps) ------------------------------------\n",
      "Max loss: 0.025353608652949333\n",
      "Min loss: 0.006233016029000282\n",
      "Mean loss: 0.012496261314178506\n",
      "Std loss: 0.006946681467114963\n",
      "Total Loss: 0.07497756788507104\n",
      "saved model at ./weights/model_5901.pth\n",
      "------------------------------------ epoch 5902 (35406 steps) ------------------------------------\n",
      "Max loss: 0.01800558716058731\n",
      "Min loss: 0.006593090482056141\n",
      "Mean loss: 0.012352988589555025\n",
      "Std loss: 0.004024655427201491\n",
      "Total Loss: 0.07411793153733015\n",
      "------------------------------------ epoch 5903 (35412 steps) ------------------------------------\n",
      "Max loss: 0.01728377863764763\n",
      "Min loss: 0.0050916410982608795\n",
      "Mean loss: 0.011612507825096449\n",
      "Std loss: 0.004252060094988131\n",
      "Total Loss: 0.06967504695057869\n",
      "------------------------------------ epoch 5904 (35418 steps) ------------------------------------\n",
      "Max loss: 0.043805092573165894\n",
      "Min loss: 0.005010101478546858\n",
      "Mean loss: 0.014238617150112987\n",
      "Std loss: 0.013472562924648315\n",
      "Total Loss: 0.08543170290067792\n",
      "------------------------------------ epoch 5905 (35424 steps) ------------------------------------\n",
      "Max loss: 0.028318747878074646\n",
      "Min loss: 0.005711182951927185\n",
      "Mean loss: 0.013957911791900793\n",
      "Std loss: 0.008694105289953607\n",
      "Total Loss: 0.08374747075140476\n",
      "------------------------------------ epoch 5906 (35430 steps) ------------------------------------\n",
      "Max loss: 0.0169215127825737\n",
      "Min loss: 0.005969656631350517\n",
      "Mean loss: 0.009647227978954712\n",
      "Std loss: 0.004094464255094536\n",
      "Total Loss: 0.057883367873728275\n",
      "------------------------------------ epoch 5907 (35436 steps) ------------------------------------\n",
      "Max loss: 0.0182998925447464\n",
      "Min loss: 0.005422639660537243\n",
      "Mean loss: 0.01027613552287221\n",
      "Std loss: 0.004889877138997982\n",
      "Total Loss: 0.06165681313723326\n",
      "------------------------------------ epoch 5908 (35442 steps) ------------------------------------\n",
      "Max loss: 0.020882774144411087\n",
      "Min loss: 0.005693783517926931\n",
      "Mean loss: 0.013120686868205667\n",
      "Std loss: 0.00601248337709174\n",
      "Total Loss: 0.078724121209234\n",
      "------------------------------------ epoch 5909 (35448 steps) ------------------------------------\n",
      "Max loss: 0.027226854115724564\n",
      "Min loss: 0.004697613883763552\n",
      "Mean loss: 0.014745046307022372\n",
      "Std loss: 0.009102240108438811\n",
      "Total Loss: 0.08847027784213424\n",
      "------------------------------------ epoch 5910 (35454 steps) ------------------------------------\n",
      "Max loss: 0.013934537768363953\n",
      "Min loss: 0.005659396760165691\n",
      "Mean loss: 0.010062514493862787\n",
      "Std loss: 0.0025599307724174275\n",
      "Total Loss: 0.06037508696317673\n",
      "------------------------------------ epoch 5911 (35460 steps) ------------------------------------\n",
      "Max loss: 0.04141312092542648\n",
      "Min loss: 0.01046839915215969\n",
      "Mean loss: 0.020930204074829817\n",
      "Std loss: 0.011398717897514338\n",
      "Total Loss: 0.1255812244489789\n",
      "------------------------------------ epoch 5912 (35466 steps) ------------------------------------\n",
      "Max loss: 0.01636122725903988\n",
      "Min loss: 0.006540544331073761\n",
      "Mean loss: 0.01236618465433518\n",
      "Std loss: 0.003292408241131666\n",
      "Total Loss: 0.07419710792601109\n",
      "------------------------------------ epoch 5913 (35472 steps) ------------------------------------\n",
      "Max loss: 0.01966089755296707\n",
      "Min loss: 0.006463052239269018\n",
      "Mean loss: 0.012367869804923734\n",
      "Std loss: 0.004610071223009371\n",
      "Total Loss: 0.0742072188295424\n",
      "------------------------------------ epoch 5914 (35478 steps) ------------------------------------\n",
      "Max loss: 0.02980010211467743\n",
      "Min loss: 0.005984300281852484\n",
      "Mean loss: 0.01175899237083892\n",
      "Std loss: 0.008267770132007442\n",
      "Total Loss: 0.07055395422503352\n",
      "------------------------------------ epoch 5915 (35484 steps) ------------------------------------\n",
      "Max loss: 0.023753179237246513\n",
      "Min loss: 0.005847680848091841\n",
      "Mean loss: 0.010727622623865804\n",
      "Std loss: 0.006078916595979664\n",
      "Total Loss: 0.06436573574319482\n",
      "------------------------------------ epoch 5916 (35490 steps) ------------------------------------\n",
      "Max loss: 0.0157608725130558\n",
      "Min loss: 0.004268823191523552\n",
      "Mean loss: 0.008950365552057823\n",
      "Std loss: 0.004330787639550479\n",
      "Total Loss: 0.053702193312346935\n",
      "------------------------------------ epoch 5917 (35496 steps) ------------------------------------\n",
      "Max loss: 0.012662546709179878\n",
      "Min loss: 0.0054923370480537415\n",
      "Mean loss: 0.008547277810672918\n",
      "Std loss: 0.0026112988184006365\n",
      "Total Loss: 0.051283666864037514\n",
      "------------------------------------ epoch 5918 (35502 steps) ------------------------------------\n",
      "Max loss: 0.007621977478265762\n",
      "Min loss: 0.004761960357427597\n",
      "Mean loss: 0.00655219432276984\n",
      "Std loss: 0.000979939127898238\n",
      "Total Loss: 0.03931316593661904\n",
      "------------------------------------ epoch 5919 (35508 steps) ------------------------------------\n",
      "Max loss: 0.008554227650165558\n",
      "Min loss: 0.0063141281716525555\n",
      "Mean loss: 0.007485681679099798\n",
      "Std loss: 0.0010058930293978395\n",
      "Total Loss: 0.04491409007459879\n",
      "------------------------------------ epoch 5920 (35514 steps) ------------------------------------\n",
      "Max loss: 0.0119797233492136\n",
      "Min loss: 0.005200468003749847\n",
      "Mean loss: 0.00808701904801031\n",
      "Std loss: 0.00221226932676843\n",
      "Total Loss: 0.04852211428806186\n",
      "------------------------------------ epoch 5921 (35520 steps) ------------------------------------\n",
      "Max loss: 0.013928575441241264\n",
      "Min loss: 0.005334071349352598\n",
      "Mean loss: 0.008184945598865548\n",
      "Std loss: 0.003066199956755473\n",
      "Total Loss: 0.04910967359319329\n",
      "------------------------------------ epoch 5922 (35526 steps) ------------------------------------\n",
      "Max loss: 0.025554724037647247\n",
      "Min loss: 0.004376650787889957\n",
      "Mean loss: 0.009352926242475709\n",
      "Std loss: 0.007314773619787567\n",
      "Total Loss: 0.05611755745485425\n",
      "------------------------------------ epoch 5923 (35532 steps) ------------------------------------\n",
      "Max loss: 0.020846864208579063\n",
      "Min loss: 0.007105852477252483\n",
      "Mean loss: 0.011913409767051538\n",
      "Std loss: 0.005003274527298279\n",
      "Total Loss: 0.07148045860230923\n",
      "------------------------------------ epoch 5924 (35538 steps) ------------------------------------\n",
      "Max loss: 0.012199333868920803\n",
      "Min loss: 0.0039938404224812984\n",
      "Mean loss: 0.007122220083450277\n",
      "Std loss: 0.002574295128648468\n",
      "Total Loss: 0.042733320500701666\n",
      "------------------------------------ epoch 5925 (35544 steps) ------------------------------------\n",
      "Max loss: 0.027890807017683983\n",
      "Min loss: 0.0049310484901070595\n",
      "Mean loss: 0.01409057341516018\n",
      "Std loss: 0.007875745320271904\n",
      "Total Loss: 0.08454344049096107\n",
      "------------------------------------ epoch 5926 (35550 steps) ------------------------------------\n",
      "Max loss: 0.0237509123980999\n",
      "Min loss: 0.006133814342319965\n",
      "Mean loss: 0.011045959467689196\n",
      "Std loss: 0.00609334204924469\n",
      "Total Loss: 0.06627575680613518\n",
      "------------------------------------ epoch 5927 (35556 steps) ------------------------------------\n",
      "Max loss: 0.024463094770908356\n",
      "Min loss: 0.006415854673832655\n",
      "Mean loss: 0.012283555309598645\n",
      "Std loss: 0.005892688604264908\n",
      "Total Loss: 0.07370133185759187\n",
      "------------------------------------ epoch 5928 (35562 steps) ------------------------------------\n",
      "Max loss: 0.013093060813844204\n",
      "Min loss: 0.0051913270726799965\n",
      "Mean loss: 0.007409275509417057\n",
      "Std loss: 0.002704905375377686\n",
      "Total Loss: 0.04445565305650234\n",
      "------------------------------------ epoch 5929 (35568 steps) ------------------------------------\n",
      "Max loss: 0.02228909730911255\n",
      "Min loss: 0.004180884920060635\n",
      "Mean loss: 0.009231545651952425\n",
      "Std loss: 0.0060092579436829795\n",
      "Total Loss: 0.055389273911714554\n",
      "------------------------------------ epoch 5930 (35574 steps) ------------------------------------\n",
      "Max loss: 0.013688634149730206\n",
      "Min loss: 0.006790309213101864\n",
      "Mean loss: 0.009494055217752853\n",
      "Std loss: 0.002303062651531862\n",
      "Total Loss: 0.056964331306517124\n",
      "------------------------------------ epoch 5931 (35580 steps) ------------------------------------\n",
      "Max loss: 0.016821108758449554\n",
      "Min loss: 0.0060662440955638885\n",
      "Mean loss: 0.010041110217571259\n",
      "Std loss: 0.003410126304201166\n",
      "Total Loss: 0.06024666130542755\n",
      "------------------------------------ epoch 5932 (35586 steps) ------------------------------------\n",
      "Max loss: 0.028554430231451988\n",
      "Min loss: 0.005916871130466461\n",
      "Mean loss: 0.011957693379372358\n",
      "Std loss: 0.008579773831606229\n",
      "Total Loss: 0.07174616027623415\n",
      "------------------------------------ epoch 5933 (35592 steps) ------------------------------------\n",
      "Max loss: 0.04104146733880043\n",
      "Min loss: 0.005109917372465134\n",
      "Mean loss: 0.01643248670734465\n",
      "Std loss: 0.014040520724373947\n",
      "Total Loss: 0.09859492024406791\n",
      "------------------------------------ epoch 5934 (35598 steps) ------------------------------------\n",
      "Max loss: 0.025507815182209015\n",
      "Min loss: 0.009844382293522358\n",
      "Mean loss: 0.01625113080566128\n",
      "Std loss: 0.0052674139433356686\n",
      "Total Loss: 0.09750678483396769\n",
      "------------------------------------ epoch 5935 (35604 steps) ------------------------------------\n",
      "Max loss: 0.044229909777641296\n",
      "Min loss: 0.012335146777331829\n",
      "Mean loss: 0.023853697037945192\n",
      "Std loss: 0.009877959171107667\n",
      "Total Loss: 0.14312218222767115\n",
      "------------------------------------ epoch 5936 (35610 steps) ------------------------------------\n",
      "Max loss: 0.023548327386379242\n",
      "Min loss: 0.00927518866956234\n",
      "Mean loss: 0.016539452131837606\n",
      "Std loss: 0.005481387857802253\n",
      "Total Loss: 0.09923671279102564\n",
      "------------------------------------ epoch 5937 (35616 steps) ------------------------------------\n",
      "Max loss: 0.032340358942747116\n",
      "Min loss: 0.010715818032622337\n",
      "Mean loss: 0.015753794616709154\n",
      "Std loss: 0.007492043365402582\n",
      "Total Loss: 0.09452276770025492\n",
      "------------------------------------ epoch 5938 (35622 steps) ------------------------------------\n",
      "Max loss: 0.01798565313220024\n",
      "Min loss: 0.009025977924466133\n",
      "Mean loss: 0.011770203554381927\n",
      "Std loss: 0.003634941906411948\n",
      "Total Loss: 0.07062122132629156\n",
      "------------------------------------ epoch 5939 (35628 steps) ------------------------------------\n",
      "Max loss: 0.025046393275260925\n",
      "Min loss: 0.009204286150634289\n",
      "Mean loss: 0.013146953657269478\n",
      "Std loss: 0.005498502423594446\n",
      "Total Loss: 0.07888172194361687\n",
      "------------------------------------ epoch 5940 (35634 steps) ------------------------------------\n",
      "Max loss: 0.037029705941677094\n",
      "Min loss: 0.008287661708891392\n",
      "Mean loss: 0.016878810555984575\n",
      "Std loss: 0.010525101828345325\n",
      "Total Loss: 0.10127286333590746\n",
      "------------------------------------ epoch 5941 (35640 steps) ------------------------------------\n",
      "Max loss: 0.026255596429109573\n",
      "Min loss: 0.007021085359156132\n",
      "Mean loss: 0.01560132143398126\n",
      "Std loss: 0.006787163337218915\n",
      "Total Loss: 0.09360792860388756\n",
      "------------------------------------ epoch 5942 (35646 steps) ------------------------------------\n",
      "Max loss: 0.018278799951076508\n",
      "Min loss: 0.01052216999232769\n",
      "Mean loss: 0.013116775080561638\n",
      "Std loss: 0.0028616687187860905\n",
      "Total Loss: 0.07870065048336983\n",
      "------------------------------------ epoch 5943 (35652 steps) ------------------------------------\n",
      "Max loss: 0.012564470991492271\n",
      "Min loss: 0.005523707717657089\n",
      "Mean loss: 0.009108778710166613\n",
      "Std loss: 0.0022313543047503227\n",
      "Total Loss: 0.05465267226099968\n",
      "------------------------------------ epoch 5944 (35658 steps) ------------------------------------\n",
      "Max loss: 0.022387489676475525\n",
      "Min loss: 0.005800626706331968\n",
      "Mean loss: 0.0111521251189212\n",
      "Std loss: 0.005498158989930379\n",
      "Total Loss: 0.0669127507135272\n",
      "------------------------------------ epoch 5945 (35664 steps) ------------------------------------\n",
      "Max loss: 0.02780676633119583\n",
      "Min loss: 0.005795147269964218\n",
      "Mean loss: 0.013670790009200573\n",
      "Std loss: 0.0077559488887917125\n",
      "Total Loss: 0.08202474005520344\n",
      "------------------------------------ epoch 5946 (35670 steps) ------------------------------------\n",
      "Max loss: 0.026645494624972343\n",
      "Min loss: 0.0062190028838813305\n",
      "Mean loss: 0.012763082437838117\n",
      "Std loss: 0.007217894207685084\n",
      "Total Loss: 0.0765784946270287\n",
      "------------------------------------ epoch 5947 (35676 steps) ------------------------------------\n",
      "Max loss: 0.0245915986597538\n",
      "Min loss: 0.008111592382192612\n",
      "Mean loss: 0.012211637881894907\n",
      "Std loss: 0.005764601935279595\n",
      "Total Loss: 0.07326982729136944\n",
      "------------------------------------ epoch 5948 (35682 steps) ------------------------------------\n",
      "Max loss: 0.03218088299036026\n",
      "Min loss: 0.006048732437193394\n",
      "Mean loss: 0.01464668195694685\n",
      "Std loss: 0.009021814696787278\n",
      "Total Loss: 0.0878800917416811\n",
      "------------------------------------ epoch 5949 (35688 steps) ------------------------------------\n",
      "Max loss: 0.017745956778526306\n",
      "Min loss: 0.005601084791123867\n",
      "Mean loss: 0.01133372588083148\n",
      "Std loss: 0.004341147313256285\n",
      "Total Loss: 0.06800235528498888\n",
      "------------------------------------ epoch 5950 (35694 steps) ------------------------------------\n",
      "Max loss: 0.02146969363093376\n",
      "Min loss: 0.006556237116456032\n",
      "Mean loss: 0.009751228848472238\n",
      "Std loss: 0.005278373952191162\n",
      "Total Loss: 0.058507373090833426\n",
      "------------------------------------ epoch 5951 (35700 steps) ------------------------------------\n",
      "Max loss: 0.017777500674128532\n",
      "Min loss: 0.0049079908058047295\n",
      "Mean loss: 0.007992667301247517\n",
      "Std loss: 0.004433729634640371\n",
      "Total Loss: 0.047956003807485104\n",
      "------------------------------------ epoch 5952 (35706 steps) ------------------------------------\n",
      "Max loss: 0.015636909753084183\n",
      "Min loss: 0.005160611122846603\n",
      "Mean loss: 0.007764515001326799\n",
      "Std loss: 0.0036446172842473866\n",
      "Total Loss: 0.046587090007960796\n",
      "------------------------------------ epoch 5953 (35712 steps) ------------------------------------\n",
      "Max loss: 0.016238564625382423\n",
      "Min loss: 0.005024419631808996\n",
      "Mean loss: 0.010358776819581786\n",
      "Std loss: 0.00401537896318624\n",
      "Total Loss: 0.06215266091749072\n",
      "------------------------------------ epoch 5954 (35718 steps) ------------------------------------\n",
      "Max loss: 0.015485206618905067\n",
      "Min loss: 0.005398785229772329\n",
      "Mean loss: 0.008562300742293397\n",
      "Std loss: 0.0032079631363492616\n",
      "Total Loss: 0.051373804453760386\n",
      "------------------------------------ epoch 5955 (35724 steps) ------------------------------------\n",
      "Max loss: 0.01085861586034298\n",
      "Min loss: 0.005100419744849205\n",
      "Mean loss: 0.007577615712458889\n",
      "Std loss: 0.0019327487383133289\n",
      "Total Loss: 0.04546569427475333\n",
      "------------------------------------ epoch 5956 (35730 steps) ------------------------------------\n",
      "Max loss: 0.026106981560587883\n",
      "Min loss: 0.005359867587685585\n",
      "Mean loss: 0.011567268598203858\n",
      "Std loss: 0.007099779362636041\n",
      "Total Loss: 0.06940361158922315\n",
      "------------------------------------ epoch 5957 (35736 steps) ------------------------------------\n",
      "Max loss: 0.027553509920835495\n",
      "Min loss: 0.006325317546725273\n",
      "Mean loss: 0.012627773297329744\n",
      "Std loss: 0.0070889612666460025\n",
      "Total Loss: 0.07576663978397846\n",
      "------------------------------------ epoch 5958 (35742 steps) ------------------------------------\n",
      "Max loss: 0.033479586243629456\n",
      "Min loss: 0.005557835102081299\n",
      "Mean loss: 0.013205058562258879\n",
      "Std loss: 0.009307359449907368\n",
      "Total Loss: 0.07923035137355328\n",
      "------------------------------------ epoch 5959 (35748 steps) ------------------------------------\n",
      "Max loss: 0.013863010331988335\n",
      "Min loss: 0.00523495813831687\n",
      "Mean loss: 0.009545624954625964\n",
      "Std loss: 0.0035438295001723745\n",
      "Total Loss: 0.057273749727755785\n",
      "------------------------------------ epoch 5960 (35754 steps) ------------------------------------\n",
      "Max loss: 0.01787560246884823\n",
      "Min loss: 0.005127549171447754\n",
      "Mean loss: 0.009535928489640355\n",
      "Std loss: 0.004413295102341652\n",
      "Total Loss: 0.05721557093784213\n",
      "------------------------------------ epoch 5961 (35760 steps) ------------------------------------\n",
      "Max loss: 0.019325153902173042\n",
      "Min loss: 0.004575404804199934\n",
      "Mean loss: 0.008690537108729282\n",
      "Std loss: 0.005175607777526998\n",
      "Total Loss: 0.0521432226523757\n",
      "------------------------------------ epoch 5962 (35766 steps) ------------------------------------\n",
      "Max loss: 0.04222717881202698\n",
      "Min loss: 0.00512149091809988\n",
      "Mean loss: 0.013965846039354801\n",
      "Std loss: 0.012877755515424002\n",
      "Total Loss: 0.08379507623612881\n",
      "------------------------------------ epoch 5963 (35772 steps) ------------------------------------\n",
      "Max loss: 0.021760746836662292\n",
      "Min loss: 0.0059691378846764565\n",
      "Mean loss: 0.012375476925323406\n",
      "Std loss: 0.0061366000265355635\n",
      "Total Loss: 0.07425286155194044\n",
      "------------------------------------ epoch 5964 (35778 steps) ------------------------------------\n",
      "Max loss: 0.023132827132940292\n",
      "Min loss: 0.007183987647294998\n",
      "Mean loss: 0.01251415116712451\n",
      "Std loss: 0.005466535398821714\n",
      "Total Loss: 0.07508490700274706\n",
      "------------------------------------ epoch 5965 (35784 steps) ------------------------------------\n",
      "Max loss: 0.013812676072120667\n",
      "Min loss: 0.006556456442922354\n",
      "Mean loss: 0.008969180984422565\n",
      "Std loss: 0.0022843965660209547\n",
      "Total Loss: 0.05381508590653539\n",
      "------------------------------------ epoch 5966 (35790 steps) ------------------------------------\n",
      "Max loss: 0.025840533897280693\n",
      "Min loss: 0.0065491413697600365\n",
      "Mean loss: 0.014889651133368412\n",
      "Std loss: 0.007735357581350946\n",
      "Total Loss: 0.08933790680021048\n",
      "------------------------------------ epoch 5967 (35796 steps) ------------------------------------\n",
      "Max loss: 0.02077299728989601\n",
      "Min loss: 0.007157871033996344\n",
      "Mean loss: 0.012966827567045888\n",
      "Std loss: 0.005168816574018853\n",
      "Total Loss: 0.07780096540227532\n",
      "------------------------------------ epoch 5968 (35802 steps) ------------------------------------\n",
      "Max loss: 0.04467092081904411\n",
      "Min loss: 0.005948184989392757\n",
      "Mean loss: 0.015721259483446676\n",
      "Std loss: 0.013669394046037831\n",
      "Total Loss: 0.09432755690068007\n",
      "------------------------------------ epoch 5969 (35808 steps) ------------------------------------\n",
      "Max loss: 0.014579126611351967\n",
      "Min loss: 0.005915041547268629\n",
      "Mean loss: 0.010007159551605582\n",
      "Std loss: 0.0026236192938945174\n",
      "Total Loss: 0.06004295730963349\n",
      "------------------------------------ epoch 5970 (35814 steps) ------------------------------------\n",
      "Max loss: 0.026815250515937805\n",
      "Min loss: 0.008428315632045269\n",
      "Mean loss: 0.019095157117893297\n",
      "Std loss: 0.007513174426175169\n",
      "Total Loss: 0.11457094270735979\n",
      "------------------------------------ epoch 5971 (35820 steps) ------------------------------------\n",
      "Max loss: 0.023134738206863403\n",
      "Min loss: 0.009304886683821678\n",
      "Mean loss: 0.01635419049610694\n",
      "Std loss: 0.005092053671014944\n",
      "Total Loss: 0.09812514297664165\n",
      "------------------------------------ epoch 5972 (35826 steps) ------------------------------------\n",
      "Max loss: 0.018665486946702003\n",
      "Min loss: 0.006705890409648418\n",
      "Mean loss: 0.011013431552176675\n",
      "Std loss: 0.004353462835433676\n",
      "Total Loss: 0.06608058931306005\n",
      "------------------------------------ epoch 5973 (35832 steps) ------------------------------------\n",
      "Max loss: 0.013040738180279732\n",
      "Min loss: 0.0061125545762479305\n",
      "Mean loss: 0.008686218255509933\n",
      "Std loss: 0.002310599568267448\n",
      "Total Loss: 0.0521173095330596\n",
      "------------------------------------ epoch 5974 (35838 steps) ------------------------------------\n",
      "Max loss: 0.20229516923427582\n",
      "Min loss: 0.010091559961438179\n",
      "Mean loss: 0.062225971991817154\n",
      "Std loss: 0.06566138549154343\n",
      "Total Loss: 0.37335583195090294\n",
      "------------------------------------ epoch 5975 (35844 steps) ------------------------------------\n",
      "Max loss: 0.15063577890396118\n",
      "Min loss: 0.06994671374559402\n",
      "Mean loss: 0.09331361825267474\n",
      "Std loss: 0.028559783297946596\n",
      "Total Loss: 0.5598817095160484\n",
      "------------------------------------ epoch 5976 (35850 steps) ------------------------------------\n",
      "Max loss: 0.1778288334608078\n",
      "Min loss: 0.0662202313542366\n",
      "Mean loss: 0.11892363056540489\n",
      "Std loss: 0.038698905267743694\n",
      "Total Loss: 0.7135417833924294\n",
      "------------------------------------ epoch 5977 (35856 steps) ------------------------------------\n",
      "Max loss: 0.14933477342128754\n",
      "Min loss: 0.07606414705514908\n",
      "Mean loss: 0.10489349191387494\n",
      "Std loss: 0.03039738206631724\n",
      "Total Loss: 0.6293609514832497\n",
      "------------------------------------ epoch 5978 (35862 steps) ------------------------------------\n",
      "Max loss: 0.11876000463962555\n",
      "Min loss: 0.05360062047839165\n",
      "Mean loss: 0.07665168369809787\n",
      "Std loss: 0.021327541708703254\n",
      "Total Loss: 0.4599101021885872\n",
      "------------------------------------ epoch 5979 (35868 steps) ------------------------------------\n",
      "Max loss: 0.046897195279598236\n",
      "Min loss: 0.04071911796927452\n",
      "Mean loss: 0.044080535570780434\n",
      "Std loss: 0.0022097729367122243\n",
      "Total Loss: 0.2644832134246826\n",
      "------------------------------------ epoch 5980 (35874 steps) ------------------------------------\n",
      "Max loss: 0.07409664243459702\n",
      "Min loss: 0.024557646363973618\n",
      "Mean loss: 0.04252584899465243\n",
      "Std loss: 0.015608255654787753\n",
      "Total Loss: 0.2551550939679146\n",
      "------------------------------------ epoch 5981 (35880 steps) ------------------------------------\n",
      "Max loss: 0.04181826114654541\n",
      "Min loss: 0.020512796938419342\n",
      "Mean loss: 0.028068109105030697\n",
      "Std loss: 0.007136926473568318\n",
      "Total Loss: 0.16840865463018417\n",
      "------------------------------------ epoch 5982 (35886 steps) ------------------------------------\n",
      "Max loss: 0.03685161843895912\n",
      "Min loss: 0.01644686423242092\n",
      "Mean loss: 0.024959416439135868\n",
      "Std loss: 0.0072526427904303584\n",
      "Total Loss: 0.14975649863481522\n",
      "------------------------------------ epoch 5983 (35892 steps) ------------------------------------\n",
      "Max loss: 0.030511848628520966\n",
      "Min loss: 0.013294383883476257\n",
      "Mean loss: 0.022823625554641087\n",
      "Std loss: 0.006161684316337949\n",
      "Total Loss: 0.13694175332784653\n",
      "------------------------------------ epoch 5984 (35898 steps) ------------------------------------\n",
      "Max loss: 0.04356926307082176\n",
      "Min loss: 0.01234270166605711\n",
      "Mean loss: 0.022776601525644462\n",
      "Std loss: 0.010265587565190418\n",
      "Total Loss: 0.13665960915386677\n",
      "------------------------------------ epoch 5985 (35904 steps) ------------------------------------\n",
      "Max loss: 0.031869083642959595\n",
      "Min loss: 0.011942493729293346\n",
      "Mean loss: 0.017652909581859905\n",
      "Std loss: 0.0068214874994362755\n",
      "Total Loss: 0.10591745749115944\n",
      "------------------------------------ epoch 5986 (35910 steps) ------------------------------------\n",
      "Max loss: 0.027006952092051506\n",
      "Min loss: 0.011020204052329063\n",
      "Mean loss: 0.01718129465977351\n",
      "Std loss: 0.00545781782552815\n",
      "Total Loss: 0.10308776795864105\n",
      "------------------------------------ epoch 5987 (35916 steps) ------------------------------------\n",
      "Max loss: 0.04261375963687897\n",
      "Min loss: 0.008909820578992367\n",
      "Mean loss: 0.02123444325601061\n",
      "Std loss: 0.011772695766833501\n",
      "Total Loss: 0.12740665953606367\n",
      "------------------------------------ epoch 5988 (35922 steps) ------------------------------------\n",
      "Max loss: 0.01796353980898857\n",
      "Min loss: 0.010207582265138626\n",
      "Mean loss: 0.012462836690247059\n",
      "Std loss: 0.002609728823934448\n",
      "Total Loss: 0.07477702014148235\n",
      "------------------------------------ epoch 5989 (35928 steps) ------------------------------------\n",
      "Max loss: 0.025352193042635918\n",
      "Min loss: 0.008562213741242886\n",
      "Mean loss: 0.014080852114905914\n",
      "Std loss: 0.005613750359497654\n",
      "Total Loss: 0.08448511268943548\n",
      "------------------------------------ epoch 5990 (35934 steps) ------------------------------------\n",
      "Max loss: 0.032353319227695465\n",
      "Min loss: 0.010205277241766453\n",
      "Mean loss: 0.018162442060808342\n",
      "Std loss: 0.007936439578016105\n",
      "Total Loss: 0.10897465236485004\n",
      "------------------------------------ epoch 5991 (35940 steps) ------------------------------------\n",
      "Max loss: 0.012418268248438835\n",
      "Min loss: 0.007587534841150045\n",
      "Mean loss: 0.009147021841878692\n",
      "Std loss: 0.0016227299423492228\n",
      "Total Loss: 0.054882131051272154\n",
      "------------------------------------ epoch 5992 (35946 steps) ------------------------------------\n",
      "Max loss: 0.022053364664316177\n",
      "Min loss: 0.006676217075437307\n",
      "Mean loss: 0.012435693526640534\n",
      "Std loss: 0.005140409874807094\n",
      "Total Loss: 0.0746141611598432\n",
      "------------------------------------ epoch 5993 (35952 steps) ------------------------------------\n",
      "Max loss: 0.021386077627539635\n",
      "Min loss: 0.008959352970123291\n",
      "Mean loss: 0.012491918945064148\n",
      "Std loss: 0.0041441894545663225\n",
      "Total Loss: 0.07495151367038488\n",
      "------------------------------------ epoch 5994 (35958 steps) ------------------------------------\n",
      "Max loss: 0.013604163192212582\n",
      "Min loss: 0.007511158939450979\n",
      "Mean loss: 0.01122871219801406\n",
      "Std loss: 0.0021103674510793598\n",
      "Total Loss: 0.06737227318808436\n",
      "------------------------------------ epoch 5995 (35964 steps) ------------------------------------\n",
      "Max loss: 0.04417099058628082\n",
      "Min loss: 0.005923858843743801\n",
      "Mean loss: 0.020846491136277717\n",
      "Std loss: 0.015434178150886827\n",
      "Total Loss: 0.1250789468176663\n",
      "------------------------------------ epoch 5996 (35970 steps) ------------------------------------\n",
      "Max loss: 0.04341286048293114\n",
      "Min loss: 0.00680816825479269\n",
      "Mean loss: 0.016253300632039707\n",
      "Std loss: 0.012365393192608808\n",
      "Total Loss: 0.09751980379223824\n",
      "------------------------------------ epoch 5997 (35976 steps) ------------------------------------\n",
      "Max loss: 0.016537252813577652\n",
      "Min loss: 0.00634798314422369\n",
      "Mean loss: 0.00977708725258708\n",
      "Std loss: 0.0033753018179869102\n",
      "Total Loss: 0.05866252351552248\n",
      "------------------------------------ epoch 5998 (35982 steps) ------------------------------------\n",
      "Max loss: 0.04242953658103943\n",
      "Min loss: 0.007369094993919134\n",
      "Mean loss: 0.02109019543665151\n",
      "Std loss: 0.011635062049378907\n",
      "Total Loss: 0.12654117261990905\n",
      "------------------------------------ epoch 5999 (35988 steps) ------------------------------------\n",
      "Max loss: 0.010684887878596783\n",
      "Min loss: 0.0073007456958293915\n",
      "Mean loss: 0.008370753222455582\n",
      "Std loss: 0.0010864738476417414\n",
      "Total Loss: 0.050224519334733486\n",
      "------------------------------------ epoch 6000 (35994 steps) ------------------------------------\n",
      "Max loss: 0.01533315610140562\n",
      "Min loss: 0.006739165633916855\n",
      "Mean loss: 0.010802552880098423\n",
      "Std loss: 0.00326811894495038\n",
      "Total Loss: 0.06481531728059053\n",
      "------------------------------------ epoch 6001 (36000 steps) ------------------------------------\n",
      "Max loss: 0.02060340903699398\n",
      "Min loss: 0.00935768336057663\n",
      "Mean loss: 0.013395318761467934\n",
      "Std loss: 0.0035615685806111375\n",
      "Total Loss: 0.0803719125688076\n",
      "saved model at ./weights/model_6001.pth\n",
      "------------------------------------ epoch 6002 (36006 steps) ------------------------------------\n",
      "Max loss: 0.031109152361750603\n",
      "Min loss: 0.009198009967803955\n",
      "Mean loss: 0.01769560854882002\n",
      "Std loss: 0.007863204567159965\n",
      "Total Loss: 0.10617365129292011\n",
      "------------------------------------ epoch 6003 (36012 steps) ------------------------------------\n",
      "Max loss: 0.011172814294695854\n",
      "Min loss: 0.006255242973566055\n",
      "Mean loss: 0.007753276887039344\n",
      "Std loss: 0.0016655031628401787\n",
      "Total Loss: 0.04651966132223606\n",
      "------------------------------------ epoch 6004 (36018 steps) ------------------------------------\n",
      "Max loss: 0.02148744836449623\n",
      "Min loss: 0.008061195723712444\n",
      "Mean loss: 0.013550932984799147\n",
      "Std loss: 0.005120679624923027\n",
      "Total Loss: 0.08130559790879488\n",
      "------------------------------------ epoch 6005 (36024 steps) ------------------------------------\n",
      "Max loss: 0.028616486117243767\n",
      "Min loss: 0.008544440381228924\n",
      "Mean loss: 0.012874014675617218\n",
      "Std loss: 0.0071740868295774845\n",
      "Total Loss: 0.07724408805370331\n",
      "------------------------------------ epoch 6006 (36030 steps) ------------------------------------\n",
      "Max loss: 0.014545097947120667\n",
      "Min loss: 0.00737310666590929\n",
      "Mean loss: 0.009942227353652319\n",
      "Std loss: 0.002220921310410154\n",
      "Total Loss: 0.05965336412191391\n",
      "------------------------------------ epoch 6007 (36036 steps) ------------------------------------\n",
      "Max loss: 0.021724557504057884\n",
      "Min loss: 0.006162894424051046\n",
      "Mean loss: 0.009989575948566198\n",
      "Std loss: 0.005349730834879297\n",
      "Total Loss: 0.05993745569139719\n",
      "------------------------------------ epoch 6008 (36042 steps) ------------------------------------\n",
      "Max loss: 0.015727784484624863\n",
      "Min loss: 0.005997765809297562\n",
      "Mean loss: 0.010139903364082178\n",
      "Std loss: 0.003323113516452501\n",
      "Total Loss: 0.060839420184493065\n",
      "------------------------------------ epoch 6009 (36048 steps) ------------------------------------\n",
      "Max loss: 0.012321030721068382\n",
      "Min loss: 0.005763323046267033\n",
      "Mean loss: 0.008690370169157783\n",
      "Std loss: 0.002140449489111882\n",
      "Total Loss: 0.0521422210149467\n",
      "------------------------------------ epoch 6010 (36054 steps) ------------------------------------\n",
      "Max loss: 0.014984211884438992\n",
      "Min loss: 0.0055290767922997475\n",
      "Mean loss: 0.008825828709329167\n",
      "Std loss: 0.002986547647164037\n",
      "Total Loss: 0.05295497225597501\n",
      "------------------------------------ epoch 6011 (36060 steps) ------------------------------------\n",
      "Max loss: 0.030234811827540398\n",
      "Min loss: 0.007802275009453297\n",
      "Mean loss: 0.01642158068716526\n",
      "Std loss: 0.008769968321704345\n",
      "Total Loss: 0.09852948412299156\n",
      "------------------------------------ epoch 6012 (36066 steps) ------------------------------------\n",
      "Max loss: 0.06045806407928467\n",
      "Min loss: 0.007013413123786449\n",
      "Mean loss: 0.017588315065950155\n",
      "Std loss: 0.019246065802635357\n",
      "Total Loss: 0.10552989039570093\n",
      "------------------------------------ epoch 6013 (36072 steps) ------------------------------------\n",
      "Max loss: 0.010037325322628021\n",
      "Min loss: 0.005291061010211706\n",
      "Mean loss: 0.008741166985904178\n",
      "Std loss: 0.0016316423924399937\n",
      "Total Loss: 0.05244700191542506\n",
      "------------------------------------ epoch 6014 (36078 steps) ------------------------------------\n",
      "Max loss: 0.020607562735676765\n",
      "Min loss: 0.007945647463202477\n",
      "Mean loss: 0.014601607962201038\n",
      "Std loss: 0.003954077943901509\n",
      "Total Loss: 0.08760964777320623\n",
      "------------------------------------ epoch 6015 (36084 steps) ------------------------------------\n",
      "Max loss: 0.023906206712126732\n",
      "Min loss: 0.006111702881753445\n",
      "Mean loss: 0.012503538280725479\n",
      "Std loss: 0.005837591768916733\n",
      "Total Loss: 0.07502122968435287\n",
      "------------------------------------ epoch 6016 (36090 steps) ------------------------------------\n",
      "Max loss: 0.013404881581664085\n",
      "Min loss: 0.00564440106973052\n",
      "Mean loss: 0.009897500664616624\n",
      "Std loss: 0.0025863752188819656\n",
      "Total Loss: 0.05938500398769975\n",
      "------------------------------------ epoch 6017 (36096 steps) ------------------------------------\n",
      "Max loss: 0.03268132731318474\n",
      "Min loss: 0.0070609478279948235\n",
      "Mean loss: 0.01444162055850029\n",
      "Std loss: 0.008578312261890625\n",
      "Total Loss: 0.08664972335100174\n",
      "------------------------------------ epoch 6018 (36102 steps) ------------------------------------\n",
      "Max loss: 0.0157761350274086\n",
      "Min loss: 0.005607565399259329\n",
      "Mean loss: 0.00932458663980166\n",
      "Std loss: 0.003384812865717097\n",
      "Total Loss: 0.05594751983880997\n",
      "------------------------------------ epoch 6019 (36108 steps) ------------------------------------\n",
      "Max loss: 0.013529847376048565\n",
      "Min loss: 0.0060052121989429\n",
      "Mean loss: 0.009303568163886666\n",
      "Std loss: 0.0025602705042473035\n",
      "Total Loss: 0.05582140898332\n",
      "------------------------------------ epoch 6020 (36114 steps) ------------------------------------\n",
      "Max loss: 0.021463707089424133\n",
      "Min loss: 0.006526726298034191\n",
      "Mean loss: 0.012195719250788292\n",
      "Std loss: 0.004982588449934476\n",
      "Total Loss: 0.07317431550472975\n",
      "------------------------------------ epoch 6021 (36120 steps) ------------------------------------\n",
      "Max loss: 0.016357094049453735\n",
      "Min loss: 0.0049330382607877254\n",
      "Mean loss: 0.010010039201006293\n",
      "Std loss: 0.004097044127283017\n",
      "Total Loss: 0.06006023520603776\n",
      "------------------------------------ epoch 6022 (36126 steps) ------------------------------------\n",
      "Max loss: 0.029608402401208878\n",
      "Min loss: 0.0048740701749920845\n",
      "Mean loss: 0.013002672465518117\n",
      "Std loss: 0.00852664301665281\n",
      "Total Loss: 0.0780160347931087\n",
      "------------------------------------ epoch 6023 (36132 steps) ------------------------------------\n",
      "Max loss: 0.02141701802611351\n",
      "Min loss: 0.007353572174906731\n",
      "Mean loss: 0.011182269624744853\n",
      "Std loss: 0.004854147617496454\n",
      "Total Loss: 0.06709361774846911\n",
      "------------------------------------ epoch 6024 (36138 steps) ------------------------------------\n",
      "Max loss: 0.014676129445433617\n",
      "Min loss: 0.005129147320985794\n",
      "Mean loss: 0.009742543799802661\n",
      "Std loss: 0.003609510555458121\n",
      "Total Loss: 0.058455262798815966\n",
      "------------------------------------ epoch 6025 (36144 steps) ------------------------------------\n",
      "Max loss: 0.028825707733631134\n",
      "Min loss: 0.008329639211297035\n",
      "Mean loss: 0.014744466791550318\n",
      "Std loss: 0.007562396666358086\n",
      "Total Loss: 0.08846680074930191\n",
      "------------------------------------ epoch 6026 (36150 steps) ------------------------------------\n",
      "Max loss: 0.01666712388396263\n",
      "Min loss: 0.006488288752734661\n",
      "Mean loss: 0.010100204808016619\n",
      "Std loss: 0.0035482540885190487\n",
      "Total Loss: 0.06060122884809971\n",
      "------------------------------------ epoch 6027 (36156 steps) ------------------------------------\n",
      "Max loss: 0.01284471433609724\n",
      "Min loss: 0.006370916962623596\n",
      "Mean loss: 0.00853836901175479\n",
      "Std loss: 0.0022757946496318984\n",
      "Total Loss: 0.051230214070528746\n",
      "------------------------------------ epoch 6028 (36162 steps) ------------------------------------\n",
      "Max loss: 0.024186722934246063\n",
      "Min loss: 0.0064319223165512085\n",
      "Mean loss: 0.011366830362627903\n",
      "Std loss: 0.005971624033833602\n",
      "Total Loss: 0.06820098217576742\n",
      "------------------------------------ epoch 6029 (36168 steps) ------------------------------------\n",
      "Max loss: 0.017028316855430603\n",
      "Min loss: 0.0071315402165055275\n",
      "Mean loss: 0.0122194637854894\n",
      "Std loss: 0.003072846344351158\n",
      "Total Loss: 0.0733167827129364\n",
      "------------------------------------ epoch 6030 (36174 steps) ------------------------------------\n",
      "Max loss: 0.0680031105875969\n",
      "Min loss: 0.006734701804816723\n",
      "Mean loss: 0.01772500326236089\n",
      "Std loss: 0.022522292597655702\n",
      "Total Loss: 0.10635001957416534\n",
      "------------------------------------ epoch 6031 (36180 steps) ------------------------------------\n",
      "Max loss: 0.01641014590859413\n",
      "Min loss: 0.006805995013564825\n",
      "Mean loss: 0.011012432398274541\n",
      "Std loss: 0.0030956400521611727\n",
      "Total Loss: 0.06607459438964725\n",
      "------------------------------------ epoch 6032 (36186 steps) ------------------------------------\n",
      "Max loss: 0.022283917292952538\n",
      "Min loss: 0.0061097717843949795\n",
      "Mean loss: 0.011324143425251046\n",
      "Std loss: 0.005129368278920911\n",
      "Total Loss: 0.06794486055150628\n",
      "------------------------------------ epoch 6033 (36192 steps) ------------------------------------\n",
      "Max loss: 0.013030711561441422\n",
      "Min loss: 0.0052855173125863075\n",
      "Mean loss: 0.007539805024862289\n",
      "Std loss: 0.002669108505900129\n",
      "Total Loss: 0.04523883014917374\n",
      "------------------------------------ epoch 6034 (36198 steps) ------------------------------------\n",
      "Max loss: 0.017135217785835266\n",
      "Min loss: 0.006597307976335287\n",
      "Mean loss: 0.009751529200002551\n",
      "Std loss: 0.0034645122030047618\n",
      "Total Loss: 0.058509175200015306\n",
      "------------------------------------ epoch 6035 (36204 steps) ------------------------------------\n",
      "Max loss: 0.05211693048477173\n",
      "Min loss: 0.006693737581372261\n",
      "Mean loss: 0.018510613900919754\n",
      "Std loss: 0.016492872359298097\n",
      "Total Loss: 0.11106368340551853\n",
      "------------------------------------ epoch 6036 (36210 steps) ------------------------------------\n",
      "Max loss: 0.010817042551934719\n",
      "Min loss: 0.006206981837749481\n",
      "Mean loss: 0.008331371781726679\n",
      "Std loss: 0.0018483494670630373\n",
      "Total Loss: 0.04998823069036007\n",
      "------------------------------------ epoch 6037 (36216 steps) ------------------------------------\n",
      "Max loss: 0.024928968399763107\n",
      "Min loss: 0.0052741169929504395\n",
      "Mean loss: 0.01090696050475041\n",
      "Std loss: 0.006562471713856294\n",
      "Total Loss: 0.06544176302850246\n",
      "------------------------------------ epoch 6038 (36222 steps) ------------------------------------\n",
      "Max loss: 0.024573683738708496\n",
      "Min loss: 0.008141051977872849\n",
      "Mean loss: 0.013841938382635513\n",
      "Std loss: 0.005924784288097284\n",
      "Total Loss: 0.08305163029581308\n",
      "------------------------------------ epoch 6039 (36228 steps) ------------------------------------\n",
      "Max loss: 0.025570426136255264\n",
      "Min loss: 0.004738454706966877\n",
      "Mean loss: 0.011645006326337656\n",
      "Std loss: 0.00685372131191471\n",
      "Total Loss: 0.06987003795802593\n",
      "------------------------------------ epoch 6040 (36234 steps) ------------------------------------\n",
      "Max loss: 0.012955322861671448\n",
      "Min loss: 0.005572743713855743\n",
      "Mean loss: 0.008728232700377703\n",
      "Std loss: 0.002547389771672455\n",
      "Total Loss: 0.052369396202266216\n",
      "------------------------------------ epoch 6041 (36240 steps) ------------------------------------\n",
      "Max loss: 0.01558054517954588\n",
      "Min loss: 0.005478336475789547\n",
      "Mean loss: 0.00877401310329636\n",
      "Std loss: 0.0034209280193599253\n",
      "Total Loss: 0.052644078619778156\n",
      "------------------------------------ epoch 6042 (36246 steps) ------------------------------------\n",
      "Max loss: 0.022823192179203033\n",
      "Min loss: 0.006643248256295919\n",
      "Mean loss: 0.010227342989916602\n",
      "Std loss: 0.005738582937261058\n",
      "Total Loss: 0.06136405793949962\n",
      "------------------------------------ epoch 6043 (36252 steps) ------------------------------------\n",
      "Max loss: 0.01280309073626995\n",
      "Min loss: 0.004752007313072681\n",
      "Mean loss: 0.007387131918221712\n",
      "Std loss: 0.0026821441496394038\n",
      "Total Loss: 0.04432279150933027\n",
      "------------------------------------ epoch 6044 (36258 steps) ------------------------------------\n",
      "Max loss: 0.01437048614025116\n",
      "Min loss: 0.004851398523896933\n",
      "Mean loss: 0.008942442558084926\n",
      "Std loss: 0.0035569486052039762\n",
      "Total Loss: 0.05365465534850955\n",
      "------------------------------------ epoch 6045 (36264 steps) ------------------------------------\n",
      "Max loss: 0.01006085891276598\n",
      "Min loss: 0.004867522045969963\n",
      "Mean loss: 0.0076207440967361135\n",
      "Std loss: 0.0019239463271028216\n",
      "Total Loss: 0.04572446458041668\n",
      "------------------------------------ epoch 6046 (36270 steps) ------------------------------------\n",
      "Max loss: 0.01678207889199257\n",
      "Min loss: 0.005296609364449978\n",
      "Mean loss: 0.00856339760745565\n",
      "Std loss: 0.004128306567517915\n",
      "Total Loss: 0.051380385644733906\n",
      "------------------------------------ epoch 6047 (36276 steps) ------------------------------------\n",
      "Max loss: 0.012022982351481915\n",
      "Min loss: 0.005829413887113333\n",
      "Mean loss: 0.008933768840506673\n",
      "Std loss: 0.0022865846771011828\n",
      "Total Loss: 0.05360261304304004\n",
      "------------------------------------ epoch 6048 (36282 steps) ------------------------------------\n",
      "Max loss: 0.008301403373479843\n",
      "Min loss: 0.005029484163969755\n",
      "Mean loss: 0.006563042368118961\n",
      "Std loss: 0.0010887087306585327\n",
      "Total Loss: 0.03937825420871377\n",
      "------------------------------------ epoch 6049 (36288 steps) ------------------------------------\n",
      "Max loss: 0.0165927242487669\n",
      "Min loss: 0.005282506346702576\n",
      "Mean loss: 0.009925515856593847\n",
      "Std loss: 0.0040778165799469565\n",
      "Total Loss: 0.059553095139563084\n",
      "------------------------------------ epoch 6050 (36294 steps) ------------------------------------\n",
      "Max loss: 0.04705177992582321\n",
      "Min loss: 0.004850203171372414\n",
      "Mean loss: 0.014834611831853787\n",
      "Std loss: 0.014789935845491247\n",
      "Total Loss: 0.08900767099112272\n",
      "------------------------------------ epoch 6051 (36300 steps) ------------------------------------\n",
      "Max loss: 0.022980868816375732\n",
      "Min loss: 0.006348916329443455\n",
      "Mean loss: 0.011891623182843128\n",
      "Std loss: 0.005241571328683141\n",
      "Total Loss: 0.07134973909705877\n",
      "------------------------------------ epoch 6052 (36306 steps) ------------------------------------\n",
      "Max loss: 0.012383883818984032\n",
      "Min loss: 0.005571427755057812\n",
      "Mean loss: 0.008238651556894183\n",
      "Std loss: 0.0022135980490267685\n",
      "Total Loss: 0.0494319093413651\n",
      "------------------------------------ epoch 6053 (36312 steps) ------------------------------------\n",
      "Max loss: 0.035706404596567154\n",
      "Min loss: 0.007110056467354298\n",
      "Mean loss: 0.01325844181701541\n",
      "Std loss: 0.010161695789784993\n",
      "Total Loss: 0.07955065090209246\n",
      "------------------------------------ epoch 6054 (36318 steps) ------------------------------------\n",
      "Max loss: 0.019455574452877045\n",
      "Min loss: 0.006571829319000244\n",
      "Mean loss: 0.00999245971130828\n",
      "Std loss: 0.004395896608435979\n",
      "Total Loss: 0.059954758267849684\n",
      "------------------------------------ epoch 6055 (36324 steps) ------------------------------------\n",
      "Max loss: 0.009909601882100105\n",
      "Min loss: 0.004504583775997162\n",
      "Mean loss: 0.0063653800170868635\n",
      "Std loss: 0.0018427516651273574\n",
      "Total Loss: 0.03819228010252118\n",
      "------------------------------------ epoch 6056 (36330 steps) ------------------------------------\n",
      "Max loss: 0.013920404016971588\n",
      "Min loss: 0.006091126706451178\n",
      "Mean loss: 0.008788676466792822\n",
      "Std loss: 0.0027846621710077638\n",
      "Total Loss: 0.05273205880075693\n",
      "------------------------------------ epoch 6057 (36336 steps) ------------------------------------\n",
      "Max loss: 0.02409631572663784\n",
      "Min loss: 0.004732287023216486\n",
      "Mean loss: 0.010548587966089448\n",
      "Std loss: 0.006971965429998376\n",
      "Total Loss: 0.06329152779653668\n",
      "------------------------------------ epoch 6058 (36342 steps) ------------------------------------\n",
      "Max loss: 0.009498989209532738\n",
      "Min loss: 0.005611587315797806\n",
      "Mean loss: 0.007063816689575712\n",
      "Std loss: 0.001568524671185668\n",
      "Total Loss: 0.04238290013745427\n",
      "------------------------------------ epoch 6059 (36348 steps) ------------------------------------\n",
      "Max loss: 0.012275461107492447\n",
      "Min loss: 0.0058165243826806545\n",
      "Mean loss: 0.008330174023285508\n",
      "Std loss: 0.0020993143818543794\n",
      "Total Loss: 0.04998104413971305\n",
      "------------------------------------ epoch 6060 (36354 steps) ------------------------------------\n",
      "Max loss: 0.013358521275222301\n",
      "Min loss: 0.005134164355695248\n",
      "Mean loss: 0.008457459354152283\n",
      "Std loss: 0.0032180042381542142\n",
      "Total Loss: 0.05074475612491369\n",
      "------------------------------------ epoch 6061 (36360 steps) ------------------------------------\n",
      "Max loss: 0.05287928506731987\n",
      "Min loss: 0.005405669566243887\n",
      "Mean loss: 0.014779580213750402\n",
      "Std loss: 0.01712240606850677\n",
      "Total Loss: 0.08867748128250241\n",
      "------------------------------------ epoch 6062 (36366 steps) ------------------------------------\n",
      "Max loss: 0.038297586143016815\n",
      "Min loss: 0.004712826572358608\n",
      "Mean loss: 0.011278406794493398\n",
      "Std loss: 0.012132863093754767\n",
      "Total Loss: 0.06767044076696038\n",
      "------------------------------------ epoch 6063 (36372 steps) ------------------------------------\n",
      "Max loss: 0.015157029032707214\n",
      "Min loss: 0.005638731177896261\n",
      "Mean loss: 0.010736021601284543\n",
      "Std loss: 0.002759122522021386\n",
      "Total Loss: 0.06441612960770726\n",
      "------------------------------------ epoch 6064 (36378 steps) ------------------------------------\n",
      "Max loss: 0.023541003465652466\n",
      "Min loss: 0.0056402310729026794\n",
      "Mean loss: 0.010824229335412383\n",
      "Std loss: 0.005914809360557735\n",
      "Total Loss: 0.0649453760124743\n",
      "------------------------------------ epoch 6065 (36384 steps) ------------------------------------\n",
      "Max loss: 0.037213727831840515\n",
      "Min loss: 0.006893429905176163\n",
      "Mean loss: 0.018703488167375326\n",
      "Std loss: 0.01047009555881798\n",
      "Total Loss: 0.11222092900425196\n",
      "------------------------------------ epoch 6066 (36390 steps) ------------------------------------\n",
      "Max loss: 0.019280709326267242\n",
      "Min loss: 0.004561603534966707\n",
      "Mean loss: 0.010192925964171687\n",
      "Std loss: 0.0055163376452745305\n",
      "Total Loss: 0.06115755578503013\n",
      "------------------------------------ epoch 6067 (36396 steps) ------------------------------------\n",
      "Max loss: 0.011894467286765575\n",
      "Min loss: 0.00628626486286521\n",
      "Mean loss: 0.009029451369618377\n",
      "Std loss: 0.002013460137000594\n",
      "Total Loss: 0.05417670821771026\n",
      "------------------------------------ epoch 6068 (36402 steps) ------------------------------------\n",
      "Max loss: 0.016177857294678688\n",
      "Min loss: 0.005159562453627586\n",
      "Mean loss: 0.0095791508598874\n",
      "Std loss: 0.003348071555939835\n",
      "Total Loss: 0.05747490515932441\n",
      "------------------------------------ epoch 6069 (36408 steps) ------------------------------------\n",
      "Max loss: 0.017546899616718292\n",
      "Min loss: 0.006390143185853958\n",
      "Mean loss: 0.010953998814026514\n",
      "Std loss: 0.0034471272934451726\n",
      "Total Loss: 0.06572399288415909\n",
      "------------------------------------ epoch 6070 (36414 steps) ------------------------------------\n",
      "Max loss: 0.015572759322822094\n",
      "Min loss: 0.005031281616538763\n",
      "Mean loss: 0.009363794544090828\n",
      "Std loss: 0.004256583537980469\n",
      "Total Loss: 0.056182767264544964\n",
      "------------------------------------ epoch 6071 (36420 steps) ------------------------------------\n",
      "Max loss: 0.017502617090940475\n",
      "Min loss: 0.006148068234324455\n",
      "Mean loss: 0.0116969533264637\n",
      "Std loss: 0.00455947613044325\n",
      "Total Loss: 0.0701817199587822\n",
      "------------------------------------ epoch 6072 (36426 steps) ------------------------------------\n",
      "Max loss: 0.013571268878877163\n",
      "Min loss: 0.004836484789848328\n",
      "Mean loss: 0.009192016441375017\n",
      "Std loss: 0.0029482067840336597\n",
      "Total Loss: 0.0551520986482501\n",
      "------------------------------------ epoch 6073 (36432 steps) ------------------------------------\n",
      "Max loss: 0.009940238669514656\n",
      "Min loss: 0.006557290907949209\n",
      "Mean loss: 0.007825815817341208\n",
      "Std loss: 0.0010710831052013195\n",
      "Total Loss: 0.04695489490404725\n",
      "------------------------------------ epoch 6074 (36438 steps) ------------------------------------\n",
      "Max loss: 0.010772712528705597\n",
      "Min loss: 0.008766250684857368\n",
      "Mean loss: 0.009402047377079725\n",
      "Std loss: 0.000697518613059904\n",
      "Total Loss: 0.05641228426247835\n",
      "------------------------------------ epoch 6075 (36444 steps) ------------------------------------\n",
      "Max loss: 0.02308398112654686\n",
      "Min loss: 0.0055360510013997555\n",
      "Mean loss: 0.010698353871703148\n",
      "Std loss: 0.0059964047255395385\n",
      "Total Loss: 0.06419012323021889\n",
      "------------------------------------ epoch 6076 (36450 steps) ------------------------------------\n",
      "Max loss: 0.01301257498562336\n",
      "Min loss: 0.0045767356641590595\n",
      "Mean loss: 0.007722032178814213\n",
      "Std loss: 0.003573372440011143\n",
      "Total Loss: 0.046332193072885275\n",
      "------------------------------------ epoch 6077 (36456 steps) ------------------------------------\n",
      "Max loss: 0.04081381857395172\n",
      "Min loss: 0.004851270001381636\n",
      "Mean loss: 0.014397986078013977\n",
      "Std loss: 0.012119217139359323\n",
      "Total Loss: 0.08638791646808386\n",
      "------------------------------------ epoch 6078 (36462 steps) ------------------------------------\n",
      "Max loss: 0.013311756774783134\n",
      "Min loss: 0.004722976125776768\n",
      "Mean loss: 0.007796764373779297\n",
      "Std loss: 0.002962089335449676\n",
      "Total Loss: 0.04678058624267578\n",
      "------------------------------------ epoch 6079 (36468 steps) ------------------------------------\n",
      "Max loss: 0.028022870421409607\n",
      "Min loss: 0.0064480965957045555\n",
      "Mean loss: 0.010779068184395632\n",
      "Std loss: 0.007738656085276408\n",
      "Total Loss: 0.06467440910637379\n",
      "------------------------------------ epoch 6080 (36474 steps) ------------------------------------\n",
      "Max loss: 0.01871194690465927\n",
      "Min loss: 0.005485471338033676\n",
      "Mean loss: 0.011464160168543458\n",
      "Std loss: 0.00494848746288122\n",
      "Total Loss: 0.06878496101126075\n",
      "------------------------------------ epoch 6081 (36480 steps) ------------------------------------\n",
      "Max loss: 0.015688158571720123\n",
      "Min loss: 0.0054933978244662285\n",
      "Mean loss: 0.008211005634317795\n",
      "Std loss: 0.0034694889858596424\n",
      "Total Loss: 0.04926603380590677\n",
      "------------------------------------ epoch 6082 (36486 steps) ------------------------------------\n",
      "Max loss: 0.043281883001327515\n",
      "Min loss: 0.006879802793264389\n",
      "Mean loss: 0.014854103326797485\n",
      "Std loss: 0.012890369071746413\n",
      "Total Loss: 0.08912461996078491\n",
      "------------------------------------ epoch 6083 (36492 steps) ------------------------------------\n",
      "Max loss: 0.012537597678601742\n",
      "Min loss: 0.004551579710096121\n",
      "Mean loss: 0.008378272643312812\n",
      "Std loss: 0.003025424310236053\n",
      "Total Loss: 0.05026963585987687\n",
      "------------------------------------ epoch 6084 (36498 steps) ------------------------------------\n",
      "Max loss: 0.04494728147983551\n",
      "Min loss: 0.004946061410009861\n",
      "Mean loss: 0.014876607184608778\n",
      "Std loss: 0.013641277319373982\n",
      "Total Loss: 0.08925964310765266\n",
      "------------------------------------ epoch 6085 (36504 steps) ------------------------------------\n",
      "Max loss: 0.013877585530281067\n",
      "Min loss: 0.006387694738805294\n",
      "Mean loss: 0.009423363643387953\n",
      "Std loss: 0.0029406540929337592\n",
      "Total Loss: 0.05654018186032772\n",
      "------------------------------------ epoch 6086 (36510 steps) ------------------------------------\n",
      "Max loss: 0.016320783644914627\n",
      "Min loss: 0.006229030899703503\n",
      "Mean loss: 0.009604446667556962\n",
      "Std loss: 0.003799723248353384\n",
      "Total Loss: 0.05762668000534177\n",
      "------------------------------------ epoch 6087 (36516 steps) ------------------------------------\n",
      "Max loss: 0.024439703673124313\n",
      "Min loss: 0.005053063854575157\n",
      "Mean loss: 0.01415214982504646\n",
      "Std loss: 0.007334380425503512\n",
      "Total Loss: 0.08491289895027876\n",
      "------------------------------------ epoch 6088 (36522 steps) ------------------------------------\n",
      "Max loss: 0.02103019505739212\n",
      "Min loss: 0.004634934477508068\n",
      "Mean loss: 0.010725588072091341\n",
      "Std loss: 0.005067069134958087\n",
      "Total Loss: 0.06435352843254805\n",
      "------------------------------------ epoch 6089 (36528 steps) ------------------------------------\n",
      "Max loss: 0.014068415388464928\n",
      "Min loss: 0.005769789218902588\n",
      "Mean loss: 0.010102277155965567\n",
      "Std loss: 0.0033980630464221064\n",
      "Total Loss: 0.0606136629357934\n",
      "------------------------------------ epoch 6090 (36534 steps) ------------------------------------\n",
      "Max loss: 0.032583411782979965\n",
      "Min loss: 0.004671873524785042\n",
      "Mean loss: 0.011822189126784602\n",
      "Std loss: 0.009952788264742213\n",
      "Total Loss: 0.07093313476070762\n",
      "------------------------------------ epoch 6091 (36540 steps) ------------------------------------\n",
      "Max loss: 0.022628232836723328\n",
      "Min loss: 0.005094422958791256\n",
      "Mean loss: 0.011921091238036752\n",
      "Std loss: 0.005979496268135186\n",
      "Total Loss: 0.07152654742822051\n",
      "------------------------------------ epoch 6092 (36546 steps) ------------------------------------\n",
      "Max loss: 0.010893136262893677\n",
      "Min loss: 0.005766573827713728\n",
      "Mean loss: 0.007995672601585587\n",
      "Std loss: 0.0017212748033624053\n",
      "Total Loss: 0.04797403560951352\n",
      "------------------------------------ epoch 6093 (36552 steps) ------------------------------------\n",
      "Max loss: 0.01777002215385437\n",
      "Min loss: 0.00663706474006176\n",
      "Mean loss: 0.011304892444362244\n",
      "Std loss: 0.003762824545671261\n",
      "Total Loss: 0.06782935466617346\n",
      "------------------------------------ epoch 6094 (36558 steps) ------------------------------------\n",
      "Max loss: 0.012401212006807327\n",
      "Min loss: 0.005964824464172125\n",
      "Mean loss: 0.00907856683867673\n",
      "Std loss: 0.002231390064342155\n",
      "Total Loss: 0.054471401032060385\n",
      "------------------------------------ epoch 6095 (36564 steps) ------------------------------------\n",
      "Max loss: 0.016230612993240356\n",
      "Min loss: 0.0044182646088302135\n",
      "Mean loss: 0.008798968124513825\n",
      "Std loss: 0.004212074483569607\n",
      "Total Loss: 0.05279380874708295\n",
      "------------------------------------ epoch 6096 (36570 steps) ------------------------------------\n",
      "Max loss: 0.012787966057658195\n",
      "Min loss: 0.006180780939757824\n",
      "Mean loss: 0.009281976148486137\n",
      "Std loss: 0.0022411479761441804\n",
      "Total Loss: 0.055691856890916824\n",
      "------------------------------------ epoch 6097 (36576 steps) ------------------------------------\n",
      "Max loss: 0.017391569912433624\n",
      "Min loss: 0.005379906855523586\n",
      "Mean loss: 0.009604378758619228\n",
      "Std loss: 0.0038197254274702857\n",
      "Total Loss: 0.057626272551715374\n",
      "------------------------------------ epoch 6098 (36582 steps) ------------------------------------\n",
      "Max loss: 0.02849210426211357\n",
      "Min loss: 0.0047480338253080845\n",
      "Mean loss: 0.01058367818283538\n",
      "Std loss: 0.008204329393769253\n",
      "Total Loss: 0.06350206909701228\n",
      "------------------------------------ epoch 6099 (36588 steps) ------------------------------------\n",
      "Max loss: 0.023420339450240135\n",
      "Min loss: 0.005482691805809736\n",
      "Mean loss: 0.011367020585263768\n",
      "Std loss: 0.00622427909168683\n",
      "Total Loss: 0.06820212351158261\n",
      "------------------------------------ epoch 6100 (36594 steps) ------------------------------------\n",
      "Max loss: 0.03701632469892502\n",
      "Min loss: 0.006000116001814604\n",
      "Mean loss: 0.014636176483084759\n",
      "Std loss: 0.011393103339268868\n",
      "Total Loss: 0.08781705889850855\n",
      "------------------------------------ epoch 6101 (36600 steps) ------------------------------------\n",
      "Max loss: 0.026282187551259995\n",
      "Min loss: 0.005720106884837151\n",
      "Mean loss: 0.013818474641690651\n",
      "Std loss: 0.008649644168882123\n",
      "Total Loss: 0.08291084785014391\n",
      "saved model at ./weights/model_6101.pth\n",
      "------------------------------------ epoch 6102 (36606 steps) ------------------------------------\n",
      "Max loss: 0.018633469939231873\n",
      "Min loss: 0.0054991235956549644\n",
      "Mean loss: 0.01076077250763774\n",
      "Std loss: 0.00444554652708996\n",
      "Total Loss: 0.06456463504582644\n",
      "------------------------------------ epoch 6103 (36612 steps) ------------------------------------\n",
      "Max loss: 0.012149995192885399\n",
      "Min loss: 0.004930370952934027\n",
      "Mean loss: 0.008709869425122937\n",
      "Std loss: 0.002437848432732687\n",
      "Total Loss: 0.05225921655073762\n",
      "------------------------------------ epoch 6104 (36618 steps) ------------------------------------\n",
      "Max loss: 0.03558715432882309\n",
      "Min loss: 0.006591259501874447\n",
      "Mean loss: 0.0137662123888731\n",
      "Std loss: 0.010037954089114785\n",
      "Total Loss: 0.0825972743332386\n",
      "------------------------------------ epoch 6105 (36624 steps) ------------------------------------\n",
      "Max loss: 0.029898878186941147\n",
      "Min loss: 0.005282414611428976\n",
      "Mean loss: 0.012184911174699664\n",
      "Std loss: 0.009529183838216474\n",
      "Total Loss: 0.07310946704819798\n",
      "------------------------------------ epoch 6106 (36630 steps) ------------------------------------\n",
      "Max loss: 0.052303701639175415\n",
      "Min loss: 0.006647919770330191\n",
      "Mean loss: 0.022614945735161502\n",
      "Std loss: 0.01699784684013519\n",
      "Total Loss: 0.13568967441096902\n",
      "------------------------------------ epoch 6107 (36636 steps) ------------------------------------\n",
      "Max loss: 0.020439205691218376\n",
      "Min loss: 0.007617811672389507\n",
      "Mean loss: 0.012675927796711525\n",
      "Std loss: 0.005343568916855587\n",
      "Total Loss: 0.07605556678026915\n",
      "------------------------------------ epoch 6108 (36642 steps) ------------------------------------\n",
      "Max loss: 0.03327034041285515\n",
      "Min loss: 0.009473998099565506\n",
      "Mean loss: 0.017798825943221647\n",
      "Std loss: 0.007439817907027298\n",
      "Total Loss: 0.10679295565932989\n",
      "------------------------------------ epoch 6109 (36648 steps) ------------------------------------\n",
      "Max loss: 0.018912922590970993\n",
      "Min loss: 0.007773211225867271\n",
      "Mean loss: 0.012370409754415354\n",
      "Std loss: 0.003946203163354235\n",
      "Total Loss: 0.07422245852649212\n",
      "------------------------------------ epoch 6110 (36654 steps) ------------------------------------\n",
      "Max loss: 0.007474155630916357\n",
      "Min loss: 0.00506568094715476\n",
      "Mean loss: 0.006133974917853872\n",
      "Std loss: 0.0007325306970930271\n",
      "Total Loss: 0.03680384950712323\n",
      "------------------------------------ epoch 6111 (36660 steps) ------------------------------------\n",
      "Max loss: 0.014260394498705864\n",
      "Min loss: 0.007020560558885336\n",
      "Mean loss: 0.010347379449134072\n",
      "Std loss: 0.002302774276865568\n",
      "Total Loss: 0.06208427669480443\n",
      "------------------------------------ epoch 6112 (36666 steps) ------------------------------------\n",
      "Max loss: 0.015168990008533001\n",
      "Min loss: 0.00859616044908762\n",
      "Mean loss: 0.011649499026437601\n",
      "Std loss: 0.002390230592972249\n",
      "Total Loss: 0.0698969941586256\n",
      "------------------------------------ epoch 6113 (36672 steps) ------------------------------------\n",
      "Max loss: 0.01011216826736927\n",
      "Min loss: 0.00436319038271904\n",
      "Mean loss: 0.007549010682851076\n",
      "Std loss: 0.0018939777678647339\n",
      "Total Loss: 0.04529406409710646\n",
      "------------------------------------ epoch 6114 (36678 steps) ------------------------------------\n",
      "Max loss: 0.0144862812012434\n",
      "Min loss: 0.004446901381015778\n",
      "Mean loss: 0.007478631644820173\n",
      "Std loss: 0.0032986474476393445\n",
      "Total Loss: 0.04487178986892104\n",
      "------------------------------------ epoch 6115 (36684 steps) ------------------------------------\n",
      "Max loss: 0.008449334651231766\n",
      "Min loss: 0.004334039054811001\n",
      "Mean loss: 0.006730890289569895\n",
      "Std loss: 0.001648977842988749\n",
      "Total Loss: 0.04038534173741937\n",
      "------------------------------------ epoch 6116 (36690 steps) ------------------------------------\n",
      "Max loss: 0.013319852761924267\n",
      "Min loss: 0.004598922561854124\n",
      "Mean loss: 0.008449689717963338\n",
      "Std loss: 0.0030829243245083476\n",
      "Total Loss: 0.05069813830778003\n",
      "------------------------------------ epoch 6117 (36696 steps) ------------------------------------\n",
      "Max loss: 0.010647032409906387\n",
      "Min loss: 0.0046428507193923\n",
      "Mean loss: 0.007563798067470391\n",
      "Std loss: 0.0022596025006120066\n",
      "Total Loss: 0.04538278840482235\n",
      "------------------------------------ epoch 6118 (36702 steps) ------------------------------------\n",
      "Max loss: 0.019210243597626686\n",
      "Min loss: 0.004550037439912558\n",
      "Mean loss: 0.011438596916074554\n",
      "Std loss: 0.0052332860033566355\n",
      "Total Loss: 0.06863158149644732\n",
      "------------------------------------ epoch 6119 (36708 steps) ------------------------------------\n",
      "Max loss: 0.013078107498586178\n",
      "Min loss: 0.004614926874637604\n",
      "Mean loss: 0.007912116202836236\n",
      "Std loss: 0.0026114207374325054\n",
      "Total Loss: 0.04747269721701741\n",
      "------------------------------------ epoch 6120 (36714 steps) ------------------------------------\n",
      "Max loss: 0.017986945807933807\n",
      "Min loss: 0.004825476557016373\n",
      "Mean loss: 0.009556485495219627\n",
      "Std loss: 0.004468684987973005\n",
      "Total Loss: 0.05733891297131777\n",
      "------------------------------------ epoch 6121 (36720 steps) ------------------------------------\n",
      "Max loss: 0.029086029157042503\n",
      "Min loss: 0.004753295332193375\n",
      "Mean loss: 0.013485790540774664\n",
      "Std loss: 0.00965516429154702\n",
      "Total Loss: 0.08091474324464798\n",
      "------------------------------------ epoch 6122 (36726 steps) ------------------------------------\n",
      "Max loss: 0.025889446958899498\n",
      "Min loss: 0.003960465081036091\n",
      "Mean loss: 0.01205643406137824\n",
      "Std loss: 0.00723111588779673\n",
      "Total Loss: 0.07233860436826944\n",
      "------------------------------------ epoch 6123 (36732 steps) ------------------------------------\n",
      "Max loss: 0.00840018317103386\n",
      "Min loss: 0.005230973474681377\n",
      "Mean loss: 0.006540144328027964\n",
      "Std loss: 0.0012476414313405421\n",
      "Total Loss: 0.03924086596816778\n",
      "------------------------------------ epoch 6124 (36738 steps) ------------------------------------\n",
      "Max loss: 0.013509221374988556\n",
      "Min loss: 0.005168219096958637\n",
      "Mean loss: 0.007907692575827241\n",
      "Std loss: 0.002880362822852657\n",
      "Total Loss: 0.047446155454963446\n",
      "------------------------------------ epoch 6125 (36744 steps) ------------------------------------\n",
      "Max loss: 0.01135121937841177\n",
      "Min loss: 0.004646852612495422\n",
      "Mean loss: 0.007404657701651256\n",
      "Std loss: 0.0025423262158522286\n",
      "Total Loss: 0.04442794620990753\n",
      "------------------------------------ epoch 6126 (36750 steps) ------------------------------------\n",
      "Max loss: 0.02167925238609314\n",
      "Min loss: 0.005412766709923744\n",
      "Mean loss: 0.013669976110880574\n",
      "Std loss: 0.006704200647895635\n",
      "Total Loss: 0.08201985666528344\n",
      "------------------------------------ epoch 6127 (36756 steps) ------------------------------------\n",
      "Max loss: 0.0296078622341156\n",
      "Min loss: 0.004429358057677746\n",
      "Mean loss: 0.014002409918854633\n",
      "Std loss: 0.008110986306598568\n",
      "Total Loss: 0.0840144595131278\n",
      "------------------------------------ epoch 6128 (36762 steps) ------------------------------------\n",
      "Max loss: 0.0189370084553957\n",
      "Min loss: 0.0045187100768089294\n",
      "Mean loss: 0.011634970859934887\n",
      "Std loss: 0.004656282582102419\n",
      "Total Loss: 0.06980982515960932\n",
      "------------------------------------ epoch 6129 (36768 steps) ------------------------------------\n",
      "Max loss: 0.017162907868623734\n",
      "Min loss: 0.005840439349412918\n",
      "Mean loss: 0.011482109936575094\n",
      "Std loss: 0.004095756047187253\n",
      "Total Loss: 0.06889265961945057\n",
      "------------------------------------ epoch 6130 (36774 steps) ------------------------------------\n",
      "Max loss: 0.016478635370731354\n",
      "Min loss: 0.004388617351651192\n",
      "Mean loss: 0.008804935806741318\n",
      "Std loss: 0.0043250198628585635\n",
      "Total Loss: 0.0528296148404479\n",
      "------------------------------------ epoch 6131 (36780 steps) ------------------------------------\n",
      "Max loss: 0.01475190743803978\n",
      "Min loss: 0.005231050308793783\n",
      "Mean loss: 0.008578449487686157\n",
      "Std loss: 0.0030513432605775233\n",
      "Total Loss: 0.05147069692611694\n",
      "------------------------------------ epoch 6132 (36786 steps) ------------------------------------\n",
      "Max loss: 0.012229937128722668\n",
      "Min loss: 0.0054937428794801235\n",
      "Mean loss: 0.008638877576837936\n",
      "Std loss: 0.002856848052903206\n",
      "Total Loss: 0.05183326546102762\n",
      "------------------------------------ epoch 6133 (36792 steps) ------------------------------------\n",
      "Max loss: 0.014148525893688202\n",
      "Min loss: 0.005417335778474808\n",
      "Mean loss: 0.0080017134702454\n",
      "Std loss: 0.002883608544108978\n",
      "Total Loss: 0.048010280821472406\n",
      "------------------------------------ epoch 6134 (36798 steps) ------------------------------------\n",
      "Max loss: 0.016289863735437393\n",
      "Min loss: 0.00516484584659338\n",
      "Mean loss: 0.008207028266042471\n",
      "Std loss: 0.0037455229795810324\n",
      "Total Loss: 0.049242169596254826\n",
      "------------------------------------ epoch 6135 (36804 steps) ------------------------------------\n",
      "Max loss: 0.019687632098793983\n",
      "Min loss: 0.0063598426058888435\n",
      "Mean loss: 0.011124542836720744\n",
      "Std loss: 0.004929341960464877\n",
      "Total Loss: 0.06674725702032447\n",
      "------------------------------------ epoch 6136 (36810 steps) ------------------------------------\n",
      "Max loss: 0.035871073603630066\n",
      "Min loss: 0.005028043873608112\n",
      "Mean loss: 0.015076340564216176\n",
      "Std loss: 0.010898873017541237\n",
      "Total Loss: 0.09045804338529706\n",
      "------------------------------------ epoch 6137 (36816 steps) ------------------------------------\n",
      "Max loss: 0.017726633697748184\n",
      "Min loss: 0.004561667330563068\n",
      "Mean loss: 0.010233798141901692\n",
      "Std loss: 0.005190637466087119\n",
      "Total Loss: 0.06140278885141015\n",
      "------------------------------------ epoch 6138 (36822 steps) ------------------------------------\n",
      "Max loss: 0.025653675198554993\n",
      "Min loss: 0.007800380699336529\n",
      "Mean loss: 0.011591871734708548\n",
      "Std loss: 0.006317673284618241\n",
      "Total Loss: 0.06955123040825129\n",
      "------------------------------------ epoch 6139 (36828 steps) ------------------------------------\n",
      "Max loss: 0.014110411517322063\n",
      "Min loss: 0.004981917794793844\n",
      "Mean loss: 0.007556941205014785\n",
      "Std loss: 0.003052274498797186\n",
      "Total Loss: 0.04534164723008871\n",
      "------------------------------------ epoch 6140 (36834 steps) ------------------------------------\n",
      "Max loss: 0.015765538439154625\n",
      "Min loss: 0.0050682611763477325\n",
      "Mean loss: 0.00993082874144117\n",
      "Std loss: 0.003992825973464721\n",
      "Total Loss: 0.05958497244864702\n",
      "------------------------------------ epoch 6141 (36840 steps) ------------------------------------\n",
      "Max loss: 0.018946971744298935\n",
      "Min loss: 0.0054832687601447105\n",
      "Mean loss: 0.010738840714717904\n",
      "Std loss: 0.0049735006286118185\n",
      "Total Loss: 0.06443304428830743\n",
      "------------------------------------ epoch 6142 (36846 steps) ------------------------------------\n",
      "Max loss: 0.012769266963005066\n",
      "Min loss: 0.005920202936977148\n",
      "Mean loss: 0.008756446884945035\n",
      "Std loss: 0.0022174122361896483\n",
      "Total Loss: 0.05253868130967021\n",
      "------------------------------------ epoch 6143 (36852 steps) ------------------------------------\n",
      "Max loss: 0.013669883832335472\n",
      "Min loss: 0.004108084831386805\n",
      "Mean loss: 0.007008144243930777\n",
      "Std loss: 0.003158738250078357\n",
      "Total Loss: 0.04204886546358466\n",
      "------------------------------------ epoch 6144 (36858 steps) ------------------------------------\n",
      "Max loss: 0.010033069178462029\n",
      "Min loss: 0.004959119483828545\n",
      "Mean loss: 0.006939999681587021\n",
      "Std loss: 0.0016149110354644447\n",
      "Total Loss: 0.04163999808952212\n",
      "------------------------------------ epoch 6145 (36864 steps) ------------------------------------\n",
      "Max loss: 0.021579574793577194\n",
      "Min loss: 0.0063338009640574455\n",
      "Mean loss: 0.010483599578340849\n",
      "Std loss: 0.005089643475213762\n",
      "Total Loss: 0.06290159747004509\n",
      "------------------------------------ epoch 6146 (36870 steps) ------------------------------------\n",
      "Max loss: 0.017232563346624374\n",
      "Min loss: 0.0056557320058345795\n",
      "Mean loss: 0.011242383935799202\n",
      "Std loss: 0.0044068921743309585\n",
      "Total Loss: 0.06745430361479521\n",
      "------------------------------------ epoch 6147 (36876 steps) ------------------------------------\n",
      "Max loss: 0.0220761951059103\n",
      "Min loss: 0.005423351190984249\n",
      "Mean loss: 0.010293834454690417\n",
      "Std loss: 0.005822045171031338\n",
      "Total Loss: 0.0617630067281425\n",
      "------------------------------------ epoch 6148 (36882 steps) ------------------------------------\n",
      "Max loss: 0.02114908955991268\n",
      "Min loss: 0.00473575945943594\n",
      "Mean loss: 0.010119369253516197\n",
      "Std loss: 0.005395335998237756\n",
      "Total Loss: 0.06071621552109718\n",
      "------------------------------------ epoch 6149 (36888 steps) ------------------------------------\n",
      "Max loss: 0.020441439002752304\n",
      "Min loss: 0.005191742908209562\n",
      "Mean loss: 0.011736977767820159\n",
      "Std loss: 0.005904394509045727\n",
      "Total Loss: 0.07042186660692096\n",
      "------------------------------------ epoch 6150 (36894 steps) ------------------------------------\n",
      "Max loss: 0.011578451842069626\n",
      "Min loss: 0.005048112478107214\n",
      "Mean loss: 0.007428742557143171\n",
      "Std loss: 0.0021847959926488743\n",
      "Total Loss: 0.04457245534285903\n",
      "------------------------------------ epoch 6151 (36900 steps) ------------------------------------\n",
      "Max loss: 0.03839968144893646\n",
      "Min loss: 0.004871668294072151\n",
      "Mean loss: 0.015682688914239407\n",
      "Std loss: 0.011265218285118473\n",
      "Total Loss: 0.09409613348543644\n",
      "------------------------------------ epoch 6152 (36906 steps) ------------------------------------\n",
      "Max loss: 0.023655345663428307\n",
      "Min loss: 0.006358347833156586\n",
      "Mean loss: 0.015669244651993115\n",
      "Std loss: 0.00553673303195867\n",
      "Total Loss: 0.0940154679119587\n",
      "------------------------------------ epoch 6153 (36912 steps) ------------------------------------\n",
      "Max loss: 0.023760486394166946\n",
      "Min loss: 0.006413241382688284\n",
      "Mean loss: 0.012016617615396777\n",
      "Std loss: 0.005733456930972417\n",
      "Total Loss: 0.07209970569238067\n",
      "------------------------------------ epoch 6154 (36918 steps) ------------------------------------\n",
      "Max loss: 0.01311248168349266\n",
      "Min loss: 0.005435450933873653\n",
      "Mean loss: 0.009615882920722166\n",
      "Std loss: 0.0031130944889945066\n",
      "Total Loss: 0.057695297524333\n",
      "------------------------------------ epoch 6155 (36924 steps) ------------------------------------\n",
      "Max loss: 0.01710895262658596\n",
      "Min loss: 0.0039405436255037785\n",
      "Mean loss: 0.009797281818464398\n",
      "Std loss: 0.004342261040063443\n",
      "Total Loss: 0.05878369091078639\n",
      "------------------------------------ epoch 6156 (36930 steps) ------------------------------------\n",
      "Max loss: 0.014083219692111015\n",
      "Min loss: 0.005999212618917227\n",
      "Mean loss: 0.0088204275816679\n",
      "Std loss: 0.0032405190533702595\n",
      "Total Loss: 0.0529225654900074\n",
      "------------------------------------ epoch 6157 (36936 steps) ------------------------------------\n",
      "Max loss: 0.013513049110770226\n",
      "Min loss: 0.0061090788803994656\n",
      "Mean loss: 0.009448537214969596\n",
      "Std loss: 0.002382088583817654\n",
      "Total Loss: 0.05669122328981757\n",
      "------------------------------------ epoch 6158 (36942 steps) ------------------------------------\n",
      "Max loss: 0.018579017370939255\n",
      "Min loss: 0.0039039202965795994\n",
      "Mean loss: 0.009224176484470567\n",
      "Std loss: 0.004681697031776592\n",
      "Total Loss: 0.0553450589068234\n",
      "------------------------------------ epoch 6159 (36948 steps) ------------------------------------\n",
      "Max loss: 0.00816931389272213\n",
      "Min loss: 0.00447630463168025\n",
      "Mean loss: 0.006364734843373299\n",
      "Std loss: 0.0013544537139593784\n",
      "Total Loss: 0.03818840906023979\n",
      "------------------------------------ epoch 6160 (36954 steps) ------------------------------------\n",
      "Max loss: 0.008157867006957531\n",
      "Min loss: 0.004652473609894514\n",
      "Mean loss: 0.005841718676189582\n",
      "Std loss: 0.0011169687042446885\n",
      "Total Loss: 0.03505031205713749\n",
      "------------------------------------ epoch 6161 (36960 steps) ------------------------------------\n",
      "Max loss: 0.03509807586669922\n",
      "Min loss: 0.004992899484932423\n",
      "Mean loss: 0.01112083881162107\n",
      "Std loss: 0.010788585548453515\n",
      "Total Loss: 0.06672503286972642\n",
      "------------------------------------ epoch 6162 (36966 steps) ------------------------------------\n",
      "Max loss: 0.020059406757354736\n",
      "Min loss: 0.006369770970195532\n",
      "Mean loss: 0.012342333095148206\n",
      "Std loss: 0.004290319014025142\n",
      "Total Loss: 0.07405399857088923\n",
      "------------------------------------ epoch 6163 (36972 steps) ------------------------------------\n",
      "Max loss: 0.0159209705889225\n",
      "Min loss: 0.004753295332193375\n",
      "Mean loss: 0.009804428011799851\n",
      "Std loss: 0.00462753446752343\n",
      "Total Loss: 0.05882656807079911\n",
      "------------------------------------ epoch 6164 (36978 steps) ------------------------------------\n",
      "Max loss: 0.011119512841105461\n",
      "Min loss: 0.004807393066585064\n",
      "Mean loss: 0.0073479236258814735\n",
      "Std loss: 0.0020915500206397514\n",
      "Total Loss: 0.04408754175528884\n",
      "------------------------------------ epoch 6165 (36984 steps) ------------------------------------\n",
      "Max loss: 0.019525809213519096\n",
      "Min loss: 0.008121855556964874\n",
      "Mean loss: 0.015567087878783544\n",
      "Std loss: 0.004159617193097513\n",
      "Total Loss: 0.09340252727270126\n",
      "------------------------------------ epoch 6166 (36990 steps) ------------------------------------\n",
      "Max loss: 0.014778073877096176\n",
      "Min loss: 0.00570629071444273\n",
      "Mean loss: 0.011686304273704687\n",
      "Std loss: 0.003000848182042368\n",
      "Total Loss: 0.07011782564222813\n",
      "------------------------------------ epoch 6167 (36996 steps) ------------------------------------\n",
      "Max loss: 0.017023950815200806\n",
      "Min loss: 0.006276906933635473\n",
      "Mean loss: 0.009385429322719574\n",
      "Std loss: 0.003753083556323743\n",
      "Total Loss: 0.056312575936317444\n",
      "------------------------------------ epoch 6168 (37002 steps) ------------------------------------\n",
      "Max loss: 0.024743815883994102\n",
      "Min loss: 0.006333788391202688\n",
      "Mean loss: 0.013054967081795136\n",
      "Std loss: 0.00764720922096605\n",
      "Total Loss: 0.07832980249077082\n",
      "------------------------------------ epoch 6169 (37008 steps) ------------------------------------\n",
      "Max loss: 0.01822761446237564\n",
      "Min loss: 0.005164198111742735\n",
      "Mean loss: 0.00818363786675036\n",
      "Std loss: 0.004547097189788147\n",
      "Total Loss: 0.04910182720050216\n",
      "------------------------------------ epoch 6170 (37014 steps) ------------------------------------\n",
      "Max loss: 0.013311443850398064\n",
      "Min loss: 0.004351027309894562\n",
      "Mean loss: 0.006658342278872927\n",
      "Std loss: 0.003112211311948745\n",
      "Total Loss: 0.03995005367323756\n",
      "------------------------------------ epoch 6171 (37020 steps) ------------------------------------\n",
      "Max loss: 0.02473325841128826\n",
      "Min loss: 0.004996964242309332\n",
      "Mean loss: 0.010984456865116954\n",
      "Std loss: 0.0067951801805551946\n",
      "Total Loss: 0.06590674119070172\n",
      "------------------------------------ epoch 6172 (37026 steps) ------------------------------------\n",
      "Max loss: 0.024137502536177635\n",
      "Min loss: 0.00838969461619854\n",
      "Mean loss: 0.01343966011578838\n",
      "Std loss: 0.005824657135653613\n",
      "Total Loss: 0.08063796069473028\n",
      "------------------------------------ epoch 6173 (37032 steps) ------------------------------------\n",
      "Max loss: 0.020023025572299957\n",
      "Min loss: 0.006611563265323639\n",
      "Mean loss: 0.011817274770389\n",
      "Std loss: 0.004896998004372837\n",
      "Total Loss: 0.070903648622334\n",
      "------------------------------------ epoch 6174 (37038 steps) ------------------------------------\n",
      "Max loss: 0.018997466191649437\n",
      "Min loss: 0.004779797978699207\n",
      "Mean loss: 0.010298708376164237\n",
      "Std loss: 0.005215486098302991\n",
      "Total Loss: 0.061792250256985426\n",
      "------------------------------------ epoch 6175 (37044 steps) ------------------------------------\n",
      "Max loss: 0.01219446025788784\n",
      "Min loss: 0.004777790978550911\n",
      "Mean loss: 0.008253528891752163\n",
      "Std loss: 0.0023083817642693843\n",
      "Total Loss: 0.04952117335051298\n",
      "------------------------------------ epoch 6176 (37050 steps) ------------------------------------\n",
      "Max loss: 0.008716035634279251\n",
      "Min loss: 0.004969629924744368\n",
      "Mean loss: 0.00661564152687788\n",
      "Std loss: 0.0014016029087800539\n",
      "Total Loss: 0.03969384916126728\n",
      "------------------------------------ epoch 6177 (37056 steps) ------------------------------------\n",
      "Max loss: 0.03145895153284073\n",
      "Min loss: 0.006024833768606186\n",
      "Mean loss: 0.013062925388415655\n",
      "Std loss: 0.008825096386621807\n",
      "Total Loss: 0.07837755233049393\n",
      "------------------------------------ epoch 6178 (37062 steps) ------------------------------------\n",
      "Max loss: 0.012264885008335114\n",
      "Min loss: 0.005012566223740578\n",
      "Mean loss: 0.00822124327532947\n",
      "Std loss: 0.002660518110415502\n",
      "Total Loss: 0.049327459651976824\n",
      "------------------------------------ epoch 6179 (37068 steps) ------------------------------------\n",
      "Max loss: 0.009955579414963722\n",
      "Min loss: 0.004430841654539108\n",
      "Mean loss: 0.00695275313531359\n",
      "Std loss: 0.002094658382898644\n",
      "Total Loss: 0.04171651881188154\n",
      "------------------------------------ epoch 6180 (37074 steps) ------------------------------------\n",
      "Max loss: 0.008763592690229416\n",
      "Min loss: 0.005295361392199993\n",
      "Mean loss: 0.006845189879337947\n",
      "Std loss: 0.0011967176301474645\n",
      "Total Loss: 0.04107113927602768\n",
      "------------------------------------ epoch 6181 (37080 steps) ------------------------------------\n",
      "Max loss: 0.03359140828251839\n",
      "Min loss: 0.004851921461522579\n",
      "Mean loss: 0.014378115612392625\n",
      "Std loss: 0.011071251195380827\n",
      "Total Loss: 0.08626869367435575\n",
      "------------------------------------ epoch 6182 (37086 steps) ------------------------------------\n",
      "Max loss: 0.009186949580907822\n",
      "Min loss: 0.004202533513307571\n",
      "Mean loss: 0.006257292892162998\n",
      "Std loss: 0.001987888428667961\n",
      "Total Loss: 0.03754375735297799\n",
      "------------------------------------ epoch 6183 (37092 steps) ------------------------------------\n",
      "Max loss: 0.013379238545894623\n",
      "Min loss: 0.007210260257124901\n",
      "Mean loss: 0.011319454604138931\n",
      "Std loss: 0.0019894963938764053\n",
      "Total Loss: 0.06791672762483358\n",
      "------------------------------------ epoch 6184 (37098 steps) ------------------------------------\n",
      "Max loss: 0.01184520311653614\n",
      "Min loss: 0.005006472580134869\n",
      "Mean loss: 0.008036660806586346\n",
      "Std loss: 0.0024806021994164622\n",
      "Total Loss: 0.04821996483951807\n",
      "------------------------------------ epoch 6185 (37104 steps) ------------------------------------\n",
      "Max loss: 0.019516268745064735\n",
      "Min loss: 0.0046301912516355515\n",
      "Mean loss: 0.009798599251856407\n",
      "Std loss: 0.00533028623425301\n",
      "Total Loss: 0.05879159551113844\n",
      "------------------------------------ epoch 6186 (37110 steps) ------------------------------------\n",
      "Max loss: 0.02473476156592369\n",
      "Min loss: 0.004987206310033798\n",
      "Mean loss: 0.010011602581168214\n",
      "Std loss: 0.006715008682358935\n",
      "Total Loss: 0.06006961548700929\n",
      "------------------------------------ epoch 6187 (37116 steps) ------------------------------------\n",
      "Max loss: 0.01106145791709423\n",
      "Min loss: 0.004576744511723518\n",
      "Mean loss: 0.00695454499994715\n",
      "Std loss: 0.0023355128358737726\n",
      "Total Loss: 0.0417272699996829\n",
      "------------------------------------ epoch 6188 (37122 steps) ------------------------------------\n",
      "Max loss: 0.04252639412879944\n",
      "Min loss: 0.005187198985368013\n",
      "Mean loss: 0.013845919088150064\n",
      "Std loss: 0.01295692413337457\n",
      "Total Loss: 0.08307551452890038\n",
      "------------------------------------ epoch 6189 (37128 steps) ------------------------------------\n",
      "Max loss: 0.02647416666150093\n",
      "Min loss: 0.0056915502063930035\n",
      "Mean loss: 0.012817567524810633\n",
      "Std loss: 0.00771692656156063\n",
      "Total Loss: 0.07690540514886379\n",
      "------------------------------------ epoch 6190 (37134 steps) ------------------------------------\n",
      "Max loss: 0.014761475846171379\n",
      "Min loss: 0.005008704029023647\n",
      "Mean loss: 0.009922262746840715\n",
      "Std loss: 0.0033488504777405307\n",
      "Total Loss: 0.05953357648104429\n",
      "------------------------------------ epoch 6191 (37140 steps) ------------------------------------\n",
      "Max loss: 0.013817637227475643\n",
      "Min loss: 0.005540861748158932\n",
      "Mean loss: 0.010104558120171228\n",
      "Std loss: 0.0029334961130589744\n",
      "Total Loss: 0.060627348721027374\n",
      "------------------------------------ epoch 6192 (37146 steps) ------------------------------------\n",
      "Max loss: 0.011729958467185497\n",
      "Min loss: 0.005031620617955923\n",
      "Mean loss: 0.0067015227396041155\n",
      "Std loss: 0.00232950301898793\n",
      "Total Loss: 0.04020913643762469\n",
      "------------------------------------ epoch 6193 (37152 steps) ------------------------------------\n",
      "Max loss: 0.010056987404823303\n",
      "Min loss: 0.005188539624214172\n",
      "Mean loss: 0.006671691856657465\n",
      "Std loss: 0.001855947556434258\n",
      "Total Loss: 0.04003015113994479\n",
      "------------------------------------ epoch 6194 (37158 steps) ------------------------------------\n",
      "Max loss: 0.016203120350837708\n",
      "Min loss: 0.006367522291839123\n",
      "Mean loss: 0.009583452716469765\n",
      "Std loss: 0.0033675392064205253\n",
      "Total Loss: 0.05750071629881859\n",
      "------------------------------------ epoch 6195 (37164 steps) ------------------------------------\n",
      "Max loss: 0.02141263708472252\n",
      "Min loss: 0.005160576198250055\n",
      "Mean loss: 0.010727823944762349\n",
      "Std loss: 0.006243356278809189\n",
      "Total Loss: 0.0643669436685741\n",
      "------------------------------------ epoch 6196 (37170 steps) ------------------------------------\n",
      "Max loss: 0.02353326603770256\n",
      "Min loss: 0.005178501363843679\n",
      "Mean loss: 0.012149258516728878\n",
      "Std loss: 0.007253939843771305\n",
      "Total Loss: 0.07289555110037327\n",
      "------------------------------------ epoch 6197 (37176 steps) ------------------------------------\n",
      "Max loss: 0.01057024858891964\n",
      "Min loss: 0.004574110731482506\n",
      "Mean loss: 0.007034217085068424\n",
      "Std loss: 0.0018994823803408455\n",
      "Total Loss: 0.04220530251041055\n",
      "------------------------------------ epoch 6198 (37182 steps) ------------------------------------\n",
      "Max loss: 0.014175079762935638\n",
      "Min loss: 0.005020175129175186\n",
      "Mean loss: 0.008623272568608323\n",
      "Std loss: 0.0031126063210881847\n",
      "Total Loss: 0.05173963541164994\n",
      "------------------------------------ epoch 6199 (37188 steps) ------------------------------------\n",
      "Max loss: 0.011712782084941864\n",
      "Min loss: 0.004316503182053566\n",
      "Mean loss: 0.006590323289856315\n",
      "Std loss: 0.0025358339822899017\n",
      "Total Loss: 0.03954193973913789\n",
      "------------------------------------ epoch 6200 (37194 steps) ------------------------------------\n",
      "Max loss: 0.008900733664631844\n",
      "Min loss: 0.005980938207358122\n",
      "Mean loss: 0.007374022776881854\n",
      "Std loss: 0.0010640424256545794\n",
      "Total Loss: 0.04424413666129112\n",
      "------------------------------------ epoch 6201 (37200 steps) ------------------------------------\n",
      "Max loss: 0.014694447629153728\n",
      "Min loss: 0.004927404224872589\n",
      "Mean loss: 0.006990488385781646\n",
      "Std loss: 0.003457157916966351\n",
      "Total Loss: 0.041942930314689875\n",
      "saved model at ./weights/model_6201.pth\n",
      "------------------------------------ epoch 6202 (37206 steps) ------------------------------------\n",
      "Max loss: 0.013233142904937267\n",
      "Min loss: 0.004243604838848114\n",
      "Mean loss: 0.008867978739241758\n",
      "Std loss: 0.0034534265755461744\n",
      "Total Loss: 0.053207872435450554\n",
      "------------------------------------ epoch 6203 (37212 steps) ------------------------------------\n",
      "Max loss: 0.01765628159046173\n",
      "Min loss: 0.004884319379925728\n",
      "Mean loss: 0.008571461153527101\n",
      "Std loss: 0.004400054111239362\n",
      "Total Loss: 0.051428766921162605\n",
      "------------------------------------ epoch 6204 (37218 steps) ------------------------------------\n",
      "Max loss: 0.01833745650947094\n",
      "Min loss: 0.006412072107195854\n",
      "Mean loss: 0.010850833884129921\n",
      "Std loss: 0.0036733026536510164\n",
      "Total Loss: 0.06510500330477953\n",
      "------------------------------------ epoch 6205 (37224 steps) ------------------------------------\n",
      "Max loss: 0.011044180952012539\n",
      "Min loss: 0.004504400305449963\n",
      "Mean loss: 0.008130715073396763\n",
      "Std loss: 0.002251133425684905\n",
      "Total Loss: 0.04878429044038057\n",
      "------------------------------------ epoch 6206 (37230 steps) ------------------------------------\n",
      "Max loss: 0.038091350346803665\n",
      "Min loss: 0.005008158273994923\n",
      "Mean loss: 0.012864196595425407\n",
      "Std loss: 0.011891932130661111\n",
      "Total Loss: 0.07718517957255244\n",
      "------------------------------------ epoch 6207 (37236 steps) ------------------------------------\n",
      "Max loss: 0.021532177925109863\n",
      "Min loss: 0.0039865393191576\n",
      "Mean loss: 0.00898290821351111\n",
      "Std loss: 0.006102720733300346\n",
      "Total Loss: 0.053897449281066656\n",
      "------------------------------------ epoch 6208 (37242 steps) ------------------------------------\n",
      "Max loss: 0.037189170718193054\n",
      "Min loss: 0.004815485328435898\n",
      "Mean loss: 0.014405386056751013\n",
      "Std loss: 0.012164534847778878\n",
      "Total Loss: 0.08643231634050608\n",
      "------------------------------------ epoch 6209 (37248 steps) ------------------------------------\n",
      "Max loss: 0.01623883657157421\n",
      "Min loss: 0.007060985546559095\n",
      "Mean loss: 0.011582078024124106\n",
      "Std loss: 0.003460829052374332\n",
      "Total Loss: 0.06949246814474463\n",
      "------------------------------------ epoch 6210 (37254 steps) ------------------------------------\n",
      "Max loss: 0.03475658968091011\n",
      "Min loss: 0.006316939368844032\n",
      "Mean loss: 0.013546695389474431\n",
      "Std loss: 0.010098945115964994\n",
      "Total Loss: 0.08128017233684659\n",
      "------------------------------------ epoch 6211 (37260 steps) ------------------------------------\n",
      "Max loss: 0.02877780608832836\n",
      "Min loss: 0.006827918346971273\n",
      "Mean loss: 0.013340589357540011\n",
      "Std loss: 0.007625592544830065\n",
      "Total Loss: 0.08004353614524007\n",
      "------------------------------------ epoch 6212 (37266 steps) ------------------------------------\n",
      "Max loss: 0.048519670963287354\n",
      "Min loss: 0.006330931559205055\n",
      "Mean loss: 0.015266058966517448\n",
      "Std loss: 0.01495422982258712\n",
      "Total Loss: 0.09159635379910469\n",
      "------------------------------------ epoch 6213 (37272 steps) ------------------------------------\n",
      "Max loss: 0.024229709059000015\n",
      "Min loss: 0.00529772974550724\n",
      "Mean loss: 0.010717244896416863\n",
      "Std loss: 0.006993937730988278\n",
      "Total Loss: 0.06430346937850118\n",
      "------------------------------------ epoch 6214 (37278 steps) ------------------------------------\n",
      "Max loss: 0.027982687577605247\n",
      "Min loss: 0.005039949901401997\n",
      "Mean loss: 0.01272283090899388\n",
      "Std loss: 0.00843995545498288\n",
      "Total Loss: 0.07633698545396328\n",
      "------------------------------------ epoch 6215 (37284 steps) ------------------------------------\n",
      "Max loss: 0.022594522684812546\n",
      "Min loss: 0.0047633955255150795\n",
      "Mean loss: 0.013670680966849128\n",
      "Std loss: 0.0062384123047106435\n",
      "Total Loss: 0.08202408580109477\n",
      "------------------------------------ epoch 6216 (37290 steps) ------------------------------------\n",
      "Max loss: 0.011699207127094269\n",
      "Min loss: 0.006505495868623257\n",
      "Mean loss: 0.00894949360129734\n",
      "Std loss: 0.0018975225031380534\n",
      "Total Loss: 0.05369696160778403\n",
      "------------------------------------ epoch 6217 (37296 steps) ------------------------------------\n",
      "Max loss: 0.021051853895187378\n",
      "Min loss: 0.005688070319592953\n",
      "Mean loss: 0.012459720484912395\n",
      "Std loss: 0.005165791396239237\n",
      "Total Loss: 0.07475832290947437\n",
      "------------------------------------ epoch 6218 (37302 steps) ------------------------------------\n",
      "Max loss: 0.015063757076859474\n",
      "Min loss: 0.007341504096984863\n",
      "Mean loss: 0.010425748148312172\n",
      "Std loss: 0.0026496680561195704\n",
      "Total Loss: 0.06255448888987303\n",
      "------------------------------------ epoch 6219 (37308 steps) ------------------------------------\n",
      "Max loss: 0.021706964820623398\n",
      "Min loss: 0.007660761009901762\n",
      "Mean loss: 0.012558840758477649\n",
      "Std loss: 0.004533482367969331\n",
      "Total Loss: 0.07535304455086589\n",
      "------------------------------------ epoch 6220 (37314 steps) ------------------------------------\n",
      "Max loss: 0.028077885508537292\n",
      "Min loss: 0.00545080890879035\n",
      "Mean loss: 0.01326866657473147\n",
      "Std loss: 0.008109334095570025\n",
      "Total Loss: 0.07961199944838881\n",
      "------------------------------------ epoch 6221 (37320 steps) ------------------------------------\n",
      "Max loss: 0.013519657775759697\n",
      "Min loss: 0.005164028145372868\n",
      "Mean loss: 0.008715678161631027\n",
      "Std loss: 0.0027597146263207578\n",
      "Total Loss: 0.05229406896978617\n",
      "------------------------------------ epoch 6222 (37326 steps) ------------------------------------\n",
      "Max loss: 0.011147111654281616\n",
      "Min loss: 0.005041725467890501\n",
      "Mean loss: 0.0077875876644005375\n",
      "Std loss: 0.0023282140754428762\n",
      "Total Loss: 0.04672552598640323\n",
      "------------------------------------ epoch 6223 (37332 steps) ------------------------------------\n",
      "Max loss: 0.009075693786144257\n",
      "Min loss: 0.007180181331932545\n",
      "Mean loss: 0.008460826706141233\n",
      "Std loss: 0.0006268649345068984\n",
      "Total Loss: 0.0507649602368474\n",
      "------------------------------------ epoch 6224 (37338 steps) ------------------------------------\n",
      "Max loss: 0.009558520279824734\n",
      "Min loss: 0.004608985967934132\n",
      "Mean loss: 0.006758107338100672\n",
      "Std loss: 0.0020724092272150487\n",
      "Total Loss: 0.04054864402860403\n",
      "------------------------------------ epoch 6225 (37344 steps) ------------------------------------\n",
      "Max loss: 0.02234402857720852\n",
      "Min loss: 0.005864308215677738\n",
      "Mean loss: 0.0110159523319453\n",
      "Std loss: 0.005692802677115066\n",
      "Total Loss: 0.0660957139916718\n",
      "------------------------------------ epoch 6226 (37350 steps) ------------------------------------\n",
      "Max loss: 0.016723092645406723\n",
      "Min loss: 0.0060080355033278465\n",
      "Mean loss: 0.009404169395565987\n",
      "Std loss: 0.0035974419792283536\n",
      "Total Loss: 0.05642501637339592\n",
      "------------------------------------ epoch 6227 (37356 steps) ------------------------------------\n",
      "Max loss: 0.039755985140800476\n",
      "Min loss: 0.006162721663713455\n",
      "Mean loss: 0.013358432489136854\n",
      "Std loss: 0.011974984558546519\n",
      "Total Loss: 0.08015059493482113\n",
      "------------------------------------ epoch 6228 (37362 steps) ------------------------------------\n",
      "Max loss: 0.01854037493467331\n",
      "Min loss: 0.0047636618837714195\n",
      "Mean loss: 0.009098097837219635\n",
      "Std loss: 0.005118641191038067\n",
      "Total Loss: 0.054588587023317814\n",
      "------------------------------------ epoch 6229 (37368 steps) ------------------------------------\n",
      "Max loss: 0.014164382591843605\n",
      "Min loss: 0.004669848829507828\n",
      "Mean loss: 0.007288643702243765\n",
      "Std loss: 0.0031563047392567187\n",
      "Total Loss: 0.04373186221346259\n",
      "------------------------------------ epoch 6230 (37374 steps) ------------------------------------\n",
      "Max loss: 0.012601442635059357\n",
      "Min loss: 0.005971142090857029\n",
      "Mean loss: 0.007937820861116052\n",
      "Std loss: 0.0022343446060333985\n",
      "Total Loss: 0.04762692516669631\n",
      "------------------------------------ epoch 6231 (37380 steps) ------------------------------------\n",
      "Max loss: 0.0177469402551651\n",
      "Min loss: 0.005117114633321762\n",
      "Mean loss: 0.008530027621115247\n",
      "Std loss: 0.004464701818202623\n",
      "Total Loss: 0.051180165726691484\n",
      "------------------------------------ epoch 6232 (37386 steps) ------------------------------------\n",
      "Max loss: 0.00803547166287899\n",
      "Min loss: 0.00481408229097724\n",
      "Mean loss: 0.0064197012688964605\n",
      "Std loss: 0.0013525338370779145\n",
      "Total Loss: 0.03851820761337876\n",
      "------------------------------------ epoch 6233 (37392 steps) ------------------------------------\n",
      "Max loss: 0.010817892849445343\n",
      "Min loss: 0.0037023923359811306\n",
      "Mean loss: 0.007110100472345948\n",
      "Std loss: 0.0029333248784583713\n",
      "Total Loss: 0.04266060283407569\n",
      "------------------------------------ epoch 6234 (37398 steps) ------------------------------------\n",
      "Max loss: 0.010135569609701633\n",
      "Min loss: 0.004961814731359482\n",
      "Mean loss: 0.00743828007640938\n",
      "Std loss: 0.002173641974478266\n",
      "Total Loss: 0.04462968045845628\n",
      "------------------------------------ epoch 6235 (37404 steps) ------------------------------------\n",
      "Max loss: 0.013442352414131165\n",
      "Min loss: 0.004433694761246443\n",
      "Mean loss: 0.008474195841699839\n",
      "Std loss: 0.002776908149081359\n",
      "Total Loss: 0.05084517505019903\n",
      "------------------------------------ epoch 6236 (37410 steps) ------------------------------------\n",
      "Max loss: 0.016042394563555717\n",
      "Min loss: 0.004978931043297052\n",
      "Mean loss: 0.009165452482799688\n",
      "Std loss: 0.003811770708901463\n",
      "Total Loss: 0.054992714896798134\n",
      "------------------------------------ epoch 6237 (37416 steps) ------------------------------------\n",
      "Max loss: 0.023395521566271782\n",
      "Min loss: 0.004442222416400909\n",
      "Mean loss: 0.012356927075112859\n",
      "Std loss: 0.006466323281946557\n",
      "Total Loss: 0.07414156245067716\n",
      "------------------------------------ epoch 6238 (37422 steps) ------------------------------------\n",
      "Max loss: 0.022237269207835197\n",
      "Min loss: 0.004881393164396286\n",
      "Mean loss: 0.010699704599877199\n",
      "Std loss: 0.005890257760882395\n",
      "Total Loss: 0.06419822759926319\n",
      "------------------------------------ epoch 6239 (37428 steps) ------------------------------------\n",
      "Max loss: 0.008672602474689484\n",
      "Min loss: 0.004313160665333271\n",
      "Mean loss: 0.006503001709158222\n",
      "Std loss: 0.001373840738159697\n",
      "Total Loss: 0.03901801025494933\n",
      "------------------------------------ epoch 6240 (37434 steps) ------------------------------------\n",
      "Max loss: 0.013741428963840008\n",
      "Min loss: 0.00513100903481245\n",
      "Mean loss: 0.008130182744935155\n",
      "Std loss: 0.0032688670998022263\n",
      "Total Loss: 0.04878109646961093\n",
      "------------------------------------ epoch 6241 (37440 steps) ------------------------------------\n",
      "Max loss: 0.023304104804992676\n",
      "Min loss: 0.007716778665781021\n",
      "Mean loss: 0.016079346959789593\n",
      "Std loss: 0.0061406754773065675\n",
      "Total Loss: 0.09647608175873756\n",
      "------------------------------------ epoch 6242 (37446 steps) ------------------------------------\n",
      "Max loss: 0.01822262816131115\n",
      "Min loss: 0.005570821464061737\n",
      "Mean loss: 0.010984631488099694\n",
      "Std loss: 0.00475323455176955\n",
      "Total Loss: 0.06590778892859817\n",
      "------------------------------------ epoch 6243 (37452 steps) ------------------------------------\n",
      "Max loss: 0.044195692986249924\n",
      "Min loss: 0.004432350397109985\n",
      "Mean loss: 0.013696807203814387\n",
      "Std loss: 0.01416517299758459\n",
      "Total Loss: 0.08218084322288632\n",
      "------------------------------------ epoch 6244 (37458 steps) ------------------------------------\n",
      "Max loss: 0.01710609719157219\n",
      "Min loss: 0.005727999843657017\n",
      "Mean loss: 0.009250171793003878\n",
      "Std loss: 0.003886290818894279\n",
      "Total Loss: 0.05550103075802326\n",
      "------------------------------------ epoch 6245 (37464 steps) ------------------------------------\n",
      "Max loss: 0.019155845046043396\n",
      "Min loss: 0.0059354729019105434\n",
      "Mean loss: 0.01156951324082911\n",
      "Std loss: 0.005433448122309154\n",
      "Total Loss: 0.06941707944497466\n",
      "------------------------------------ epoch 6246 (37470 steps) ------------------------------------\n",
      "Max loss: 0.014118192717432976\n",
      "Min loss: 0.006228141486644745\n",
      "Mean loss: 0.009246373471493522\n",
      "Std loss: 0.002773444441664099\n",
      "Total Loss: 0.055478240828961134\n",
      "------------------------------------ epoch 6247 (37476 steps) ------------------------------------\n",
      "Max loss: 0.01325842272490263\n",
      "Min loss: 0.004504254087805748\n",
      "Mean loss: 0.00902673702997466\n",
      "Std loss: 0.003135061342496555\n",
      "Total Loss: 0.054160422179847956\n",
      "------------------------------------ epoch 6248 (37482 steps) ------------------------------------\n",
      "Max loss: 0.011701161973178387\n",
      "Min loss: 0.003886238671839237\n",
      "Mean loss: 0.0068225449261566\n",
      "Std loss: 0.0024162451107770993\n",
      "Total Loss: 0.0409352695569396\n",
      "------------------------------------ epoch 6249 (37488 steps) ------------------------------------\n",
      "Max loss: 0.009909594431519508\n",
      "Min loss: 0.004758045077323914\n",
      "Mean loss: 0.007960519986227155\n",
      "Std loss: 0.0017722624682165742\n",
      "Total Loss: 0.04776311991736293\n",
      "------------------------------------ epoch 6250 (37494 steps) ------------------------------------\n",
      "Max loss: 0.032620664685964584\n",
      "Min loss: 0.006029683630913496\n",
      "Mean loss: 0.014940731615448991\n",
      "Std loss: 0.011777011790796144\n",
      "Total Loss: 0.08964438969269395\n",
      "------------------------------------ epoch 6251 (37500 steps) ------------------------------------\n",
      "Max loss: 0.02607833221554756\n",
      "Min loss: 0.007355723064392805\n",
      "Mean loss: 0.01477566633063058\n",
      "Std loss: 0.0074587615912902385\n",
      "Total Loss: 0.08865399798378348\n",
      "------------------------------------ epoch 6252 (37506 steps) ------------------------------------\n",
      "Max loss: 0.029610764235258102\n",
      "Min loss: 0.005690280348062515\n",
      "Mean loss: 0.013142712724705538\n",
      "Std loss: 0.008813309853412526\n",
      "Total Loss: 0.07885627634823322\n",
      "------------------------------------ epoch 6253 (37512 steps) ------------------------------------\n",
      "Max loss: 0.02616332471370697\n",
      "Min loss: 0.0058199334889650345\n",
      "Mean loss: 0.011613906516383091\n",
      "Std loss: 0.006900765005664388\n",
      "Total Loss: 0.06968343909829855\n",
      "------------------------------------ epoch 6254 (37518 steps) ------------------------------------\n",
      "Max loss: 0.02057887613773346\n",
      "Min loss: 0.0061222268268466\n",
      "Mean loss: 0.009530119054640332\n",
      "Std loss: 0.0050686000616424875\n",
      "Total Loss: 0.057180714327842\n",
      "------------------------------------ epoch 6255 (37524 steps) ------------------------------------\n",
      "Max loss: 0.01011337898671627\n",
      "Min loss: 0.006051947828382254\n",
      "Mean loss: 0.007611831417307258\n",
      "Std loss: 0.0015296356219093674\n",
      "Total Loss: 0.045670988503843546\n",
      "------------------------------------ epoch 6256 (37530 steps) ------------------------------------\n",
      "Max loss: 0.018590198829770088\n",
      "Min loss: 0.00497450539842248\n",
      "Mean loss: 0.009699663380160928\n",
      "Std loss: 0.004432886405427966\n",
      "Total Loss: 0.05819798028096557\n",
      "------------------------------------ epoch 6257 (37536 steps) ------------------------------------\n",
      "Max loss: 0.012397564016282558\n",
      "Min loss: 0.005251653492450714\n",
      "Mean loss: 0.007734732857594888\n",
      "Std loss: 0.0025123430308192996\n",
      "Total Loss: 0.046408397145569324\n",
      "------------------------------------ epoch 6258 (37542 steps) ------------------------------------\n",
      "Max loss: 0.022234490141272545\n",
      "Min loss: 0.003740308340638876\n",
      "Mean loss: 0.011121898889541626\n",
      "Std loss: 0.0065844859179454915\n",
      "Total Loss: 0.06673139333724976\n",
      "------------------------------------ epoch 6259 (37548 steps) ------------------------------------\n",
      "Max loss: 0.007734662853181362\n",
      "Min loss: 0.004528742749243975\n",
      "Mean loss: 0.006446136860176921\n",
      "Std loss: 0.001067966761845185\n",
      "Total Loss: 0.038676821161061525\n",
      "------------------------------------ epoch 6260 (37554 steps) ------------------------------------\n",
      "Max loss: 0.02184765413403511\n",
      "Min loss: 0.004437343217432499\n",
      "Mean loss: 0.01017576347415646\n",
      "Std loss: 0.006142133078267244\n",
      "Total Loss: 0.061054580844938755\n",
      "------------------------------------ epoch 6261 (37560 steps) ------------------------------------\n",
      "Max loss: 0.029284730553627014\n",
      "Min loss: 0.0044429367408156395\n",
      "Mean loss: 0.010299470508471131\n",
      "Std loss: 0.008669811026209086\n",
      "Total Loss: 0.06179682305082679\n",
      "------------------------------------ epoch 6262 (37566 steps) ------------------------------------\n",
      "Max loss: 0.00980944000184536\n",
      "Min loss: 0.0045002843253314495\n",
      "Mean loss: 0.006280449451878667\n",
      "Std loss: 0.0018804712886838864\n",
      "Total Loss: 0.037682696711272\n",
      "------------------------------------ epoch 6263 (37572 steps) ------------------------------------\n",
      "Max loss: 0.008778543211519718\n",
      "Min loss: 0.004219158552587032\n",
      "Mean loss: 0.006675341011335452\n",
      "Std loss: 0.0014444309807019702\n",
      "Total Loss: 0.040052046068012714\n",
      "------------------------------------ epoch 6264 (37578 steps) ------------------------------------\n",
      "Max loss: 0.019000299274921417\n",
      "Min loss: 0.0054054223001003265\n",
      "Mean loss: 0.00917214333700637\n",
      "Std loss: 0.004888264040005402\n",
      "Total Loss: 0.05503286002203822\n",
      "------------------------------------ epoch 6265 (37584 steps) ------------------------------------\n",
      "Max loss: 0.02484961412847042\n",
      "Min loss: 0.00505588436499238\n",
      "Mean loss: 0.011958481821541985\n",
      "Std loss: 0.006547454079531419\n",
      "Total Loss: 0.07175089092925191\n",
      "------------------------------------ epoch 6266 (37590 steps) ------------------------------------\n",
      "Max loss: 0.015111139044165611\n",
      "Min loss: 0.003964421804994345\n",
      "Mean loss: 0.0077337852368752165\n",
      "Std loss: 0.003574258233272951\n",
      "Total Loss: 0.0464027114212513\n",
      "------------------------------------ epoch 6267 (37596 steps) ------------------------------------\n",
      "Max loss: 0.013646334409713745\n",
      "Min loss: 0.00485741114243865\n",
      "Mean loss: 0.007591774920001626\n",
      "Std loss: 0.00286239764482429\n",
      "Total Loss: 0.045550649520009756\n",
      "------------------------------------ epoch 6268 (37602 steps) ------------------------------------\n",
      "Max loss: 0.02136690728366375\n",
      "Min loss: 0.00482673104852438\n",
      "Mean loss: 0.009696593973785639\n",
      "Std loss: 0.005677142063878303\n",
      "Total Loss: 0.05817956384271383\n",
      "------------------------------------ epoch 6269 (37608 steps) ------------------------------------\n",
      "Max loss: 0.023937206715345383\n",
      "Min loss: 0.008412035182118416\n",
      "Mean loss: 0.013747499324381351\n",
      "Std loss: 0.006022483001050221\n",
      "Total Loss: 0.08248499594628811\n",
      "------------------------------------ epoch 6270 (37614 steps) ------------------------------------\n",
      "Max loss: 0.029229208827018738\n",
      "Min loss: 0.005845475941896439\n",
      "Mean loss: 0.014531054922069112\n",
      "Std loss: 0.007770450939024206\n",
      "Total Loss: 0.08718632953241467\n",
      "------------------------------------ epoch 6271 (37620 steps) ------------------------------------\n",
      "Max loss: 0.017058230936527252\n",
      "Min loss: 0.005901309661567211\n",
      "Mean loss: 0.010061126512785753\n",
      "Std loss: 0.004337315018463281\n",
      "Total Loss: 0.060366759076714516\n",
      "------------------------------------ epoch 6272 (37626 steps) ------------------------------------\n",
      "Max loss: 0.01598319411277771\n",
      "Min loss: 0.005749505944550037\n",
      "Mean loss: 0.00794264154198269\n",
      "Std loss: 0.003648408556018225\n",
      "Total Loss: 0.04765584925189614\n",
      "------------------------------------ epoch 6273 (37632 steps) ------------------------------------\n",
      "Max loss: 0.01765727624297142\n",
      "Min loss: 0.005605824291706085\n",
      "Mean loss: 0.00817567886163791\n",
      "Std loss: 0.004287426647205367\n",
      "Total Loss: 0.04905407316982746\n",
      "------------------------------------ epoch 6274 (37638 steps) ------------------------------------\n",
      "Max loss: 0.011449644342064857\n",
      "Min loss: 0.004365496337413788\n",
      "Mean loss: 0.008283950931703052\n",
      "Std loss: 0.0030349916567066794\n",
      "Total Loss: 0.049703705590218306\n",
      "------------------------------------ epoch 6275 (37644 steps) ------------------------------------\n",
      "Max loss: 0.024754216894507408\n",
      "Min loss: 0.0048588113859295845\n",
      "Mean loss: 0.009717336234947046\n",
      "Std loss: 0.006951746412265861\n",
      "Total Loss: 0.058304017409682274\n",
      "------------------------------------ epoch 6276 (37650 steps) ------------------------------------\n",
      "Max loss: 0.018486004322767258\n",
      "Min loss: 0.005138088017702103\n",
      "Mean loss: 0.009968998764331142\n",
      "Std loss: 0.005243786785025992\n",
      "Total Loss: 0.05981399258598685\n",
      "------------------------------------ epoch 6277 (37656 steps) ------------------------------------\n",
      "Max loss: 0.01595095545053482\n",
      "Min loss: 0.004623311571776867\n",
      "Mean loss: 0.009015083778649569\n",
      "Std loss: 0.003820077637443507\n",
      "Total Loss: 0.05409050267189741\n",
      "------------------------------------ epoch 6278 (37662 steps) ------------------------------------\n",
      "Max loss: 0.017569366842508316\n",
      "Min loss: 0.005446101538836956\n",
      "Mean loss: 0.008806549245491624\n",
      "Std loss: 0.004212066251405914\n",
      "Total Loss: 0.05283929547294974\n",
      "------------------------------------ epoch 6279 (37668 steps) ------------------------------------\n",
      "Max loss: 0.015382681041955948\n",
      "Min loss: 0.004882674664258957\n",
      "Mean loss: 0.008063960975656906\n",
      "Std loss: 0.0033746954387298806\n",
      "Total Loss: 0.04838376585394144\n",
      "------------------------------------ epoch 6280 (37674 steps) ------------------------------------\n",
      "Max loss: 0.014824945479631424\n",
      "Min loss: 0.005840185564011335\n",
      "Mean loss: 0.009417332786445817\n",
      "Std loss: 0.003209774582251919\n",
      "Total Loss: 0.0565039967186749\n",
      "------------------------------------ epoch 6281 (37680 steps) ------------------------------------\n",
      "Max loss: 0.015585261397063732\n",
      "Min loss: 0.006757661700248718\n",
      "Mean loss: 0.009769148969401916\n",
      "Std loss: 0.003996244525768044\n",
      "Total Loss: 0.058614893816411495\n",
      "------------------------------------ epoch 6282 (37686 steps) ------------------------------------\n",
      "Max loss: 0.008467230014503002\n",
      "Min loss: 0.00497679365798831\n",
      "Mean loss: 0.006576297959933679\n",
      "Std loss: 0.0011254311306593207\n",
      "Total Loss: 0.03945778775960207\n",
      "------------------------------------ epoch 6283 (37692 steps) ------------------------------------\n",
      "Max loss: 0.011278463527560234\n",
      "Min loss: 0.006309503689408302\n",
      "Mean loss: 0.0077729174712051945\n",
      "Std loss: 0.001684897524309958\n",
      "Total Loss: 0.04663750482723117\n",
      "------------------------------------ epoch 6284 (37698 steps) ------------------------------------\n",
      "Max loss: 0.0108940489590168\n",
      "Min loss: 0.004371483810245991\n",
      "Mean loss: 0.006717537762597203\n",
      "Std loss: 0.0025411652336833047\n",
      "Total Loss: 0.04030522657558322\n",
      "------------------------------------ epoch 6285 (37704 steps) ------------------------------------\n",
      "Max loss: 0.018147800117731094\n",
      "Min loss: 0.006204856093972921\n",
      "Mean loss: 0.010136921890079975\n",
      "Std loss: 0.004283890661932362\n",
      "Total Loss: 0.06082153134047985\n",
      "------------------------------------ epoch 6286 (37710 steps) ------------------------------------\n",
      "Max loss: 0.012398190796375275\n",
      "Min loss: 0.0036358085926622152\n",
      "Mean loss: 0.006397843166875343\n",
      "Std loss: 0.0028314082148538887\n",
      "Total Loss: 0.038387059001252055\n",
      "------------------------------------ epoch 6287 (37716 steps) ------------------------------------\n",
      "Max loss: 0.00942601915448904\n",
      "Min loss: 0.005329973064363003\n",
      "Mean loss: 0.006257288934042056\n",
      "Std loss: 0.0014405216353921984\n",
      "Total Loss: 0.03754373360425234\n",
      "------------------------------------ epoch 6288 (37722 steps) ------------------------------------\n",
      "Max loss: 0.011080059222877026\n",
      "Min loss: 0.005024504382163286\n",
      "Mean loss: 0.006926227438574036\n",
      "Std loss: 0.0021318042265161944\n",
      "Total Loss: 0.041557364631444216\n",
      "------------------------------------ epoch 6289 (37728 steps) ------------------------------------\n",
      "Max loss: 0.006296965759247541\n",
      "Min loss: 0.00381280854344368\n",
      "Mean loss: 0.0048530367979158955\n",
      "Std loss: 0.0008176456175930241\n",
      "Total Loss: 0.029118220787495375\n",
      "------------------------------------ epoch 6290 (37734 steps) ------------------------------------\n",
      "Max loss: 0.010629236698150635\n",
      "Min loss: 0.004521775059401989\n",
      "Mean loss: 0.006600564578548074\n",
      "Std loss: 0.002002658772113208\n",
      "Total Loss: 0.03960338747128844\n",
      "------------------------------------ epoch 6291 (37740 steps) ------------------------------------\n",
      "Max loss: 0.017310116440057755\n",
      "Min loss: 0.00468268059194088\n",
      "Mean loss: 0.00753215483079354\n",
      "Std loss: 0.004422636931104054\n",
      "Total Loss: 0.04519292898476124\n",
      "------------------------------------ epoch 6292 (37746 steps) ------------------------------------\n",
      "Max loss: 0.013886954635381699\n",
      "Min loss: 0.005325864069163799\n",
      "Mean loss: 0.009191247324148813\n",
      "Std loss: 0.0031043511974522407\n",
      "Total Loss: 0.05514748394489288\n",
      "------------------------------------ epoch 6293 (37752 steps) ------------------------------------\n",
      "Max loss: 0.00979787576943636\n",
      "Min loss: 0.005050851032137871\n",
      "Mean loss: 0.006724275726204117\n",
      "Std loss: 0.0015702631000109777\n",
      "Total Loss: 0.0403456543572247\n",
      "------------------------------------ epoch 6294 (37758 steps) ------------------------------------\n",
      "Max loss: 0.021215511485934258\n",
      "Min loss: 0.004007874056696892\n",
      "Mean loss: 0.00922265206463635\n",
      "Std loss: 0.00557214443784463\n",
      "Total Loss: 0.0553359123878181\n",
      "------------------------------------ epoch 6295 (37764 steps) ------------------------------------\n",
      "Max loss: 0.010191814973950386\n",
      "Min loss: 0.005122063215821981\n",
      "Mean loss: 0.007847002319370707\n",
      "Std loss: 0.00206305202164337\n",
      "Total Loss: 0.04708201391622424\n",
      "------------------------------------ epoch 6296 (37770 steps) ------------------------------------\n",
      "Max loss: 0.011980156414210796\n",
      "Min loss: 0.004386863671243191\n",
      "Mean loss: 0.00786272568317751\n",
      "Std loss: 0.0027076181391625887\n",
      "Total Loss: 0.047176354099065065\n",
      "------------------------------------ epoch 6297 (37776 steps) ------------------------------------\n",
      "Max loss: 0.015015989542007446\n",
      "Min loss: 0.0045503354631364346\n",
      "Mean loss: 0.008223959244787693\n",
      "Std loss: 0.0036018940681169457\n",
      "Total Loss: 0.04934375546872616\n",
      "------------------------------------ epoch 6298 (37782 steps) ------------------------------------\n",
      "Max loss: 0.012103483080863953\n",
      "Min loss: 0.005133139900863171\n",
      "Mean loss: 0.008188612526282668\n",
      "Std loss: 0.0028050686314960403\n",
      "Total Loss: 0.04913167515769601\n",
      "------------------------------------ epoch 6299 (37788 steps) ------------------------------------\n",
      "Max loss: 0.0189820509403944\n",
      "Min loss: 0.004628867842257023\n",
      "Mean loss: 0.01074488398929437\n",
      "Std loss: 0.005149982772840045\n",
      "Total Loss: 0.06446930393576622\n",
      "------------------------------------ epoch 6300 (37794 steps) ------------------------------------\n",
      "Max loss: 0.008518378250300884\n",
      "Min loss: 0.00453939288854599\n",
      "Mean loss: 0.007035978526497881\n",
      "Std loss: 0.0015170925594527257\n",
      "Total Loss: 0.042215871158987284\n",
      "------------------------------------ epoch 6301 (37800 steps) ------------------------------------\n",
      "Max loss: 0.015180706977844238\n",
      "Min loss: 0.0037304824218153954\n",
      "Mean loss: 0.00870145771962901\n",
      "Std loss: 0.0038672862228744287\n",
      "Total Loss: 0.05220874631777406\n",
      "saved model at ./weights/model_6301.pth\n",
      "------------------------------------ epoch 6302 (37806 steps) ------------------------------------\n",
      "Max loss: 0.038923125714063644\n",
      "Min loss: 0.0038705819752067327\n",
      "Mean loss: 0.011803083626242975\n",
      "Std loss: 0.012399708366647047\n",
      "Total Loss: 0.07081850175745785\n",
      "------------------------------------ epoch 6303 (37812 steps) ------------------------------------\n",
      "Max loss: 0.04641488194465637\n",
      "Min loss: 0.0040247468277812\n",
      "Mean loss: 0.016652936736742657\n",
      "Std loss: 0.013892900195027121\n",
      "Total Loss: 0.09991762042045593\n",
      "------------------------------------ epoch 6304 (37818 steps) ------------------------------------\n",
      "Max loss: 0.020410262048244476\n",
      "Min loss: 0.0054368264973163605\n",
      "Mean loss: 0.010146143768603602\n",
      "Std loss: 0.004883341873551573\n",
      "Total Loss: 0.06087686261162162\n",
      "------------------------------------ epoch 6305 (37824 steps) ------------------------------------\n",
      "Max loss: 0.015091387555003166\n",
      "Min loss: 0.004783987998962402\n",
      "Mean loss: 0.009083385346457362\n",
      "Std loss: 0.003545191988853072\n",
      "Total Loss: 0.05450031207874417\n",
      "------------------------------------ epoch 6306 (37830 steps) ------------------------------------\n",
      "Max loss: 0.01601361483335495\n",
      "Min loss: 0.005389761179685593\n",
      "Mean loss: 0.009325095374758044\n",
      "Std loss: 0.0037948572857625833\n",
      "Total Loss: 0.05595057224854827\n",
      "------------------------------------ epoch 6307 (37836 steps) ------------------------------------\n",
      "Max loss: 0.02364996448159218\n",
      "Min loss: 0.005354567430913448\n",
      "Mean loss: 0.013291136749709645\n",
      "Std loss: 0.006759627077126106\n",
      "Total Loss: 0.07974682049825788\n",
      "------------------------------------ epoch 6308 (37842 steps) ------------------------------------\n",
      "Max loss: 0.022906837984919548\n",
      "Min loss: 0.0046479227021336555\n",
      "Mean loss: 0.010262276977300644\n",
      "Std loss: 0.005959413088293314\n",
      "Total Loss: 0.061573661863803864\n",
      "------------------------------------ epoch 6309 (37848 steps) ------------------------------------\n",
      "Max loss: 0.018947094678878784\n",
      "Min loss: 0.006376142613589764\n",
      "Mean loss: 0.009946851991117\n",
      "Std loss: 0.004288912135971961\n",
      "Total Loss: 0.059681111946702003\n",
      "------------------------------------ epoch 6310 (37854 steps) ------------------------------------\n",
      "Max loss: 0.033519912511110306\n",
      "Min loss: 0.005680243484675884\n",
      "Mean loss: 0.01618434675037861\n",
      "Std loss: 0.008633069727184525\n",
      "Total Loss: 0.09710608050227165\n",
      "------------------------------------ epoch 6311 (37860 steps) ------------------------------------\n",
      "Max loss: 0.010104252956807613\n",
      "Min loss: 0.0063123381696641445\n",
      "Mean loss: 0.007686544442549348\n",
      "Std loss: 0.0014062525671987376\n",
      "Total Loss: 0.04611926665529609\n",
      "------------------------------------ epoch 6312 (37866 steps) ------------------------------------\n",
      "Max loss: 0.019475843757390976\n",
      "Min loss: 0.0066094771027565\n",
      "Mean loss: 0.010932436057676872\n",
      "Std loss: 0.004903442055864336\n",
      "Total Loss: 0.06559461634606123\n",
      "------------------------------------ epoch 6313 (37872 steps) ------------------------------------\n",
      "Max loss: 0.0068405806086957455\n",
      "Min loss: 0.004644657485187054\n",
      "Mean loss: 0.0058431838018198805\n",
      "Std loss: 0.0006877867578178932\n",
      "Total Loss: 0.035059102810919285\n",
      "------------------------------------ epoch 6314 (37878 steps) ------------------------------------\n",
      "Max loss: 0.024559427052736282\n",
      "Min loss: 0.00480036623775959\n",
      "Mean loss: 0.011727214402829608\n",
      "Std loss: 0.006804992710150588\n",
      "Total Loss: 0.07036328641697764\n",
      "------------------------------------ epoch 6315 (37884 steps) ------------------------------------\n",
      "Max loss: 0.01459383126348257\n",
      "Min loss: 0.0045209117233753204\n",
      "Mean loss: 0.009437818080186844\n",
      "Std loss: 0.003453093347449856\n",
      "Total Loss: 0.05662690848112106\n",
      "------------------------------------ epoch 6316 (37890 steps) ------------------------------------\n",
      "Max loss: 0.008906912058591843\n",
      "Min loss: 0.0048589822836220264\n",
      "Mean loss: 0.006748898498093088\n",
      "Std loss: 0.001458576960594104\n",
      "Total Loss: 0.04049339098855853\n",
      "------------------------------------ epoch 6317 (37896 steps) ------------------------------------\n",
      "Max loss: 0.013150426559150219\n",
      "Min loss: 0.004093450959771872\n",
      "Mean loss: 0.008481014830370745\n",
      "Std loss: 0.003308464678128118\n",
      "Total Loss: 0.050886088982224464\n",
      "------------------------------------ epoch 6318 (37902 steps) ------------------------------------\n",
      "Max loss: 0.01824362389743328\n",
      "Min loss: 0.004704602062702179\n",
      "Mean loss: 0.009301294106990099\n",
      "Std loss: 0.004342579291232523\n",
      "Total Loss: 0.055807764641940594\n",
      "------------------------------------ epoch 6319 (37908 steps) ------------------------------------\n",
      "Max loss: 0.014027654193341732\n",
      "Min loss: 0.004530399106442928\n",
      "Mean loss: 0.007556080042074124\n",
      "Std loss: 0.0032226732312796756\n",
      "Total Loss: 0.045336480252444744\n",
      "------------------------------------ epoch 6320 (37914 steps) ------------------------------------\n",
      "Max loss: 0.03118734061717987\n",
      "Min loss: 0.008552582934498787\n",
      "Mean loss: 0.01338084259380897\n",
      "Std loss: 0.008034576072705028\n",
      "Total Loss: 0.08028505556285381\n",
      "------------------------------------ epoch 6321 (37920 steps) ------------------------------------\n",
      "Max loss: 0.010871730744838715\n",
      "Min loss: 0.00535355880856514\n",
      "Mean loss: 0.008224250360702475\n",
      "Std loss: 0.0018011604865012826\n",
      "Total Loss: 0.04934550216421485\n",
      "------------------------------------ epoch 6322 (37926 steps) ------------------------------------\n",
      "Max loss: 0.010523771867156029\n",
      "Min loss: 0.004092994146049023\n",
      "Mean loss: 0.006740132890020807\n",
      "Std loss: 0.002127291006061371\n",
      "Total Loss: 0.040440797340124846\n",
      "------------------------------------ epoch 6323 (37932 steps) ------------------------------------\n",
      "Max loss: 0.028785465285182\n",
      "Min loss: 0.005465896800160408\n",
      "Mean loss: 0.01173191067452232\n",
      "Std loss: 0.007831930115185355\n",
      "Total Loss: 0.07039146404713392\n",
      "------------------------------------ epoch 6324 (37938 steps) ------------------------------------\n",
      "Max loss: 0.010897955857217312\n",
      "Min loss: 0.0057132793590426445\n",
      "Mean loss: 0.0072982873146732645\n",
      "Std loss: 0.0017051982655752836\n",
      "Total Loss: 0.04378972388803959\n",
      "------------------------------------ epoch 6325 (37944 steps) ------------------------------------\n",
      "Max loss: 0.03816280886530876\n",
      "Min loss: 0.004946121480315924\n",
      "Mean loss: 0.013166393230979642\n",
      "Std loss: 0.011430920498427545\n",
      "Total Loss: 0.07899835938587785\n",
      "------------------------------------ epoch 6326 (37950 steps) ------------------------------------\n",
      "Max loss: 0.016784105449914932\n",
      "Min loss: 0.005727443844079971\n",
      "Mean loss: 0.011050245103736719\n",
      "Std loss: 0.004420598772651528\n",
      "Total Loss: 0.06630147062242031\n",
      "------------------------------------ epoch 6327 (37956 steps) ------------------------------------\n",
      "Max loss: 0.014958932064473629\n",
      "Min loss: 0.006193629465997219\n",
      "Mean loss: 0.011377709917724133\n",
      "Std loss: 0.0032062103177507243\n",
      "Total Loss: 0.0682662595063448\n",
      "------------------------------------ epoch 6328 (37962 steps) ------------------------------------\n",
      "Max loss: 0.03000520169734955\n",
      "Min loss: 0.007365092635154724\n",
      "Mean loss: 0.011910992208868265\n",
      "Std loss: 0.008128112986573446\n",
      "Total Loss: 0.07146595325320959\n",
      "------------------------------------ epoch 6329 (37968 steps) ------------------------------------\n",
      "Max loss: 0.0168624147772789\n",
      "Min loss: 0.004844561219215393\n",
      "Mean loss: 0.009003017796203494\n",
      "Std loss: 0.004070261816678104\n",
      "Total Loss: 0.054018106777220964\n",
      "------------------------------------ epoch 6330 (37974 steps) ------------------------------------\n",
      "Max loss: 0.022359773516654968\n",
      "Min loss: 0.005114113446325064\n",
      "Mean loss: 0.012538466214512786\n",
      "Std loss: 0.006228339316872482\n",
      "Total Loss: 0.07523079728707671\n",
      "------------------------------------ epoch 6331 (37980 steps) ------------------------------------\n",
      "Max loss: 0.00931522436439991\n",
      "Min loss: 0.005098935682326555\n",
      "Mean loss: 0.007287691968182723\n",
      "Std loss: 0.001230489970884992\n",
      "Total Loss: 0.043726151809096336\n",
      "------------------------------------ epoch 6332 (37986 steps) ------------------------------------\n",
      "Max loss: 0.019066274166107178\n",
      "Min loss: 0.007196580525487661\n",
      "Mean loss: 0.014336341448749105\n",
      "Std loss: 0.004011285156930626\n",
      "Total Loss: 0.08601804869249463\n",
      "------------------------------------ epoch 6333 (37992 steps) ------------------------------------\n",
      "Max loss: 0.011250238865613937\n",
      "Min loss: 0.005268504377454519\n",
      "Mean loss: 0.0076756104826927185\n",
      "Std loss: 0.0021015328487729106\n",
      "Total Loss: 0.04605366289615631\n",
      "------------------------------------ epoch 6334 (37998 steps) ------------------------------------\n",
      "Max loss: 0.020789647474884987\n",
      "Min loss: 0.005530168768018484\n",
      "Mean loss: 0.012774130096659064\n",
      "Std loss: 0.004566443685466309\n",
      "Total Loss: 0.07664478057995439\n",
      "------------------------------------ epoch 6335 (38004 steps) ------------------------------------\n",
      "Max loss: 0.020573895424604416\n",
      "Min loss: 0.005044479854404926\n",
      "Mean loss: 0.012247192207723856\n",
      "Std loss: 0.006598179718706872\n",
      "Total Loss: 0.07348315324634314\n",
      "------------------------------------ epoch 6336 (38010 steps) ------------------------------------\n",
      "Max loss: 0.024638330563902855\n",
      "Min loss: 0.0048805889673531055\n",
      "Mean loss: 0.011541164247319102\n",
      "Std loss: 0.007145004067999346\n",
      "Total Loss: 0.06924698548391461\n",
      "------------------------------------ epoch 6337 (38016 steps) ------------------------------------\n",
      "Max loss: 0.017262769863009453\n",
      "Min loss: 0.005394862964749336\n",
      "Mean loss: 0.009212287220483025\n",
      "Std loss: 0.004143927241791996\n",
      "Total Loss: 0.05527372332289815\n",
      "------------------------------------ epoch 6338 (38022 steps) ------------------------------------\n",
      "Max loss: 0.02677454799413681\n",
      "Min loss: 0.005430683493614197\n",
      "Mean loss: 0.014156919438391924\n",
      "Std loss: 0.006600192282510095\n",
      "Total Loss: 0.08494151663035154\n",
      "------------------------------------ epoch 6339 (38028 steps) ------------------------------------\n",
      "Max loss: 0.02587944082915783\n",
      "Min loss: 0.005812309682369232\n",
      "Mean loss: 0.012821776947627464\n",
      "Std loss: 0.007039719658086533\n",
      "Total Loss: 0.07693066168576479\n",
      "------------------------------------ epoch 6340 (38034 steps) ------------------------------------\n",
      "Max loss: 0.013880586251616478\n",
      "Min loss: 0.006436966359615326\n",
      "Mean loss: 0.009528910275548697\n",
      "Std loss: 0.002379118736296873\n",
      "Total Loss: 0.05717346165329218\n",
      "------------------------------------ epoch 6341 (38040 steps) ------------------------------------\n",
      "Max loss: 0.015718473121523857\n",
      "Min loss: 0.004298019222915173\n",
      "Mean loss: 0.00877863533484439\n",
      "Std loss: 0.003940688048811626\n",
      "Total Loss: 0.05267181200906634\n",
      "------------------------------------ epoch 6342 (38046 steps) ------------------------------------\n",
      "Max loss: 0.016894325613975525\n",
      "Min loss: 0.004330229014158249\n",
      "Mean loss: 0.00841206886495153\n",
      "Std loss: 0.004337121435753838\n",
      "Total Loss: 0.05047241318970919\n",
      "------------------------------------ epoch 6343 (38052 steps) ------------------------------------\n",
      "Max loss: 0.030662819743156433\n",
      "Min loss: 0.004675736650824547\n",
      "Mean loss: 0.011440332358082136\n",
      "Std loss: 0.008810882725766137\n",
      "Total Loss: 0.06864199414849281\n",
      "------------------------------------ epoch 6344 (38058 steps) ------------------------------------\n",
      "Max loss: 0.03959530591964722\n",
      "Min loss: 0.006023521535098553\n",
      "Mean loss: 0.013244662589083115\n",
      "Std loss: 0.011968015626440589\n",
      "Total Loss: 0.07946797553449869\n",
      "------------------------------------ epoch 6345 (38064 steps) ------------------------------------\n",
      "Max loss: 0.030397720634937286\n",
      "Min loss: 0.0061057982966303825\n",
      "Mean loss: 0.013865215238183737\n",
      "Std loss: 0.008310703621372957\n",
      "Total Loss: 0.08319129142910242\n",
      "------------------------------------ epoch 6346 (38070 steps) ------------------------------------\n",
      "Max loss: 0.017217762768268585\n",
      "Min loss: 0.0066121239215135574\n",
      "Mean loss: 0.010624489514157176\n",
      "Std loss: 0.00353318994401957\n",
      "Total Loss: 0.06374693708494306\n",
      "------------------------------------ epoch 6347 (38076 steps) ------------------------------------\n",
      "Max loss: 0.018685225397348404\n",
      "Min loss: 0.005815798416733742\n",
      "Mean loss: 0.011095251111934582\n",
      "Std loss: 0.004399071545713138\n",
      "Total Loss: 0.0665715066716075\n",
      "------------------------------------ epoch 6348 (38082 steps) ------------------------------------\n",
      "Max loss: 0.019579995423555374\n",
      "Min loss: 0.0055427998304367065\n",
      "Mean loss: 0.013113195542246103\n",
      "Std loss: 0.005330034051087895\n",
      "Total Loss: 0.07867917325347662\n",
      "------------------------------------ epoch 6349 (38088 steps) ------------------------------------\n",
      "Max loss: 0.016489356756210327\n",
      "Min loss: 0.005041623953729868\n",
      "Mean loss: 0.008757449065645536\n",
      "Std loss: 0.0037301890096193982\n",
      "Total Loss: 0.052544694393873215\n",
      "------------------------------------ epoch 6350 (38094 steps) ------------------------------------\n",
      "Max loss: 0.012305877171456814\n",
      "Min loss: 0.004391947761178017\n",
      "Mean loss: 0.007297484204173088\n",
      "Std loss: 0.0027611348830898176\n",
      "Total Loss: 0.04378490522503853\n",
      "------------------------------------ epoch 6351 (38100 steps) ------------------------------------\n",
      "Max loss: 0.020761754363775253\n",
      "Min loss: 0.00590949272736907\n",
      "Mean loss: 0.011239367925251523\n",
      "Std loss: 0.005786455678514066\n",
      "Total Loss: 0.06743620755150914\n",
      "------------------------------------ epoch 6352 (38106 steps) ------------------------------------\n",
      "Max loss: 0.02225460857152939\n",
      "Min loss: 0.007233412936329842\n",
      "Mean loss: 0.011289897685249647\n",
      "Std loss: 0.005092577136819343\n",
      "Total Loss: 0.06773938611149788\n",
      "------------------------------------ epoch 6353 (38112 steps) ------------------------------------\n",
      "Max loss: 0.028029482811689377\n",
      "Min loss: 0.0056129633449018\n",
      "Mean loss: 0.010998177109286189\n",
      "Std loss: 0.007753149939629734\n",
      "Total Loss: 0.06598906265571713\n",
      "------------------------------------ epoch 6354 (38118 steps) ------------------------------------\n",
      "Max loss: 0.009557118639349937\n",
      "Min loss: 0.006088443100452423\n",
      "Mean loss: 0.007371180923655629\n",
      "Std loss: 0.001150071591228134\n",
      "Total Loss: 0.044227085541933775\n",
      "------------------------------------ epoch 6355 (38124 steps) ------------------------------------\n",
      "Max loss: 0.015909841284155846\n",
      "Min loss: 0.005162052810192108\n",
      "Mean loss: 0.010339999183391532\n",
      "Std loss: 0.0036219287865710895\n",
      "Total Loss: 0.06203999510034919\n",
      "------------------------------------ epoch 6356 (38130 steps) ------------------------------------\n",
      "Max loss: 0.0245977770537138\n",
      "Min loss: 0.005966754164546728\n",
      "Mean loss: 0.012921502891307076\n",
      "Std loss: 0.007349668674775549\n",
      "Total Loss: 0.07752901734784245\n",
      "------------------------------------ epoch 6357 (38136 steps) ------------------------------------\n",
      "Max loss: 0.0184489618986845\n",
      "Min loss: 0.005391337908804417\n",
      "Mean loss: 0.010045224102213979\n",
      "Std loss: 0.004523033891426051\n",
      "Total Loss: 0.06027134461328387\n",
      "------------------------------------ epoch 6358 (38142 steps) ------------------------------------\n",
      "Max loss: 0.011727790348231792\n",
      "Min loss: 0.0088497968390584\n",
      "Mean loss: 0.010064076321820417\n",
      "Std loss: 0.0011108478869236535\n",
      "Total Loss: 0.06038445793092251\n",
      "------------------------------------ epoch 6359 (38148 steps) ------------------------------------\n",
      "Max loss: 0.007398064713925123\n",
      "Min loss: 0.0045226989313960075\n",
      "Mean loss: 0.0058647961510966224\n",
      "Std loss: 0.001266861496631076\n",
      "Total Loss: 0.03518877690657973\n",
      "------------------------------------ epoch 6360 (38154 steps) ------------------------------------\n",
      "Max loss: 0.026449214667081833\n",
      "Min loss: 0.005739427637308836\n",
      "Mean loss: 0.012025082173446814\n",
      "Std loss: 0.0072855086734569825\n",
      "Total Loss: 0.07215049304068089\n",
      "------------------------------------ epoch 6361 (38160 steps) ------------------------------------\n",
      "Max loss: 0.016796547919511795\n",
      "Min loss: 0.005027934908866882\n",
      "Mean loss: 0.010095506363237897\n",
      "Std loss: 0.004548943334801706\n",
      "Total Loss: 0.060573038179427385\n",
      "------------------------------------ epoch 6362 (38166 steps) ------------------------------------\n",
      "Max loss: 0.0128904078155756\n",
      "Min loss: 0.004418965429067612\n",
      "Mean loss: 0.008049360010772943\n",
      "Std loss: 0.00316237638268337\n",
      "Total Loss: 0.04829616006463766\n",
      "------------------------------------ epoch 6363 (38172 steps) ------------------------------------\n",
      "Max loss: 0.01567523553967476\n",
      "Min loss: 0.004973802249878645\n",
      "Mean loss: 0.009095432702451944\n",
      "Std loss: 0.0033363881828638057\n",
      "Total Loss: 0.054572596214711666\n",
      "------------------------------------ epoch 6364 (38178 steps) ------------------------------------\n",
      "Max loss: 0.013256474398076534\n",
      "Min loss: 0.004347267094999552\n",
      "Mean loss: 0.007521009538322687\n",
      "Std loss: 0.002978866599513225\n",
      "Total Loss: 0.04512605722993612\n",
      "------------------------------------ epoch 6365 (38184 steps) ------------------------------------\n",
      "Max loss: 0.010372674092650414\n",
      "Min loss: 0.006130575202405453\n",
      "Mean loss: 0.007789649923021595\n",
      "Std loss: 0.0017711517319047502\n",
      "Total Loss: 0.04673789953812957\n",
      "------------------------------------ epoch 6366 (38190 steps) ------------------------------------\n",
      "Max loss: 0.010459758341312408\n",
      "Min loss: 0.005096382461488247\n",
      "Mean loss: 0.006932095779726903\n",
      "Std loss: 0.0017242370239082259\n",
      "Total Loss: 0.041592574678361416\n",
      "------------------------------------ epoch 6367 (38196 steps) ------------------------------------\n",
      "Max loss: 0.018280617892742157\n",
      "Min loss: 0.0053477175533771515\n",
      "Mean loss: 0.010012137548377117\n",
      "Std loss: 0.0057640517001400155\n",
      "Total Loss: 0.0600728252902627\n",
      "------------------------------------ epoch 6368 (38202 steps) ------------------------------------\n",
      "Max loss: 0.024489549919962883\n",
      "Min loss: 0.006123988889157772\n",
      "Mean loss: 0.013348264697318276\n",
      "Std loss: 0.006662670627614429\n",
      "Total Loss: 0.08008958818390965\n",
      "------------------------------------ epoch 6369 (38208 steps) ------------------------------------\n",
      "Max loss: 0.01672070287168026\n",
      "Min loss: 0.0051918067038059235\n",
      "Mean loss: 0.010720424043635527\n",
      "Std loss: 0.0036835025524072787\n",
      "Total Loss: 0.06432254426181316\n",
      "------------------------------------ epoch 6370 (38214 steps) ------------------------------------\n",
      "Max loss: 0.009334219619631767\n",
      "Min loss: 0.00479518948122859\n",
      "Mean loss: 0.006525604364772637\n",
      "Std loss: 0.0015776496620416582\n",
      "Total Loss: 0.039153626188635826\n",
      "------------------------------------ epoch 6371 (38220 steps) ------------------------------------\n",
      "Max loss: 0.01551581360399723\n",
      "Min loss: 0.004501115530729294\n",
      "Mean loss: 0.007723284264405568\n",
      "Std loss: 0.003667081093360829\n",
      "Total Loss: 0.04633970558643341\n",
      "------------------------------------ epoch 6372 (38226 steps) ------------------------------------\n",
      "Max loss: 0.010783381760120392\n",
      "Min loss: 0.0050269062630832195\n",
      "Mean loss: 0.008060660678893328\n",
      "Std loss: 0.0021399656532979303\n",
      "Total Loss: 0.048363964073359966\n",
      "------------------------------------ epoch 6373 (38232 steps) ------------------------------------\n",
      "Max loss: 0.014591837301850319\n",
      "Min loss: 0.0043713683262467384\n",
      "Mean loss: 0.008004909846931696\n",
      "Std loss: 0.0033117436129026625\n",
      "Total Loss: 0.048029459081590176\n",
      "------------------------------------ epoch 6374 (38238 steps) ------------------------------------\n",
      "Max loss: 0.014244504272937775\n",
      "Min loss: 0.005338477902114391\n",
      "Mean loss: 0.0073886561828355\n",
      "Std loss: 0.003104214244391392\n",
      "Total Loss: 0.044331937097013\n",
      "------------------------------------ epoch 6375 (38244 steps) ------------------------------------\n",
      "Max loss: 0.011577457189559937\n",
      "Min loss: 0.0049069905653595924\n",
      "Mean loss: 0.008053157478570938\n",
      "Std loss: 0.002578090189747228\n",
      "Total Loss: 0.04831894487142563\n",
      "------------------------------------ epoch 6376 (38250 steps) ------------------------------------\n",
      "Max loss: 0.012213233858346939\n",
      "Min loss: 0.0038953954353928566\n",
      "Mean loss: 0.008189785682285825\n",
      "Std loss: 0.0026491303632501084\n",
      "Total Loss: 0.04913871409371495\n",
      "------------------------------------ epoch 6377 (38256 steps) ------------------------------------\n",
      "Max loss: 0.022688424214720726\n",
      "Min loss: 0.0044392868876457214\n",
      "Mean loss: 0.013052727561444044\n",
      "Std loss: 0.005691340927333373\n",
      "Total Loss: 0.07831636536866426\n",
      "------------------------------------ epoch 6378 (38262 steps) ------------------------------------\n",
      "Max loss: 0.013771338388323784\n",
      "Min loss: 0.004025466740131378\n",
      "Mean loss: 0.008194972295314074\n",
      "Std loss: 0.003441955594784525\n",
      "Total Loss: 0.04916983377188444\n",
      "------------------------------------ epoch 6379 (38268 steps) ------------------------------------\n",
      "Max loss: 0.0276661179959774\n",
      "Min loss: 0.004296586383134127\n",
      "Mean loss: 0.01242493100774785\n",
      "Std loss: 0.007859493952322437\n",
      "Total Loss: 0.07454958604648709\n",
      "------------------------------------ epoch 6380 (38274 steps) ------------------------------------\n",
      "Max loss: 0.0076897405087947845\n",
      "Min loss: 0.004900287836790085\n",
      "Mean loss: 0.006231538175294797\n",
      "Std loss: 0.0010369373399044955\n",
      "Total Loss: 0.03738922905176878\n",
      "------------------------------------ epoch 6381 (38280 steps) ------------------------------------\n",
      "Max loss: 0.038026854395866394\n",
      "Min loss: 0.005296043120324612\n",
      "Mean loss: 0.01728640574341019\n",
      "Std loss: 0.01271533262851778\n",
      "Total Loss: 0.10371843446046114\n",
      "------------------------------------ epoch 6382 (38286 steps) ------------------------------------\n",
      "Max loss: 0.050981711596250534\n",
      "Min loss: 0.010010307654738426\n",
      "Mean loss: 0.02442503860220313\n",
      "Std loss: 0.014919610654268655\n",
      "Total Loss: 0.14655023161321878\n",
      "------------------------------------ epoch 6383 (38292 steps) ------------------------------------\n",
      "Max loss: 0.018647635355591774\n",
      "Min loss: 0.008777393028140068\n",
      "Mean loss: 0.014420096452037493\n",
      "Std loss: 0.004190732710493858\n",
      "Total Loss: 0.08652057871222496\n",
      "------------------------------------ epoch 6384 (38298 steps) ------------------------------------\n",
      "Max loss: 0.014921076595783234\n",
      "Min loss: 0.008811095729470253\n",
      "Mean loss: 0.011967308974514404\n",
      "Std loss: 0.002202506318920579\n",
      "Total Loss: 0.07180385384708643\n",
      "------------------------------------ epoch 6385 (38304 steps) ------------------------------------\n",
      "Max loss: 0.01270056888461113\n",
      "Min loss: 0.004732292145490646\n",
      "Mean loss: 0.007672896919151147\n",
      "Std loss: 0.0024229910451237457\n",
      "Total Loss: 0.04603738151490688\n",
      "------------------------------------ epoch 6386 (38310 steps) ------------------------------------\n",
      "Max loss: 0.028950190171599388\n",
      "Min loss: 0.005528173875063658\n",
      "Mean loss: 0.011301399829487005\n",
      "Std loss: 0.008563788980290268\n",
      "Total Loss: 0.06780839897692204\n",
      "------------------------------------ epoch 6387 (38316 steps) ------------------------------------\n",
      "Max loss: 0.025498278439044952\n",
      "Min loss: 0.005233560688793659\n",
      "Mean loss: 0.012883601554979881\n",
      "Std loss: 0.007341824830642147\n",
      "Total Loss: 0.07730160932987928\n",
      "------------------------------------ epoch 6388 (38322 steps) ------------------------------------\n",
      "Max loss: 0.05379864573478699\n",
      "Min loss: 0.00604134239256382\n",
      "Mean loss: 0.024413739951948326\n",
      "Std loss: 0.01993796015602764\n",
      "Total Loss: 0.14648243971168995\n",
      "------------------------------------ epoch 6389 (38328 steps) ------------------------------------\n",
      "Max loss: 0.02218303084373474\n",
      "Min loss: 0.008690369315445423\n",
      "Mean loss: 0.01247837378953894\n",
      "Std loss: 0.004560535093076076\n",
      "Total Loss: 0.07487024273723364\n",
      "------------------------------------ epoch 6390 (38334 steps) ------------------------------------\n",
      "Max loss: 0.015285334549844265\n",
      "Min loss: 0.005549855064600706\n",
      "Mean loss: 0.00890157736527423\n",
      "Std loss: 0.003174242645159285\n",
      "Total Loss: 0.053409464191645384\n",
      "------------------------------------ epoch 6391 (38340 steps) ------------------------------------\n",
      "Max loss: 0.05945684015750885\n",
      "Min loss: 0.007310707122087479\n",
      "Mean loss: 0.02248880049834649\n",
      "Std loss: 0.017563719900409763\n",
      "Total Loss: 0.13493280299007893\n",
      "------------------------------------ epoch 6392 (38346 steps) ------------------------------------\n",
      "Max loss: 0.03334406390786171\n",
      "Min loss: 0.006467278115451336\n",
      "Mean loss: 0.013564216283460459\n",
      "Std loss: 0.00934249700107103\n",
      "Total Loss: 0.08138529770076275\n",
      "------------------------------------ epoch 6393 (38352 steps) ------------------------------------\n",
      "Max loss: 0.02437392994761467\n",
      "Min loss: 0.006508669815957546\n",
      "Mean loss: 0.012574154185131192\n",
      "Std loss: 0.006303792035839806\n",
      "Total Loss: 0.07544492511078715\n",
      "------------------------------------ epoch 6394 (38358 steps) ------------------------------------\n",
      "Max loss: 0.026377592235803604\n",
      "Min loss: 0.0049323467537760735\n",
      "Mean loss: 0.013056619868924221\n",
      "Std loss: 0.008632279466096557\n",
      "Total Loss: 0.07833971921354532\n",
      "------------------------------------ epoch 6395 (38364 steps) ------------------------------------\n",
      "Max loss: 0.013375362381339073\n",
      "Min loss: 0.005569811444729567\n",
      "Mean loss: 0.009813330989951888\n",
      "Std loss: 0.0030803682119273303\n",
      "Total Loss: 0.05887998593971133\n",
      "------------------------------------ epoch 6396 (38370 steps) ------------------------------------\n",
      "Max loss: 0.0337744764983654\n",
      "Min loss: 0.005854256451129913\n",
      "Mean loss: 0.015418891096487641\n",
      "Std loss: 0.011032703827935236\n",
      "Total Loss: 0.09251334657892585\n",
      "------------------------------------ epoch 6397 (38376 steps) ------------------------------------\n",
      "Max loss: 0.048665981739759445\n",
      "Min loss: 0.00538293831050396\n",
      "Mean loss: 0.01644335376719634\n",
      "Std loss: 0.015360965361939842\n",
      "Total Loss: 0.09866012260317802\n",
      "------------------------------------ epoch 6398 (38382 steps) ------------------------------------\n",
      "Max loss: 0.020247921347618103\n",
      "Min loss: 0.00869486853480339\n",
      "Mean loss: 0.011724965025981268\n",
      "Std loss: 0.004043528994994474\n",
      "Total Loss: 0.0703497901558876\n",
      "------------------------------------ epoch 6399 (38388 steps) ------------------------------------\n",
      "Max loss: 0.051479414105415344\n",
      "Min loss: 0.009384192526340485\n",
      "Mean loss: 0.023688916116952896\n",
      "Std loss: 0.014473052983408466\n",
      "Total Loss: 0.14213349670171738\n",
      "------------------------------------ epoch 6400 (38394 steps) ------------------------------------\n",
      "Max loss: 0.030503785237669945\n",
      "Min loss: 0.008038374595344067\n",
      "Mean loss: 0.01553867245092988\n",
      "Std loss: 0.007274164824166286\n",
      "Total Loss: 0.09323203470557928\n",
      "------------------------------------ epoch 6401 (38400 steps) ------------------------------------\n",
      "Max loss: 0.03418204188346863\n",
      "Min loss: 0.008852316997945309\n",
      "Mean loss: 0.01969705196097493\n",
      "Std loss: 0.009391988019606352\n",
      "Total Loss: 0.11818231176584959\n",
      "saved model at ./weights/model_6401.pth\n",
      "------------------------------------ epoch 6402 (38406 steps) ------------------------------------\n",
      "Max loss: 0.017296001315116882\n",
      "Min loss: 0.0054894015192985535\n",
      "Mean loss: 0.010998295930524668\n",
      "Std loss: 0.003689311152570846\n",
      "Total Loss: 0.065989775583148\n",
      "------------------------------------ epoch 6403 (38412 steps) ------------------------------------\n",
      "Max loss: 0.02357097528874874\n",
      "Min loss: 0.006940088700503111\n",
      "Mean loss: 0.013915637275204062\n",
      "Std loss: 0.006119475573514909\n",
      "Total Loss: 0.08349382365122437\n",
      "------------------------------------ epoch 6404 (38418 steps) ------------------------------------\n",
      "Max loss: 0.02739815227687359\n",
      "Min loss: 0.006506018340587616\n",
      "Mean loss: 0.013556157083561024\n",
      "Std loss: 0.007042457125034564\n",
      "Total Loss: 0.08133694250136614\n",
      "------------------------------------ epoch 6405 (38424 steps) ------------------------------------\n",
      "Max loss: 0.01647059991955757\n",
      "Min loss: 0.0057864016853272915\n",
      "Mean loss: 0.009439289337024093\n",
      "Std loss: 0.0035344004377696886\n",
      "Total Loss: 0.056635736022144556\n",
      "------------------------------------ epoch 6406 (38430 steps) ------------------------------------\n",
      "Max loss: 0.02999497763812542\n",
      "Min loss: 0.005731950514018536\n",
      "Mean loss: 0.014510280918329954\n",
      "Std loss: 0.008133219955165782\n",
      "Total Loss: 0.08706168550997972\n",
      "------------------------------------ epoch 6407 (38436 steps) ------------------------------------\n",
      "Max loss: 0.02031157724559307\n",
      "Min loss: 0.00563517352566123\n",
      "Mean loss: 0.010485158612330755\n",
      "Std loss: 0.0053509593895634585\n",
      "Total Loss: 0.06291095167398453\n",
      "------------------------------------ epoch 6408 (38442 steps) ------------------------------------\n",
      "Max loss: 0.012691309675574303\n",
      "Min loss: 0.005190318450331688\n",
      "Mean loss: 0.008380653491864601\n",
      "Std loss: 0.0029160538043426906\n",
      "Total Loss: 0.05028392095118761\n",
      "------------------------------------ epoch 6409 (38448 steps) ------------------------------------\n",
      "Max loss: 0.015108151361346245\n",
      "Min loss: 0.004936202894896269\n",
      "Mean loss: 0.00816968409344554\n",
      "Std loss: 0.0034484727580403472\n",
      "Total Loss: 0.04901810456067324\n",
      "------------------------------------ epoch 6410 (38454 steps) ------------------------------------\n",
      "Max loss: 0.03002747893333435\n",
      "Min loss: 0.0054363347589969635\n",
      "Mean loss: 0.011551200179383159\n",
      "Std loss: 0.008601247288233835\n",
      "Total Loss: 0.06930720107629895\n",
      "------------------------------------ epoch 6411 (38460 steps) ------------------------------------\n",
      "Max loss: 0.03875288367271423\n",
      "Min loss: 0.0048834532499313354\n",
      "Mean loss: 0.015179996999601523\n",
      "Std loss: 0.011678449233700108\n",
      "Total Loss: 0.09107998199760914\n",
      "------------------------------------ epoch 6412 (38466 steps) ------------------------------------\n",
      "Max loss: 0.018189556896686554\n",
      "Min loss: 0.005234360694885254\n",
      "Mean loss: 0.010469866062824925\n",
      "Std loss: 0.004537965151187591\n",
      "Total Loss: 0.06281919637694955\n",
      "------------------------------------ epoch 6413 (38472 steps) ------------------------------------\n",
      "Max loss: 0.010929631069302559\n",
      "Min loss: 0.004773047752678394\n",
      "Mean loss: 0.007857812646155557\n",
      "Std loss: 0.0020095180271136985\n",
      "Total Loss: 0.047146875876933336\n",
      "------------------------------------ epoch 6414 (38478 steps) ------------------------------------\n",
      "Max loss: 0.029791241511702538\n",
      "Min loss: 0.004644717089831829\n",
      "Mean loss: 0.014478392743815979\n",
      "Std loss: 0.008133415930187186\n",
      "Total Loss: 0.08687035646289587\n",
      "------------------------------------ epoch 6415 (38484 steps) ------------------------------------\n",
      "Max loss: 0.01276930421590805\n",
      "Min loss: 0.005239227320998907\n",
      "Mean loss: 0.007616608248402675\n",
      "Std loss: 0.002391359924789447\n",
      "Total Loss: 0.04569964949041605\n",
      "------------------------------------ epoch 6416 (38490 steps) ------------------------------------\n",
      "Max loss: 0.015532678924500942\n",
      "Min loss: 0.004404033534228802\n",
      "Mean loss: 0.007099043655519684\n",
      "Std loss: 0.0038796049927084897\n",
      "Total Loss: 0.042594261933118105\n",
      "------------------------------------ epoch 6417 (38496 steps) ------------------------------------\n",
      "Max loss: 0.0303794052451849\n",
      "Min loss: 0.004744546487927437\n",
      "Mean loss: 0.010903430636972189\n",
      "Std loss: 0.009242929855095006\n",
      "Total Loss: 0.06542058382183313\n",
      "------------------------------------ epoch 6418 (38502 steps) ------------------------------------\n",
      "Max loss: 0.011399395763874054\n",
      "Min loss: 0.005030314438045025\n",
      "Mean loss: 0.006880936678498983\n",
      "Std loss: 0.0023302222914691056\n",
      "Total Loss: 0.0412856200709939\n",
      "------------------------------------ epoch 6419 (38508 steps) ------------------------------------\n",
      "Max loss: 0.012368766590952873\n",
      "Min loss: 0.004391489550471306\n",
      "Mean loss: 0.006957379635423422\n",
      "Std loss: 0.003008544832648248\n",
      "Total Loss: 0.04174427781254053\n",
      "------------------------------------ epoch 6420 (38514 steps) ------------------------------------\n",
      "Max loss: 0.012027483433485031\n",
      "Min loss: 0.004859494045376778\n",
      "Mean loss: 0.008972973097115755\n",
      "Std loss: 0.002635185586841424\n",
      "Total Loss: 0.05383783858269453\n",
      "------------------------------------ epoch 6421 (38520 steps) ------------------------------------\n",
      "Max loss: 0.019555388018488884\n",
      "Min loss: 0.004496365785598755\n",
      "Mean loss: 0.009127436050524315\n",
      "Std loss: 0.005088968616209718\n",
      "Total Loss: 0.054764616303145885\n",
      "------------------------------------ epoch 6422 (38526 steps) ------------------------------------\n",
      "Max loss: 0.02602565288543701\n",
      "Min loss: 0.004816692788153887\n",
      "Mean loss: 0.01060262982112666\n",
      "Std loss: 0.0071311800038684715\n",
      "Total Loss: 0.06361577892675996\n",
      "------------------------------------ epoch 6423 (38532 steps) ------------------------------------\n",
      "Max loss: 0.030148297548294067\n",
      "Min loss: 0.005048651248216629\n",
      "Mean loss: 0.010996713070198894\n",
      "Std loss: 0.00871481799579051\n",
      "Total Loss: 0.06598027842119336\n",
      "------------------------------------ epoch 6424 (38538 steps) ------------------------------------\n",
      "Max loss: 0.019316181540489197\n",
      "Min loss: 0.00617096945643425\n",
      "Mean loss: 0.009657349049424132\n",
      "Std loss: 0.004468709672902027\n",
      "Total Loss: 0.05794409429654479\n",
      "------------------------------------ epoch 6425 (38544 steps) ------------------------------------\n",
      "Max loss: 0.008432861417531967\n",
      "Min loss: 0.004282521083950996\n",
      "Mean loss: 0.006119444190214078\n",
      "Std loss: 0.0014836641561023175\n",
      "Total Loss: 0.036716665141284466\n",
      "------------------------------------ epoch 6426 (38550 steps) ------------------------------------\n",
      "Max loss: 0.017223678529262543\n",
      "Min loss: 0.006954164244234562\n",
      "Mean loss: 0.010085656928519407\n",
      "Std loss: 0.00352036125978404\n",
      "Total Loss: 0.06051394157111645\n",
      "------------------------------------ epoch 6427 (38556 steps) ------------------------------------\n",
      "Max loss: 0.017874957993626595\n",
      "Min loss: 0.0041597699746489525\n",
      "Mean loss: 0.008391042860845724\n",
      "Std loss: 0.004568832129508413\n",
      "Total Loss: 0.05034625716507435\n",
      "------------------------------------ epoch 6428 (38562 steps) ------------------------------------\n",
      "Max loss: 0.015398910269141197\n",
      "Min loss: 0.004888473078608513\n",
      "Mean loss: 0.007331443950533867\n",
      "Std loss: 0.003757931477934767\n",
      "Total Loss: 0.0439886637032032\n",
      "------------------------------------ epoch 6429 (38568 steps) ------------------------------------\n",
      "Max loss: 0.04324626177549362\n",
      "Min loss: 0.004992806352674961\n",
      "Mean loss: 0.014491562343512973\n",
      "Std loss: 0.01362333997585654\n",
      "Total Loss: 0.08694937406107783\n",
      "------------------------------------ epoch 6430 (38574 steps) ------------------------------------\n",
      "Max loss: 0.017372792586684227\n",
      "Min loss: 0.004199591930955648\n",
      "Mean loss: 0.009212424357732138\n",
      "Std loss: 0.004567108704028139\n",
      "Total Loss: 0.05527454614639282\n",
      "------------------------------------ epoch 6431 (38580 steps) ------------------------------------\n",
      "Max loss: 0.011116805486381054\n",
      "Min loss: 0.005691160913556814\n",
      "Mean loss: 0.008298822523405155\n",
      "Std loss: 0.002002654043733566\n",
      "Total Loss: 0.04979293514043093\n",
      "------------------------------------ epoch 6432 (38586 steps) ------------------------------------\n",
      "Max loss: 0.012322432361543179\n",
      "Min loss: 0.006285829469561577\n",
      "Mean loss: 0.008408903221910199\n",
      "Std loss: 0.002155507186452453\n",
      "Total Loss: 0.05045341933146119\n",
      "------------------------------------ epoch 6433 (38592 steps) ------------------------------------\n",
      "Max loss: 0.013990111649036407\n",
      "Min loss: 0.004361898172646761\n",
      "Mean loss: 0.01021680150491496\n",
      "Std loss: 0.004018041425905582\n",
      "Total Loss: 0.061300809029489756\n",
      "------------------------------------ epoch 6434 (38598 steps) ------------------------------------\n",
      "Max loss: 0.02268807962536812\n",
      "Min loss: 0.005352091044187546\n",
      "Mean loss: 0.010052796763678392\n",
      "Std loss: 0.006138738124463358\n",
      "Total Loss: 0.06031678058207035\n",
      "------------------------------------ epoch 6435 (38604 steps) ------------------------------------\n",
      "Max loss: 0.012657134793698788\n",
      "Min loss: 0.004795881919562817\n",
      "Mean loss: 0.008008791133761406\n",
      "Std loss: 0.0031490779817263744\n",
      "Total Loss: 0.048052746802568436\n",
      "------------------------------------ epoch 6436 (38610 steps) ------------------------------------\n",
      "Max loss: 0.06417851150035858\n",
      "Min loss: 0.005649907514452934\n",
      "Mean loss: 0.018579561573763687\n",
      "Std loss: 0.020539380707785358\n",
      "Total Loss: 0.11147736944258213\n",
      "------------------------------------ epoch 6437 (38616 steps) ------------------------------------\n",
      "Max loss: 0.015644878149032593\n",
      "Min loss: 0.005462951958179474\n",
      "Mean loss: 0.008172046237935623\n",
      "Std loss: 0.0034689731081140565\n",
      "Total Loss: 0.049032277427613735\n",
      "------------------------------------ epoch 6438 (38622 steps) ------------------------------------\n",
      "Max loss: 0.030002644285559654\n",
      "Min loss: 0.004681467544287443\n",
      "Mean loss: 0.011907214997336268\n",
      "Std loss: 0.008318031923353076\n",
      "Total Loss: 0.07144328998401761\n",
      "------------------------------------ epoch 6439 (38628 steps) ------------------------------------\n",
      "Max loss: 0.04873853176832199\n",
      "Min loss: 0.006667090579867363\n",
      "Mean loss: 0.01564760459586978\n",
      "Std loss: 0.01491318892979626\n",
      "Total Loss: 0.09388562757521868\n",
      "------------------------------------ epoch 6440 (38634 steps) ------------------------------------\n",
      "Max loss: 0.01007816381752491\n",
      "Min loss: 0.004092348739504814\n",
      "Mean loss: 0.007243141531944275\n",
      "Std loss: 0.001969344614439344\n",
      "Total Loss: 0.04345884919166565\n",
      "------------------------------------ epoch 6441 (38640 steps) ------------------------------------\n",
      "Max loss: 0.033138208091259\n",
      "Min loss: 0.003970342688262463\n",
      "Mean loss: 0.010718999508147439\n",
      "Std loss: 0.010289028537853372\n",
      "Total Loss: 0.06431399704888463\n",
      "------------------------------------ epoch 6442 (38646 steps) ------------------------------------\n",
      "Max loss: 0.01857929304242134\n",
      "Min loss: 0.004915090277791023\n",
      "Mean loss: 0.010463438152025143\n",
      "Std loss: 0.004391638235974969\n",
      "Total Loss: 0.06278062891215086\n",
      "------------------------------------ epoch 6443 (38652 steps) ------------------------------------\n",
      "Max loss: 0.05548778176307678\n",
      "Min loss: 0.004533419851213694\n",
      "Mean loss: 0.014091553166508675\n",
      "Std loss: 0.018546556572060953\n",
      "Total Loss: 0.08454931899905205\n",
      "------------------------------------ epoch 6444 (38658 steps) ------------------------------------\n",
      "Max loss: 0.012472031638026237\n",
      "Min loss: 0.004425885155797005\n",
      "Mean loss: 0.007166225075100859\n",
      "Std loss: 0.002902564215768984\n",
      "Total Loss: 0.042997350450605154\n",
      "------------------------------------ epoch 6445 (38664 steps) ------------------------------------\n",
      "Max loss: 0.019821427762508392\n",
      "Min loss: 0.0054763793013989925\n",
      "Mean loss: 0.010442585529138645\n",
      "Std loss: 0.005291331880867005\n",
      "Total Loss: 0.06265551317483187\n",
      "------------------------------------ epoch 6446 (38670 steps) ------------------------------------\n",
      "Max loss: 0.00893859937787056\n",
      "Min loss: 0.0047777388244867325\n",
      "Mean loss: 0.0070313094183802605\n",
      "Std loss: 0.0015684912375239972\n",
      "Total Loss: 0.04218785651028156\n",
      "------------------------------------ epoch 6447 (38676 steps) ------------------------------------\n",
      "Max loss: 0.021355150267481804\n",
      "Min loss: 0.009913614951074123\n",
      "Mean loss: 0.014189814062168201\n",
      "Std loss: 0.003611311161614975\n",
      "Total Loss: 0.0851388843730092\n",
      "------------------------------------ epoch 6448 (38682 steps) ------------------------------------\n",
      "Max loss: 0.02904265746474266\n",
      "Min loss: 0.004403605125844479\n",
      "Mean loss: 0.011673567273343602\n",
      "Std loss: 0.008641912697948988\n",
      "Total Loss: 0.07004140364006162\n",
      "------------------------------------ epoch 6449 (38688 steps) ------------------------------------\n",
      "Max loss: 0.01025803666561842\n",
      "Min loss: 0.005591212771832943\n",
      "Mean loss: 0.0077900812805940705\n",
      "Std loss: 0.0017735202506121712\n",
      "Total Loss: 0.046740487683564425\n",
      "------------------------------------ epoch 6450 (38694 steps) ------------------------------------\n",
      "Max loss: 0.04004661738872528\n",
      "Min loss: 0.004072016105055809\n",
      "Mean loss: 0.01717673024783532\n",
      "Std loss: 0.013132481265422203\n",
      "Total Loss: 0.10306038148701191\n",
      "------------------------------------ epoch 6451 (38700 steps) ------------------------------------\n",
      "Max loss: 0.04137859493494034\n",
      "Min loss: 0.005608703941106796\n",
      "Mean loss: 0.019355984559903543\n",
      "Std loss: 0.013671851219638749\n",
      "Total Loss: 0.11613590735942125\n",
      "------------------------------------ epoch 6452 (38706 steps) ------------------------------------\n",
      "Max loss: 0.023215577006340027\n",
      "Min loss: 0.006284200586378574\n",
      "Mean loss: 0.012458151827255884\n",
      "Std loss: 0.005680185229717479\n",
      "Total Loss: 0.07474891096353531\n",
      "------------------------------------ epoch 6453 (38712 steps) ------------------------------------\n",
      "Max loss: 0.03714187070727348\n",
      "Min loss: 0.006308991461992264\n",
      "Mean loss: 0.01456808221216003\n",
      "Std loss: 0.01030114872562039\n",
      "Total Loss: 0.08740849327296019\n",
      "------------------------------------ epoch 6454 (38718 steps) ------------------------------------\n",
      "Max loss: 0.016066838055849075\n",
      "Min loss: 0.005328381434082985\n",
      "Mean loss: 0.01035865854161481\n",
      "Std loss: 0.004332641919151288\n",
      "Total Loss: 0.062151951249688864\n",
      "------------------------------------ epoch 6455 (38724 steps) ------------------------------------\n",
      "Max loss: 0.01037617214024067\n",
      "Min loss: 0.005382319446653128\n",
      "Mean loss: 0.007609393913298845\n",
      "Std loss: 0.0019893991417952494\n",
      "Total Loss: 0.04565636347979307\n",
      "------------------------------------ epoch 6456 (38730 steps) ------------------------------------\n",
      "Max loss: 0.024514196440577507\n",
      "Min loss: 0.004433941561728716\n",
      "Mean loss: 0.010624972948183617\n",
      "Std loss: 0.007184432404354392\n",
      "Total Loss: 0.0637498376891017\n",
      "------------------------------------ epoch 6457 (38736 steps) ------------------------------------\n",
      "Max loss: 0.009820716455578804\n",
      "Min loss: 0.004204600118100643\n",
      "Mean loss: 0.006970341627796491\n",
      "Std loss: 0.001994946108498254\n",
      "Total Loss: 0.041822049766778946\n",
      "------------------------------------ epoch 6458 (38742 steps) ------------------------------------\n",
      "Max loss: 0.017871901392936707\n",
      "Min loss: 0.005084664560854435\n",
      "Mean loss: 0.010241628934939703\n",
      "Std loss: 0.004704345798415163\n",
      "Total Loss: 0.061449773609638214\n",
      "------------------------------------ epoch 6459 (38748 steps) ------------------------------------\n",
      "Max loss: 0.019051533192396164\n",
      "Min loss: 0.004554847255349159\n",
      "Mean loss: 0.009301231397936741\n",
      "Std loss: 0.004954123996017997\n",
      "Total Loss: 0.05580738838762045\n",
      "------------------------------------ epoch 6460 (38754 steps) ------------------------------------\n",
      "Max loss: 0.02068331092596054\n",
      "Min loss: 0.00441688671708107\n",
      "Mean loss: 0.009273905695105592\n",
      "Std loss: 0.00652265522186733\n",
      "Total Loss: 0.055643434170633554\n",
      "------------------------------------ epoch 6461 (38760 steps) ------------------------------------\n",
      "Max loss: 0.007792784832417965\n",
      "Min loss: 0.0049710338935256\n",
      "Mean loss: 0.006384483926619093\n",
      "Std loss: 0.000838312285852673\n",
      "Total Loss: 0.038306903559714556\n",
      "------------------------------------ epoch 6462 (38766 steps) ------------------------------------\n",
      "Max loss: 0.017315803095698357\n",
      "Min loss: 0.004979046061635017\n",
      "Mean loss: 0.008495419286191463\n",
      "Std loss: 0.004437025940829734\n",
      "Total Loss: 0.05097251571714878\n",
      "------------------------------------ epoch 6463 (38772 steps) ------------------------------------\n",
      "Max loss: 0.029393207281827927\n",
      "Min loss: 0.004141333047300577\n",
      "Mean loss: 0.013514541943247119\n",
      "Std loss: 0.008825199358794236\n",
      "Total Loss: 0.08108725165948272\n",
      "------------------------------------ epoch 6464 (38778 steps) ------------------------------------\n",
      "Max loss: 0.01382711436599493\n",
      "Min loss: 0.005458315368741751\n",
      "Mean loss: 0.00920667367366453\n",
      "Std loss: 0.003463250317503349\n",
      "Total Loss: 0.05524004204198718\n",
      "------------------------------------ epoch 6465 (38784 steps) ------------------------------------\n",
      "Max loss: 0.03979429602622986\n",
      "Min loss: 0.006499729119241238\n",
      "Mean loss: 0.022836757513384025\n",
      "Std loss: 0.014941094966425452\n",
      "Total Loss: 0.13702054508030415\n",
      "------------------------------------ epoch 6466 (38790 steps) ------------------------------------\n",
      "Max loss: 0.05525168776512146\n",
      "Min loss: 0.008524348959326744\n",
      "Mean loss: 0.019540972386797268\n",
      "Std loss: 0.016461473432604873\n",
      "Total Loss: 0.11724583432078362\n",
      "------------------------------------ epoch 6467 (38796 steps) ------------------------------------\n",
      "Max loss: 0.027784090489149094\n",
      "Min loss: 0.006163515150547028\n",
      "Mean loss: 0.016180474931995075\n",
      "Std loss: 0.007938160407015124\n",
      "Total Loss: 0.09708284959197044\n",
      "------------------------------------ epoch 6468 (38802 steps) ------------------------------------\n",
      "Max loss: 0.017428863793611526\n",
      "Min loss: 0.007995767518877983\n",
      "Mean loss: 0.011823116801679134\n",
      "Std loss: 0.0028857235750708674\n",
      "Total Loss: 0.0709387008100748\n",
      "------------------------------------ epoch 6469 (38808 steps) ------------------------------------\n",
      "Max loss: 0.03913725167512894\n",
      "Min loss: 0.011489586904644966\n",
      "Mean loss: 0.018944260974725086\n",
      "Std loss: 0.009501678724923607\n",
      "Total Loss: 0.11366556584835052\n",
      "------------------------------------ epoch 6470 (38814 steps) ------------------------------------\n",
      "Max loss: 0.020654380321502686\n",
      "Min loss: 0.005253764800727367\n",
      "Mean loss: 0.009352800125877062\n",
      "Std loss: 0.005277584305125227\n",
      "Total Loss: 0.056116800755262375\n",
      "------------------------------------ epoch 6471 (38820 steps) ------------------------------------\n",
      "Max loss: 0.024869244545698166\n",
      "Min loss: 0.0058386134915053844\n",
      "Mean loss: 0.010552263430630168\n",
      "Std loss: 0.00662143701197613\n",
      "Total Loss: 0.063313580583781\n",
      "------------------------------------ epoch 6472 (38826 steps) ------------------------------------\n",
      "Max loss: 0.04575914144515991\n",
      "Min loss: 0.0050690146163105965\n",
      "Mean loss: 0.016647703169534605\n",
      "Std loss: 0.014064874132354247\n",
      "Total Loss: 0.09988621901720762\n",
      "------------------------------------ epoch 6473 (38832 steps) ------------------------------------\n",
      "Max loss: 0.037027109414339066\n",
      "Min loss: 0.005194267723709345\n",
      "Mean loss: 0.013147758164753517\n",
      "Std loss: 0.011141218276336209\n",
      "Total Loss: 0.0788865489885211\n",
      "------------------------------------ epoch 6474 (38838 steps) ------------------------------------\n",
      "Max loss: 0.04653817042708397\n",
      "Min loss: 0.005900081247091293\n",
      "Mean loss: 0.015291856446613869\n",
      "Std loss: 0.014113736719976433\n",
      "Total Loss: 0.09175113867968321\n",
      "------------------------------------ epoch 6475 (38844 steps) ------------------------------------\n",
      "Max loss: 0.01901351474225521\n",
      "Min loss: 0.007426477037370205\n",
      "Mean loss: 0.012402041349560022\n",
      "Std loss: 0.00350024559170371\n",
      "Total Loss: 0.07441224809736013\n",
      "------------------------------------ epoch 6476 (38850 steps) ------------------------------------\n",
      "Max loss: 0.026298418641090393\n",
      "Min loss: 0.004928411915898323\n",
      "Mean loss: 0.010441665770485997\n",
      "Std loss: 0.007174676516786746\n",
      "Total Loss: 0.06264999462291598\n",
      "------------------------------------ epoch 6477 (38856 steps) ------------------------------------\n",
      "Max loss: 0.01862506940960884\n",
      "Min loss: 0.00583493709564209\n",
      "Mean loss: 0.011362132305900255\n",
      "Std loss: 0.004119864641922219\n",
      "Total Loss: 0.06817279383540154\n",
      "------------------------------------ epoch 6478 (38862 steps) ------------------------------------\n",
      "Max loss: 0.011608543805778027\n",
      "Min loss: 0.00576894311234355\n",
      "Mean loss: 0.007746260923643907\n",
      "Std loss: 0.00192372107600789\n",
      "Total Loss: 0.04647756554186344\n",
      "------------------------------------ epoch 6479 (38868 steps) ------------------------------------\n",
      "Max loss: 0.015733487904071808\n",
      "Min loss: 0.004784103948622942\n",
      "Mean loss: 0.008976001137246689\n",
      "Std loss: 0.00382349203133683\n",
      "Total Loss: 0.05385600682348013\n",
      "------------------------------------ epoch 6480 (38874 steps) ------------------------------------\n",
      "Max loss: 0.02325628325343132\n",
      "Min loss: 0.005255033727735281\n",
      "Mean loss: 0.010630368255078793\n",
      "Std loss: 0.00675808544820545\n",
      "Total Loss: 0.06378220953047276\n",
      "------------------------------------ epoch 6481 (38880 steps) ------------------------------------\n",
      "Max loss: 0.022735431790351868\n",
      "Min loss: 0.0043930793181061745\n",
      "Mean loss: 0.010482316603884101\n",
      "Std loss: 0.006048003103174804\n",
      "Total Loss: 0.0628938996233046\n",
      "------------------------------------ epoch 6482 (38886 steps) ------------------------------------\n",
      "Max loss: 0.01643187366425991\n",
      "Min loss: 0.004865456372499466\n",
      "Mean loss: 0.00857635160597662\n",
      "Std loss: 0.004167794925081512\n",
      "Total Loss: 0.05145810963585973\n",
      "------------------------------------ epoch 6483 (38892 steps) ------------------------------------\n",
      "Max loss: 0.009057006798684597\n",
      "Min loss: 0.005499682854861021\n",
      "Mean loss: 0.007987169781699777\n",
      "Std loss: 0.0012095865160171371\n",
      "Total Loss: 0.04792301869019866\n",
      "------------------------------------ epoch 6484 (38898 steps) ------------------------------------\n",
      "Max loss: 0.044995635747909546\n",
      "Min loss: 0.004363571759313345\n",
      "Mean loss: 0.016226744279265404\n",
      "Std loss: 0.014878310889244411\n",
      "Total Loss: 0.09736046567559242\n",
      "------------------------------------ epoch 6485 (38904 steps) ------------------------------------\n",
      "Max loss: 0.0190022774040699\n",
      "Min loss: 0.004531480837613344\n",
      "Mean loss: 0.008930331406493982\n",
      "Std loss: 0.004896742125319457\n",
      "Total Loss: 0.05358198843896389\n",
      "------------------------------------ epoch 6486 (38910 steps) ------------------------------------\n",
      "Max loss: 0.02501039020717144\n",
      "Min loss: 0.006019634660333395\n",
      "Mean loss: 0.012714841170236468\n",
      "Std loss: 0.006274630907827211\n",
      "Total Loss: 0.07628904702141881\n",
      "------------------------------------ epoch 6487 (38916 steps) ------------------------------------\n",
      "Max loss: 0.020216945558786392\n",
      "Min loss: 0.004699865356087685\n",
      "Mean loss: 0.009672681412970027\n",
      "Std loss: 0.005024651265621933\n",
      "Total Loss: 0.05803608847782016\n",
      "------------------------------------ epoch 6488 (38922 steps) ------------------------------------\n",
      "Max loss: 0.011092312633991241\n",
      "Min loss: 0.0051142629235982895\n",
      "Mean loss: 0.008424297130356232\n",
      "Std loss: 0.002051055541031357\n",
      "Total Loss: 0.050545782782137394\n",
      "------------------------------------ epoch 6489 (38928 steps) ------------------------------------\n",
      "Max loss: 0.02621757797896862\n",
      "Min loss: 0.006341863423585892\n",
      "Mean loss: 0.01075058119992415\n",
      "Std loss: 0.0070105385230485396\n",
      "Total Loss: 0.0645034871995449\n",
      "------------------------------------ epoch 6490 (38934 steps) ------------------------------------\n",
      "Max loss: 0.014817334711551666\n",
      "Min loss: 0.004724716302007437\n",
      "Mean loss: 0.010611663572490215\n",
      "Std loss: 0.0036611187731652793\n",
      "Total Loss: 0.06366998143494129\n",
      "------------------------------------ epoch 6491 (38940 steps) ------------------------------------\n",
      "Max loss: 0.010787198320031166\n",
      "Min loss: 0.005203426815569401\n",
      "Mean loss: 0.006903539877384901\n",
      "Std loss: 0.001859508246442896\n",
      "Total Loss: 0.041421239264309406\n",
      "------------------------------------ epoch 6492 (38946 steps) ------------------------------------\n",
      "Max loss: 0.012128513306379318\n",
      "Min loss: 0.00465283403173089\n",
      "Mean loss: 0.007615851859251658\n",
      "Std loss: 0.0023569891724163103\n",
      "Total Loss: 0.04569511115550995\n",
      "------------------------------------ epoch 6493 (38952 steps) ------------------------------------\n",
      "Max loss: 0.02110046148300171\n",
      "Min loss: 0.006334717385470867\n",
      "Mean loss: 0.010605943234016499\n",
      "Std loss: 0.005125145571584816\n",
      "Total Loss: 0.06363565940409899\n",
      "------------------------------------ epoch 6494 (38958 steps) ------------------------------------\n",
      "Max loss: 0.012554059736430645\n",
      "Min loss: 0.005123713053762913\n",
      "Mean loss: 0.007350365398451686\n",
      "Std loss: 0.002749555775634943\n",
      "Total Loss: 0.044102192390710115\n",
      "------------------------------------ epoch 6495 (38964 steps) ------------------------------------\n",
      "Max loss: 0.02174416184425354\n",
      "Min loss: 0.004484412260353565\n",
      "Mean loss: 0.009172130996982256\n",
      "Std loss: 0.006137595612100024\n",
      "Total Loss: 0.05503278598189354\n",
      "------------------------------------ epoch 6496 (38970 steps) ------------------------------------\n",
      "Max loss: 0.014652365818619728\n",
      "Min loss: 0.005550541914999485\n",
      "Mean loss: 0.009070654166862369\n",
      "Std loss: 0.003186427583486466\n",
      "Total Loss: 0.05442392500117421\n",
      "------------------------------------ epoch 6497 (38976 steps) ------------------------------------\n",
      "Max loss: 0.01725553162395954\n",
      "Min loss: 0.007717716973274946\n",
      "Mean loss: 0.011430988010639945\n",
      "Std loss: 0.0033902967011631637\n",
      "Total Loss: 0.06858592806383967\n",
      "------------------------------------ epoch 6498 (38982 steps) ------------------------------------\n",
      "Max loss: 0.021131381392478943\n",
      "Min loss: 0.005034185945987701\n",
      "Mean loss: 0.01095567635881404\n",
      "Std loss: 0.005096631404556178\n",
      "Total Loss: 0.06573405815288424\n",
      "------------------------------------ epoch 6499 (38988 steps) ------------------------------------\n",
      "Max loss: 0.01338111236691475\n",
      "Min loss: 0.0053756325505673885\n",
      "Mean loss: 0.010546477433914939\n",
      "Std loss: 0.0024887497696375028\n",
      "Total Loss: 0.06327886460348964\n",
      "------------------------------------ epoch 6500 (38994 steps) ------------------------------------\n",
      "Max loss: 0.013410551473498344\n",
      "Min loss: 0.004763161763548851\n",
      "Mean loss: 0.0076496486241618795\n",
      "Std loss: 0.002958540166212765\n",
      "Total Loss: 0.045897891744971275\n",
      "------------------------------------ epoch 6501 (39000 steps) ------------------------------------\n",
      "Max loss: 0.011672231368720531\n",
      "Min loss: 0.003953893668949604\n",
      "Mean loss: 0.008110330440104008\n",
      "Std loss: 0.0025691087216666007\n",
      "Total Loss: 0.048661982640624046\n",
      "saved model at ./weights/model_6501.pth\n",
      "------------------------------------ epoch 6502 (39006 steps) ------------------------------------\n",
      "Max loss: 0.034255754202604294\n",
      "Min loss: 0.007078626658767462\n",
      "Mean loss: 0.01335992788275083\n",
      "Std loss: 0.009530862228277138\n",
      "Total Loss: 0.08015956729650497\n",
      "------------------------------------ epoch 6503 (39012 steps) ------------------------------------\n",
      "Max loss: 0.00970547180622816\n",
      "Min loss: 0.005274954717606306\n",
      "Mean loss: 0.007193573983386159\n",
      "Std loss: 0.0014518567981817553\n",
      "Total Loss: 0.043161443900316954\n",
      "------------------------------------ epoch 6504 (39018 steps) ------------------------------------\n",
      "Max loss: 0.011504310183227062\n",
      "Min loss: 0.00401199609041214\n",
      "Mean loss: 0.0075820442289114\n",
      "Std loss: 0.0022704110060223137\n",
      "Total Loss: 0.0454922653734684\n",
      "------------------------------------ epoch 6505 (39024 steps) ------------------------------------\n",
      "Max loss: 0.008982015773653984\n",
      "Min loss: 0.005560997873544693\n",
      "Mean loss: 0.007192813403283556\n",
      "Std loss: 0.0014550128627244922\n",
      "Total Loss: 0.04315688041970134\n",
      "------------------------------------ epoch 6506 (39030 steps) ------------------------------------\n",
      "Max loss: 0.03637351095676422\n",
      "Min loss: 0.004448296502232552\n",
      "Mean loss: 0.012125726323574781\n",
      "Std loss: 0.011066213863016816\n",
      "Total Loss: 0.07275435794144869\n",
      "------------------------------------ epoch 6507 (39036 steps) ------------------------------------\n",
      "Max loss: 0.013446865603327751\n",
      "Min loss: 0.004921189974993467\n",
      "Mean loss: 0.00938453859028717\n",
      "Std loss: 0.003270262686122976\n",
      "Total Loss: 0.05630723154172301\n",
      "------------------------------------ epoch 6508 (39042 steps) ------------------------------------\n",
      "Max loss: 0.014921705238521099\n",
      "Min loss: 0.006269888021051884\n",
      "Mean loss: 0.010018268755326668\n",
      "Std loss: 0.0028186662031841592\n",
      "Total Loss: 0.06010961253196001\n",
      "------------------------------------ epoch 6509 (39048 steps) ------------------------------------\n",
      "Max loss: 0.016768567264080048\n",
      "Min loss: 0.004758176393806934\n",
      "Mean loss: 0.01121506979689002\n",
      "Std loss: 0.003608923736431204\n",
      "Total Loss: 0.06729041878134012\n",
      "------------------------------------ epoch 6510 (39054 steps) ------------------------------------\n",
      "Max loss: 0.010424375534057617\n",
      "Min loss: 0.003972968086600304\n",
      "Mean loss: 0.006783779710531235\n",
      "Std loss: 0.0022943665639088627\n",
      "Total Loss: 0.04070267826318741\n",
      "------------------------------------ epoch 6511 (39060 steps) ------------------------------------\n",
      "Max loss: 0.016341976821422577\n",
      "Min loss: 0.0050772204995155334\n",
      "Mean loss: 0.009368567572285732\n",
      "Std loss: 0.0037763380203295737\n",
      "Total Loss: 0.05621140543371439\n",
      "------------------------------------ epoch 6512 (39066 steps) ------------------------------------\n",
      "Max loss: 0.025567293167114258\n",
      "Min loss: 0.005216510035097599\n",
      "Mean loss: 0.010832237623011073\n",
      "Std loss: 0.007052976610682264\n",
      "Total Loss: 0.06499342573806643\n",
      "------------------------------------ epoch 6513 (39072 steps) ------------------------------------\n",
      "Max loss: 0.025887710973620415\n",
      "Min loss: 0.005062061361968517\n",
      "Mean loss: 0.011191101977601647\n",
      "Std loss: 0.007189537202092314\n",
      "Total Loss: 0.06714661186560988\n",
      "------------------------------------ epoch 6514 (39078 steps) ------------------------------------\n",
      "Max loss: 0.012429758906364441\n",
      "Min loss: 0.006690387614071369\n",
      "Mean loss: 0.010214633774012327\n",
      "Std loss: 0.001865457655731708\n",
      "Total Loss: 0.06128780264407396\n",
      "------------------------------------ epoch 6515 (39084 steps) ------------------------------------\n",
      "Max loss: 0.009650188498198986\n",
      "Min loss: 0.0054875584319233894\n",
      "Mean loss: 0.0077234799197564525\n",
      "Std loss: 0.0016505625178110006\n",
      "Total Loss: 0.04634087951853871\n",
      "------------------------------------ epoch 6516 (39090 steps) ------------------------------------\n",
      "Max loss: 0.018943417817354202\n",
      "Min loss: 0.005687061697244644\n",
      "Mean loss: 0.011007087615629038\n",
      "Std loss: 0.004691440895303066\n",
      "Total Loss: 0.06604252569377422\n",
      "------------------------------------ epoch 6517 (39096 steps) ------------------------------------\n",
      "Max loss: 0.03524091839790344\n",
      "Min loss: 0.005767523311078548\n",
      "Mean loss: 0.012967192490274707\n",
      "Std loss: 0.010327983376484105\n",
      "Total Loss: 0.07780315494164824\n",
      "------------------------------------ epoch 6518 (39102 steps) ------------------------------------\n",
      "Max loss: 0.031852852553129196\n",
      "Min loss: 0.005412299185991287\n",
      "Mean loss: 0.012216430467863878\n",
      "Std loss: 0.0089388795507734\n",
      "Total Loss: 0.07329858280718327\n",
      "------------------------------------ epoch 6519 (39108 steps) ------------------------------------\n",
      "Max loss: 0.01924799010157585\n",
      "Min loss: 0.006464836653321981\n",
      "Mean loss: 0.013021195462594429\n",
      "Std loss: 0.005005314361161789\n",
      "Total Loss: 0.07812717277556658\n",
      "------------------------------------ epoch 6520 (39114 steps) ------------------------------------\n",
      "Max loss: 0.03718918189406395\n",
      "Min loss: 0.0058991555124521255\n",
      "Mean loss: 0.02067891089245677\n",
      "Std loss: 0.012340746514894146\n",
      "Total Loss: 0.12407346535474062\n",
      "------------------------------------ epoch 6521 (39120 steps) ------------------------------------\n",
      "Max loss: 0.024234702810645103\n",
      "Min loss: 0.005714531987905502\n",
      "Mean loss: 0.01303626069178184\n",
      "Std loss: 0.006025416115497736\n",
      "Total Loss: 0.07821756415069103\n",
      "------------------------------------ epoch 6522 (39126 steps) ------------------------------------\n",
      "Max loss: 0.035103730857372284\n",
      "Min loss: 0.00696297874674201\n",
      "Mean loss: 0.013239036003748575\n",
      "Std loss: 0.009965694600809413\n",
      "Total Loss: 0.07943421602249146\n",
      "------------------------------------ epoch 6523 (39132 steps) ------------------------------------\n",
      "Max loss: 0.014451283030211926\n",
      "Min loss: 0.006348594091832638\n",
      "Mean loss: 0.01070804800838232\n",
      "Std loss: 0.002682372471213459\n",
      "Total Loss: 0.06424828805029392\n",
      "------------------------------------ epoch 6524 (39138 steps) ------------------------------------\n",
      "Max loss: 0.014436169527471066\n",
      "Min loss: 0.005132068879902363\n",
      "Mean loss: 0.009102264884859324\n",
      "Std loss: 0.0030535287018124725\n",
      "Total Loss: 0.05461358930915594\n",
      "------------------------------------ epoch 6525 (39144 steps) ------------------------------------\n",
      "Max loss: 0.04264724254608154\n",
      "Min loss: 0.006165340542793274\n",
      "Mean loss: 0.013916748575866222\n",
      "Std loss: 0.012950961019822614\n",
      "Total Loss: 0.08350049145519733\n",
      "------------------------------------ epoch 6526 (39150 steps) ------------------------------------\n",
      "Max loss: 0.012586201541125774\n",
      "Min loss: 0.004603350535035133\n",
      "Mean loss: 0.007952722099920114\n",
      "Std loss: 0.002630477293105454\n",
      "Total Loss: 0.04771633259952068\n",
      "------------------------------------ epoch 6527 (39156 steps) ------------------------------------\n",
      "Max loss: 0.018565639853477478\n",
      "Min loss: 0.005578509531915188\n",
      "Mean loss: 0.011024327483028173\n",
      "Std loss: 0.005166439696139533\n",
      "Total Loss: 0.06614596489816904\n",
      "------------------------------------ epoch 6528 (39162 steps) ------------------------------------\n",
      "Max loss: 0.018846113234758377\n",
      "Min loss: 0.005481486674398184\n",
      "Mean loss: 0.008632448424274722\n",
      "Std loss: 0.004629639107214763\n",
      "Total Loss: 0.051794690545648336\n",
      "------------------------------------ epoch 6529 (39168 steps) ------------------------------------\n",
      "Max loss: 0.03809565305709839\n",
      "Min loss: 0.005638684146106243\n",
      "Mean loss: 0.012817505669469634\n",
      "Std loss: 0.011405608270073841\n",
      "Total Loss: 0.07690503401681781\n",
      "------------------------------------ epoch 6530 (39174 steps) ------------------------------------\n",
      "Max loss: 0.025252219289541245\n",
      "Min loss: 0.004549853969365358\n",
      "Mean loss: 0.011140154131377736\n",
      "Std loss: 0.00657441284479405\n",
      "Total Loss: 0.06684092478826642\n",
      "------------------------------------ epoch 6531 (39180 steps) ------------------------------------\n",
      "Max loss: 0.022753417491912842\n",
      "Min loss: 0.006364337168633938\n",
      "Mean loss: 0.010979276150465012\n",
      "Std loss: 0.005426446121293096\n",
      "Total Loss: 0.06587565690279007\n",
      "------------------------------------ epoch 6532 (39186 steps) ------------------------------------\n",
      "Max loss: 0.013611232861876488\n",
      "Min loss: 0.005165385082364082\n",
      "Mean loss: 0.008316177176311612\n",
      "Std loss: 0.0030179337858751787\n",
      "Total Loss: 0.04989706305786967\n",
      "------------------------------------ epoch 6533 (39192 steps) ------------------------------------\n",
      "Max loss: 0.010366838425397873\n",
      "Min loss: 0.004716841503977776\n",
      "Mean loss: 0.0069180200807750225\n",
      "Std loss: 0.0019312899288541727\n",
      "Total Loss: 0.041508120484650135\n",
      "------------------------------------ epoch 6534 (39198 steps) ------------------------------------\n",
      "Max loss: 0.029465703293681145\n",
      "Min loss: 0.005690179765224457\n",
      "Mean loss: 0.014349736738950014\n",
      "Std loss: 0.00955000850739906\n",
      "Total Loss: 0.08609842043370008\n",
      "------------------------------------ epoch 6535 (39204 steps) ------------------------------------\n",
      "Max loss: 0.019055750221014023\n",
      "Min loss: 0.004595967009663582\n",
      "Mean loss: 0.00998416442113618\n",
      "Std loss: 0.004964128874265612\n",
      "Total Loss: 0.05990498652681708\n",
      "------------------------------------ epoch 6536 (39210 steps) ------------------------------------\n",
      "Max loss: 0.0156949982047081\n",
      "Min loss: 0.005136307328939438\n",
      "Mean loss: 0.010124066844582558\n",
      "Std loss: 0.00434389543513091\n",
      "Total Loss: 0.060744401067495346\n",
      "------------------------------------ epoch 6537 (39216 steps) ------------------------------------\n",
      "Max loss: 0.012126468122005463\n",
      "Min loss: 0.004631289280951023\n",
      "Mean loss: 0.007771072909235954\n",
      "Std loss: 0.003073014250105679\n",
      "Total Loss: 0.046626437455415726\n",
      "------------------------------------ epoch 6538 (39222 steps) ------------------------------------\n",
      "Max loss: 0.01151879783719778\n",
      "Min loss: 0.005666846409440041\n",
      "Mean loss: 0.008390547552456459\n",
      "Std loss: 0.0018359550290319823\n",
      "Total Loss: 0.05034328531473875\n",
      "------------------------------------ epoch 6539 (39228 steps) ------------------------------------\n",
      "Max loss: 0.010665204375982285\n",
      "Min loss: 0.004299607127904892\n",
      "Mean loss: 0.0062057808196793\n",
      "Std loss: 0.0021646864717803035\n",
      "Total Loss: 0.0372346849180758\n",
      "------------------------------------ epoch 6540 (39234 steps) ------------------------------------\n",
      "Max loss: 0.03252529352903366\n",
      "Min loss: 0.004922332242131233\n",
      "Mean loss: 0.013899720429132381\n",
      "Std loss: 0.00892348798903276\n",
      "Total Loss: 0.08339832257479429\n",
      "------------------------------------ epoch 6541 (39240 steps) ------------------------------------\n",
      "Max loss: 0.027257900685071945\n",
      "Min loss: 0.004964934661984444\n",
      "Mean loss: 0.010464068269357085\n",
      "Std loss: 0.007915391154138616\n",
      "Total Loss: 0.06278440961614251\n",
      "------------------------------------ epoch 6542 (39246 steps) ------------------------------------\n",
      "Max loss: 0.01491622906178236\n",
      "Min loss: 0.004794138949364424\n",
      "Mean loss: 0.010011353918040792\n",
      "Std loss: 0.003266083769006852\n",
      "Total Loss: 0.06006812350824475\n",
      "------------------------------------ epoch 6543 (39252 steps) ------------------------------------\n",
      "Max loss: 0.00749311875551939\n",
      "Min loss: 0.005392908118665218\n",
      "Mean loss: 0.006387344483907024\n",
      "Std loss: 0.0007968916546681983\n",
      "Total Loss: 0.038324066903442144\n",
      "------------------------------------ epoch 6544 (39258 steps) ------------------------------------\n",
      "Max loss: 0.014939708635210991\n",
      "Min loss: 0.005470178555697203\n",
      "Mean loss: 0.009683747543022037\n",
      "Std loss: 0.003234835360514785\n",
      "Total Loss: 0.05810248525813222\n",
      "------------------------------------ epoch 6545 (39264 steps) ------------------------------------\n",
      "Max loss: 0.011483456939458847\n",
      "Min loss: 0.004404663108289242\n",
      "Mean loss: 0.007001966082801421\n",
      "Std loss: 0.0022201851699396925\n",
      "Total Loss: 0.04201179649680853\n",
      "------------------------------------ epoch 6546 (39270 steps) ------------------------------------\n",
      "Max loss: 0.02494186908006668\n",
      "Min loss: 0.006146376486867666\n",
      "Mean loss: 0.011765741975978017\n",
      "Std loss: 0.0063025800360956615\n",
      "Total Loss: 0.0705944518558681\n",
      "------------------------------------ epoch 6547 (39276 steps) ------------------------------------\n",
      "Max loss: 0.02534702979028225\n",
      "Min loss: 0.00551743246614933\n",
      "Mean loss: 0.012180470240612825\n",
      "Std loss: 0.006553137747071965\n",
      "Total Loss: 0.07308282144367695\n",
      "------------------------------------ epoch 6548 (39282 steps) ------------------------------------\n",
      "Max loss: 0.024110868573188782\n",
      "Min loss: 0.005989436991512775\n",
      "Mean loss: 0.012993398743371168\n",
      "Std loss: 0.006444008106493242\n",
      "Total Loss: 0.07796039246022701\n",
      "------------------------------------ epoch 6549 (39288 steps) ------------------------------------\n",
      "Max loss: 0.009584229439496994\n",
      "Min loss: 0.0044623627327382565\n",
      "Mean loss: 0.006442626239731908\n",
      "Std loss: 0.0018469832545385402\n",
      "Total Loss: 0.03865575743839145\n",
      "------------------------------------ epoch 6550 (39294 steps) ------------------------------------\n",
      "Max loss: 0.01155926939100027\n",
      "Min loss: 0.0053071207366883755\n",
      "Mean loss: 0.008720879132548967\n",
      "Std loss: 0.002251322727113685\n",
      "Total Loss: 0.05232527479529381\n",
      "------------------------------------ epoch 6551 (39300 steps) ------------------------------------\n",
      "Max loss: 0.011455558240413666\n",
      "Min loss: 0.007012392394244671\n",
      "Mean loss: 0.009021647119273743\n",
      "Std loss: 0.0015881221862526693\n",
      "Total Loss: 0.05412988271564245\n",
      "------------------------------------ epoch 6552 (39306 steps) ------------------------------------\n",
      "Max loss: 0.026731237769126892\n",
      "Min loss: 0.004797921981662512\n",
      "Mean loss: 0.01712920788365106\n",
      "Std loss: 0.0064713233669238856\n",
      "Total Loss: 0.10277524730190635\n",
      "------------------------------------ epoch 6553 (39312 steps) ------------------------------------\n",
      "Max loss: 0.02063881605863571\n",
      "Min loss: 0.005112483166158199\n",
      "Mean loss: 0.009158975832785169\n",
      "Std loss: 0.0053018575643324455\n",
      "Total Loss: 0.054953854996711016\n",
      "------------------------------------ epoch 6554 (39318 steps) ------------------------------------\n",
      "Max loss: 0.01699236035346985\n",
      "Min loss: 0.006478605326265097\n",
      "Mean loss: 0.010604467010125518\n",
      "Std loss: 0.0039630114264316155\n",
      "Total Loss: 0.06362680206075311\n",
      "------------------------------------ epoch 6555 (39324 steps) ------------------------------------\n",
      "Max loss: 0.01675717905163765\n",
      "Min loss: 0.005949209909886122\n",
      "Mean loss: 0.01199772108035783\n",
      "Std loss: 0.0037639588535392744\n",
      "Total Loss: 0.07198632648214698\n",
      "------------------------------------ epoch 6556 (39330 steps) ------------------------------------\n",
      "Max loss: 0.010102897882461548\n",
      "Min loss: 0.004812663421034813\n",
      "Mean loss: 0.006483017932623625\n",
      "Std loss: 0.001831994331050448\n",
      "Total Loss: 0.03889810759574175\n",
      "------------------------------------ epoch 6557 (39336 steps) ------------------------------------\n",
      "Max loss: 0.019444596022367477\n",
      "Min loss: 0.00543186953291297\n",
      "Mean loss: 0.010943610376367966\n",
      "Std loss: 0.004924951465187669\n",
      "Total Loss: 0.0656616622582078\n",
      "------------------------------------ epoch 6558 (39342 steps) ------------------------------------\n",
      "Max loss: 0.0448051318526268\n",
      "Min loss: 0.004401796497404575\n",
      "Mean loss: 0.013090567275260886\n",
      "Std loss: 0.01430173808727594\n",
      "Total Loss: 0.07854340365156531\n",
      "------------------------------------ epoch 6559 (39348 steps) ------------------------------------\n",
      "Max loss: 0.05377202481031418\n",
      "Min loss: 0.006575280334800482\n",
      "Mean loss: 0.023844499529028933\n",
      "Std loss: 0.017860502844435053\n",
      "Total Loss: 0.1430669971741736\n",
      "------------------------------------ epoch 6560 (39354 steps) ------------------------------------\n",
      "Max loss: 0.019083773717284203\n",
      "Min loss: 0.006886833813041449\n",
      "Mean loss: 0.012291912377501527\n",
      "Std loss: 0.0038079529343556886\n",
      "Total Loss: 0.07375147426500916\n",
      "------------------------------------ epoch 6561 (39360 steps) ------------------------------------\n",
      "Max loss: 0.03945023566484451\n",
      "Min loss: 0.008343441411852837\n",
      "Mean loss: 0.022221469941238563\n",
      "Std loss: 0.011784597659639217\n",
      "Total Loss: 0.13332881964743137\n",
      "------------------------------------ epoch 6562 (39366 steps) ------------------------------------\n",
      "Max loss: 0.01854642480611801\n",
      "Min loss: 0.006553961895406246\n",
      "Mean loss: 0.011105038536091646\n",
      "Std loss: 0.004388812795512576\n",
      "Total Loss: 0.06663023121654987\n",
      "------------------------------------ epoch 6563 (39372 steps) ------------------------------------\n",
      "Max loss: 0.03730703517794609\n",
      "Min loss: 0.007021021097898483\n",
      "Mean loss: 0.0156996149259309\n",
      "Std loss: 0.010423205057165338\n",
      "Total Loss: 0.09419768955558538\n",
      "------------------------------------ epoch 6564 (39378 steps) ------------------------------------\n",
      "Max loss: 0.013970300555229187\n",
      "Min loss: 0.00619500270113349\n",
      "Mean loss: 0.008709959918633103\n",
      "Std loss: 0.002802126577017006\n",
      "Total Loss: 0.05225975951179862\n",
      "------------------------------------ epoch 6565 (39384 steps) ------------------------------------\n",
      "Max loss: 0.029914801940321922\n",
      "Min loss: 0.005765377543866634\n",
      "Mean loss: 0.011531584740926823\n",
      "Std loss: 0.00839931738150231\n",
      "Total Loss: 0.06918950844556093\n",
      "------------------------------------ epoch 6566 (39390 steps) ------------------------------------\n",
      "Max loss: 0.05434395372867584\n",
      "Min loss: 0.005498831160366535\n",
      "Mean loss: 0.016871604680394132\n",
      "Std loss: 0.017158219320238078\n",
      "Total Loss: 0.1012296280823648\n",
      "------------------------------------ epoch 6567 (39396 steps) ------------------------------------\n",
      "Max loss: 0.012585021555423737\n",
      "Min loss: 0.004815701395273209\n",
      "Mean loss: 0.007908407521123687\n",
      "Std loss: 0.0023507306044776755\n",
      "Total Loss: 0.047450445126742125\n",
      "------------------------------------ epoch 6568 (39402 steps) ------------------------------------\n",
      "Max loss: 0.013247838243842125\n",
      "Min loss: 0.0049719903618097305\n",
      "Mean loss: 0.009747543449824056\n",
      "Std loss: 0.003465607560343668\n",
      "Total Loss: 0.05848526069894433\n",
      "------------------------------------ epoch 6569 (39408 steps) ------------------------------------\n",
      "Max loss: 0.023610033094882965\n",
      "Min loss: 0.004797372501343489\n",
      "Mean loss: 0.009821420535445213\n",
      "Std loss: 0.0063195713575481096\n",
      "Total Loss: 0.05892852321267128\n",
      "------------------------------------ epoch 6570 (39414 steps) ------------------------------------\n",
      "Max loss: 0.039165325462818146\n",
      "Min loss: 0.010081376880407333\n",
      "Mean loss: 0.016512191233535606\n",
      "Std loss: 0.010247255410974217\n",
      "Total Loss: 0.09907314740121365\n",
      "------------------------------------ epoch 6571 (39420 steps) ------------------------------------\n",
      "Max loss: 0.017519252374768257\n",
      "Min loss: 0.005956633947789669\n",
      "Mean loss: 0.00997225862617294\n",
      "Std loss: 0.004155957028664091\n",
      "Total Loss: 0.05983355175703764\n",
      "------------------------------------ epoch 6572 (39426 steps) ------------------------------------\n",
      "Max loss: 0.027884937822818756\n",
      "Min loss: 0.008110035210847855\n",
      "Mean loss: 0.014485725512107214\n",
      "Std loss: 0.0069080130247543906\n",
      "Total Loss: 0.08691435307264328\n",
      "------------------------------------ epoch 6573 (39432 steps) ------------------------------------\n",
      "Max loss: 0.046132005751132965\n",
      "Min loss: 0.006466264836490154\n",
      "Mean loss: 0.017242291166136663\n",
      "Std loss: 0.014300860445240876\n",
      "Total Loss: 0.10345374699681997\n",
      "------------------------------------ epoch 6574 (39438 steps) ------------------------------------\n",
      "Max loss: 0.01787411794066429\n",
      "Min loss: 0.005063338205218315\n",
      "Mean loss: 0.010343518340960145\n",
      "Std loss: 0.00458490838538894\n",
      "Total Loss: 0.06206111004576087\n",
      "------------------------------------ epoch 6575 (39444 steps) ------------------------------------\n",
      "Max loss: 0.024436764419078827\n",
      "Min loss: 0.005931161344051361\n",
      "Mean loss: 0.011459043327098092\n",
      "Std loss: 0.007092512524550724\n",
      "Total Loss: 0.06875425996258855\n",
      "------------------------------------ epoch 6576 (39450 steps) ------------------------------------\n",
      "Max loss: 0.018424153327941895\n",
      "Min loss: 0.006583273410797119\n",
      "Mean loss: 0.01028762369727095\n",
      "Std loss: 0.0038768649314462433\n",
      "Total Loss: 0.0617257421836257\n",
      "------------------------------------ epoch 6577 (39456 steps) ------------------------------------\n",
      "Max loss: 0.008596614934504032\n",
      "Min loss: 0.005258041433990002\n",
      "Mean loss: 0.006804844519744317\n",
      "Std loss: 0.0013139688687232227\n",
      "Total Loss: 0.0408290671184659\n",
      "------------------------------------ epoch 6578 (39462 steps) ------------------------------------\n",
      "Max loss: 0.03456982225179672\n",
      "Min loss: 0.004676317796111107\n",
      "Mean loss: 0.014301523566246033\n",
      "Std loss: 0.010390609129452508\n",
      "Total Loss: 0.0858091413974762\n",
      "------------------------------------ epoch 6579 (39468 steps) ------------------------------------\n",
      "Max loss: 0.033500030636787415\n",
      "Min loss: 0.005751488264650106\n",
      "Mean loss: 0.012356951367110014\n",
      "Std loss: 0.009539145849184529\n",
      "Total Loss: 0.07414170820266008\n",
      "------------------------------------ epoch 6580 (39474 steps) ------------------------------------\n",
      "Max loss: 0.02404237911105156\n",
      "Min loss: 0.004929825197905302\n",
      "Mean loss: 0.01235324889421463\n",
      "Std loss: 0.007408815351101467\n",
      "Total Loss: 0.07411949336528778\n",
      "------------------------------------ epoch 6581 (39480 steps) ------------------------------------\n",
      "Max loss: 0.018358634784817696\n",
      "Min loss: 0.008151770569384098\n",
      "Mean loss: 0.011632077551136414\n",
      "Std loss: 0.003270311958365685\n",
      "Total Loss: 0.06979246530681849\n",
      "------------------------------------ epoch 6582 (39486 steps) ------------------------------------\n",
      "Max loss: 0.010817289352416992\n",
      "Min loss: 0.006324729882180691\n",
      "Mean loss: 0.008799065680553516\n",
      "Std loss: 0.0017551084094338588\n",
      "Total Loss: 0.052794394083321095\n",
      "------------------------------------ epoch 6583 (39492 steps) ------------------------------------\n",
      "Max loss: 0.026294752955436707\n",
      "Min loss: 0.004842572379857302\n",
      "Mean loss: 0.011383949390922984\n",
      "Std loss: 0.00693477025343976\n",
      "Total Loss: 0.0683036963455379\n",
      "------------------------------------ epoch 6584 (39498 steps) ------------------------------------\n",
      "Max loss: 0.026549089699983597\n",
      "Min loss: 0.0049184756353497505\n",
      "Mean loss: 0.010485273165007433\n",
      "Std loss: 0.0072959642766595115\n",
      "Total Loss: 0.0629116389900446\n",
      "------------------------------------ epoch 6585 (39504 steps) ------------------------------------\n",
      "Max loss: 0.01834890805184841\n",
      "Min loss: 0.005159440450370312\n",
      "Mean loss: 0.011223049911980828\n",
      "Std loss: 0.005242779289149854\n",
      "Total Loss: 0.06733829947188497\n",
      "------------------------------------ epoch 6586 (39510 steps) ------------------------------------\n",
      "Max loss: 0.010413967072963715\n",
      "Min loss: 0.006626843474805355\n",
      "Mean loss: 0.00892432468632857\n",
      "Std loss: 0.0015311683681279008\n",
      "Total Loss: 0.05354594811797142\n",
      "------------------------------------ epoch 6587 (39516 steps) ------------------------------------\n",
      "Max loss: 0.014058694243431091\n",
      "Min loss: 0.004788548685610294\n",
      "Mean loss: 0.007760726458703478\n",
      "Std loss: 0.0032459001491099105\n",
      "Total Loss: 0.04656435875222087\n",
      "------------------------------------ epoch 6588 (39522 steps) ------------------------------------\n",
      "Max loss: 0.011179087683558464\n",
      "Min loss: 0.005175409372895956\n",
      "Mean loss: 0.006738209553683798\n",
      "Std loss: 0.0021097336988384506\n",
      "Total Loss: 0.040429257322102785\n",
      "------------------------------------ epoch 6589 (39528 steps) ------------------------------------\n",
      "Max loss: 0.010355021804571152\n",
      "Min loss: 0.0037413674872368574\n",
      "Mean loss: 0.0069679118460044265\n",
      "Std loss: 0.0026645981240511884\n",
      "Total Loss: 0.04180747107602656\n",
      "------------------------------------ epoch 6590 (39534 steps) ------------------------------------\n",
      "Max loss: 0.013989081606268883\n",
      "Min loss: 0.003759165992960334\n",
      "Mean loss: 0.0075848875955368085\n",
      "Std loss: 0.003318018846918618\n",
      "Total Loss: 0.04550932557322085\n",
      "------------------------------------ epoch 6591 (39540 steps) ------------------------------------\n",
      "Max loss: 0.015685560181736946\n",
      "Min loss: 0.00471763638779521\n",
      "Mean loss: 0.010098886288081607\n",
      "Std loss: 0.003869665084418909\n",
      "Total Loss: 0.06059331772848964\n",
      "------------------------------------ epoch 6592 (39546 steps) ------------------------------------\n",
      "Max loss: 0.04036862403154373\n",
      "Min loss: 0.003915809094905853\n",
      "Mean loss: 0.014611021615564823\n",
      "Std loss: 0.01184635102070905\n",
      "Total Loss: 0.08766612969338894\n",
      "------------------------------------ epoch 6593 (39552 steps) ------------------------------------\n",
      "Max loss: 0.016521669924259186\n",
      "Min loss: 0.007445621304214001\n",
      "Mean loss: 0.011103314192344746\n",
      "Std loss: 0.003602595915446795\n",
      "Total Loss: 0.06661988515406847\n",
      "------------------------------------ epoch 6594 (39558 steps) ------------------------------------\n",
      "Max loss: 0.04729443043470383\n",
      "Min loss: 0.005852826870977879\n",
      "Mean loss: 0.017190588017304737\n",
      "Std loss: 0.014024634849583044\n",
      "Total Loss: 0.10314352810382843\n",
      "------------------------------------ epoch 6595 (39564 steps) ------------------------------------\n",
      "Max loss: 0.01790398173034191\n",
      "Min loss: 0.006662802305072546\n",
      "Mean loss: 0.01093753093543152\n",
      "Std loss: 0.003958146160049529\n",
      "Total Loss: 0.06562518561258912\n",
      "------------------------------------ epoch 6596 (39570 steps) ------------------------------------\n",
      "Max loss: 0.02698352187871933\n",
      "Min loss: 0.004902927204966545\n",
      "Mean loss: 0.011831619155903658\n",
      "Std loss: 0.0074870396194707265\n",
      "Total Loss: 0.07098971493542194\n",
      "------------------------------------ epoch 6597 (39576 steps) ------------------------------------\n",
      "Max loss: 0.01256628893315792\n",
      "Min loss: 0.0061952294781804085\n",
      "Mean loss: 0.009396857426812252\n",
      "Std loss: 0.0023913810699797873\n",
      "Total Loss: 0.05638114456087351\n",
      "------------------------------------ epoch 6598 (39582 steps) ------------------------------------\n",
      "Max loss: 0.014655401930212975\n",
      "Min loss: 0.005992613732814789\n",
      "Mean loss: 0.009963737179835638\n",
      "Std loss: 0.003155594844651639\n",
      "Total Loss: 0.059782423079013824\n",
      "------------------------------------ epoch 6599 (39588 steps) ------------------------------------\n",
      "Max loss: 0.009869350120425224\n",
      "Min loss: 0.005518503487110138\n",
      "Mean loss: 0.007550207121918599\n",
      "Std loss: 0.0017895975999496303\n",
      "Total Loss: 0.04530124273151159\n",
      "------------------------------------ epoch 6600 (39594 steps) ------------------------------------\n",
      "Max loss: 0.029811162501573563\n",
      "Min loss: 0.006879919674247503\n",
      "Mean loss: 0.012480242100233832\n",
      "Std loss: 0.008073049635461939\n",
      "Total Loss: 0.074881452601403\n",
      "------------------------------------ epoch 6601 (39600 steps) ------------------------------------\n",
      "Max loss: 0.016552839428186417\n",
      "Min loss: 0.004447204992175102\n",
      "Mean loss: 0.009683929694195589\n",
      "Std loss: 0.0043218320669068215\n",
      "Total Loss: 0.05810357816517353\n",
      "saved model at ./weights/model_6601.pth\n",
      "------------------------------------ epoch 6602 (39606 steps) ------------------------------------\n",
      "Max loss: 0.016110647469758987\n",
      "Min loss: 0.005285091232508421\n",
      "Mean loss: 0.009755501911665002\n",
      "Std loss: 0.004508569102329634\n",
      "Total Loss: 0.058533011469990015\n",
      "------------------------------------ epoch 6603 (39612 steps) ------------------------------------\n",
      "Max loss: 0.011478989385068417\n",
      "Min loss: 0.005533984862267971\n",
      "Mean loss: 0.00835488123508791\n",
      "Std loss: 0.0022524938283279327\n",
      "Total Loss: 0.05012928741052747\n",
      "------------------------------------ epoch 6604 (39618 steps) ------------------------------------\n",
      "Max loss: 0.011109678074717522\n",
      "Min loss: 0.00421240646392107\n",
      "Mean loss: 0.007341845581928889\n",
      "Std loss: 0.00211963250347798\n",
      "Total Loss: 0.044051073491573334\n",
      "------------------------------------ epoch 6605 (39624 steps) ------------------------------------\n",
      "Max loss: 0.01636754348874092\n",
      "Min loss: 0.004431538749486208\n",
      "Mean loss: 0.009134259230146805\n",
      "Std loss: 0.004048671690563696\n",
      "Total Loss: 0.05480555538088083\n",
      "------------------------------------ epoch 6606 (39630 steps) ------------------------------------\n",
      "Max loss: 0.022780891507864\n",
      "Min loss: 0.005256671458482742\n",
      "Mean loss: 0.011229950934648514\n",
      "Std loss: 0.006354927138168236\n",
      "Total Loss: 0.06737970560789108\n",
      "------------------------------------ epoch 6607 (39636 steps) ------------------------------------\n",
      "Max loss: 0.02312013879418373\n",
      "Min loss: 0.006444997154176235\n",
      "Mean loss: 0.015504091667632261\n",
      "Std loss: 0.005083627727610084\n",
      "Total Loss: 0.09302455000579357\n",
      "------------------------------------ epoch 6608 (39642 steps) ------------------------------------\n",
      "Max loss: 0.02671852521598339\n",
      "Min loss: 0.006138999946415424\n",
      "Mean loss: 0.01144272182136774\n",
      "Std loss: 0.007041014420709476\n",
      "Total Loss: 0.06865633092820644\n",
      "------------------------------------ epoch 6609 (39648 steps) ------------------------------------\n",
      "Max loss: 0.05916551500558853\n",
      "Min loss: 0.005637315567582846\n",
      "Mean loss: 0.0210954446811229\n",
      "Std loss: 0.018070468907183845\n",
      "Total Loss: 0.1265726680867374\n",
      "------------------------------------ epoch 6610 (39654 steps) ------------------------------------\n",
      "Max loss: 0.02058752439916134\n",
      "Min loss: 0.007829686626791954\n",
      "Mean loss: 0.013976609179129204\n",
      "Std loss: 0.004552929005460774\n",
      "Total Loss: 0.08385965507477522\n",
      "------------------------------------ epoch 6611 (39660 steps) ------------------------------------\n",
      "Max loss: 0.04470746964216232\n",
      "Min loss: 0.007244751788675785\n",
      "Mean loss: 0.017855057027190924\n",
      "Std loss: 0.013239993863421555\n",
      "Total Loss: 0.10713034216314554\n",
      "------------------------------------ epoch 6612 (39666 steps) ------------------------------------\n",
      "Max loss: 0.0188295841217041\n",
      "Min loss: 0.006122184917330742\n",
      "Mean loss: 0.010219838004559278\n",
      "Std loss: 0.004168146785211604\n",
      "Total Loss: 0.06131902802735567\n",
      "------------------------------------ epoch 6613 (39672 steps) ------------------------------------\n",
      "Max loss: 0.019922275096178055\n",
      "Min loss: 0.006356166675686836\n",
      "Mean loss: 0.014145897002890706\n",
      "Std loss: 0.0056275086240964245\n",
      "Total Loss: 0.08487538201734424\n",
      "------------------------------------ epoch 6614 (39678 steps) ------------------------------------\n",
      "Max loss: 0.025170115754008293\n",
      "Min loss: 0.008092495612800121\n",
      "Mean loss: 0.013808762499441704\n",
      "Std loss: 0.0057695834728300425\n",
      "Total Loss: 0.08285257499665022\n",
      "------------------------------------ epoch 6615 (39684 steps) ------------------------------------\n",
      "Max loss: 0.01853330247104168\n",
      "Min loss: 0.005952256731688976\n",
      "Mean loss: 0.010370590879271427\n",
      "Std loss: 0.003998686656523958\n",
      "Total Loss: 0.06222354527562857\n",
      "------------------------------------ epoch 6616 (39690 steps) ------------------------------------\n",
      "Max loss: 0.01635928265750408\n",
      "Min loss: 0.005908586084842682\n",
      "Mean loss: 0.008120484417304397\n",
      "Std loss: 0.003728368206814329\n",
      "Total Loss: 0.04872290650382638\n",
      "------------------------------------ epoch 6617 (39696 steps) ------------------------------------\n",
      "Max loss: 0.011178402230143547\n",
      "Min loss: 0.006640019826591015\n",
      "Mean loss: 0.008652942255139351\n",
      "Std loss: 0.0014660700980463133\n",
      "Total Loss: 0.051917653530836105\n",
      "------------------------------------ epoch 6618 (39702 steps) ------------------------------------\n",
      "Max loss: 0.029861759394407272\n",
      "Min loss: 0.005617555230855942\n",
      "Mean loss: 0.01193156853939096\n",
      "Std loss: 0.00813105368507902\n",
      "Total Loss: 0.07158941123634577\n",
      "------------------------------------ epoch 6619 (39708 steps) ------------------------------------\n",
      "Max loss: 0.014048482291400433\n",
      "Min loss: 0.008039494976401329\n",
      "Mean loss: 0.01034800661727786\n",
      "Std loss: 0.002228534992951236\n",
      "Total Loss: 0.062088039703667164\n",
      "------------------------------------ epoch 6620 (39714 steps) ------------------------------------\n",
      "Max loss: 0.016507653519511223\n",
      "Min loss: 0.0050600431859493256\n",
      "Mean loss: 0.009422562783583999\n",
      "Std loss: 0.004554656729066343\n",
      "Total Loss: 0.05653537670150399\n",
      "------------------------------------ epoch 6621 (39720 steps) ------------------------------------\n",
      "Max loss: 0.011494239792227745\n",
      "Min loss: 0.005568879656493664\n",
      "Mean loss: 0.008056906207154194\n",
      "Std loss: 0.00231131672483933\n",
      "Total Loss: 0.04834143724292517\n",
      "------------------------------------ epoch 6622 (39726 steps) ------------------------------------\n",
      "Max loss: 0.010523838922381401\n",
      "Min loss: 0.004649709910154343\n",
      "Mean loss: 0.007378757077579697\n",
      "Std loss: 0.0018582023926787606\n",
      "Total Loss: 0.04427254246547818\n",
      "------------------------------------ epoch 6623 (39732 steps) ------------------------------------\n",
      "Max loss: 0.013626906089484692\n",
      "Min loss: 0.00447139423340559\n",
      "Mean loss: 0.00790819269604981\n",
      "Std loss: 0.003549217704011778\n",
      "Total Loss: 0.04744915617629886\n",
      "------------------------------------ epoch 6624 (39738 steps) ------------------------------------\n",
      "Max loss: 0.014323330484330654\n",
      "Min loss: 0.005096292123198509\n",
      "Mean loss: 0.008047081995755434\n",
      "Std loss: 0.0029228458976833993\n",
      "Total Loss: 0.048282491974532604\n",
      "------------------------------------ epoch 6625 (39744 steps) ------------------------------------\n",
      "Max loss: 0.03640063479542732\n",
      "Min loss: 0.004755862057209015\n",
      "Mean loss: 0.011628951178863645\n",
      "Std loss: 0.011256531835617912\n",
      "Total Loss: 0.06977370707318187\n",
      "------------------------------------ epoch 6626 (39750 steps) ------------------------------------\n",
      "Max loss: 0.024395491927862167\n",
      "Min loss: 0.006617601029574871\n",
      "Mean loss: 0.011325254182641705\n",
      "Std loss: 0.0060456997982421615\n",
      "Total Loss: 0.06795152509585023\n",
      "------------------------------------ epoch 6627 (39756 steps) ------------------------------------\n",
      "Max loss: 0.037842653691768646\n",
      "Min loss: 0.00687616690993309\n",
      "Mean loss: 0.015161197166889906\n",
      "Std loss: 0.010474811895889058\n",
      "Total Loss: 0.09096718300133944\n",
      "------------------------------------ epoch 6628 (39762 steps) ------------------------------------\n",
      "Max loss: 0.023246347904205322\n",
      "Min loss: 0.007368780672550201\n",
      "Mean loss: 0.013403901364654303\n",
      "Std loss: 0.006639889767943019\n",
      "Total Loss: 0.08042340818792582\n",
      "------------------------------------ epoch 6629 (39768 steps) ------------------------------------\n",
      "Max loss: 0.01420505903661251\n",
      "Min loss: 0.00589776923879981\n",
      "Mean loss: 0.008975555654615164\n",
      "Std loss: 0.0025624826917868115\n",
      "Total Loss: 0.05385333392769098\n",
      "------------------------------------ epoch 6630 (39774 steps) ------------------------------------\n",
      "Max loss: 0.03651130944490433\n",
      "Min loss: 0.004280941095203161\n",
      "Mean loss: 0.015199347864836454\n",
      "Std loss: 0.012204034080480378\n",
      "Total Loss: 0.09119608718901873\n",
      "------------------------------------ epoch 6631 (39780 steps) ------------------------------------\n",
      "Max loss: 0.012820249423384666\n",
      "Min loss: 0.0038752572145313025\n",
      "Mean loss: 0.008352083425658444\n",
      "Std loss: 0.0032412787774188244\n",
      "Total Loss: 0.05011250055395067\n",
      "------------------------------------ epoch 6632 (39786 steps) ------------------------------------\n",
      "Max loss: 0.016412537544965744\n",
      "Min loss: 0.004076455719769001\n",
      "Mean loss: 0.009530271481101712\n",
      "Std loss: 0.004041253223279932\n",
      "Total Loss: 0.05718162888661027\n",
      "------------------------------------ epoch 6633 (39792 steps) ------------------------------------\n",
      "Max loss: 0.010820033960044384\n",
      "Min loss: 0.006538850720971823\n",
      "Mean loss: 0.008720676569888989\n",
      "Std loss: 0.0017482902154501822\n",
      "Total Loss: 0.052324059419333935\n",
      "------------------------------------ epoch 6634 (39798 steps) ------------------------------------\n",
      "Max loss: 0.009497720748186111\n",
      "Min loss: 0.004302266053855419\n",
      "Mean loss: 0.006208861789976557\n",
      "Std loss: 0.0022345757383214785\n",
      "Total Loss: 0.03725317073985934\n",
      "------------------------------------ epoch 6635 (39804 steps) ------------------------------------\n",
      "Max loss: 0.022757012397050858\n",
      "Min loss: 0.006905323825776577\n",
      "Mean loss: 0.013403198138500253\n",
      "Std loss: 0.006523620162462692\n",
      "Total Loss: 0.08041918883100152\n",
      "------------------------------------ epoch 6636 (39810 steps) ------------------------------------\n",
      "Max loss: 0.01887727528810501\n",
      "Min loss: 0.005757936742156744\n",
      "Mean loss: 0.011008080327883363\n",
      "Std loss: 0.005104341008603153\n",
      "Total Loss: 0.06604848196730018\n",
      "------------------------------------ epoch 6637 (39816 steps) ------------------------------------\n",
      "Max loss: 0.013141326606273651\n",
      "Min loss: 0.005432078614830971\n",
      "Mean loss: 0.007937812091161808\n",
      "Std loss: 0.002466856101110423\n",
      "Total Loss: 0.047626872546970844\n",
      "------------------------------------ epoch 6638 (39822 steps) ------------------------------------\n",
      "Max loss: 0.007574472110718489\n",
      "Min loss: 0.004856660962104797\n",
      "Mean loss: 0.005801798077300191\n",
      "Std loss: 0.0009388731736264495\n",
      "Total Loss: 0.034810788463801146\n",
      "------------------------------------ epoch 6639 (39828 steps) ------------------------------------\n",
      "Max loss: 0.016246436163783073\n",
      "Min loss: 0.00388300116173923\n",
      "Mean loss: 0.010009500935363272\n",
      "Std loss: 0.004648025837978815\n",
      "Total Loss: 0.06005700561217964\n",
      "------------------------------------ epoch 6640 (39834 steps) ------------------------------------\n",
      "Max loss: 0.02417990379035473\n",
      "Min loss: 0.005558628123253584\n",
      "Mean loss: 0.01001185403826336\n",
      "Std loss: 0.006439948346666081\n",
      "Total Loss: 0.060071124229580164\n",
      "------------------------------------ epoch 6641 (39840 steps) ------------------------------------\n",
      "Max loss: 0.013858357444405556\n",
      "Min loss: 0.004386330023407936\n",
      "Mean loss: 0.009157893558343252\n",
      "Std loss: 0.003691326058465523\n",
      "Total Loss: 0.05494736135005951\n",
      "------------------------------------ epoch 6642 (39846 steps) ------------------------------------\n",
      "Max loss: 0.0153549425303936\n",
      "Min loss: 0.0053762272000312805\n",
      "Mean loss: 0.008858477774386605\n",
      "Std loss: 0.004016845979584254\n",
      "Total Loss: 0.05315086664631963\n",
      "------------------------------------ epoch 6643 (39852 steps) ------------------------------------\n",
      "Max loss: 0.013691676780581474\n",
      "Min loss: 0.005077692214399576\n",
      "Mean loss: 0.00872330111451447\n",
      "Std loss: 0.002937592078337557\n",
      "Total Loss: 0.05233980668708682\n",
      "------------------------------------ epoch 6644 (39858 steps) ------------------------------------\n",
      "Max loss: 0.019325241446495056\n",
      "Min loss: 0.0063349781557917595\n",
      "Mean loss: 0.011325689653555552\n",
      "Std loss: 0.004382760042734269\n",
      "Total Loss: 0.06795413792133331\n",
      "------------------------------------ epoch 6645 (39864 steps) ------------------------------------\n",
      "Max loss: 0.0119695495814085\n",
      "Min loss: 0.005945488344877958\n",
      "Mean loss: 0.008728081050018469\n",
      "Std loss: 0.0021067290189518695\n",
      "Total Loss: 0.05236848630011082\n",
      "------------------------------------ epoch 6646 (39870 steps) ------------------------------------\n",
      "Max loss: 0.02346184477210045\n",
      "Min loss: 0.003802228718996048\n",
      "Mean loss: 0.008585624629631639\n",
      "Std loss: 0.00677120350389835\n",
      "Total Loss: 0.05151374777778983\n",
      "------------------------------------ epoch 6647 (39876 steps) ------------------------------------\n",
      "Max loss: 0.021948624402284622\n",
      "Min loss: 0.00685222540050745\n",
      "Mean loss: 0.010747801900530854\n",
      "Std loss: 0.005491309478243501\n",
      "Total Loss: 0.06448681140318513\n",
      "------------------------------------ epoch 6648 (39882 steps) ------------------------------------\n",
      "Max loss: 0.010925265029072762\n",
      "Min loss: 0.005027399398386478\n",
      "Mean loss: 0.007838481881966194\n",
      "Std loss: 0.0021047107288114345\n",
      "Total Loss: 0.04703089129179716\n",
      "------------------------------------ epoch 6649 (39888 steps) ------------------------------------\n",
      "Max loss: 0.02494507096707821\n",
      "Min loss: 0.0051668547093868256\n",
      "Mean loss: 0.010838212134937445\n",
      "Std loss: 0.0067718290511425945\n",
      "Total Loss: 0.06502927280962467\n",
      "------------------------------------ epoch 6650 (39894 steps) ------------------------------------\n",
      "Max loss: 0.013253772631287575\n",
      "Min loss: 0.005656879395246506\n",
      "Mean loss: 0.009024532278999686\n",
      "Std loss: 0.003091977604790519\n",
      "Total Loss: 0.05414719367399812\n",
      "------------------------------------ epoch 6651 (39900 steps) ------------------------------------\n",
      "Max loss: 0.01536162756383419\n",
      "Min loss: 0.005954999476671219\n",
      "Mean loss: 0.009553007471064726\n",
      "Std loss: 0.0032275363865209034\n",
      "Total Loss: 0.05731804482638836\n",
      "------------------------------------ epoch 6652 (39906 steps) ------------------------------------\n",
      "Max loss: 0.014857332222163677\n",
      "Min loss: 0.005288015119731426\n",
      "Mean loss: 0.009532789854953686\n",
      "Std loss: 0.0030463610191524535\n",
      "Total Loss: 0.05719673912972212\n",
      "------------------------------------ epoch 6653 (39912 steps) ------------------------------------\n",
      "Max loss: 0.018086202442646027\n",
      "Min loss: 0.006438472308218479\n",
      "Mean loss: 0.009346198523417115\n",
      "Std loss: 0.003958559825286894\n",
      "Total Loss: 0.05607719114050269\n",
      "------------------------------------ epoch 6654 (39918 steps) ------------------------------------\n",
      "Max loss: 0.047093868255615234\n",
      "Min loss: 0.004130749963223934\n",
      "Mean loss: 0.015716031193733215\n",
      "Std loss: 0.014252487642479088\n",
      "Total Loss: 0.09429618716239929\n",
      "------------------------------------ epoch 6655 (39924 steps) ------------------------------------\n",
      "Max loss: 0.035625554621219635\n",
      "Min loss: 0.0060957372188568115\n",
      "Mean loss: 0.016868015596022207\n",
      "Std loss: 0.011352309955735034\n",
      "Total Loss: 0.10120809357613325\n",
      "------------------------------------ epoch 6656 (39930 steps) ------------------------------------\n",
      "Max loss: 0.03040984831750393\n",
      "Min loss: 0.006973288953304291\n",
      "Mean loss: 0.01204042291889588\n",
      "Std loss: 0.008340806863852161\n",
      "Total Loss: 0.07224253751337528\n",
      "------------------------------------ epoch 6657 (39936 steps) ------------------------------------\n",
      "Max loss: 0.016775811091065407\n",
      "Min loss: 0.005443292669951916\n",
      "Mean loss: 0.009111259443064531\n",
      "Std loss: 0.004105567286386655\n",
      "Total Loss: 0.054667556658387184\n",
      "------------------------------------ epoch 6658 (39942 steps) ------------------------------------\n",
      "Max loss: 0.011562409810721874\n",
      "Min loss: 0.00442812405526638\n",
      "Mean loss: 0.007509510110442837\n",
      "Std loss: 0.0022460895885878405\n",
      "Total Loss: 0.04505706066265702\n",
      "------------------------------------ epoch 6659 (39948 steps) ------------------------------------\n",
      "Max loss: 0.01233144011348486\n",
      "Min loss: 0.004472207278013229\n",
      "Mean loss: 0.0067700947013994055\n",
      "Std loss: 0.0027201552076717957\n",
      "Total Loss: 0.040620568208396435\n",
      "------------------------------------ epoch 6660 (39954 steps) ------------------------------------\n",
      "Max loss: 0.01550348848104477\n",
      "Min loss: 0.004411913454532623\n",
      "Mean loss: 0.008045391412451863\n",
      "Std loss: 0.003938572441027882\n",
      "Total Loss: 0.04827234847471118\n",
      "------------------------------------ epoch 6661 (39960 steps) ------------------------------------\n",
      "Max loss: 0.011811262927949429\n",
      "Min loss: 0.005587302148342133\n",
      "Mean loss: 0.008437383997564515\n",
      "Std loss: 0.0021163219506390896\n",
      "Total Loss: 0.05062430398538709\n",
      "------------------------------------ epoch 6662 (39966 steps) ------------------------------------\n",
      "Max loss: 0.009529009461402893\n",
      "Min loss: 0.004427827894687653\n",
      "Mean loss: 0.007364776063089569\n",
      "Std loss: 0.001843467507434924\n",
      "Total Loss: 0.044188656378537416\n",
      "------------------------------------ epoch 6663 (39972 steps) ------------------------------------\n",
      "Max loss: 0.009099571965634823\n",
      "Min loss: 0.003902789205312729\n",
      "Mean loss: 0.006508908001706004\n",
      "Std loss: 0.0016164221528745435\n",
      "Total Loss: 0.039053448010236025\n",
      "------------------------------------ epoch 6664 (39978 steps) ------------------------------------\n",
      "Max loss: 0.019155483692884445\n",
      "Min loss: 0.0053717284463346004\n",
      "Mean loss: 0.009900633012875915\n",
      "Std loss: 0.004612230511372888\n",
      "Total Loss: 0.05940379807725549\n",
      "------------------------------------ epoch 6665 (39984 steps) ------------------------------------\n",
      "Max loss: 0.017533887177705765\n",
      "Min loss: 0.004083562642335892\n",
      "Mean loss: 0.007472837110981345\n",
      "Std loss: 0.004644296233169169\n",
      "Total Loss: 0.04483702266588807\n",
      "------------------------------------ epoch 6666 (39990 steps) ------------------------------------\n",
      "Max loss: 0.05069491267204285\n",
      "Min loss: 0.006919582840055227\n",
      "Mean loss: 0.01995316209892432\n",
      "Std loss: 0.01670962446613417\n",
      "Total Loss: 0.11971897259354591\n",
      "------------------------------------ epoch 6667 (39996 steps) ------------------------------------\n",
      "Max loss: 0.01638386771082878\n",
      "Min loss: 0.005928066559135914\n",
      "Mean loss: 0.009852534160017967\n",
      "Std loss: 0.003957796944310218\n",
      "Total Loss: 0.0591152049601078\n",
      "------------------------------------ epoch 6668 (40002 steps) ------------------------------------\n",
      "Max loss: 0.013430358842015266\n",
      "Min loss: 0.004917427897453308\n",
      "Mean loss: 0.00921256560832262\n",
      "Std loss: 0.0027532507211331775\n",
      "Total Loss: 0.05527539364993572\n",
      "------------------------------------ epoch 6669 (40008 steps) ------------------------------------\n",
      "Max loss: 0.01506302785128355\n",
      "Min loss: 0.007047530263662338\n",
      "Mean loss: 0.009857059301187595\n",
      "Std loss: 0.0025427124371070757\n",
      "Total Loss: 0.05914235580712557\n",
      "------------------------------------ epoch 6670 (40014 steps) ------------------------------------\n",
      "Max loss: 0.011422562412917614\n",
      "Min loss: 0.0065866815857589245\n",
      "Mean loss: 0.008313227134446302\n",
      "Std loss: 0.0015776984152027155\n",
      "Total Loss: 0.04987936280667782\n",
      "------------------------------------ epoch 6671 (40020 steps) ------------------------------------\n",
      "Max loss: 0.031328096985816956\n",
      "Min loss: 0.006866054143756628\n",
      "Mean loss: 0.015223328412200013\n",
      "Std loss: 0.007748399484736329\n",
      "Total Loss: 0.09133997047320008\n",
      "------------------------------------ epoch 6672 (40026 steps) ------------------------------------\n",
      "Max loss: 0.04338841140270233\n",
      "Min loss: 0.005329744424670935\n",
      "Mean loss: 0.015048288507387042\n",
      "Std loss: 0.012931331253905961\n",
      "Total Loss: 0.09028973104432225\n",
      "------------------------------------ epoch 6673 (40032 steps) ------------------------------------\n",
      "Max loss: 0.01948947086930275\n",
      "Min loss: 0.006946404464542866\n",
      "Mean loss: 0.011234407623608908\n",
      "Std loss: 0.00531268641670669\n",
      "Total Loss: 0.06740644574165344\n",
      "------------------------------------ epoch 6674 (40038 steps) ------------------------------------\n",
      "Max loss: 0.01478602271527052\n",
      "Min loss: 0.0053680166602134705\n",
      "Mean loss: 0.008626765416314205\n",
      "Std loss: 0.0033288252504879373\n",
      "Total Loss: 0.05176059249788523\n",
      "------------------------------------ epoch 6675 (40044 steps) ------------------------------------\n",
      "Max loss: 0.01658918708562851\n",
      "Min loss: 0.005517239682376385\n",
      "Mean loss: 0.010919727385044098\n",
      "Std loss: 0.004138954569349648\n",
      "Total Loss: 0.06551836431026459\n",
      "------------------------------------ epoch 6676 (40050 steps) ------------------------------------\n",
      "Max loss: 0.02225273847579956\n",
      "Min loss: 0.0055351718328893185\n",
      "Mean loss: 0.010320516070351005\n",
      "Std loss: 0.006625126385557982\n",
      "Total Loss: 0.06192309642210603\n",
      "------------------------------------ epoch 6677 (40056 steps) ------------------------------------\n",
      "Max loss: 0.023173626512289047\n",
      "Min loss: 0.006111507769674063\n",
      "Mean loss: 0.010275004974876841\n",
      "Std loss: 0.0058464476455941695\n",
      "Total Loss: 0.061650029849261045\n",
      "------------------------------------ epoch 6678 (40062 steps) ------------------------------------\n",
      "Max loss: 0.010404001921415329\n",
      "Min loss: 0.0042672003619372845\n",
      "Mean loss: 0.007181080523878336\n",
      "Std loss: 0.0018790974678975151\n",
      "Total Loss: 0.043086483143270016\n",
      "------------------------------------ epoch 6679 (40068 steps) ------------------------------------\n",
      "Max loss: 0.010704322718083858\n",
      "Min loss: 0.004802193958312273\n",
      "Mean loss: 0.006873020126173894\n",
      "Std loss: 0.0018641812303234458\n",
      "Total Loss: 0.04123812075704336\n",
      "------------------------------------ epoch 6680 (40074 steps) ------------------------------------\n",
      "Max loss: 0.02328687533736229\n",
      "Min loss: 0.005257719662040472\n",
      "Mean loss: 0.010346704861149192\n",
      "Std loss: 0.0064068877817114595\n",
      "Total Loss: 0.06208022916689515\n",
      "------------------------------------ epoch 6681 (40080 steps) ------------------------------------\n",
      "Max loss: 0.010553000494837761\n",
      "Min loss: 0.004356367513537407\n",
      "Mean loss: 0.00694398822573324\n",
      "Std loss: 0.001972071069584738\n",
      "Total Loss: 0.04166392935439944\n",
      "------------------------------------ epoch 6682 (40086 steps) ------------------------------------\n",
      "Max loss: 0.020404605194926262\n",
      "Min loss: 0.007765596266835928\n",
      "Mean loss: 0.012296477720762292\n",
      "Std loss: 0.004534220712687522\n",
      "Total Loss: 0.07377886632457376\n",
      "------------------------------------ epoch 6683 (40092 steps) ------------------------------------\n",
      "Max loss: 0.014292830601334572\n",
      "Min loss: 0.004222917836159468\n",
      "Mean loss: 0.00821086453894774\n",
      "Std loss: 0.0034838670736551027\n",
      "Total Loss: 0.04926518723368645\n",
      "------------------------------------ epoch 6684 (40098 steps) ------------------------------------\n",
      "Max loss: 0.012079942040145397\n",
      "Min loss: 0.004663215484470129\n",
      "Mean loss: 0.008978097466751933\n",
      "Std loss: 0.002992731830391478\n",
      "Total Loss: 0.0538685848005116\n",
      "------------------------------------ epoch 6685 (40104 steps) ------------------------------------\n",
      "Max loss: 0.020909136161208153\n",
      "Min loss: 0.004769817925989628\n",
      "Mean loss: 0.009668892715126276\n",
      "Std loss: 0.005750946954797137\n",
      "Total Loss: 0.058013356290757656\n",
      "------------------------------------ epoch 6686 (40110 steps) ------------------------------------\n",
      "Max loss: 0.008255083113908768\n",
      "Min loss: 0.0038169424515217543\n",
      "Mean loss: 0.006123261797862749\n",
      "Std loss: 0.0013860943562750644\n",
      "Total Loss: 0.03673957078717649\n",
      "------------------------------------ epoch 6687 (40116 steps) ------------------------------------\n",
      "Max loss: 0.037182390689849854\n",
      "Min loss: 0.006159802433103323\n",
      "Mean loss: 0.015586671729882559\n",
      "Std loss: 0.010561333346444764\n",
      "Total Loss: 0.09352003037929535\n",
      "------------------------------------ epoch 6688 (40122 steps) ------------------------------------\n",
      "Max loss: 0.011244496330618858\n",
      "Min loss: 0.005041562952101231\n",
      "Mean loss: 0.00790910942790409\n",
      "Std loss: 0.002413603396797362\n",
      "Total Loss: 0.047454656567424536\n",
      "------------------------------------ epoch 6689 (40128 steps) ------------------------------------\n",
      "Max loss: 0.0168850589543581\n",
      "Min loss: 0.005287709645926952\n",
      "Mean loss: 0.009912355337291956\n",
      "Std loss: 0.003910071366210316\n",
      "Total Loss: 0.059474132023751736\n",
      "------------------------------------ epoch 6690 (40134 steps) ------------------------------------\n",
      "Max loss: 0.01806691661477089\n",
      "Min loss: 0.005199896637350321\n",
      "Mean loss: 0.010164796685179075\n",
      "Std loss: 0.005636463491246941\n",
      "Total Loss: 0.06098878011107445\n",
      "------------------------------------ epoch 6691 (40140 steps) ------------------------------------\n",
      "Max loss: 0.0251769982278347\n",
      "Min loss: 0.006154391914606094\n",
      "Mean loss: 0.010876861012851199\n",
      "Std loss: 0.00676063831313549\n",
      "Total Loss: 0.06526116607710719\n",
      "------------------------------------ epoch 6692 (40146 steps) ------------------------------------\n",
      "Max loss: 0.012416800484061241\n",
      "Min loss: 0.005653781816363335\n",
      "Mean loss: 0.008019280309478441\n",
      "Std loss: 0.002285456720389871\n",
      "Total Loss: 0.04811568185687065\n",
      "------------------------------------ epoch 6693 (40152 steps) ------------------------------------\n",
      "Max loss: 0.012520302087068558\n",
      "Min loss: 0.005041257478296757\n",
      "Mean loss: 0.008576083385075131\n",
      "Std loss: 0.0029718692779507625\n",
      "Total Loss: 0.05145650031045079\n",
      "------------------------------------ epoch 6694 (40158 steps) ------------------------------------\n",
      "Max loss: 0.009540862403810024\n",
      "Min loss: 0.004991006106138229\n",
      "Mean loss: 0.006076951744034886\n",
      "Std loss: 0.0015783627039423467\n",
      "Total Loss: 0.03646171046420932\n",
      "------------------------------------ epoch 6695 (40164 steps) ------------------------------------\n",
      "Max loss: 0.018018364906311035\n",
      "Min loss: 0.004684144631028175\n",
      "Mean loss: 0.00838907800304393\n",
      "Std loss: 0.004489871792156811\n",
      "Total Loss: 0.05033446801826358\n",
      "------------------------------------ epoch 6696 (40170 steps) ------------------------------------\n",
      "Max loss: 0.021266095340251923\n",
      "Min loss: 0.00457519618794322\n",
      "Mean loss: 0.010387036561345061\n",
      "Std loss: 0.005432606214643511\n",
      "Total Loss: 0.062322219368070364\n",
      "------------------------------------ epoch 6697 (40176 steps) ------------------------------------\n",
      "Max loss: 0.012406399473547935\n",
      "Min loss: 0.0038963507395237684\n",
      "Mean loss: 0.007435699497970442\n",
      "Std loss: 0.003275777178936819\n",
      "Total Loss: 0.04461419698782265\n",
      "------------------------------------ epoch 6698 (40182 steps) ------------------------------------\n",
      "Max loss: 0.01091121044009924\n",
      "Min loss: 0.004541109316051006\n",
      "Mean loss: 0.0077645079387972755\n",
      "Std loss: 0.0022702280065624914\n",
      "Total Loss: 0.04658704763278365\n",
      "------------------------------------ epoch 6699 (40188 steps) ------------------------------------\n",
      "Max loss: 0.015246596187353134\n",
      "Min loss: 0.005638229660689831\n",
      "Mean loss: 0.009687886340543628\n",
      "Std loss: 0.003563450610872873\n",
      "Total Loss: 0.058127318043261766\n",
      "------------------------------------ epoch 6700 (40194 steps) ------------------------------------\n",
      "Max loss: 0.007130987010896206\n",
      "Min loss: 0.005290140863507986\n",
      "Mean loss: 0.0060747109819203615\n",
      "Std loss: 0.0006194706797408158\n",
      "Total Loss: 0.03644826589152217\n",
      "------------------------------------ epoch 6701 (40200 steps) ------------------------------------\n",
      "Max loss: 0.025133749470114708\n",
      "Min loss: 0.005496086087077856\n",
      "Mean loss: 0.011612927230695883\n",
      "Std loss: 0.0069506959102865135\n",
      "Total Loss: 0.0696775633841753\n",
      "saved model at ./weights/model_6701.pth\n",
      "------------------------------------ epoch 6702 (40206 steps) ------------------------------------\n",
      "Max loss: 0.009778965264558792\n",
      "Min loss: 0.005153893027454615\n",
      "Mean loss: 0.008098290069028735\n",
      "Std loss: 0.0014617906709807889\n",
      "Total Loss: 0.04858974041417241\n",
      "------------------------------------ epoch 6703 (40212 steps) ------------------------------------\n",
      "Max loss: 0.02588289976119995\n",
      "Min loss: 0.004802139010280371\n",
      "Mean loss: 0.010346400318667293\n",
      "Std loss: 0.007249774554287462\n",
      "Total Loss: 0.062078401912003756\n",
      "------------------------------------ epoch 6704 (40218 steps) ------------------------------------\n",
      "Max loss: 0.017338458448648453\n",
      "Min loss: 0.005556868389248848\n",
      "Mean loss: 0.011182349951316914\n",
      "Std loss: 0.004191363204194816\n",
      "Total Loss: 0.06709409970790148\n",
      "------------------------------------ epoch 6705 (40224 steps) ------------------------------------\n",
      "Max loss: 0.014640608802437782\n",
      "Min loss: 0.005483092274516821\n",
      "Mean loss: 0.008138189868380627\n",
      "Std loss: 0.003094168523065807\n",
      "Total Loss: 0.048829139210283756\n",
      "------------------------------------ epoch 6706 (40230 steps) ------------------------------------\n",
      "Max loss: 0.03967205435037613\n",
      "Min loss: 0.007477075792849064\n",
      "Mean loss: 0.01618944954437514\n",
      "Std loss: 0.011383395879521164\n",
      "Total Loss: 0.09713669726625085\n",
      "------------------------------------ epoch 6707 (40236 steps) ------------------------------------\n",
      "Max loss: 0.022800147533416748\n",
      "Min loss: 0.005393519066274166\n",
      "Mean loss: 0.013309219852089882\n",
      "Std loss: 0.005936739423678422\n",
      "Total Loss: 0.07985531911253929\n",
      "------------------------------------ epoch 6708 (40242 steps) ------------------------------------\n",
      "Max loss: 0.020320065319538116\n",
      "Min loss: 0.006925676949322224\n",
      "Mean loss: 0.01267356425523758\n",
      "Std loss: 0.004345005551995312\n",
      "Total Loss: 0.07604138553142548\n",
      "------------------------------------ epoch 6709 (40248 steps) ------------------------------------\n",
      "Max loss: 0.02552036941051483\n",
      "Min loss: 0.008698929101228714\n",
      "Mean loss: 0.01646687975153327\n",
      "Std loss: 0.006577807853342031\n",
      "Total Loss: 0.09880127850919962\n",
      "------------------------------------ epoch 6710 (40254 steps) ------------------------------------\n",
      "Max loss: 0.020062988623976707\n",
      "Min loss: 0.006412993650883436\n",
      "Mean loss: 0.013443594876055917\n",
      "Std loss: 0.005036342047281242\n",
      "Total Loss: 0.0806615692563355\n",
      "------------------------------------ epoch 6711 (40260 steps) ------------------------------------\n",
      "Max loss: 0.017170747742056847\n",
      "Min loss: 0.0073755825869739056\n",
      "Mean loss: 0.01207832689397037\n",
      "Std loss: 0.0034993117433749577\n",
      "Total Loss: 0.07246996136382222\n",
      "------------------------------------ epoch 6712 (40266 steps) ------------------------------------\n",
      "Max loss: 0.01324271596968174\n",
      "Min loss: 0.004918977618217468\n",
      "Mean loss: 0.009132355373973647\n",
      "Std loss: 0.003253734587338087\n",
      "Total Loss: 0.054794132243841887\n",
      "------------------------------------ epoch 6713 (40272 steps) ------------------------------------\n",
      "Max loss: 0.01537473127245903\n",
      "Min loss: 0.004937866237014532\n",
      "Mean loss: 0.009110478141034642\n",
      "Std loss: 0.004244891190921459\n",
      "Total Loss: 0.05466286884620786\n",
      "------------------------------------ epoch 6714 (40278 steps) ------------------------------------\n",
      "Max loss: 0.00883769616484642\n",
      "Min loss: 0.0040356311947107315\n",
      "Mean loss: 0.007009309716522694\n",
      "Std loss: 0.00180303515510739\n",
      "Total Loss: 0.04205585829913616\n",
      "------------------------------------ epoch 6715 (40284 steps) ------------------------------------\n",
      "Max loss: 0.04943851754069328\n",
      "Min loss: 0.00682879239320755\n",
      "Mean loss: 0.02170133466521899\n",
      "Std loss: 0.014100186895573288\n",
      "Total Loss: 0.13020800799131393\n",
      "------------------------------------ epoch 6716 (40290 steps) ------------------------------------\n",
      "Max loss: 0.07118801772594452\n",
      "Min loss: 0.01414340641349554\n",
      "Mean loss: 0.04121106661235293\n",
      "Std loss: 0.02153719250354911\n",
      "Total Loss: 0.24726639967411757\n",
      "------------------------------------ epoch 6717 (40296 steps) ------------------------------------\n",
      "Max loss: 0.056846246123313904\n",
      "Min loss: 0.01403885893523693\n",
      "Mean loss: 0.03200145221004883\n",
      "Std loss: 0.016202769828758446\n",
      "Total Loss: 0.192008713260293\n",
      "------------------------------------ epoch 6718 (40302 steps) ------------------------------------\n",
      "Max loss: 0.04872850328683853\n",
      "Min loss: 0.017544593662023544\n",
      "Mean loss: 0.02897959202528\n",
      "Std loss: 0.011538053535569056\n",
      "Total Loss: 0.17387755215168\n",
      "------------------------------------ epoch 6719 (40308 steps) ------------------------------------\n",
      "Max loss: 0.04595986008644104\n",
      "Min loss: 0.014997845515608788\n",
      "Mean loss: 0.030520279581348102\n",
      "Std loss: 0.011931455485797327\n",
      "Total Loss: 0.1831216774880886\n",
      "------------------------------------ epoch 6720 (40314 steps) ------------------------------------\n",
      "Max loss: 0.06354516744613647\n",
      "Min loss: 0.01696319505572319\n",
      "Mean loss: 0.032400780667861305\n",
      "Std loss: 0.01689880847421504\n",
      "Total Loss: 0.19440468400716782\n",
      "------------------------------------ epoch 6721 (40320 steps) ------------------------------------\n",
      "Max loss: 0.030217092484235764\n",
      "Min loss: 0.010374709032475948\n",
      "Mean loss: 0.017189273921151955\n",
      "Std loss: 0.006823367745433369\n",
      "Total Loss: 0.10313564352691174\n",
      "------------------------------------ epoch 6722 (40326 steps) ------------------------------------\n",
      "Max loss: 0.03795319423079491\n",
      "Min loss: 0.010778479278087616\n",
      "Mean loss: 0.02064292986566822\n",
      "Std loss: 0.00966783996434872\n",
      "Total Loss: 0.1238575791940093\n",
      "------------------------------------ epoch 6723 (40332 steps) ------------------------------------\n",
      "Max loss: 0.028420625254511833\n",
      "Min loss: 0.008194508031010628\n",
      "Mean loss: 0.014741897427787384\n",
      "Std loss: 0.0069269373153708\n",
      "Total Loss: 0.0884513845667243\n",
      "------------------------------------ epoch 6724 (40338 steps) ------------------------------------\n",
      "Max loss: 0.022455982863903046\n",
      "Min loss: 0.0081479512155056\n",
      "Mean loss: 0.01293001106629769\n",
      "Std loss: 0.005693140588134786\n",
      "Total Loss: 0.07758006639778614\n",
      "------------------------------------ epoch 6725 (40344 steps) ------------------------------------\n",
      "Max loss: 0.022032586857676506\n",
      "Min loss: 0.009209041483700275\n",
      "Mean loss: 0.01317016143972675\n",
      "Std loss: 0.004318935559680465\n",
      "Total Loss: 0.0790209686383605\n",
      "------------------------------------ epoch 6726 (40350 steps) ------------------------------------\n",
      "Max loss: 0.025317620486021042\n",
      "Min loss: 0.007494296878576279\n",
      "Mean loss: 0.013924415844182173\n",
      "Std loss: 0.00619882192401657\n",
      "Total Loss: 0.08354649506509304\n",
      "------------------------------------ epoch 6727 (40356 steps) ------------------------------------\n",
      "Max loss: 0.02081218548119068\n",
      "Min loss: 0.008644641377031803\n",
      "Mean loss: 0.013392754985640446\n",
      "Std loss: 0.004844394915292516\n",
      "Total Loss: 0.08035652991384268\n",
      "------------------------------------ epoch 6728 (40362 steps) ------------------------------------\n",
      "Max loss: 0.012395812198519707\n",
      "Min loss: 0.006568006239831448\n",
      "Mean loss: 0.009760040557011962\n",
      "Std loss: 0.001975101972956954\n",
      "Total Loss: 0.05856024334207177\n",
      "------------------------------------ epoch 6729 (40368 steps) ------------------------------------\n",
      "Max loss: 0.043872635811567307\n",
      "Min loss: 0.0064634522423148155\n",
      "Mean loss: 0.018203711913277704\n",
      "Std loss: 0.012929637487078633\n",
      "Total Loss: 0.10922227147966623\n",
      "------------------------------------ epoch 6730 (40374 steps) ------------------------------------\n",
      "Max loss: 0.02295072004199028\n",
      "Min loss: 0.01159929670393467\n",
      "Mean loss: 0.015023115711907545\n",
      "Std loss: 0.003791339421006842\n",
      "Total Loss: 0.09013869427144527\n",
      "------------------------------------ epoch 6731 (40380 steps) ------------------------------------\n",
      "Max loss: 0.024884961545467377\n",
      "Min loss: 0.006434082053601742\n",
      "Mean loss: 0.013420969558258852\n",
      "Std loss: 0.0065404891511096535\n",
      "Total Loss: 0.08052581734955311\n",
      "------------------------------------ epoch 6732 (40386 steps) ------------------------------------\n",
      "Max loss: 0.013499877415597439\n",
      "Min loss: 0.006301339715719223\n",
      "Mean loss: 0.009018263391529521\n",
      "Std loss: 0.0029652468123747746\n",
      "Total Loss: 0.05410958034917712\n",
      "------------------------------------ epoch 6733 (40392 steps) ------------------------------------\n",
      "Max loss: 0.022329969331622124\n",
      "Min loss: 0.0070445239543914795\n",
      "Mean loss: 0.013164356040457884\n",
      "Std loss: 0.005014543677078527\n",
      "Total Loss: 0.0789861362427473\n",
      "------------------------------------ epoch 6734 (40398 steps) ------------------------------------\n",
      "Max loss: 0.022620543837547302\n",
      "Min loss: 0.0062675378285348415\n",
      "Mean loss: 0.010563559286917249\n",
      "Std loss: 0.00564787677138993\n",
      "Total Loss: 0.0633813557215035\n",
      "------------------------------------ epoch 6735 (40404 steps) ------------------------------------\n",
      "Max loss: 0.022775497287511826\n",
      "Min loss: 0.00685484241694212\n",
      "Mean loss: 0.012221012419710556\n",
      "Std loss: 0.005674207776535124\n",
      "Total Loss: 0.07332607451826334\n",
      "------------------------------------ epoch 6736 (40410 steps) ------------------------------------\n",
      "Max loss: 0.015219527296721935\n",
      "Min loss: 0.006514754146337509\n",
      "Mean loss: 0.009897541720420122\n",
      "Std loss: 0.0026827316215000453\n",
      "Total Loss: 0.05938525032252073\n",
      "------------------------------------ epoch 6737 (40416 steps) ------------------------------------\n",
      "Max loss: 0.01752159371972084\n",
      "Min loss: 0.006993800401687622\n",
      "Mean loss: 0.011535075570767125\n",
      "Std loss: 0.003802952622006807\n",
      "Total Loss: 0.06921045342460275\n",
      "------------------------------------ epoch 6738 (40422 steps) ------------------------------------\n",
      "Max loss: 0.01296472828835249\n",
      "Min loss: 0.00608384795486927\n",
      "Mean loss: 0.008664916191870967\n",
      "Std loss: 0.0021691716205786324\n",
      "Total Loss: 0.051989497151225805\n",
      "------------------------------------ epoch 6739 (40428 steps) ------------------------------------\n",
      "Max loss: 0.01641523465514183\n",
      "Min loss: 0.005268963053822517\n",
      "Mean loss: 0.009545995000128945\n",
      "Std loss: 0.0037525410958863976\n",
      "Total Loss: 0.05727597000077367\n",
      "------------------------------------ epoch 6740 (40434 steps) ------------------------------------\n",
      "Max loss: 0.018667874857783318\n",
      "Min loss: 0.004947943612933159\n",
      "Mean loss: 0.01078537960226337\n",
      "Std loss: 0.0045073773749925\n",
      "Total Loss: 0.06471227761358023\n",
      "------------------------------------ epoch 6741 (40440 steps) ------------------------------------\n",
      "Max loss: 0.024173304438591003\n",
      "Min loss: 0.005472327582538128\n",
      "Mean loss: 0.011665050406008959\n",
      "Std loss: 0.006576397188709365\n",
      "Total Loss: 0.06999030243605375\n",
      "------------------------------------ epoch 6742 (40446 steps) ------------------------------------\n",
      "Max loss: 0.016175609081983566\n",
      "Min loss: 0.005452440585941076\n",
      "Mean loss: 0.009753223353375992\n",
      "Std loss: 0.003516545399567818\n",
      "Total Loss: 0.05851934012025595\n",
      "------------------------------------ epoch 6743 (40452 steps) ------------------------------------\n",
      "Max loss: 0.016811860725283623\n",
      "Min loss: 0.005948083009570837\n",
      "Mean loss: 0.009137196155885855\n",
      "Std loss: 0.0040122578328362895\n",
      "Total Loss: 0.05482317693531513\n",
      "------------------------------------ epoch 6744 (40458 steps) ------------------------------------\n",
      "Max loss: 0.013376357965171337\n",
      "Min loss: 0.006032341625541449\n",
      "Mean loss: 0.008973938257743916\n",
      "Std loss: 0.00259922165109498\n",
      "Total Loss: 0.05384362954646349\n",
      "------------------------------------ epoch 6745 (40464 steps) ------------------------------------\n",
      "Max loss: 0.036567773669958115\n",
      "Min loss: 0.005700505338609219\n",
      "Mean loss: 0.011835481505841017\n",
      "Std loss: 0.011080152808692798\n",
      "Total Loss: 0.0710128890350461\n",
      "------------------------------------ epoch 6746 (40470 steps) ------------------------------------\n",
      "Max loss: 0.0487431064248085\n",
      "Min loss: 0.006323081441223621\n",
      "Mean loss: 0.015848286760350067\n",
      "Std loss: 0.01490863736625641\n",
      "Total Loss: 0.09508972056210041\n",
      "------------------------------------ epoch 6747 (40476 steps) ------------------------------------\n",
      "Max loss: 0.01592072658240795\n",
      "Min loss: 0.0055561973713338375\n",
      "Mean loss: 0.010999881584818164\n",
      "Std loss: 0.0030448971818248793\n",
      "Total Loss: 0.06599928950890899\n",
      "------------------------------------ epoch 6748 (40482 steps) ------------------------------------\n",
      "Max loss: 0.017129871994256973\n",
      "Min loss: 0.005677323788404465\n",
      "Mean loss: 0.011182158254086971\n",
      "Std loss: 0.004106215350404002\n",
      "Total Loss: 0.06709294952452183\n",
      "------------------------------------ epoch 6749 (40488 steps) ------------------------------------\n",
      "Max loss: 0.033877089619636536\n",
      "Min loss: 0.006080248858779669\n",
      "Mean loss: 0.014060759839291373\n",
      "Std loss: 0.00942973260724478\n",
      "Total Loss: 0.08436455903574824\n",
      "------------------------------------ epoch 6750 (40494 steps) ------------------------------------\n",
      "Max loss: 0.013966372236609459\n",
      "Min loss: 0.005281319376081228\n",
      "Mean loss: 0.008419172295058766\n",
      "Std loss: 0.002795438343378103\n",
      "Total Loss: 0.0505150337703526\n",
      "------------------------------------ epoch 6751 (40500 steps) ------------------------------------\n",
      "Max loss: 0.012698414735496044\n",
      "Min loss: 0.005461361259222031\n",
      "Mean loss: 0.007815357530489564\n",
      "Std loss: 0.0023349032830993813\n",
      "Total Loss: 0.046892145182937384\n",
      "------------------------------------ epoch 6752 (40506 steps) ------------------------------------\n",
      "Max loss: 0.009591925889253616\n",
      "Min loss: 0.005065923556685448\n",
      "Mean loss: 0.00797427756090959\n",
      "Std loss: 0.001500983342652861\n",
      "Total Loss: 0.047845665365457535\n",
      "------------------------------------ epoch 6753 (40512 steps) ------------------------------------\n",
      "Max loss: 0.046827249228954315\n",
      "Min loss: 0.007683805655688047\n",
      "Mean loss: 0.015516443410888314\n",
      "Std loss: 0.014060401732178736\n",
      "Total Loss: 0.09309866046532989\n",
      "------------------------------------ epoch 6754 (40518 steps) ------------------------------------\n",
      "Max loss: 0.011893931776285172\n",
      "Min loss: 0.006487444043159485\n",
      "Mean loss: 0.009205905720591545\n",
      "Std loss: 0.0019000544888672474\n",
      "Total Loss: 0.05523543432354927\n",
      "------------------------------------ epoch 6755 (40524 steps) ------------------------------------\n",
      "Max loss: 0.029089171439409256\n",
      "Min loss: 0.005936504807323217\n",
      "Mean loss: 0.017039220702523988\n",
      "Std loss: 0.008266096731638972\n",
      "Total Loss: 0.10223532421514392\n",
      "------------------------------------ epoch 6756 (40530 steps) ------------------------------------\n",
      "Max loss: 0.05506104230880737\n",
      "Min loss: 0.00802929513156414\n",
      "Mean loss: 0.017169534073521692\n",
      "Std loss: 0.017049718950778173\n",
      "Total Loss: 0.10301720444113016\n",
      "------------------------------------ epoch 6757 (40536 steps) ------------------------------------\n",
      "Max loss: 0.020514722913503647\n",
      "Min loss: 0.008117040619254112\n",
      "Mean loss: 0.012234257068485022\n",
      "Std loss: 0.004575654281562384\n",
      "Total Loss: 0.07340554241091013\n",
      "------------------------------------ epoch 6758 (40542 steps) ------------------------------------\n",
      "Max loss: 0.019352685660123825\n",
      "Min loss: 0.0057028974406421185\n",
      "Mean loss: 0.011149063395957151\n",
      "Std loss: 0.0047743316617114865\n",
      "Total Loss: 0.06689438037574291\n",
      "------------------------------------ epoch 6759 (40548 steps) ------------------------------------\n",
      "Max loss: 0.036216720938682556\n",
      "Min loss: 0.006603564601391554\n",
      "Mean loss: 0.016688647447153926\n",
      "Std loss: 0.010802356938504867\n",
      "Total Loss: 0.10013188468292356\n",
      "------------------------------------ epoch 6760 (40554 steps) ------------------------------------\n",
      "Max loss: 0.02105741761624813\n",
      "Min loss: 0.006420202553272247\n",
      "Mean loss: 0.010597713446865479\n",
      "Std loss: 0.0048897098360255065\n",
      "Total Loss: 0.06358628068119287\n",
      "------------------------------------ epoch 6761 (40560 steps) ------------------------------------\n",
      "Max loss: 0.029080523177981377\n",
      "Min loss: 0.00855950266122818\n",
      "Mean loss: 0.016177960050602753\n",
      "Std loss: 0.007745961456971442\n",
      "Total Loss: 0.09706776030361652\n",
      "------------------------------------ epoch 6762 (40566 steps) ------------------------------------\n",
      "Max loss: 0.029992394149303436\n",
      "Min loss: 0.007086703088134527\n",
      "Mean loss: 0.012747657562916478\n",
      "Std loss: 0.008050766918616458\n",
      "Total Loss: 0.07648594537749887\n",
      "------------------------------------ epoch 6763 (40572 steps) ------------------------------------\n",
      "Max loss: 0.029130712151527405\n",
      "Min loss: 0.006684444844722748\n",
      "Mean loss: 0.014206428856899342\n",
      "Std loss: 0.0073384288270997204\n",
      "Total Loss: 0.08523857314139605\n",
      "------------------------------------ epoch 6764 (40578 steps) ------------------------------------\n",
      "Max loss: 0.02592146396636963\n",
      "Min loss: 0.00835745595395565\n",
      "Mean loss: 0.01795832036683957\n",
      "Std loss: 0.005687131493772207\n",
      "Total Loss: 0.1077499222010374\n",
      "------------------------------------ epoch 6765 (40584 steps) ------------------------------------\n",
      "Max loss: 0.022291934117674828\n",
      "Min loss: 0.005766042973846197\n",
      "Mean loss: 0.012564292720829448\n",
      "Std loss: 0.00519183800116145\n",
      "Total Loss: 0.07538575632497668\n",
      "------------------------------------ epoch 6766 (40590 steps) ------------------------------------\n",
      "Max loss: 0.023525774478912354\n",
      "Min loss: 0.0048360684886574745\n",
      "Mean loss: 0.012142831382031241\n",
      "Std loss: 0.007375304461763\n",
      "Total Loss: 0.07285698829218745\n",
      "------------------------------------ epoch 6767 (40596 steps) ------------------------------------\n",
      "Max loss: 0.04565923660993576\n",
      "Min loss: 0.006777036935091019\n",
      "Mean loss: 0.01682967847834031\n",
      "Std loss: 0.013189766262755614\n",
      "Total Loss: 0.10097807087004185\n",
      "------------------------------------ epoch 6768 (40602 steps) ------------------------------------\n",
      "Max loss: 0.024719109758734703\n",
      "Min loss: 0.005903094075620174\n",
      "Mean loss: 0.01275106294391056\n",
      "Std loss: 0.006324909559154028\n",
      "Total Loss: 0.07650637766346335\n",
      "------------------------------------ epoch 6769 (40608 steps) ------------------------------------\n",
      "Max loss: 0.016148444265127182\n",
      "Min loss: 0.007726001553237438\n",
      "Mean loss: 0.011087718574951092\n",
      "Std loss: 0.0028732138278654107\n",
      "Total Loss: 0.06652631144970655\n",
      "------------------------------------ epoch 6770 (40614 steps) ------------------------------------\n",
      "Max loss: 0.010764125734567642\n",
      "Min loss: 0.005287688225507736\n",
      "Mean loss: 0.00793499305533866\n",
      "Std loss: 0.00212479421256381\n",
      "Total Loss: 0.047609958332031965\n",
      "------------------------------------ epoch 6771 (40620 steps) ------------------------------------\n",
      "Max loss: 0.0469784140586853\n",
      "Min loss: 0.008943472057580948\n",
      "Mean loss: 0.017071064716825884\n",
      "Std loss: 0.013604161919990816\n",
      "Total Loss: 0.1024263883009553\n",
      "------------------------------------ epoch 6772 (40626 steps) ------------------------------------\n",
      "Max loss: 0.019418984651565552\n",
      "Min loss: 0.004621353931725025\n",
      "Mean loss: 0.011681360968699058\n",
      "Std loss: 0.004761314786126811\n",
      "Total Loss: 0.07008816581219435\n",
      "------------------------------------ epoch 6773 (40632 steps) ------------------------------------\n",
      "Max loss: 0.013601324521005154\n",
      "Min loss: 0.00483308220282197\n",
      "Mean loss: 0.008703494444489479\n",
      "Std loss: 0.003315110854759517\n",
      "Total Loss: 0.052220966666936874\n",
      "------------------------------------ epoch 6774 (40638 steps) ------------------------------------\n",
      "Max loss: 0.016203172504901886\n",
      "Min loss: 0.007868846878409386\n",
      "Mean loss: 0.011167817438642183\n",
      "Std loss: 0.002893405886597462\n",
      "Total Loss: 0.0670069046318531\n",
      "------------------------------------ epoch 6775 (40644 steps) ------------------------------------\n",
      "Max loss: 0.01345592550933361\n",
      "Min loss: 0.004816945176571608\n",
      "Mean loss: 0.008336312525595227\n",
      "Std loss: 0.00261049884858014\n",
      "Total Loss: 0.05001787515357137\n",
      "------------------------------------ epoch 6776 (40650 steps) ------------------------------------\n",
      "Max loss: 0.023392532020807266\n",
      "Min loss: 0.005205485969781876\n",
      "Mean loss: 0.011107712906474868\n",
      "Std loss: 0.006218309967097852\n",
      "Total Loss: 0.06664627743884921\n",
      "------------------------------------ epoch 6777 (40656 steps) ------------------------------------\n",
      "Max loss: 0.020101942121982574\n",
      "Min loss: 0.005058759823441505\n",
      "Mean loss: 0.012320540224512419\n",
      "Std loss: 0.005106342829075621\n",
      "Total Loss: 0.07392324134707451\n",
      "------------------------------------ epoch 6778 (40662 steps) ------------------------------------\n",
      "Max loss: 0.034916508942842484\n",
      "Min loss: 0.007151166908442974\n",
      "Mean loss: 0.016918925335630774\n",
      "Std loss: 0.010048061616959234\n",
      "Total Loss: 0.10151355201378465\n",
      "------------------------------------ epoch 6779 (40668 steps) ------------------------------------\n",
      "Max loss: 0.05830162763595581\n",
      "Min loss: 0.007014087401330471\n",
      "Mean loss: 0.017824807825187843\n",
      "Std loss: 0.018319521606860106\n",
      "Total Loss: 0.10694884695112705\n",
      "------------------------------------ epoch 6780 (40674 steps) ------------------------------------\n",
      "Max loss: 0.04315173253417015\n",
      "Min loss: 0.005360971204936504\n",
      "Mean loss: 0.015212209429591894\n",
      "Std loss: 0.012873219960373316\n",
      "Total Loss: 0.09127325657755136\n",
      "------------------------------------ epoch 6781 (40680 steps) ------------------------------------\n",
      "Max loss: 0.014901833608746529\n",
      "Min loss: 0.005509027745574713\n",
      "Mean loss: 0.011445636783416072\n",
      "Std loss: 0.003115094574063883\n",
      "Total Loss: 0.06867382070049644\n",
      "------------------------------------ epoch 6782 (40686 steps) ------------------------------------\n",
      "Max loss: 0.011351014487445354\n",
      "Min loss: 0.005165444687008858\n",
      "Mean loss: 0.007456863997504115\n",
      "Std loss: 0.0020844999321237502\n",
      "Total Loss: 0.04474118398502469\n",
      "------------------------------------ epoch 6783 (40692 steps) ------------------------------------\n",
      "Max loss: 0.015684373676776886\n",
      "Min loss: 0.004527837969362736\n",
      "Mean loss: 0.008314885199069977\n",
      "Std loss: 0.0037763772624878505\n",
      "Total Loss: 0.04988931119441986\n",
      "------------------------------------ epoch 6784 (40698 steps) ------------------------------------\n",
      "Max loss: 0.02094442769885063\n",
      "Min loss: 0.005459493957459927\n",
      "Mean loss: 0.011892199283465743\n",
      "Std loss: 0.00538984400579585\n",
      "Total Loss: 0.07135319570079446\n",
      "------------------------------------ epoch 6785 (40704 steps) ------------------------------------\n",
      "Max loss: 0.010285927914083004\n",
      "Min loss: 0.004455610644072294\n",
      "Mean loss: 0.00785862592359384\n",
      "Std loss: 0.002016169691874803\n",
      "Total Loss: 0.047151755541563034\n",
      "------------------------------------ epoch 6786 (40710 steps) ------------------------------------\n",
      "Max loss: 0.02526031993329525\n",
      "Min loss: 0.004773817025125027\n",
      "Mean loss: 0.01218988715360562\n",
      "Std loss: 0.007941825902545682\n",
      "Total Loss: 0.07313932292163372\n",
      "------------------------------------ epoch 6787 (40716 steps) ------------------------------------\n",
      "Max loss: 0.031181475147604942\n",
      "Min loss: 0.006172060966491699\n",
      "Mean loss: 0.014625802248095473\n",
      "Std loss: 0.008472214424904643\n",
      "Total Loss: 0.08775481348857284\n",
      "------------------------------------ epoch 6788 (40722 steps) ------------------------------------\n",
      "Max loss: 0.020902592688798904\n",
      "Min loss: 0.006258722394704819\n",
      "Mean loss: 0.010435545584186912\n",
      "Std loss: 0.005613543755250492\n",
      "Total Loss: 0.06261327350512147\n",
      "------------------------------------ epoch 6789 (40728 steps) ------------------------------------\n",
      "Max loss: 0.012518234550952911\n",
      "Min loss: 0.005184944719076157\n",
      "Mean loss: 0.009538378100842237\n",
      "Std loss: 0.002966759352770938\n",
      "Total Loss: 0.057230268605053425\n",
      "------------------------------------ epoch 6790 (40734 steps) ------------------------------------\n",
      "Max loss: 0.010373076424002647\n",
      "Min loss: 0.0055508604273200035\n",
      "Mean loss: 0.00829374953173101\n",
      "Std loss: 0.0016039862923341321\n",
      "Total Loss: 0.04976249719038606\n",
      "------------------------------------ epoch 6791 (40740 steps) ------------------------------------\n",
      "Max loss: 0.012654148042201996\n",
      "Min loss: 0.007338594179600477\n",
      "Mean loss: 0.01054008126569291\n",
      "Std loss: 0.0019301376717078184\n",
      "Total Loss: 0.06324048759415746\n",
      "------------------------------------ epoch 6792 (40746 steps) ------------------------------------\n",
      "Max loss: 0.01719033345580101\n",
      "Min loss: 0.005926694255322218\n",
      "Mean loss: 0.010890969230482975\n",
      "Std loss: 0.0046224668914237555\n",
      "Total Loss: 0.06534581538289785\n",
      "------------------------------------ epoch 6793 (40752 steps) ------------------------------------\n",
      "Max loss: 0.01155367773026228\n",
      "Min loss: 0.006645794492214918\n",
      "Mean loss: 0.009143444864700237\n",
      "Std loss: 0.001977202057484221\n",
      "Total Loss: 0.05486066918820143\n",
      "------------------------------------ epoch 6794 (40758 steps) ------------------------------------\n",
      "Max loss: 0.03578785061836243\n",
      "Min loss: 0.006850515957921743\n",
      "Mean loss: 0.012998720786223808\n",
      "Std loss: 0.010243157433905343\n",
      "Total Loss: 0.07799232471734285\n",
      "------------------------------------ epoch 6795 (40764 steps) ------------------------------------\n",
      "Max loss: 0.013874772936105728\n",
      "Min loss: 0.006034488789737225\n",
      "Mean loss: 0.010190297306204835\n",
      "Std loss: 0.0032233531612525287\n",
      "Total Loss: 0.06114178383722901\n",
      "------------------------------------ epoch 6796 (40770 steps) ------------------------------------\n",
      "Max loss: 0.03878559172153473\n",
      "Min loss: 0.008499293588101864\n",
      "Mean loss: 0.02275145969664057\n",
      "Std loss: 0.010175579277668191\n",
      "Total Loss: 0.13650875817984343\n",
      "------------------------------------ epoch 6797 (40776 steps) ------------------------------------\n",
      "Max loss: 0.024697639048099518\n",
      "Min loss: 0.008477510884404182\n",
      "Mean loss: 0.01266189788778623\n",
      "Std loss: 0.005558139134700416\n",
      "Total Loss: 0.07597138732671738\n",
      "------------------------------------ epoch 6798 (40782 steps) ------------------------------------\n",
      "Max loss: 0.026689842343330383\n",
      "Min loss: 0.005248107016086578\n",
      "Mean loss: 0.010971845826134086\n",
      "Std loss: 0.007646254118038907\n",
      "Total Loss: 0.06583107495680451\n",
      "------------------------------------ epoch 6799 (40788 steps) ------------------------------------\n",
      "Max loss: 0.014119736850261688\n",
      "Min loss: 0.00592085300013423\n",
      "Mean loss: 0.009158833185210824\n",
      "Std loss: 0.0028034292391603687\n",
      "Total Loss: 0.054952999111264944\n",
      "------------------------------------ epoch 6800 (40794 steps) ------------------------------------\n",
      "Max loss: 0.01885605975985527\n",
      "Min loss: 0.0048701949417591095\n",
      "Mean loss: 0.009573850935945908\n",
      "Std loss: 0.004607631276828074\n",
      "Total Loss: 0.05744310561567545\n",
      "------------------------------------ epoch 6801 (40800 steps) ------------------------------------\n",
      "Max loss: 0.019279975444078445\n",
      "Min loss: 0.006158787757158279\n",
      "Mean loss: 0.009400277476136884\n",
      "Std loss: 0.0045029220764711084\n",
      "Total Loss: 0.0564016648568213\n",
      "saved model at ./weights/model_6801.pth\n",
      "------------------------------------ epoch 6802 (40806 steps) ------------------------------------\n",
      "Max loss: 0.021047528833150864\n",
      "Min loss: 0.004755713976919651\n",
      "Mean loss: 0.01085723276870946\n",
      "Std loss: 0.005559389483967798\n",
      "Total Loss: 0.06514339661225677\n",
      "------------------------------------ epoch 6803 (40812 steps) ------------------------------------\n",
      "Max loss: 0.03314787149429321\n",
      "Min loss: 0.005276319570839405\n",
      "Mean loss: 0.012085767385239402\n",
      "Std loss: 0.00961257633926406\n",
      "Total Loss: 0.07251460431143641\n",
      "------------------------------------ epoch 6804 (40818 steps) ------------------------------------\n",
      "Max loss: 0.020549070090055466\n",
      "Min loss: 0.0044447872787714005\n",
      "Mean loss: 0.011659506397942701\n",
      "Std loss: 0.006286676930935812\n",
      "Total Loss: 0.06995703838765621\n",
      "------------------------------------ epoch 6805 (40824 steps) ------------------------------------\n",
      "Max loss: 0.013552090153098106\n",
      "Min loss: 0.004809456877410412\n",
      "Mean loss: 0.007290665448332827\n",
      "Std loss: 0.0029929813344193256\n",
      "Total Loss: 0.04374399268999696\n",
      "------------------------------------ epoch 6806 (40830 steps) ------------------------------------\n",
      "Max loss: 0.011321213096380234\n",
      "Min loss: 0.006502828560769558\n",
      "Mean loss: 0.007655967182169358\n",
      "Std loss: 0.001661322155644641\n",
      "Total Loss: 0.04593580309301615\n",
      "------------------------------------ epoch 6807 (40836 steps) ------------------------------------\n",
      "Max loss: 0.01184381078928709\n",
      "Min loss: 0.004797660745680332\n",
      "Mean loss: 0.007829330240686735\n",
      "Std loss: 0.002582951531369661\n",
      "Total Loss: 0.04697598144412041\n",
      "------------------------------------ epoch 6808 (40842 steps) ------------------------------------\n",
      "Max loss: 0.029148174449801445\n",
      "Min loss: 0.005955680273473263\n",
      "Mean loss: 0.012670906260609627\n",
      "Std loss: 0.007600913262977528\n",
      "Total Loss: 0.07602543756365776\n",
      "------------------------------------ epoch 6809 (40848 steps) ------------------------------------\n",
      "Max loss: 0.022671565413475037\n",
      "Min loss: 0.0043588364496827126\n",
      "Mean loss: 0.010500714415684342\n",
      "Std loss: 0.006041838300958003\n",
      "Total Loss: 0.06300428649410605\n",
      "------------------------------------ epoch 6810 (40854 steps) ------------------------------------\n",
      "Max loss: 0.022663218900561333\n",
      "Min loss: 0.0056489212438464165\n",
      "Mean loss: 0.011708517636482915\n",
      "Std loss: 0.006946272160984036\n",
      "Total Loss: 0.07025110581889749\n",
      "------------------------------------ epoch 6811 (40860 steps) ------------------------------------\n",
      "Max loss: 0.028945613652467728\n",
      "Min loss: 0.008018232882022858\n",
      "Mean loss: 0.01597625824312369\n",
      "Std loss: 0.007480734335866332\n",
      "Total Loss: 0.09585754945874214\n",
      "------------------------------------ epoch 6812 (40866 steps) ------------------------------------\n",
      "Max loss: 0.024326084181666374\n",
      "Min loss: 0.00919613242149353\n",
      "Mean loss: 0.015150366971890131\n",
      "Std loss: 0.0052892151245273735\n",
      "Total Loss: 0.09090220183134079\n",
      "------------------------------------ epoch 6813 (40872 steps) ------------------------------------\n",
      "Max loss: 0.024090897291898727\n",
      "Min loss: 0.007616727612912655\n",
      "Mean loss: 0.013611477023611466\n",
      "Std loss: 0.006431823648314796\n",
      "Total Loss: 0.0816688621416688\n",
      "------------------------------------ epoch 6814 (40878 steps) ------------------------------------\n",
      "Max loss: 0.014971746131777763\n",
      "Min loss: 0.005029299296438694\n",
      "Mean loss: 0.008049399126321077\n",
      "Std loss: 0.003232763450211721\n",
      "Total Loss: 0.048296394757926464\n",
      "------------------------------------ epoch 6815 (40884 steps) ------------------------------------\n",
      "Max loss: 0.013648073188960552\n",
      "Min loss: 0.005040372721850872\n",
      "Mean loss: 0.007702304205546777\n",
      "Std loss: 0.0028746391419680033\n",
      "Total Loss: 0.04621382523328066\n",
      "------------------------------------ epoch 6816 (40890 steps) ------------------------------------\n",
      "Max loss: 0.013740235939621925\n",
      "Min loss: 0.004554977174848318\n",
      "Mean loss: 0.007360089803114533\n",
      "Std loss: 0.0030319464721489655\n",
      "Total Loss: 0.0441605388186872\n",
      "------------------------------------ epoch 6817 (40896 steps) ------------------------------------\n",
      "Max loss: 0.038147252053022385\n",
      "Min loss: 0.006990488618612289\n",
      "Mean loss: 0.01401763316243887\n",
      "Std loss: 0.010915339851861041\n",
      "Total Loss: 0.08410579897463322\n",
      "------------------------------------ epoch 6818 (40902 steps) ------------------------------------\n",
      "Max loss: 0.012570001184940338\n",
      "Min loss: 0.004513653926551342\n",
      "Mean loss: 0.008215846959501505\n",
      "Std loss: 0.0030915482605620827\n",
      "Total Loss: 0.04929508175700903\n",
      "------------------------------------ epoch 6819 (40908 steps) ------------------------------------\n",
      "Max loss: 0.018383298069238663\n",
      "Min loss: 0.004658237565308809\n",
      "Mean loss: 0.009386237245053053\n",
      "Std loss: 0.0046505567355921845\n",
      "Total Loss: 0.05631742347031832\n",
      "------------------------------------ epoch 6820 (40914 steps) ------------------------------------\n",
      "Max loss: 0.018084587529301643\n",
      "Min loss: 0.006352604832500219\n",
      "Mean loss: 0.01252663639994959\n",
      "Std loss: 0.004853141736294648\n",
      "Total Loss: 0.07515981839969754\n",
      "------------------------------------ epoch 6821 (40920 steps) ------------------------------------\n",
      "Max loss: 0.010896772146224976\n",
      "Min loss: 0.0043454961851239204\n",
      "Mean loss: 0.00789363270935913\n",
      "Std loss: 0.00226066770660332\n",
      "Total Loss: 0.047361796256154776\n",
      "------------------------------------ epoch 6822 (40926 steps) ------------------------------------\n",
      "Max loss: 0.019128819927573204\n",
      "Min loss: 0.006269179284572601\n",
      "Mean loss: 0.009292006803055605\n",
      "Std loss: 0.0044483074797673355\n",
      "Total Loss: 0.055752040818333626\n",
      "------------------------------------ epoch 6823 (40932 steps) ------------------------------------\n",
      "Max loss: 0.009637776762247086\n",
      "Min loss: 0.005512755364179611\n",
      "Mean loss: 0.007046956491346161\n",
      "Std loss: 0.001392727930685192\n",
      "Total Loss: 0.04228173894807696\n",
      "------------------------------------ epoch 6824 (40938 steps) ------------------------------------\n",
      "Max loss: 0.02187393605709076\n",
      "Min loss: 0.005963983945548534\n",
      "Mean loss: 0.010278719011694193\n",
      "Std loss: 0.005307171895592921\n",
      "Total Loss: 0.06167231407016516\n",
      "------------------------------------ epoch 6825 (40944 steps) ------------------------------------\n",
      "Max loss: 0.01405212190002203\n",
      "Min loss: 0.005400433205068111\n",
      "Mean loss: 0.008489564604436358\n",
      "Std loss: 0.0028925235710722972\n",
      "Total Loss: 0.05093738762661815\n",
      "------------------------------------ epoch 6826 (40950 steps) ------------------------------------\n",
      "Max loss: 0.017940841615200043\n",
      "Min loss: 0.005930100567638874\n",
      "Mean loss: 0.009175221901386976\n",
      "Std loss: 0.004138398841017482\n",
      "Total Loss: 0.05505133140832186\n",
      "------------------------------------ epoch 6827 (40956 steps) ------------------------------------\n",
      "Max loss: 0.007482220884412527\n",
      "Min loss: 0.00423423433676362\n",
      "Mean loss: 0.005842151430745919\n",
      "Std loss: 0.0011726044351916378\n",
      "Total Loss: 0.03505290858447552\n",
      "------------------------------------ epoch 6828 (40962 steps) ------------------------------------\n",
      "Max loss: 0.025700539350509644\n",
      "Min loss: 0.006124085746705532\n",
      "Mean loss: 0.010732805433993539\n",
      "Std loss: 0.00703135903819096\n",
      "Total Loss: 0.06439683260396123\n",
      "------------------------------------ epoch 6829 (40968 steps) ------------------------------------\n",
      "Max loss: 0.038804441690444946\n",
      "Min loss: 0.006994286086410284\n",
      "Mean loss: 0.017404761087770265\n",
      "Std loss: 0.011520905594069934\n",
      "Total Loss: 0.10442856652662158\n",
      "------------------------------------ epoch 6830 (40974 steps) ------------------------------------\n",
      "Max loss: 0.016105076298117638\n",
      "Min loss: 0.00603672256693244\n",
      "Mean loss: 0.00882015711007019\n",
      "Std loss: 0.0038661920947172792\n",
      "Total Loss: 0.05292094266042113\n",
      "------------------------------------ epoch 6831 (40980 steps) ------------------------------------\n",
      "Max loss: 0.020169157534837723\n",
      "Min loss: 0.006550265476107597\n",
      "Mean loss: 0.011437903468807539\n",
      "Std loss: 0.0049947121890877\n",
      "Total Loss: 0.06862742081284523\n",
      "------------------------------------ epoch 6832 (40986 steps) ------------------------------------\n",
      "Max loss: 0.012342127971351147\n",
      "Min loss: 0.004059282597154379\n",
      "Mean loss: 0.007233270211145282\n",
      "Std loss: 0.0030892020964997267\n",
      "Total Loss: 0.04339962126687169\n",
      "------------------------------------ epoch 6833 (40992 steps) ------------------------------------\n",
      "Max loss: 0.012317906133830547\n",
      "Min loss: 0.004064461216330528\n",
      "Mean loss: 0.008261665313815078\n",
      "Std loss: 0.0031775437001233045\n",
      "Total Loss: 0.04956999188289046\n",
      "------------------------------------ epoch 6834 (40998 steps) ------------------------------------\n",
      "Max loss: 0.016383502632379532\n",
      "Min loss: 0.005248981527984142\n",
      "Mean loss: 0.01050553098320961\n",
      "Std loss: 0.004486596769976787\n",
      "Total Loss: 0.06303318589925766\n",
      "------------------------------------ epoch 6835 (41004 steps) ------------------------------------\n",
      "Max loss: 0.013226449489593506\n",
      "Min loss: 0.005505051463842392\n",
      "Mean loss: 0.00919132400304079\n",
      "Std loss: 0.0029187552706260024\n",
      "Total Loss: 0.05514794401824474\n",
      "------------------------------------ epoch 6836 (41010 steps) ------------------------------------\n",
      "Max loss: 0.01227122824639082\n",
      "Min loss: 0.004145502112805843\n",
      "Mean loss: 0.007952268157775203\n",
      "Std loss: 0.0029650318005278492\n",
      "Total Loss: 0.04771360894665122\n",
      "------------------------------------ epoch 6837 (41016 steps) ------------------------------------\n",
      "Max loss: 0.020987000316381454\n",
      "Min loss: 0.004979508928954601\n",
      "Mean loss: 0.008758524898439646\n",
      "Std loss: 0.005614971984953733\n",
      "Total Loss: 0.052551149390637875\n",
      "------------------------------------ epoch 6838 (41022 steps) ------------------------------------\n",
      "Max loss: 0.013991070911288261\n",
      "Min loss: 0.004974172916263342\n",
      "Mean loss: 0.008715529615680376\n",
      "Std loss: 0.0031984120569159384\n",
      "Total Loss: 0.05229317769408226\n",
      "------------------------------------ epoch 6839 (41028 steps) ------------------------------------\n",
      "Max loss: 0.014584536664187908\n",
      "Min loss: 0.005619147792458534\n",
      "Mean loss: 0.00915409803080062\n",
      "Std loss: 0.0035054953630161254\n",
      "Total Loss: 0.054924588184803724\n",
      "------------------------------------ epoch 6840 (41034 steps) ------------------------------------\n",
      "Max loss: 0.019125014543533325\n",
      "Min loss: 0.004202051553875208\n",
      "Mean loss: 0.00973575709698101\n",
      "Std loss: 0.005198438671828618\n",
      "Total Loss: 0.05841454258188605\n",
      "------------------------------------ epoch 6841 (41040 steps) ------------------------------------\n",
      "Max loss: 0.015750661492347717\n",
      "Min loss: 0.006390891037881374\n",
      "Mean loss: 0.01062630225593845\n",
      "Std loss: 0.002856932811239397\n",
      "Total Loss: 0.0637578135356307\n",
      "------------------------------------ epoch 6842 (41046 steps) ------------------------------------\n",
      "Max loss: 0.027998369187116623\n",
      "Min loss: 0.00433892197906971\n",
      "Mean loss: 0.012869728573908409\n",
      "Std loss: 0.00822493590378468\n",
      "Total Loss: 0.07721837144345045\n",
      "------------------------------------ epoch 6843 (41052 steps) ------------------------------------\n",
      "Max loss: 0.015613913536071777\n",
      "Min loss: 0.005283030681312084\n",
      "Mean loss: 0.011260707707454761\n",
      "Std loss: 0.00306422364760541\n",
      "Total Loss: 0.06756424624472857\n",
      "------------------------------------ epoch 6844 (41058 steps) ------------------------------------\n",
      "Max loss: 0.02611726149916649\n",
      "Min loss: 0.005259818397462368\n",
      "Mean loss: 0.011158715390289823\n",
      "Std loss: 0.0071360289219578816\n",
      "Total Loss: 0.06695229234173894\n",
      "------------------------------------ epoch 6845 (41064 steps) ------------------------------------\n",
      "Max loss: 0.01445552334189415\n",
      "Min loss: 0.005857772659510374\n",
      "Mean loss: 0.010773581685498357\n",
      "Std loss: 0.002995772561069552\n",
      "Total Loss: 0.06464149011299014\n",
      "------------------------------------ epoch 6846 (41070 steps) ------------------------------------\n",
      "Max loss: 0.02811446785926819\n",
      "Min loss: 0.005414248909801245\n",
      "Mean loss: 0.013084917872523269\n",
      "Std loss: 0.008044438464268484\n",
      "Total Loss: 0.07850950723513961\n",
      "------------------------------------ epoch 6847 (41076 steps) ------------------------------------\n",
      "Max loss: 0.020846804603934288\n",
      "Min loss: 0.004860570654273033\n",
      "Mean loss: 0.012259586791818341\n",
      "Std loss: 0.005309960937660252\n",
      "Total Loss: 0.07355752075091004\n",
      "------------------------------------ epoch 6848 (41082 steps) ------------------------------------\n",
      "Max loss: 0.01765843853354454\n",
      "Min loss: 0.007008406333625317\n",
      "Mean loss: 0.010929906585564217\n",
      "Std loss: 0.0036261939747822473\n",
      "Total Loss: 0.0655794395133853\n",
      "------------------------------------ epoch 6849 (41088 steps) ------------------------------------\n",
      "Max loss: 0.007653211709111929\n",
      "Min loss: 0.006168614141643047\n",
      "Mean loss: 0.00671267284390827\n",
      "Std loss: 0.000519139185252346\n",
      "Total Loss: 0.04027603706344962\n",
      "------------------------------------ epoch 6850 (41094 steps) ------------------------------------\n",
      "Max loss: 0.018682342022657394\n",
      "Min loss: 0.00541095994412899\n",
      "Mean loss: 0.009578139365961155\n",
      "Std loss: 0.004811071808839749\n",
      "Total Loss: 0.057468836195766926\n",
      "------------------------------------ epoch 6851 (41100 steps) ------------------------------------\n",
      "Max loss: 0.015441934578120708\n",
      "Min loss: 0.0044372775591909885\n",
      "Mean loss: 0.009080523702626428\n",
      "Std loss: 0.004477664076981498\n",
      "Total Loss: 0.05448314221575856\n",
      "------------------------------------ epoch 6852 (41106 steps) ------------------------------------\n",
      "Max loss: 0.012976225465536118\n",
      "Min loss: 0.003964669536799192\n",
      "Mean loss: 0.007758278011654814\n",
      "Std loss: 0.0033058682877212054\n",
      "Total Loss: 0.046549668069928885\n",
      "------------------------------------ epoch 6853 (41112 steps) ------------------------------------\n",
      "Max loss: 0.0176384299993515\n",
      "Min loss: 0.005450733006000519\n",
      "Mean loss: 0.008964826818555593\n",
      "Std loss: 0.004093828676502449\n",
      "Total Loss: 0.05378896091133356\n",
      "------------------------------------ epoch 6854 (41118 steps) ------------------------------------\n",
      "Max loss: 0.013442888855934143\n",
      "Min loss: 0.004135432653129101\n",
      "Mean loss: 0.006853747880086303\n",
      "Std loss: 0.0031060200433609526\n",
      "Total Loss: 0.04112248728051782\n",
      "------------------------------------ epoch 6855 (41124 steps) ------------------------------------\n",
      "Max loss: 0.017121585085988045\n",
      "Min loss: 0.005419978406280279\n",
      "Mean loss: 0.007851307047531009\n",
      "Std loss: 0.004154763451985438\n",
      "Total Loss: 0.04710784228518605\n",
      "------------------------------------ epoch 6856 (41130 steps) ------------------------------------\n",
      "Max loss: 0.009935029782354832\n",
      "Min loss: 0.004714705049991608\n",
      "Mean loss: 0.006145966316883762\n",
      "Std loss: 0.0017540998230004495\n",
      "Total Loss: 0.036875797901302576\n",
      "------------------------------------ epoch 6857 (41136 steps) ------------------------------------\n",
      "Max loss: 0.01508417259901762\n",
      "Min loss: 0.008077551610767841\n",
      "Mean loss: 0.011596239016701778\n",
      "Std loss: 0.002539914630547074\n",
      "Total Loss: 0.06957743410021067\n",
      "------------------------------------ epoch 6858 (41142 steps) ------------------------------------\n",
      "Max loss: 0.016598936170339584\n",
      "Min loss: 0.00520770950242877\n",
      "Mean loss: 0.009914449804152051\n",
      "Std loss: 0.004297187341282285\n",
      "Total Loss: 0.05948669882491231\n",
      "------------------------------------ epoch 6859 (41148 steps) ------------------------------------\n",
      "Max loss: 0.032384976744651794\n",
      "Min loss: 0.008606461808085442\n",
      "Mean loss: 0.018010894923160475\n",
      "Std loss: 0.00872252506297014\n",
      "Total Loss: 0.10806536953896284\n",
      "------------------------------------ epoch 6860 (41154 steps) ------------------------------------\n",
      "Max loss: 0.02470158413052559\n",
      "Min loss: 0.005314960144460201\n",
      "Mean loss: 0.01227286116530498\n",
      "Std loss: 0.0068916396022468416\n",
      "Total Loss: 0.07363716699182987\n",
      "------------------------------------ epoch 6861 (41160 steps) ------------------------------------\n",
      "Max loss: 0.011230295524001122\n",
      "Min loss: 0.0046875495463609695\n",
      "Mean loss: 0.007588186223680775\n",
      "Std loss: 0.0021843348583478724\n",
      "Total Loss: 0.045529117342084646\n",
      "------------------------------------ epoch 6862 (41166 steps) ------------------------------------\n",
      "Max loss: 0.013936943374574184\n",
      "Min loss: 0.007779132109135389\n",
      "Mean loss: 0.011003016882265607\n",
      "Std loss: 0.002076940871556162\n",
      "Total Loss: 0.06601810129359365\n",
      "------------------------------------ epoch 6863 (41172 steps) ------------------------------------\n",
      "Max loss: 0.014236606657505035\n",
      "Min loss: 0.005531043745577335\n",
      "Mean loss: 0.009973659568155805\n",
      "Std loss: 0.003567547316638419\n",
      "Total Loss: 0.05984195740893483\n",
      "------------------------------------ epoch 6864 (41178 steps) ------------------------------------\n",
      "Max loss: 0.0196346715092659\n",
      "Min loss: 0.0045143188908696175\n",
      "Mean loss: 0.008741575215632716\n",
      "Std loss: 0.005151264156873612\n",
      "Total Loss: 0.0524494512937963\n",
      "------------------------------------ epoch 6865 (41184 steps) ------------------------------------\n",
      "Max loss: 0.02881983481347561\n",
      "Min loss: 0.006873239763081074\n",
      "Mean loss: 0.01655723371853431\n",
      "Std loss: 0.007349432430701713\n",
      "Total Loss: 0.09934340231120586\n",
      "------------------------------------ epoch 6866 (41190 steps) ------------------------------------\n",
      "Max loss: 0.04487830400466919\n",
      "Min loss: 0.005597475916147232\n",
      "Mean loss: 0.017545518077289064\n",
      "Std loss: 0.013912081019297912\n",
      "Total Loss: 0.10527310846373439\n",
      "------------------------------------ epoch 6867 (41196 steps) ------------------------------------\n",
      "Max loss: 0.02323252335190773\n",
      "Min loss: 0.004685595631599426\n",
      "Mean loss: 0.01025599411999186\n",
      "Std loss: 0.006124794557926134\n",
      "Total Loss: 0.06153596471995115\n",
      "------------------------------------ epoch 6868 (41202 steps) ------------------------------------\n",
      "Max loss: 0.01571650803089142\n",
      "Min loss: 0.007663886062800884\n",
      "Mean loss: 0.012044576462358236\n",
      "Std loss: 0.0026006164060903853\n",
      "Total Loss: 0.07226745877414942\n",
      "------------------------------------ epoch 6869 (41208 steps) ------------------------------------\n",
      "Max loss: 0.024025052785873413\n",
      "Min loss: 0.004679359495639801\n",
      "Mean loss: 0.013196800214548906\n",
      "Std loss: 0.006989876319665332\n",
      "Total Loss: 0.07918080128729343\n",
      "------------------------------------ epoch 6870 (41214 steps) ------------------------------------\n",
      "Max loss: 0.01218593493103981\n",
      "Min loss: 0.005335220601409674\n",
      "Mean loss: 0.008230128713573018\n",
      "Std loss: 0.002470966407902194\n",
      "Total Loss: 0.04938077228143811\n",
      "------------------------------------ epoch 6871 (41220 steps) ------------------------------------\n",
      "Max loss: 0.026889558881521225\n",
      "Min loss: 0.005032106302678585\n",
      "Mean loss: 0.011016267817467451\n",
      "Std loss: 0.007332138252579562\n",
      "Total Loss: 0.0660976069048047\n",
      "------------------------------------ epoch 6872 (41226 steps) ------------------------------------\n",
      "Max loss: 0.01692896895110607\n",
      "Min loss: 0.007183182518929243\n",
      "Mean loss: 0.01048994941326479\n",
      "Std loss: 0.0032512042216362546\n",
      "Total Loss: 0.06293969647958875\n",
      "------------------------------------ epoch 6873 (41232 steps) ------------------------------------\n",
      "Max loss: 0.02161741629242897\n",
      "Min loss: 0.005467621143907309\n",
      "Mean loss: 0.009139542545502385\n",
      "Std loss: 0.005639442308889959\n",
      "Total Loss: 0.05483725527301431\n",
      "------------------------------------ epoch 6874 (41238 steps) ------------------------------------\n",
      "Max loss: 0.013428792357444763\n",
      "Min loss: 0.005926000885665417\n",
      "Mean loss: 0.00880056720537444\n",
      "Std loss: 0.0032284798675842065\n",
      "Total Loss: 0.05280340323224664\n",
      "------------------------------------ epoch 6875 (41244 steps) ------------------------------------\n",
      "Max loss: 0.028745224699378014\n",
      "Min loss: 0.005425034090876579\n",
      "Mean loss: 0.01210812913874785\n",
      "Std loss: 0.007776560144018976\n",
      "Total Loss: 0.0726487748324871\n",
      "------------------------------------ epoch 6876 (41250 steps) ------------------------------------\n",
      "Max loss: 0.01482376642525196\n",
      "Min loss: 0.006660129874944687\n",
      "Mean loss: 0.010732848042001327\n",
      "Std loss: 0.0025975806179003606\n",
      "Total Loss: 0.06439708825200796\n",
      "------------------------------------ epoch 6877 (41256 steps) ------------------------------------\n",
      "Max loss: 0.014805409125983715\n",
      "Min loss: 0.004410579800605774\n",
      "Mean loss: 0.007789523185541232\n",
      "Std loss: 0.0035018104266326413\n",
      "Total Loss: 0.046737139113247395\n",
      "------------------------------------ epoch 6878 (41262 steps) ------------------------------------\n",
      "Max loss: 0.011919955722987652\n",
      "Min loss: 0.004457593895494938\n",
      "Mean loss: 0.007286821259185672\n",
      "Std loss: 0.00273264704798104\n",
      "Total Loss: 0.04372092755511403\n",
      "------------------------------------ epoch 6879 (41268 steps) ------------------------------------\n",
      "Max loss: 0.014422035776078701\n",
      "Min loss: 0.003805033629760146\n",
      "Mean loss: 0.007317477953620255\n",
      "Std loss: 0.0039369803900236285\n",
      "Total Loss: 0.04390486772172153\n",
      "------------------------------------ epoch 6880 (41274 steps) ------------------------------------\n",
      "Max loss: 0.009417864494025707\n",
      "Min loss: 0.0046303789131343365\n",
      "Mean loss: 0.006570844445377588\n",
      "Std loss: 0.0015619876182678591\n",
      "Total Loss: 0.03942506667226553\n",
      "------------------------------------ epoch 6881 (41280 steps) ------------------------------------\n",
      "Max loss: 0.020237848162651062\n",
      "Min loss: 0.0048019858077168465\n",
      "Mean loss: 0.009143645021443566\n",
      "Std loss: 0.005507654096698789\n",
      "Total Loss: 0.054861870128661394\n",
      "------------------------------------ epoch 6882 (41286 steps) ------------------------------------\n",
      "Max loss: 0.022610396146774292\n",
      "Min loss: 0.004605830647051334\n",
      "Mean loss: 0.013296146411448717\n",
      "Std loss: 0.006253463882385575\n",
      "Total Loss: 0.0797768784686923\n",
      "------------------------------------ epoch 6883 (41292 steps) ------------------------------------\n",
      "Max loss: 0.02415645122528076\n",
      "Min loss: 0.004982106387615204\n",
      "Mean loss: 0.011208664005001387\n",
      "Std loss: 0.00788613421820222\n",
      "Total Loss: 0.06725198403000832\n",
      "------------------------------------ epoch 6884 (41298 steps) ------------------------------------\n",
      "Max loss: 0.02945641800761223\n",
      "Min loss: 0.00616732332855463\n",
      "Mean loss: 0.014347244674960772\n",
      "Std loss: 0.008616202726440113\n",
      "Total Loss: 0.08608346804976463\n",
      "------------------------------------ epoch 6885 (41304 steps) ------------------------------------\n",
      "Max loss: 0.013306042179465294\n",
      "Min loss: 0.005314491223543882\n",
      "Mean loss: 0.007651837154602011\n",
      "Std loss: 0.0026526411387334217\n",
      "Total Loss: 0.045911022927612066\n",
      "------------------------------------ epoch 6886 (41310 steps) ------------------------------------\n",
      "Max loss: 0.030255425721406937\n",
      "Min loss: 0.004840158391743898\n",
      "Mean loss: 0.013593700481578708\n",
      "Std loss: 0.009604317516957688\n",
      "Total Loss: 0.08156220288947225\n",
      "------------------------------------ epoch 6887 (41316 steps) ------------------------------------\n",
      "Max loss: 0.019860809668898582\n",
      "Min loss: 0.007462260778993368\n",
      "Mean loss: 0.012543078900004426\n",
      "Std loss: 0.005263593834591831\n",
      "Total Loss: 0.07525847340002656\n",
      "------------------------------------ epoch 6888 (41322 steps) ------------------------------------\n",
      "Max loss: 0.024149714037775993\n",
      "Min loss: 0.005453550722450018\n",
      "Mean loss: 0.010931747810294231\n",
      "Std loss: 0.006794308155484298\n",
      "Total Loss: 0.06559048686176538\n",
      "------------------------------------ epoch 6889 (41328 steps) ------------------------------------\n",
      "Max loss: 0.04993629455566406\n",
      "Min loss: 0.005284495186060667\n",
      "Mean loss: 0.01476830686442554\n",
      "Std loss: 0.015918592244232248\n",
      "Total Loss: 0.08860984118655324\n",
      "------------------------------------ epoch 6890 (41334 steps) ------------------------------------\n",
      "Max loss: 0.021878456696867943\n",
      "Min loss: 0.006760895252227783\n",
      "Mean loss: 0.013662889677410325\n",
      "Std loss: 0.0062461476311952355\n",
      "Total Loss: 0.08197733806446195\n",
      "------------------------------------ epoch 6891 (41340 steps) ------------------------------------\n",
      "Max loss: 0.03946882486343384\n",
      "Min loss: 0.010798095725476742\n",
      "Mean loss: 0.018187503796070814\n",
      "Std loss: 0.009688279861348334\n",
      "Total Loss: 0.10912502277642488\n",
      "------------------------------------ epoch 6892 (41346 steps) ------------------------------------\n",
      "Max loss: 0.021802475675940514\n",
      "Min loss: 0.0052118259482085705\n",
      "Mean loss: 0.011458263034000993\n",
      "Std loss: 0.005991173891490408\n",
      "Total Loss: 0.06874957820400596\n",
      "------------------------------------ epoch 6893 (41352 steps) ------------------------------------\n",
      "Max loss: 0.06372011452913284\n",
      "Min loss: 0.0051648057997226715\n",
      "Mean loss: 0.018551061861217022\n",
      "Std loss: 0.020880519226580824\n",
      "Total Loss: 0.11130637116730213\n",
      "------------------------------------ epoch 6894 (41358 steps) ------------------------------------\n",
      "Max loss: 0.02102569118142128\n",
      "Min loss: 0.0060492935590445995\n",
      "Mean loss: 0.01173041039146483\n",
      "Std loss: 0.004878486842240264\n",
      "Total Loss: 0.07038246234878898\n",
      "------------------------------------ epoch 6895 (41364 steps) ------------------------------------\n",
      "Max loss: 0.026754915714263916\n",
      "Min loss: 0.009159453213214874\n",
      "Mean loss: 0.016804708478351433\n",
      "Std loss: 0.007028707267263033\n",
      "Total Loss: 0.1008282508701086\n",
      "------------------------------------ epoch 6896 (41370 steps) ------------------------------------\n",
      "Max loss: 0.012992030009627342\n",
      "Min loss: 0.006425715051591396\n",
      "Mean loss: 0.008198686332131425\n",
      "Std loss: 0.0022603566124786154\n",
      "Total Loss: 0.04919211799278855\n",
      "------------------------------------ epoch 6897 (41376 steps) ------------------------------------\n",
      "Max loss: 0.017944209277629852\n",
      "Min loss: 0.006116054952144623\n",
      "Mean loss: 0.011878573490927616\n",
      "Std loss: 0.004305910021174408\n",
      "Total Loss: 0.0712714409455657\n",
      "------------------------------------ epoch 6898 (41382 steps) ------------------------------------\n",
      "Max loss: 0.009899340569972992\n",
      "Min loss: 0.005096427630633116\n",
      "Mean loss: 0.006877962887908022\n",
      "Std loss: 0.001875046691513412\n",
      "Total Loss: 0.04126777732744813\n",
      "------------------------------------ epoch 6899 (41388 steps) ------------------------------------\n",
      "Max loss: 0.01874440535902977\n",
      "Min loss: 0.004005090333521366\n",
      "Mean loss: 0.009277145067850748\n",
      "Std loss: 0.004841663673357658\n",
      "Total Loss: 0.05566287040710449\n",
      "------------------------------------ epoch 6900 (41394 steps) ------------------------------------\n",
      "Max loss: 0.026143064722418785\n",
      "Min loss: 0.005510538816452026\n",
      "Mean loss: 0.010277181630954146\n",
      "Std loss: 0.007215848778460814\n",
      "Total Loss: 0.06166308978572488\n",
      "------------------------------------ epoch 6901 (41400 steps) ------------------------------------\n",
      "Max loss: 0.029861172661185265\n",
      "Min loss: 0.006094187032431364\n",
      "Mean loss: 0.016487436136230826\n",
      "Std loss: 0.007679935530580415\n",
      "Total Loss: 0.09892461681738496\n",
      "saved model at ./weights/model_6901.pth\n",
      "------------------------------------ epoch 6902 (41406 steps) ------------------------------------\n",
      "Max loss: 0.03384783864021301\n",
      "Min loss: 0.005834976211190224\n",
      "Mean loss: 0.01569368379811446\n",
      "Std loss: 0.009034218356179166\n",
      "Total Loss: 0.09416210278868675\n",
      "------------------------------------ epoch 6903 (41412 steps) ------------------------------------\n",
      "Max loss: 0.020652031525969505\n",
      "Min loss: 0.007125248201191425\n",
      "Mean loss: 0.012829063149789969\n",
      "Std loss: 0.0050355100902865196\n",
      "Total Loss: 0.07697437889873981\n",
      "------------------------------------ epoch 6904 (41418 steps) ------------------------------------\n",
      "Max loss: 0.023152559995651245\n",
      "Min loss: 0.0062083229422569275\n",
      "Mean loss: 0.0141433613995711\n",
      "Std loss: 0.007420921292888374\n",
      "Total Loss: 0.0848601683974266\n",
      "------------------------------------ epoch 6905 (41424 steps) ------------------------------------\n",
      "Max loss: 0.01664501242339611\n",
      "Min loss: 0.006317727267742157\n",
      "Mean loss: 0.009260449092835188\n",
      "Std loss: 0.003632534306645723\n",
      "Total Loss: 0.05556269455701113\n",
      "------------------------------------ epoch 6906 (41430 steps) ------------------------------------\n",
      "Max loss: 0.015203328803181648\n",
      "Min loss: 0.00477052666246891\n",
      "Mean loss: 0.010023174652208885\n",
      "Std loss: 0.0038534342189210833\n",
      "Total Loss: 0.06013904791325331\n",
      "------------------------------------ epoch 6907 (41436 steps) ------------------------------------\n",
      "Max loss: 0.01109929196536541\n",
      "Min loss: 0.004265304189175367\n",
      "Mean loss: 0.0065698624433328705\n",
      "Std loss: 0.002169211033311965\n",
      "Total Loss: 0.039419174659997225\n",
      "------------------------------------ epoch 6908 (41442 steps) ------------------------------------\n",
      "Max loss: 0.0237085223197937\n",
      "Min loss: 0.0076271407306194305\n",
      "Mean loss: 0.011562046284476915\n",
      "Std loss: 0.005495543569659584\n",
      "Total Loss: 0.0693722777068615\n",
      "------------------------------------ epoch 6909 (41448 steps) ------------------------------------\n",
      "Max loss: 0.011401074938476086\n",
      "Min loss: 0.00406647939234972\n",
      "Mean loss: 0.006905976294850309\n",
      "Std loss: 0.00227255633145064\n",
      "Total Loss: 0.04143585776910186\n",
      "------------------------------------ epoch 6910 (41454 steps) ------------------------------------\n",
      "Max loss: 0.01455378346145153\n",
      "Min loss: 0.005855613853782415\n",
      "Mean loss: 0.01037796625557045\n",
      "Std loss: 0.0027150874259166753\n",
      "Total Loss: 0.06226779753342271\n",
      "------------------------------------ epoch 6911 (41460 steps) ------------------------------------\n",
      "Max loss: 0.015524348244071007\n",
      "Min loss: 0.004704741295427084\n",
      "Mean loss: 0.00801206799224019\n",
      "Std loss: 0.0035499253903579493\n",
      "Total Loss: 0.04807240795344114\n",
      "------------------------------------ epoch 6912 (41466 steps) ------------------------------------\n",
      "Max loss: 0.015667906031012535\n",
      "Min loss: 0.004593485966324806\n",
      "Mean loss: 0.009344880934804678\n",
      "Std loss: 0.003954179632701814\n",
      "Total Loss: 0.05606928560882807\n",
      "------------------------------------ epoch 6913 (41472 steps) ------------------------------------\n",
      "Max loss: 0.01480056717991829\n",
      "Min loss: 0.004230334889143705\n",
      "Mean loss: 0.009018350082139174\n",
      "Std loss: 0.0044704654258980025\n",
      "Total Loss: 0.054110100492835045\n",
      "------------------------------------ epoch 6914 (41478 steps) ------------------------------------\n",
      "Max loss: 0.03958238288760185\n",
      "Min loss: 0.005062340758740902\n",
      "Mean loss: 0.01269565251034995\n",
      "Std loss: 0.01214868108745924\n",
      "Total Loss: 0.0761739150620997\n",
      "------------------------------------ epoch 6915 (41484 steps) ------------------------------------\n",
      "Max loss: 0.02421274036169052\n",
      "Min loss: 0.004972708411514759\n",
      "Mean loss: 0.009306183705727259\n",
      "Std loss: 0.006700098972140967\n",
      "Total Loss: 0.055837102234363556\n",
      "------------------------------------ epoch 6916 (41490 steps) ------------------------------------\n",
      "Max loss: 0.013892164453864098\n",
      "Min loss: 0.004246109630912542\n",
      "Mean loss: 0.008933353781079253\n",
      "Std loss: 0.0032506731243977095\n",
      "Total Loss: 0.053600122686475515\n",
      "------------------------------------ epoch 6917 (41496 steps) ------------------------------------\n",
      "Max loss: 0.026025092229247093\n",
      "Min loss: 0.005004550330340862\n",
      "Mean loss: 0.01095560914836824\n",
      "Std loss: 0.007100909002808347\n",
      "Total Loss: 0.06573365489020944\n",
      "------------------------------------ epoch 6918 (41502 steps) ------------------------------------\n",
      "Max loss: 0.023836661130189896\n",
      "Min loss: 0.006837076973170042\n",
      "Mean loss: 0.010028838335225979\n",
      "Std loss: 0.006182208762716505\n",
      "Total Loss: 0.06017303001135588\n",
      "------------------------------------ epoch 6919 (41508 steps) ------------------------------------\n",
      "Max loss: 0.030550982803106308\n",
      "Min loss: 0.004584190901368856\n",
      "Mean loss: 0.01294077280908823\n",
      "Std loss: 0.010043060457179356\n",
      "Total Loss: 0.07764463685452938\n",
      "------------------------------------ epoch 6920 (41514 steps) ------------------------------------\n",
      "Max loss: 0.012427281588315964\n",
      "Min loss: 0.005363408476114273\n",
      "Mean loss: 0.00856766919605434\n",
      "Std loss: 0.0024067643691208602\n",
      "Total Loss: 0.051406015176326036\n",
      "------------------------------------ epoch 6921 (41520 steps) ------------------------------------\n",
      "Max loss: 0.020303234457969666\n",
      "Min loss: 0.004546714946627617\n",
      "Mean loss: 0.01335719662408034\n",
      "Std loss: 0.005355887894028251\n",
      "Total Loss: 0.08014317974448204\n",
      "------------------------------------ epoch 6922 (41526 steps) ------------------------------------\n",
      "Max loss: 0.016812661662697792\n",
      "Min loss: 0.004878683015704155\n",
      "Mean loss: 0.009204507805407047\n",
      "Std loss: 0.00479331575506281\n",
      "Total Loss: 0.055227046832442284\n",
      "------------------------------------ epoch 6923 (41532 steps) ------------------------------------\n",
      "Max loss: 0.01284614484757185\n",
      "Min loss: 0.0053899819031357765\n",
      "Mean loss: 0.007777505010987322\n",
      "Std loss: 0.002542375798764378\n",
      "Total Loss: 0.04666503006592393\n",
      "------------------------------------ epoch 6924 (41538 steps) ------------------------------------\n",
      "Max loss: 0.012656010687351227\n",
      "Min loss: 0.005959502421319485\n",
      "Mean loss: 0.008999625143284598\n",
      "Std loss: 0.0025189666766819883\n",
      "Total Loss: 0.053997750859707594\n",
      "------------------------------------ epoch 6925 (41544 steps) ------------------------------------\n",
      "Max loss: 0.022794177755713463\n",
      "Min loss: 0.005113208666443825\n",
      "Mean loss: 0.009364162028456727\n",
      "Std loss: 0.006361882356182923\n",
      "Total Loss: 0.056184972170740366\n",
      "------------------------------------ epoch 6926 (41550 steps) ------------------------------------\n",
      "Max loss: 0.017582934349775314\n",
      "Min loss: 0.004220175556838512\n",
      "Mean loss: 0.009134371532127261\n",
      "Std loss: 0.004655475887186371\n",
      "Total Loss: 0.05480622919276357\n",
      "------------------------------------ epoch 6927 (41556 steps) ------------------------------------\n",
      "Max loss: 0.013924318365752697\n",
      "Min loss: 0.005402476526796818\n",
      "Mean loss: 0.009587338427081704\n",
      "Std loss: 0.0033198694116274427\n",
      "Total Loss: 0.057524030562490225\n",
      "------------------------------------ epoch 6928 (41562 steps) ------------------------------------\n",
      "Max loss: 0.013613741844892502\n",
      "Min loss: 0.005218525882810354\n",
      "Mean loss: 0.009285674042378863\n",
      "Std loss: 0.0026048485755226793\n",
      "Total Loss: 0.055714044254273176\n",
      "------------------------------------ epoch 6929 (41568 steps) ------------------------------------\n",
      "Max loss: 0.02124098688364029\n",
      "Min loss: 0.004866848699748516\n",
      "Mean loss: 0.011461912033458551\n",
      "Std loss: 0.005899327612618519\n",
      "Total Loss: 0.0687714722007513\n",
      "------------------------------------ epoch 6930 (41574 steps) ------------------------------------\n",
      "Max loss: 0.016244467347860336\n",
      "Min loss: 0.00437172083184123\n",
      "Mean loss: 0.009014229212577144\n",
      "Std loss: 0.004065868005040129\n",
      "Total Loss: 0.054085375275462866\n",
      "------------------------------------ epoch 6931 (41580 steps) ------------------------------------\n",
      "Max loss: 0.011005109176039696\n",
      "Min loss: 0.004747480154037476\n",
      "Mean loss: 0.006719344217951099\n",
      "Std loss: 0.0021653298973704146\n",
      "Total Loss: 0.040316065307706594\n",
      "------------------------------------ epoch 6932 (41586 steps) ------------------------------------\n",
      "Max loss: 0.012218659743666649\n",
      "Min loss: 0.004999071359634399\n",
      "Mean loss: 0.007162466722850998\n",
      "Std loss: 0.00246260864738565\n",
      "Total Loss: 0.04297480033710599\n",
      "------------------------------------ epoch 6933 (41592 steps) ------------------------------------\n",
      "Max loss: 0.0154044721275568\n",
      "Min loss: 0.00487483898177743\n",
      "Mean loss: 0.008810020129506787\n",
      "Std loss: 0.0038054671004740795\n",
      "Total Loss: 0.05286012077704072\n",
      "------------------------------------ epoch 6934 (41598 steps) ------------------------------------\n",
      "Max loss: 0.020870748907327652\n",
      "Min loss: 0.008446825668215752\n",
      "Mean loss: 0.0156668433919549\n",
      "Std loss: 0.004943410528155252\n",
      "Total Loss: 0.09400106035172939\n",
      "------------------------------------ epoch 6935 (41604 steps) ------------------------------------\n",
      "Max loss: 0.027558987960219383\n",
      "Min loss: 0.004696541000157595\n",
      "Mean loss: 0.010356339393183589\n",
      "Std loss: 0.008054147156648189\n",
      "Total Loss: 0.062138036359101534\n",
      "------------------------------------ epoch 6936 (41610 steps) ------------------------------------\n",
      "Max loss: 0.01917997933924198\n",
      "Min loss: 0.004592013545334339\n",
      "Mean loss: 0.00736711701999108\n",
      "Std loss: 0.005302418687655633\n",
      "Total Loss: 0.04420270211994648\n",
      "------------------------------------ epoch 6937 (41616 steps) ------------------------------------\n",
      "Max loss: 0.021816743537783623\n",
      "Min loss: 0.0049963947385549545\n",
      "Mean loss: 0.010500074985126654\n",
      "Std loss: 0.005697728895583573\n",
      "Total Loss: 0.06300044991075993\n",
      "------------------------------------ epoch 6938 (41622 steps) ------------------------------------\n",
      "Max loss: 0.01095106266438961\n",
      "Min loss: 0.003822385100647807\n",
      "Mean loss: 0.007345658688185115\n",
      "Std loss: 0.002495494348364077\n",
      "Total Loss: 0.044073952129110694\n",
      "------------------------------------ epoch 6939 (41628 steps) ------------------------------------\n",
      "Max loss: 0.025670122355222702\n",
      "Min loss: 0.004072170704603195\n",
      "Mean loss: 0.013209062783668438\n",
      "Std loss: 0.008252376243055904\n",
      "Total Loss: 0.07925437670201063\n",
      "------------------------------------ epoch 6940 (41634 steps) ------------------------------------\n",
      "Max loss: 0.013237923383712769\n",
      "Min loss: 0.004223448224365711\n",
      "Mean loss: 0.010160692812254032\n",
      "Std loss: 0.0031521525991734197\n",
      "Total Loss: 0.06096415687352419\n",
      "------------------------------------ epoch 6941 (41640 steps) ------------------------------------\n",
      "Max loss: 0.008522909134626389\n",
      "Min loss: 0.004459453281015158\n",
      "Mean loss: 0.006990029166142146\n",
      "Std loss: 0.0013910444542546835\n",
      "Total Loss: 0.041940174996852875\n",
      "------------------------------------ epoch 6942 (41646 steps) ------------------------------------\n",
      "Max loss: 0.02364746481180191\n",
      "Min loss: 0.004865249618887901\n",
      "Mean loss: 0.012689051451161504\n",
      "Std loss: 0.007560814285638839\n",
      "Total Loss: 0.07613430870696902\n",
      "------------------------------------ epoch 6943 (41652 steps) ------------------------------------\n",
      "Max loss: 0.03355761244893074\n",
      "Min loss: 0.004442703910171986\n",
      "Mean loss: 0.013646482800443968\n",
      "Std loss: 0.009897737458899374\n",
      "Total Loss: 0.0818788968026638\n",
      "------------------------------------ epoch 6944 (41658 steps) ------------------------------------\n",
      "Max loss: 0.044414766132831573\n",
      "Min loss: 0.005808084737509489\n",
      "Mean loss: 0.017384036056076486\n",
      "Std loss: 0.013201521070180595\n",
      "Total Loss: 0.10430421633645892\n",
      "------------------------------------ epoch 6945 (41664 steps) ------------------------------------\n",
      "Max loss: 0.019684862345457077\n",
      "Min loss: 0.007097186520695686\n",
      "Mean loss: 0.010833738216509422\n",
      "Std loss: 0.004224260137783554\n",
      "Total Loss: 0.06500242929905653\n",
      "------------------------------------ epoch 6946 (41670 steps) ------------------------------------\n",
      "Max loss: 0.019779063761234283\n",
      "Min loss: 0.006415954791009426\n",
      "Mean loss: 0.01094686302045981\n",
      "Std loss: 0.005212253571482582\n",
      "Total Loss: 0.06568117812275887\n",
      "------------------------------------ epoch 6947 (41676 steps) ------------------------------------\n",
      "Max loss: 0.04796255752444267\n",
      "Min loss: 0.0052620042115449905\n",
      "Mean loss: 0.01490639530432721\n",
      "Std loss: 0.015287314711682334\n",
      "Total Loss: 0.08943837182596326\n",
      "------------------------------------ epoch 6948 (41682 steps) ------------------------------------\n",
      "Max loss: 0.033411286771297455\n",
      "Min loss: 0.007654845714569092\n",
      "Mean loss: 0.01623973607396086\n",
      "Std loss: 0.008669364183373739\n",
      "Total Loss: 0.09743841644376516\n",
      "------------------------------------ epoch 6949 (41688 steps) ------------------------------------\n",
      "Max loss: 0.015078454278409481\n",
      "Min loss: 0.0051876576617360115\n",
      "Mean loss: 0.008775896858423948\n",
      "Std loss: 0.0040911377623592425\n",
      "Total Loss: 0.05265538115054369\n",
      "------------------------------------ epoch 6950 (41694 steps) ------------------------------------\n",
      "Max loss: 0.012483883649110794\n",
      "Min loss: 0.004247936885803938\n",
      "Mean loss: 0.007777719525620341\n",
      "Std loss: 0.0026615235538020292\n",
      "Total Loss: 0.04666631715372205\n",
      "------------------------------------ epoch 6951 (41700 steps) ------------------------------------\n",
      "Max loss: 0.012145087122917175\n",
      "Min loss: 0.005674820393323898\n",
      "Mean loss: 0.007653771201148629\n",
      "Std loss: 0.0021085633801338237\n",
      "Total Loss: 0.045922627206891775\n",
      "------------------------------------ epoch 6952 (41706 steps) ------------------------------------\n",
      "Max loss: 0.03417249768972397\n",
      "Min loss: 0.0037350221537053585\n",
      "Mean loss: 0.01515802756572763\n",
      "Std loss: 0.010361298460573223\n",
      "Total Loss: 0.09094816539436579\n",
      "------------------------------------ epoch 6953 (41712 steps) ------------------------------------\n",
      "Max loss: 0.032522767782211304\n",
      "Min loss: 0.005658544600009918\n",
      "Mean loss: 0.013754377607256174\n",
      "Std loss: 0.009473394155208975\n",
      "Total Loss: 0.08252626564353704\n",
      "------------------------------------ epoch 6954 (41718 steps) ------------------------------------\n",
      "Max loss: 0.0238163061439991\n",
      "Min loss: 0.009848621673882008\n",
      "Mean loss: 0.014595724642276764\n",
      "Std loss: 0.004886943472364814\n",
      "Total Loss: 0.08757434785366058\n",
      "------------------------------------ epoch 6955 (41724 steps) ------------------------------------\n",
      "Max loss: 0.009788641706109047\n",
      "Min loss: 0.005136778112500906\n",
      "Mean loss: 0.00737327787404259\n",
      "Std loss: 0.0014727429640831252\n",
      "Total Loss: 0.04423966724425554\n",
      "------------------------------------ epoch 6956 (41730 steps) ------------------------------------\n",
      "Max loss: 0.02338869869709015\n",
      "Min loss: 0.00507222767919302\n",
      "Mean loss: 0.009078307310119271\n",
      "Std loss: 0.006481935566483049\n",
      "Total Loss: 0.05446984386071563\n",
      "------------------------------------ epoch 6957 (41736 steps) ------------------------------------\n",
      "Max loss: 0.01934753730893135\n",
      "Min loss: 0.005536624230444431\n",
      "Mean loss: 0.010522548264513413\n",
      "Std loss: 0.004316391839081421\n",
      "Total Loss: 0.06313528958708048\n",
      "------------------------------------ epoch 6958 (41742 steps) ------------------------------------\n",
      "Max loss: 0.025760158896446228\n",
      "Min loss: 0.005171533208340406\n",
      "Mean loss: 0.012185523364072045\n",
      "Std loss: 0.0069374173123786425\n",
      "Total Loss: 0.07311314018443227\n",
      "------------------------------------ epoch 6959 (41748 steps) ------------------------------------\n",
      "Max loss: 0.02484409511089325\n",
      "Min loss: 0.005374141037464142\n",
      "Mean loss: 0.014815613472213348\n",
      "Std loss: 0.007191741787593823\n",
      "Total Loss: 0.08889368083328009\n",
      "------------------------------------ epoch 6960 (41754 steps) ------------------------------------\n",
      "Max loss: 0.046364378184080124\n",
      "Min loss: 0.005755487829446793\n",
      "Mean loss: 0.016634201320509117\n",
      "Std loss: 0.015012625572816778\n",
      "Total Loss: 0.0998052079230547\n",
      "------------------------------------ epoch 6961 (41760 steps) ------------------------------------\n",
      "Max loss: 0.03053232654929161\n",
      "Min loss: 0.007625865750014782\n",
      "Mean loss: 0.019516900647431612\n",
      "Std loss: 0.007963776711417942\n",
      "Total Loss: 0.11710140388458967\n",
      "------------------------------------ epoch 6962 (41766 steps) ------------------------------------\n",
      "Max loss: 0.037071119993925095\n",
      "Min loss: 0.006895163562148809\n",
      "Mean loss: 0.015260367731874188\n",
      "Std loss: 0.010403268860202642\n",
      "Total Loss: 0.09156220639124513\n",
      "------------------------------------ epoch 6963 (41772 steps) ------------------------------------\n",
      "Max loss: 0.03281679004430771\n",
      "Min loss: 0.005987265612930059\n",
      "Mean loss: 0.0177401596835504\n",
      "Std loss: 0.008680745833093308\n",
      "Total Loss: 0.10644095810130239\n",
      "------------------------------------ epoch 6964 (41778 steps) ------------------------------------\n",
      "Max loss: 0.011570124886929989\n",
      "Min loss: 0.005920998752117157\n",
      "Mean loss: 0.008808137150481343\n",
      "Std loss: 0.0020518195256615853\n",
      "Total Loss: 0.05284882290288806\n",
      "------------------------------------ epoch 6965 (41784 steps) ------------------------------------\n",
      "Max loss: 0.014274444431066513\n",
      "Min loss: 0.0068046413362026215\n",
      "Mean loss: 0.010210342472419143\n",
      "Std loss: 0.002926230118166994\n",
      "Total Loss: 0.061262054834514856\n",
      "------------------------------------ epoch 6966 (41790 steps) ------------------------------------\n",
      "Max loss: 0.013515360653400421\n",
      "Min loss: 0.006068227346986532\n",
      "Mean loss: 0.00903700509419044\n",
      "Std loss: 0.0031418728949600206\n",
      "Total Loss: 0.05422203056514263\n",
      "------------------------------------ epoch 6967 (41796 steps) ------------------------------------\n",
      "Max loss: 0.014378647319972515\n",
      "Min loss: 0.0045560202561318874\n",
      "Mean loss: 0.009198286104947329\n",
      "Std loss: 0.003959216843186174\n",
      "Total Loss: 0.05518971662968397\n",
      "------------------------------------ epoch 6968 (41802 steps) ------------------------------------\n",
      "Max loss: 0.02159324288368225\n",
      "Min loss: 0.005051649175584316\n",
      "Mean loss: 0.01117487926967442\n",
      "Std loss: 0.0059033162894343\n",
      "Total Loss: 0.06704927561804652\n",
      "------------------------------------ epoch 6969 (41808 steps) ------------------------------------\n",
      "Max loss: 0.021389668807387352\n",
      "Min loss: 0.00588078610599041\n",
      "Mean loss: 0.011664105346426368\n",
      "Std loss: 0.00501008005648827\n",
      "Total Loss: 0.0699846320785582\n",
      "------------------------------------ epoch 6970 (41814 steps) ------------------------------------\n",
      "Max loss: 0.022240018472075462\n",
      "Min loss: 0.004783236421644688\n",
      "Mean loss: 0.009267952137937149\n",
      "Std loss: 0.005989145900982747\n",
      "Total Loss: 0.05560771282762289\n",
      "------------------------------------ epoch 6971 (41820 steps) ------------------------------------\n",
      "Max loss: 0.010264575481414795\n",
      "Min loss: 0.0037930835969746113\n",
      "Mean loss: 0.007399529218673706\n",
      "Std loss: 0.0023543247817629153\n",
      "Total Loss: 0.044397175312042236\n",
      "------------------------------------ epoch 6972 (41826 steps) ------------------------------------\n",
      "Max loss: 0.011805351823568344\n",
      "Min loss: 0.0049804928712546825\n",
      "Mean loss: 0.00794029018531243\n",
      "Std loss: 0.002013649644994136\n",
      "Total Loss: 0.04764174111187458\n",
      "------------------------------------ epoch 6973 (41832 steps) ------------------------------------\n",
      "Max loss: 0.008142626844346523\n",
      "Min loss: 0.004894299898296595\n",
      "Mean loss: 0.006211184508477648\n",
      "Std loss: 0.001102667452882462\n",
      "Total Loss: 0.03726710705086589\n",
      "------------------------------------ epoch 6974 (41838 steps) ------------------------------------\n",
      "Max loss: 0.008673097006976604\n",
      "Min loss: 0.004436683841049671\n",
      "Mean loss: 0.005719026550650597\n",
      "Std loss: 0.0013610883916490857\n",
      "Total Loss: 0.03431415930390358\n",
      "------------------------------------ epoch 6975 (41844 steps) ------------------------------------\n",
      "Max loss: 0.028034783899784088\n",
      "Min loss: 0.0044023082591593266\n",
      "Mean loss: 0.00953452211494247\n",
      "Std loss: 0.0083282520861293\n",
      "Total Loss: 0.05720713268965483\n",
      "------------------------------------ epoch 6976 (41850 steps) ------------------------------------\n",
      "Max loss: 0.015854734927415848\n",
      "Min loss: 0.005176112055778503\n",
      "Mean loss: 0.009529501354942719\n",
      "Std loss: 0.0038422825081077163\n",
      "Total Loss: 0.057177008129656315\n",
      "------------------------------------ epoch 6977 (41856 steps) ------------------------------------\n",
      "Max loss: 0.010015944018959999\n",
      "Min loss: 0.004720484837889671\n",
      "Mean loss: 0.007099763955920935\n",
      "Std loss: 0.0019443623565364554\n",
      "Total Loss: 0.04259858373552561\n",
      "------------------------------------ epoch 6978 (41862 steps) ------------------------------------\n",
      "Max loss: 0.02807285264134407\n",
      "Min loss: 0.0051322742365300655\n",
      "Mean loss: 0.010726643803839883\n",
      "Std loss: 0.008026789849083435\n",
      "Total Loss: 0.0643598628230393\n",
      "------------------------------------ epoch 6979 (41868 steps) ------------------------------------\n",
      "Max loss: 0.014126887544989586\n",
      "Min loss: 0.005684298928827047\n",
      "Mean loss: 0.008594611659646034\n",
      "Std loss: 0.0030696307385549224\n",
      "Total Loss: 0.051567669957876205\n",
      "------------------------------------ epoch 6980 (41874 steps) ------------------------------------\n",
      "Max loss: 0.030766766518354416\n",
      "Min loss: 0.004447538405656815\n",
      "Mean loss: 0.013172937712321678\n",
      "Std loss: 0.008969829272191891\n",
      "Total Loss: 0.07903762627393007\n",
      "------------------------------------ epoch 6981 (41880 steps) ------------------------------------\n",
      "Max loss: 0.016323205083608627\n",
      "Min loss: 0.004078404046595097\n",
      "Mean loss: 0.007809284375980496\n",
      "Std loss: 0.0039856359710758555\n",
      "Total Loss: 0.04685570625588298\n",
      "------------------------------------ epoch 6982 (41886 steps) ------------------------------------\n",
      "Max loss: 0.026269717141985893\n",
      "Min loss: 0.005678180605173111\n",
      "Mean loss: 0.011840959390004476\n",
      "Std loss: 0.007266066008297415\n",
      "Total Loss: 0.07104575634002686\n",
      "------------------------------------ epoch 6983 (41892 steps) ------------------------------------\n",
      "Max loss: 0.014248741790652275\n",
      "Min loss: 0.004943873733282089\n",
      "Mean loss: 0.009822345493982235\n",
      "Std loss: 0.003595181791344687\n",
      "Total Loss: 0.058934072963893414\n",
      "------------------------------------ epoch 6984 (41898 steps) ------------------------------------\n",
      "Max loss: 0.009582694619894028\n",
      "Min loss: 0.004733382724225521\n",
      "Mean loss: 0.007165408227592707\n",
      "Std loss: 0.0016805645395660204\n",
      "Total Loss: 0.04299244936555624\n",
      "------------------------------------ epoch 6985 (41904 steps) ------------------------------------\n",
      "Max loss: 0.014599015936255455\n",
      "Min loss: 0.004333995282649994\n",
      "Mean loss: 0.007904999578992525\n",
      "Std loss: 0.004061007285953046\n",
      "Total Loss: 0.047429997473955154\n",
      "------------------------------------ epoch 6986 (41910 steps) ------------------------------------\n",
      "Max loss: 0.027451610192656517\n",
      "Min loss: 0.005604078061878681\n",
      "Mean loss: 0.01157280469002823\n",
      "Std loss: 0.007403295775986745\n",
      "Total Loss: 0.06943682814016938\n",
      "------------------------------------ epoch 6987 (41916 steps) ------------------------------------\n",
      "Max loss: 0.01080058328807354\n",
      "Min loss: 0.005308025982230902\n",
      "Mean loss: 0.007422568664575617\n",
      "Std loss: 0.002055525279887131\n",
      "Total Loss: 0.0445354119874537\n",
      "------------------------------------ epoch 6988 (41922 steps) ------------------------------------\n",
      "Max loss: 0.014424828812479973\n",
      "Min loss: 0.004992889240384102\n",
      "Mean loss: 0.009860155948748192\n",
      "Std loss: 0.003752432263416818\n",
      "Total Loss: 0.05916093569248915\n",
      "------------------------------------ epoch 6989 (41928 steps) ------------------------------------\n",
      "Max loss: 0.06358000636100769\n",
      "Min loss: 0.005342262797057629\n",
      "Mean loss: 0.01653871724071602\n",
      "Std loss: 0.02107985777897991\n",
      "Total Loss: 0.09923230344429612\n",
      "------------------------------------ epoch 6990 (41934 steps) ------------------------------------\n",
      "Max loss: 0.02319318614900112\n",
      "Min loss: 0.006949424743652344\n",
      "Mean loss: 0.01277622440829873\n",
      "Std loss: 0.0050256819684554544\n",
      "Total Loss: 0.07665734644979239\n",
      "------------------------------------ epoch 6991 (41940 steps) ------------------------------------\n",
      "Max loss: 0.021921761333942413\n",
      "Min loss: 0.0046800728887319565\n",
      "Mean loss: 0.008734011945004264\n",
      "Std loss: 0.006128801005695117\n",
      "Total Loss: 0.05240407167002559\n",
      "------------------------------------ epoch 6992 (41946 steps) ------------------------------------\n",
      "Max loss: 0.02003849297761917\n",
      "Min loss: 0.005215310491621494\n",
      "Mean loss: 0.010911543775970737\n",
      "Std loss: 0.005490022158033045\n",
      "Total Loss: 0.06546926265582442\n",
      "------------------------------------ epoch 6993 (41952 steps) ------------------------------------\n",
      "Max loss: 0.03512218967080116\n",
      "Min loss: 0.00519212894141674\n",
      "Mean loss: 0.014050374506041408\n",
      "Std loss: 0.010964805395032175\n",
      "Total Loss: 0.08430224703624845\n",
      "------------------------------------ epoch 6994 (41958 steps) ------------------------------------\n",
      "Max loss: 0.03035469353199005\n",
      "Min loss: 0.005722110625356436\n",
      "Mean loss: 0.015332349808886647\n",
      "Std loss: 0.010441077810629907\n",
      "Total Loss: 0.09199409885331988\n",
      "------------------------------------ epoch 6995 (41964 steps) ------------------------------------\n",
      "Max loss: 0.013492497615516186\n",
      "Min loss: 0.00584061723202467\n",
      "Mean loss: 0.00908449113679429\n",
      "Std loss: 0.002590481878621831\n",
      "Total Loss: 0.054506946820765734\n",
      "------------------------------------ epoch 6996 (41970 steps) ------------------------------------\n",
      "Max loss: 0.015721529722213745\n",
      "Min loss: 0.005244516767561436\n",
      "Mean loss: 0.00845067366026342\n",
      "Std loss: 0.0035487576260658195\n",
      "Total Loss: 0.050704041961580515\n",
      "------------------------------------ epoch 6997 (41976 steps) ------------------------------------\n",
      "Max loss: 0.02475196123123169\n",
      "Min loss: 0.004216635599732399\n",
      "Mean loss: 0.010899270341421166\n",
      "Std loss: 0.0069102582325285735\n",
      "Total Loss: 0.065395622048527\n",
      "------------------------------------ epoch 6998 (41982 steps) ------------------------------------\n",
      "Max loss: 0.03657104820013046\n",
      "Min loss: 0.005937855690717697\n",
      "Mean loss: 0.014484247891232371\n",
      "Std loss: 0.011129234299727066\n",
      "Total Loss: 0.08690548734739423\n",
      "------------------------------------ epoch 6999 (41988 steps) ------------------------------------\n",
      "Max loss: 0.01971430703997612\n",
      "Min loss: 0.004893130622804165\n",
      "Mean loss: 0.009308629669249058\n",
      "Std loss: 0.00494853468360556\n",
      "Total Loss: 0.05585177801549435\n",
      "------------------------------------ epoch 7000 (41994 steps) ------------------------------------\n",
      "Max loss: 0.01541622169315815\n",
      "Min loss: 0.0056979297660291195\n",
      "Mean loss: 0.009332576378559073\n",
      "Std loss: 0.003255536470856329\n",
      "Total Loss: 0.05599545827135444\n",
      "------------------------------------ epoch 7001 (42000 steps) ------------------------------------\n",
      "Max loss: 0.01611580327153206\n",
      "Min loss: 0.004600981250405312\n",
      "Mean loss: 0.009988669771701097\n",
      "Std loss: 0.004661110589411304\n",
      "Total Loss: 0.059932018630206585\n",
      "saved model at ./weights/model_7001.pth\n",
      "------------------------------------ epoch 7002 (42006 steps) ------------------------------------\n",
      "Max loss: 0.026436250656843185\n",
      "Min loss: 0.005876024719327688\n",
      "Mean loss: 0.01173858189334472\n",
      "Std loss: 0.0071037517629145034\n",
      "Total Loss: 0.07043149136006832\n",
      "------------------------------------ epoch 7003 (42012 steps) ------------------------------------\n",
      "Max loss: 0.023617245256900787\n",
      "Min loss: 0.0040616425685584545\n",
      "Mean loss: 0.009334413645168146\n",
      "Std loss: 0.006582119749642929\n",
      "Total Loss: 0.05600648187100887\n",
      "------------------------------------ epoch 7004 (42018 steps) ------------------------------------\n",
      "Max loss: 0.027028705924749374\n",
      "Min loss: 0.004252595826983452\n",
      "Mean loss: 0.01100868273836871\n",
      "Std loss: 0.008317514515477043\n",
      "Total Loss: 0.06605209643021226\n",
      "------------------------------------ epoch 7005 (42024 steps) ------------------------------------\n",
      "Max loss: 0.014542995020747185\n",
      "Min loss: 0.005483274348080158\n",
      "Mean loss: 0.009255873582636317\n",
      "Std loss: 0.0031493832288820574\n",
      "Total Loss: 0.0555352414958179\n",
      "------------------------------------ epoch 7006 (42030 steps) ------------------------------------\n",
      "Max loss: 0.045470934361219406\n",
      "Min loss: 0.005429603159427643\n",
      "Mean loss: 0.01341658877208829\n",
      "Std loss: 0.014437653354472337\n",
      "Total Loss: 0.08049953263252974\n",
      "------------------------------------ epoch 7007 (42036 steps) ------------------------------------\n",
      "Max loss: 0.026352550834417343\n",
      "Min loss: 0.004964440129697323\n",
      "Mean loss: 0.010924569796770811\n",
      "Std loss: 0.007523172322198332\n",
      "Total Loss: 0.06554741878062487\n",
      "------------------------------------ epoch 7008 (42042 steps) ------------------------------------\n",
      "Max loss: 0.024966469034552574\n",
      "Min loss: 0.004781852476298809\n",
      "Mean loss: 0.011677180028830966\n",
      "Std loss: 0.006761005923759025\n",
      "Total Loss: 0.07006308017298579\n",
      "------------------------------------ epoch 7009 (42048 steps) ------------------------------------\n",
      "Max loss: 0.0656837597489357\n",
      "Min loss: 0.004753941670060158\n",
      "Mean loss: 0.020033344781647127\n",
      "Std loss: 0.021789621908084228\n",
      "Total Loss: 0.12020006868988276\n",
      "------------------------------------ epoch 7010 (42054 steps) ------------------------------------\n",
      "Max loss: 0.012500773184001446\n",
      "Min loss: 0.0041695861145854\n",
      "Mean loss: 0.007547130575403571\n",
      "Std loss: 0.0031256887748988414\n",
      "Total Loss: 0.04528278345242143\n",
      "------------------------------------ epoch 7011 (42060 steps) ------------------------------------\n",
      "Max loss: 0.018581241369247437\n",
      "Min loss: 0.0051742661744356155\n",
      "Mean loss: 0.009597698381791512\n",
      "Std loss: 0.004940843196935684\n",
      "Total Loss: 0.05758619029074907\n",
      "------------------------------------ epoch 7012 (42066 steps) ------------------------------------\n",
      "Max loss: 0.01966281607747078\n",
      "Min loss: 0.004622996784746647\n",
      "Mean loss: 0.009739244201531013\n",
      "Std loss: 0.005350242996187153\n",
      "Total Loss: 0.05843546520918608\n",
      "------------------------------------ epoch 7013 (42072 steps) ------------------------------------\n",
      "Max loss: 0.008037615567445755\n",
      "Min loss: 0.005450105294585228\n",
      "Mean loss: 0.0067375303866962595\n",
      "Std loss: 0.0011734357395645\n",
      "Total Loss: 0.040425182320177555\n",
      "------------------------------------ epoch 7014 (42078 steps) ------------------------------------\n",
      "Max loss: 0.020411117002367973\n",
      "Min loss: 0.003928835969418287\n",
      "Mean loss: 0.009298183411980668\n",
      "Std loss: 0.005272581114277713\n",
      "Total Loss: 0.05578910047188401\n",
      "------------------------------------ epoch 7015 (42084 steps) ------------------------------------\n",
      "Max loss: 0.010214779525995255\n",
      "Min loss: 0.005098428577184677\n",
      "Mean loss: 0.008095453105246028\n",
      "Std loss: 0.0016129279724461582\n",
      "Total Loss: 0.048572718631476164\n",
      "------------------------------------ epoch 7016 (42090 steps) ------------------------------------\n",
      "Max loss: 0.010631997138261795\n",
      "Min loss: 0.0038790227845311165\n",
      "Mean loss: 0.006234515846396486\n",
      "Std loss: 0.0022078871445640083\n",
      "Total Loss: 0.037407095078378916\n",
      "------------------------------------ epoch 7017 (42096 steps) ------------------------------------\n",
      "Max loss: 0.01707630418241024\n",
      "Min loss: 0.004556736908853054\n",
      "Mean loss: 0.00851740688085556\n",
      "Std loss: 0.004432917534467448\n",
      "Total Loss: 0.05110444128513336\n",
      "------------------------------------ epoch 7018 (42102 steps) ------------------------------------\n",
      "Max loss: 0.02050870656967163\n",
      "Min loss: 0.0050972518511116505\n",
      "Mean loss: 0.009078355195621649\n",
      "Std loss: 0.005365393537564638\n",
      "Total Loss: 0.0544701311737299\n",
      "------------------------------------ epoch 7019 (42108 steps) ------------------------------------\n",
      "Max loss: 0.036886781454086304\n",
      "Min loss: 0.0046319784596562386\n",
      "Mean loss: 0.011247364183266958\n",
      "Std loss: 0.011529828875116202\n",
      "Total Loss: 0.06748418509960175\n",
      "------------------------------------ epoch 7020 (42114 steps) ------------------------------------\n",
      "Max loss: 0.02260681986808777\n",
      "Min loss: 0.00436183949932456\n",
      "Mean loss: 0.010399647910768786\n",
      "Std loss: 0.005763028373078666\n",
      "Total Loss: 0.06239788746461272\n",
      "------------------------------------ epoch 7021 (42120 steps) ------------------------------------\n",
      "Max loss: 0.01685531809926033\n",
      "Min loss: 0.004690300673246384\n",
      "Mean loss: 0.00886217582349976\n",
      "Std loss: 0.0037834210953590877\n",
      "Total Loss: 0.053173054940998554\n",
      "------------------------------------ epoch 7022 (42126 steps) ------------------------------------\n",
      "Max loss: 0.018275653943419456\n",
      "Min loss: 0.0044477651827037334\n",
      "Mean loss: 0.009464627519870797\n",
      "Std loss: 0.00576864718088829\n",
      "Total Loss: 0.05678776511922479\n",
      "------------------------------------ epoch 7023 (42132 steps) ------------------------------------\n",
      "Max loss: 0.059382952749729156\n",
      "Min loss: 0.004433310125023127\n",
      "Mean loss: 0.01810807005191843\n",
      "Std loss: 0.01988211043232989\n",
      "Total Loss: 0.10864842031151056\n",
      "------------------------------------ epoch 7024 (42138 steps) ------------------------------------\n",
      "Max loss: 0.012794477865099907\n",
      "Min loss: 0.008373046293854713\n",
      "Mean loss: 0.010602085230251154\n",
      "Std loss: 0.0014842230844499902\n",
      "Total Loss: 0.06361251138150692\n",
      "------------------------------------ epoch 7025 (42144 steps) ------------------------------------\n",
      "Max loss: 0.013308791443705559\n",
      "Min loss: 0.006310572382062674\n",
      "Mean loss: 0.010147820459678769\n",
      "Std loss: 0.002192628309483643\n",
      "Total Loss: 0.060886922758072615\n",
      "------------------------------------ epoch 7026 (42150 steps) ------------------------------------\n",
      "Max loss: 0.01987536810338497\n",
      "Min loss: 0.005638244561851025\n",
      "Mean loss: 0.010429899208247662\n",
      "Std loss: 0.004779048641489726\n",
      "Total Loss: 0.06257939524948597\n",
      "------------------------------------ epoch 7027 (42156 steps) ------------------------------------\n",
      "Max loss: 0.022400889545679092\n",
      "Min loss: 0.0045061856508255005\n",
      "Mean loss: 0.008476416657989224\n",
      "Std loss: 0.006264971739559996\n",
      "Total Loss: 0.05085849994793534\n",
      "------------------------------------ epoch 7028 (42162 steps) ------------------------------------\n",
      "Max loss: 0.0230678990483284\n",
      "Min loss: 0.0064138080924749374\n",
      "Mean loss: 0.01052727565790216\n",
      "Std loss: 0.005763689157454534\n",
      "Total Loss: 0.06316365394741297\n",
      "------------------------------------ epoch 7029 (42168 steps) ------------------------------------\n",
      "Max loss: 0.013859009370207787\n",
      "Min loss: 0.004608347080647945\n",
      "Mean loss: 0.00842049236719807\n",
      "Std loss: 0.003301336988809638\n",
      "Total Loss: 0.05052295420318842\n",
      "------------------------------------ epoch 7030 (42174 steps) ------------------------------------\n",
      "Max loss: 0.009731926023960114\n",
      "Min loss: 0.005459312349557877\n",
      "Mean loss: 0.007101230168094237\n",
      "Std loss: 0.0013508710341390167\n",
      "Total Loss: 0.042607381008565426\n",
      "------------------------------------ epoch 7031 (42180 steps) ------------------------------------\n",
      "Max loss: 0.0120448749512434\n",
      "Min loss: 0.004011832177639008\n",
      "Mean loss: 0.007657092530280352\n",
      "Std loss: 0.002782354421181006\n",
      "Total Loss: 0.04594255518168211\n",
      "------------------------------------ epoch 7032 (42186 steps) ------------------------------------\n",
      "Max loss: 0.0345575213432312\n",
      "Min loss: 0.00442884024232626\n",
      "Mean loss: 0.013837938196957111\n",
      "Std loss: 0.010837068001631893\n",
      "Total Loss: 0.08302762918174267\n",
      "------------------------------------ epoch 7033 (42192 steps) ------------------------------------\n",
      "Max loss: 0.017934119328856468\n",
      "Min loss: 0.0048220036551356316\n",
      "Mean loss: 0.008613732876256108\n",
      "Std loss: 0.00433220100150197\n",
      "Total Loss: 0.05168239725753665\n",
      "------------------------------------ epoch 7034 (42198 steps) ------------------------------------\n",
      "Max loss: 0.013128217309713364\n",
      "Min loss: 0.00549228023737669\n",
      "Mean loss: 0.0075365106264750166\n",
      "Std loss: 0.002612737420320638\n",
      "Total Loss: 0.0452190637588501\n",
      "------------------------------------ epoch 7035 (42204 steps) ------------------------------------\n",
      "Max loss: 0.01461423747241497\n",
      "Min loss: 0.004293072037398815\n",
      "Mean loss: 0.0074271555834760266\n",
      "Std loss: 0.003476857477668444\n",
      "Total Loss: 0.04456293350085616\n",
      "------------------------------------ epoch 7036 (42210 steps) ------------------------------------\n",
      "Max loss: 0.011264674365520477\n",
      "Min loss: 0.004560749512165785\n",
      "Mean loss: 0.007374285099407037\n",
      "Std loss: 0.0024014977740866223\n",
      "Total Loss: 0.04424571059644222\n",
      "------------------------------------ epoch 7037 (42216 steps) ------------------------------------\n",
      "Max loss: 0.011326968669891357\n",
      "Min loss: 0.00446894159540534\n",
      "Mean loss: 0.008090602311616143\n",
      "Std loss: 0.0020435167745805637\n",
      "Total Loss: 0.048543613869696856\n",
      "------------------------------------ epoch 7038 (42222 steps) ------------------------------------\n",
      "Max loss: 0.01722179725766182\n",
      "Min loss: 0.005423054099082947\n",
      "Mean loss: 0.010121434926986694\n",
      "Std loss: 0.004204166081824425\n",
      "Total Loss: 0.060728609561920166\n",
      "------------------------------------ epoch 7039 (42228 steps) ------------------------------------\n",
      "Max loss: 0.03285551816225052\n",
      "Min loss: 0.004276704974472523\n",
      "Mean loss: 0.012289877825727066\n",
      "Std loss: 0.009438699379027398\n",
      "Total Loss: 0.07373926695436239\n",
      "------------------------------------ epoch 7040 (42234 steps) ------------------------------------\n",
      "Max loss: 0.008783667348325253\n",
      "Min loss: 0.0041824812069535255\n",
      "Mean loss: 0.006343482915932934\n",
      "Std loss: 0.0014131639126456802\n",
      "Total Loss: 0.0380608974955976\n",
      "------------------------------------ epoch 7041 (42240 steps) ------------------------------------\n",
      "Max loss: 0.00851793959736824\n",
      "Min loss: 0.003731462638825178\n",
      "Mean loss: 0.006298414043461283\n",
      "Std loss: 0.0018331331238532831\n",
      "Total Loss: 0.0377904842607677\n",
      "------------------------------------ epoch 7042 (42246 steps) ------------------------------------\n",
      "Max loss: 0.018084049224853516\n",
      "Min loss: 0.004810024052858353\n",
      "Mean loss: 0.0090152268918852\n",
      "Std loss: 0.004509037512260177\n",
      "Total Loss: 0.05409136135131121\n",
      "------------------------------------ epoch 7043 (42252 steps) ------------------------------------\n",
      "Max loss: 0.009980706498026848\n",
      "Min loss: 0.004962034523487091\n",
      "Mean loss: 0.006807130761444569\n",
      "Std loss: 0.0015560996578000649\n",
      "Total Loss: 0.04084278456866741\n",
      "------------------------------------ epoch 7044 (42258 steps) ------------------------------------\n",
      "Max loss: 0.014712750911712646\n",
      "Min loss: 0.004627324640750885\n",
      "Mean loss: 0.00837358389981091\n",
      "Std loss: 0.003865066423079785\n",
      "Total Loss: 0.05024150339886546\n",
      "------------------------------------ epoch 7045 (42264 steps) ------------------------------------\n",
      "Max loss: 0.011996130459010601\n",
      "Min loss: 0.005497422069311142\n",
      "Mean loss: 0.009182215668261051\n",
      "Std loss: 0.0026473487399603763\n",
      "Total Loss: 0.05509329400956631\n",
      "------------------------------------ epoch 7046 (42270 steps) ------------------------------------\n",
      "Max loss: 0.01186441071331501\n",
      "Min loss: 0.004745886195451021\n",
      "Mean loss: 0.007310925284400582\n",
      "Std loss: 0.0023229687313129914\n",
      "Total Loss: 0.043865551706403494\n",
      "------------------------------------ epoch 7047 (42276 steps) ------------------------------------\n",
      "Max loss: 0.016719069331884384\n",
      "Min loss: 0.005632289685308933\n",
      "Mean loss: 0.010026039478058616\n",
      "Std loss: 0.004008113177542487\n",
      "Total Loss: 0.0601562368683517\n",
      "------------------------------------ epoch 7048 (42282 steps) ------------------------------------\n",
      "Max loss: 0.008759940974414349\n",
      "Min loss: 0.005586093291640282\n",
      "Mean loss: 0.007054483828445275\n",
      "Std loss: 0.0009496550489209878\n",
      "Total Loss: 0.042326902970671654\n",
      "------------------------------------ epoch 7049 (42288 steps) ------------------------------------\n",
      "Max loss: 0.016812562942504883\n",
      "Min loss: 0.004331226926296949\n",
      "Mean loss: 0.00840192916803062\n",
      "Std loss: 0.00417385837289876\n",
      "Total Loss: 0.05041157500818372\n",
      "------------------------------------ epoch 7050 (42294 steps) ------------------------------------\n",
      "Max loss: 0.03000832349061966\n",
      "Min loss: 0.005110875703394413\n",
      "Mean loss: 0.009777292686824998\n",
      "Std loss: 0.009063550814748219\n",
      "Total Loss: 0.058663756120949984\n",
      "------------------------------------ epoch 7051 (42300 steps) ------------------------------------\n",
      "Max loss: 0.01582511141896248\n",
      "Min loss: 0.004666395951062441\n",
      "Mean loss: 0.007970030031477412\n",
      "Std loss: 0.0037616685684839475\n",
      "Total Loss: 0.04782018018886447\n",
      "------------------------------------ epoch 7052 (42306 steps) ------------------------------------\n",
      "Max loss: 0.009024245664477348\n",
      "Min loss: 0.003817788790911436\n",
      "Mean loss: 0.006199674680829048\n",
      "Std loss: 0.002111475729631406\n",
      "Total Loss: 0.03719804808497429\n",
      "------------------------------------ epoch 7053 (42312 steps) ------------------------------------\n",
      "Max loss: 0.021849755197763443\n",
      "Min loss: 0.004258156754076481\n",
      "Mean loss: 0.009590452769771218\n",
      "Std loss: 0.006126942692397539\n",
      "Total Loss: 0.05754271661862731\n",
      "------------------------------------ epoch 7054 (42318 steps) ------------------------------------\n",
      "Max loss: 0.009712697938084602\n",
      "Min loss: 0.004491583444178104\n",
      "Mean loss: 0.007160260962943236\n",
      "Std loss: 0.001937581554971393\n",
      "Total Loss: 0.042961565777659416\n",
      "------------------------------------ epoch 7055 (42324 steps) ------------------------------------\n",
      "Max loss: 0.011673609726130962\n",
      "Min loss: 0.0036821896210312843\n",
      "Mean loss: 0.006142312660813332\n",
      "Std loss: 0.002731957984448148\n",
      "Total Loss: 0.03685387596487999\n",
      "------------------------------------ epoch 7056 (42330 steps) ------------------------------------\n",
      "Max loss: 0.008794115856289864\n",
      "Min loss: 0.004456093534827232\n",
      "Mean loss: 0.006220227185015877\n",
      "Std loss: 0.001505611714576092\n",
      "Total Loss: 0.03732136311009526\n",
      "------------------------------------ epoch 7057 (42336 steps) ------------------------------------\n",
      "Max loss: 0.04377846047282219\n",
      "Min loss: 0.00612790510058403\n",
      "Mean loss: 0.020098266502221424\n",
      "Std loss: 0.01151284451352162\n",
      "Total Loss: 0.12058959901332855\n",
      "------------------------------------ epoch 7058 (42342 steps) ------------------------------------\n",
      "Max loss: 0.022764120250940323\n",
      "Min loss: 0.00434368010610342\n",
      "Mean loss: 0.0104601487206916\n",
      "Std loss: 0.006084725736549725\n",
      "Total Loss: 0.06276089232414961\n",
      "------------------------------------ epoch 7059 (42348 steps) ------------------------------------\n",
      "Max loss: 0.01332181692123413\n",
      "Min loss: 0.004471256863325834\n",
      "Mean loss: 0.00785687449388206\n",
      "Std loss: 0.003279754667855579\n",
      "Total Loss: 0.04714124696329236\n",
      "------------------------------------ epoch 7060 (42354 steps) ------------------------------------\n",
      "Max loss: 0.018429040908813477\n",
      "Min loss: 0.005928426515311003\n",
      "Mean loss: 0.011998427488530675\n",
      "Std loss: 0.0046630274044552784\n",
      "Total Loss: 0.07199056493118405\n",
      "------------------------------------ epoch 7061 (42360 steps) ------------------------------------\n",
      "Max loss: 0.021038435399532318\n",
      "Min loss: 0.004495648201555014\n",
      "Mean loss: 0.009837822911019126\n",
      "Std loss: 0.005916188467527063\n",
      "Total Loss: 0.05902693746611476\n",
      "------------------------------------ epoch 7062 (42366 steps) ------------------------------------\n",
      "Max loss: 0.015050351619720459\n",
      "Min loss: 0.005214313045144081\n",
      "Mean loss: 0.008447365447257956\n",
      "Std loss: 0.003193715004433875\n",
      "Total Loss: 0.050684192683547735\n",
      "------------------------------------ epoch 7063 (42372 steps) ------------------------------------\n",
      "Max loss: 0.013484900817275047\n",
      "Min loss: 0.004713337868452072\n",
      "Mean loss: 0.007522032829001546\n",
      "Std loss: 0.0030205027762674193\n",
      "Total Loss: 0.045132196974009275\n",
      "------------------------------------ epoch 7064 (42378 steps) ------------------------------------\n",
      "Max loss: 0.011760355904698372\n",
      "Min loss: 0.004754176363348961\n",
      "Mean loss: 0.008118397711465756\n",
      "Std loss: 0.0024468468451907545\n",
      "Total Loss: 0.04871038626879454\n",
      "------------------------------------ epoch 7065 (42384 steps) ------------------------------------\n",
      "Max loss: 0.031030267477035522\n",
      "Min loss: 0.004955149255692959\n",
      "Mean loss: 0.01318557932972908\n",
      "Std loss: 0.00930756489872994\n",
      "Total Loss: 0.07911347597837448\n",
      "------------------------------------ epoch 7066 (42390 steps) ------------------------------------\n",
      "Max loss: 0.012298215180635452\n",
      "Min loss: 0.0036122880410403013\n",
      "Mean loss: 0.007004387017029027\n",
      "Std loss: 0.0029798016024345157\n",
      "Total Loss: 0.04202632210217416\n",
      "------------------------------------ epoch 7067 (42396 steps) ------------------------------------\n",
      "Max loss: 0.018744617700576782\n",
      "Min loss: 0.004134985618293285\n",
      "Mean loss: 0.010155886101226011\n",
      "Std loss: 0.005269140702515953\n",
      "Total Loss: 0.06093531660735607\n",
      "------------------------------------ epoch 7068 (42402 steps) ------------------------------------\n",
      "Max loss: 0.013545266352593899\n",
      "Min loss: 0.0065117995254695415\n",
      "Mean loss: 0.008447540380681554\n",
      "Std loss: 0.0023715700090855066\n",
      "Total Loss: 0.05068524228408933\n",
      "------------------------------------ epoch 7069 (42408 steps) ------------------------------------\n",
      "Max loss: 0.018507808446884155\n",
      "Min loss: 0.005053670611232519\n",
      "Mean loss: 0.010781390049184362\n",
      "Std loss: 0.004221614316552708\n",
      "Total Loss: 0.06468834029510617\n",
      "------------------------------------ epoch 7070 (42414 steps) ------------------------------------\n",
      "Max loss: 0.009085006080567837\n",
      "Min loss: 0.005897826980799437\n",
      "Mean loss: 0.007539068271095554\n",
      "Std loss: 0.0011881030470117049\n",
      "Total Loss: 0.045234409626573324\n",
      "------------------------------------ epoch 7071 (42420 steps) ------------------------------------\n",
      "Max loss: 0.022775065153837204\n",
      "Min loss: 0.0043097431771457195\n",
      "Mean loss: 0.011931980339189371\n",
      "Std loss: 0.007601874913042457\n",
      "Total Loss: 0.07159188203513622\n",
      "------------------------------------ epoch 7072 (42426 steps) ------------------------------------\n",
      "Max loss: 0.017790798097848892\n",
      "Min loss: 0.0061561232432723045\n",
      "Mean loss: 0.009879569367816051\n",
      "Std loss: 0.004226297962779933\n",
      "Total Loss: 0.059277416206896305\n",
      "------------------------------------ epoch 7073 (42432 steps) ------------------------------------\n",
      "Max loss: 0.016736093908548355\n",
      "Min loss: 0.0049400473944842815\n",
      "Mean loss: 0.010392643511295319\n",
      "Std loss: 0.0042083607537657585\n",
      "Total Loss: 0.06235586106777191\n",
      "------------------------------------ epoch 7074 (42438 steps) ------------------------------------\n",
      "Max loss: 0.016875550150871277\n",
      "Min loss: 0.004306773655116558\n",
      "Mean loss: 0.009545676099757353\n",
      "Std loss: 0.004543592649091722\n",
      "Total Loss: 0.05727405659854412\n",
      "------------------------------------ epoch 7075 (42444 steps) ------------------------------------\n",
      "Max loss: 0.017650268971920013\n",
      "Min loss: 0.004572106525301933\n",
      "Mean loss: 0.007592463244994481\n",
      "Std loss: 0.004607313533044406\n",
      "Total Loss: 0.04555477946996689\n",
      "------------------------------------ epoch 7076 (42450 steps) ------------------------------------\n",
      "Max loss: 0.008764250203967094\n",
      "Min loss: 0.00426071509718895\n",
      "Mean loss: 0.00676379802947243\n",
      "Std loss: 0.001821026394112435\n",
      "Total Loss: 0.04058278817683458\n",
      "------------------------------------ epoch 7077 (42456 steps) ------------------------------------\n",
      "Max loss: 0.008542212657630444\n",
      "Min loss: 0.005288106855005026\n",
      "Mean loss: 0.006795323997115095\n",
      "Std loss: 0.0010073589768708469\n",
      "Total Loss: 0.04077194398269057\n",
      "------------------------------------ epoch 7078 (42462 steps) ------------------------------------\n",
      "Max loss: 0.029052376747131348\n",
      "Min loss: 0.0035649631172418594\n",
      "Mean loss: 0.010514426743611693\n",
      "Std loss: 0.008522928853801213\n",
      "Total Loss: 0.06308656046167016\n",
      "------------------------------------ epoch 7079 (42468 steps) ------------------------------------\n",
      "Max loss: 0.01802091673016548\n",
      "Min loss: 0.006136901676654816\n",
      "Mean loss: 0.010516207742815217\n",
      "Std loss: 0.004304186253694045\n",
      "Total Loss: 0.0630972464568913\n",
      "------------------------------------ epoch 7080 (42474 steps) ------------------------------------\n",
      "Max loss: 0.011347013525664806\n",
      "Min loss: 0.005232475697994232\n",
      "Mean loss: 0.007923422614112496\n",
      "Std loss: 0.0020954306221671747\n",
      "Total Loss: 0.04754053568467498\n",
      "------------------------------------ epoch 7081 (42480 steps) ------------------------------------\n",
      "Max loss: 0.009533855132758617\n",
      "Min loss: 0.004815980792045593\n",
      "Mean loss: 0.006550896835202972\n",
      "Std loss: 0.001486522839012242\n",
      "Total Loss: 0.03930538101121783\n",
      "------------------------------------ epoch 7082 (42486 steps) ------------------------------------\n",
      "Max loss: 0.012542881071567535\n",
      "Min loss: 0.004902869462966919\n",
      "Mean loss: 0.00771432804564635\n",
      "Std loss: 0.002756725524726128\n",
      "Total Loss: 0.0462859682738781\n",
      "------------------------------------ epoch 7083 (42492 steps) ------------------------------------\n",
      "Max loss: 0.011403128504753113\n",
      "Min loss: 0.004396372474730015\n",
      "Mean loss: 0.007630439164737861\n",
      "Std loss: 0.002421965318493395\n",
      "Total Loss: 0.04578263498842716\n",
      "------------------------------------ epoch 7084 (42498 steps) ------------------------------------\n",
      "Max loss: 0.02074328437447548\n",
      "Min loss: 0.004564320668578148\n",
      "Mean loss: 0.011552552382151285\n",
      "Std loss: 0.006343139199778169\n",
      "Total Loss: 0.06931531429290771\n",
      "------------------------------------ epoch 7085 (42504 steps) ------------------------------------\n",
      "Max loss: 0.016728173941373825\n",
      "Min loss: 0.004808109253644943\n",
      "Mean loss: 0.009342401598890623\n",
      "Std loss: 0.004309263926343465\n",
      "Total Loss: 0.056054409593343735\n",
      "------------------------------------ epoch 7086 (42510 steps) ------------------------------------\n",
      "Max loss: 0.027562305331230164\n",
      "Min loss: 0.004960894584655762\n",
      "Mean loss: 0.010065456386655569\n",
      "Std loss: 0.007972880433925807\n",
      "Total Loss: 0.060392738319933414\n",
      "------------------------------------ epoch 7087 (42516 steps) ------------------------------------\n",
      "Max loss: 0.011021658778190613\n",
      "Min loss: 0.0049676839262247086\n",
      "Mean loss: 0.0069969789280245704\n",
      "Std loss: 0.002276646220287156\n",
      "Total Loss: 0.04198187356814742\n",
      "------------------------------------ epoch 7088 (42522 steps) ------------------------------------\n",
      "Max loss: 0.011856859549880028\n",
      "Min loss: 0.004955599084496498\n",
      "Mean loss: 0.0072994716465473175\n",
      "Std loss: 0.0022665126092333023\n",
      "Total Loss: 0.043796829879283905\n",
      "------------------------------------ epoch 7089 (42528 steps) ------------------------------------\n",
      "Max loss: 0.03727129101753235\n",
      "Min loss: 0.004563538357615471\n",
      "Mean loss: 0.011291322764009237\n",
      "Std loss: 0.01167157575757196\n",
      "Total Loss: 0.06774793658405542\n",
      "------------------------------------ epoch 7090 (42534 steps) ------------------------------------\n",
      "Max loss: 0.016003120690584183\n",
      "Min loss: 0.004521393217146397\n",
      "Mean loss: 0.01021901285275817\n",
      "Std loss: 0.004122061026391076\n",
      "Total Loss: 0.061314077116549015\n",
      "------------------------------------ epoch 7091 (42540 steps) ------------------------------------\n",
      "Max loss: 0.02410365268588066\n",
      "Min loss: 0.009594143368303776\n",
      "Mean loss: 0.01407168727988998\n",
      "Std loss: 0.004676460081033891\n",
      "Total Loss: 0.08443012367933989\n",
      "------------------------------------ epoch 7092 (42546 steps) ------------------------------------\n",
      "Max loss: 0.01535158883780241\n",
      "Min loss: 0.007295561023056507\n",
      "Mean loss: 0.010908897034823895\n",
      "Std loss: 0.0028541448594774795\n",
      "Total Loss: 0.06545338220894337\n",
      "------------------------------------ epoch 7093 (42552 steps) ------------------------------------\n",
      "Max loss: 0.037506841123104095\n",
      "Min loss: 0.006335807032883167\n",
      "Mean loss: 0.015517923515290022\n",
      "Std loss: 0.010475650953015132\n",
      "Total Loss: 0.09310754109174013\n",
      "------------------------------------ epoch 7094 (42558 steps) ------------------------------------\n",
      "Max loss: 0.031196972355246544\n",
      "Min loss: 0.006698085926473141\n",
      "Mean loss: 0.017937555055444438\n",
      "Std loss: 0.00925334357207737\n",
      "Total Loss: 0.10762533033266664\n",
      "------------------------------------ epoch 7095 (42564 steps) ------------------------------------\n",
      "Max loss: 0.0410756841301918\n",
      "Min loss: 0.006548827048391104\n",
      "Mean loss: 0.01813933680144449\n",
      "Std loss: 0.01320746897323852\n",
      "Total Loss: 0.10883602080866694\n",
      "------------------------------------ epoch 7096 (42570 steps) ------------------------------------\n",
      "Max loss: 0.021162554621696472\n",
      "Min loss: 0.008694319054484367\n",
      "Mean loss: 0.013161146081984043\n",
      "Std loss: 0.00515953125065369\n",
      "Total Loss: 0.07896687649190426\n",
      "------------------------------------ epoch 7097 (42576 steps) ------------------------------------\n",
      "Max loss: 0.041784193366765976\n",
      "Min loss: 0.009932749904692173\n",
      "Mean loss: 0.021748057411362726\n",
      "Std loss: 0.011150625218046184\n",
      "Total Loss: 0.13048834446817636\n",
      "------------------------------------ epoch 7098 (42582 steps) ------------------------------------\n",
      "Max loss: 0.043616555631160736\n",
      "Min loss: 0.006990750785917044\n",
      "Mean loss: 0.016636039518440764\n",
      "Std loss: 0.012852938737982968\n",
      "Total Loss: 0.09981623711064458\n",
      "------------------------------------ epoch 7099 (42588 steps) ------------------------------------\n",
      "Max loss: 0.03262320160865784\n",
      "Min loss: 0.007101118098944426\n",
      "Mean loss: 0.015025608629609147\n",
      "Std loss: 0.008720014719615738\n",
      "Total Loss: 0.09015365177765489\n",
      "------------------------------------ epoch 7100 (42594 steps) ------------------------------------\n",
      "Max loss: 0.01881861314177513\n",
      "Min loss: 0.005763293709605932\n",
      "Mean loss: 0.009121765925859412\n",
      "Std loss: 0.0044194389739282735\n",
      "Total Loss: 0.05473059555515647\n",
      "------------------------------------ epoch 7101 (42600 steps) ------------------------------------\n",
      "Max loss: 0.01398731954395771\n",
      "Min loss: 0.006686256732791662\n",
      "Mean loss: 0.01015885011292994\n",
      "Std loss: 0.0024606975753412607\n",
      "Total Loss: 0.06095310067757964\n",
      "saved model at ./weights/model_7101.pth\n",
      "------------------------------------ epoch 7102 (42606 steps) ------------------------------------\n",
      "Max loss: 0.03316536173224449\n",
      "Min loss: 0.006401129998266697\n",
      "Mean loss: 0.01851090556010604\n",
      "Std loss: 0.01119502610330462\n",
      "Total Loss: 0.11106543336063623\n",
      "------------------------------------ epoch 7103 (42612 steps) ------------------------------------\n",
      "Max loss: 0.023996755480766296\n",
      "Min loss: 0.005745799280703068\n",
      "Mean loss: 0.011580609250813723\n",
      "Std loss: 0.00774600389644066\n",
      "Total Loss: 0.06948365550488234\n",
      "------------------------------------ epoch 7104 (42618 steps) ------------------------------------\n",
      "Max loss: 0.0515400730073452\n",
      "Min loss: 0.0066794282756745815\n",
      "Mean loss: 0.01759409826869766\n",
      "Std loss: 0.01617334812815782\n",
      "Total Loss: 0.10556458961218596\n",
      "------------------------------------ epoch 7105 (42624 steps) ------------------------------------\n",
      "Max loss: 0.018681149929761887\n",
      "Min loss: 0.004896113649010658\n",
      "Mean loss: 0.01100605105360349\n",
      "Std loss: 0.004236885889686552\n",
      "Total Loss: 0.06603630632162094\n",
      "------------------------------------ epoch 7106 (42630 steps) ------------------------------------\n",
      "Max loss: 0.01786063238978386\n",
      "Min loss: 0.005566954147070646\n",
      "Mean loss: 0.009527105061958233\n",
      "Std loss: 0.004214177062712899\n",
      "Total Loss: 0.0571626303717494\n",
      "------------------------------------ epoch 7107 (42636 steps) ------------------------------------\n",
      "Max loss: 0.011210369877517223\n",
      "Min loss: 0.005284424871206284\n",
      "Mean loss: 0.008142131613567472\n",
      "Std loss: 0.0018454264463950217\n",
      "Total Loss: 0.04885278968140483\n",
      "------------------------------------ epoch 7108 (42642 steps) ------------------------------------\n",
      "Max loss: 0.021541856229305267\n",
      "Min loss: 0.005189795978367329\n",
      "Mean loss: 0.011191778971503178\n",
      "Std loss: 0.00625497011568135\n",
      "Total Loss: 0.06715067382901907\n",
      "------------------------------------ epoch 7109 (42648 steps) ------------------------------------\n",
      "Max loss: 0.05388524383306503\n",
      "Min loss: 0.005002973135560751\n",
      "Mean loss: 0.017573733115568757\n",
      "Std loss: 0.016579698530035953\n",
      "Total Loss: 0.10544239869341254\n",
      "------------------------------------ epoch 7110 (42654 steps) ------------------------------------\n",
      "Max loss: 0.012934970669448376\n",
      "Min loss: 0.005637214984744787\n",
      "Mean loss: 0.01025203064394494\n",
      "Std loss: 0.002606388730373777\n",
      "Total Loss: 0.061512183863669634\n",
      "------------------------------------ epoch 7111 (42660 steps) ------------------------------------\n",
      "Max loss: 0.06507155299186707\n",
      "Min loss: 0.005862213205546141\n",
      "Mean loss: 0.01832261324549715\n",
      "Std loss: 0.021033621777950498\n",
      "Total Loss: 0.10993567947298288\n",
      "------------------------------------ epoch 7112 (42666 steps) ------------------------------------\n",
      "Max loss: 0.012813447043299675\n",
      "Min loss: 0.006545281037688255\n",
      "Mean loss: 0.008711670758202672\n",
      "Std loss: 0.00229658607374033\n",
      "Total Loss: 0.05227002454921603\n",
      "------------------------------------ epoch 7113 (42672 steps) ------------------------------------\n",
      "Max loss: 0.026471786201000214\n",
      "Min loss: 0.006257172673940659\n",
      "Mean loss: 0.011491886107251048\n",
      "Std loss: 0.007243360345563808\n",
      "Total Loss: 0.06895131664350629\n",
      "------------------------------------ epoch 7114 (42678 steps) ------------------------------------\n",
      "Max loss: 0.011345818638801575\n",
      "Min loss: 0.005927233956754208\n",
      "Mean loss: 0.008454281060645977\n",
      "Std loss: 0.0017479300752987108\n",
      "Total Loss: 0.050725686363875866\n",
      "------------------------------------ epoch 7115 (42684 steps) ------------------------------------\n",
      "Max loss: 0.013641192577779293\n",
      "Min loss: 0.005559494253247976\n",
      "Mean loss: 0.008731674791003266\n",
      "Std loss: 0.003009685892318026\n",
      "Total Loss: 0.0523900487460196\n",
      "------------------------------------ epoch 7116 (42690 steps) ------------------------------------\n",
      "Max loss: 0.010532578453421593\n",
      "Min loss: 0.004030343610793352\n",
      "Mean loss: 0.008082660768801967\n",
      "Std loss: 0.0022811885004405817\n",
      "Total Loss: 0.048495964612811804\n",
      "------------------------------------ epoch 7117 (42696 steps) ------------------------------------\n",
      "Max loss: 0.01891762763261795\n",
      "Min loss: 0.003573962952941656\n",
      "Mean loss: 0.0116240701948603\n",
      "Std loss: 0.00549018959067515\n",
      "Total Loss: 0.0697444211691618\n",
      "------------------------------------ epoch 7118 (42702 steps) ------------------------------------\n",
      "Max loss: 0.014164574444293976\n",
      "Min loss: 0.005475608166307211\n",
      "Mean loss: 0.009149206258977452\n",
      "Std loss: 0.003578519045807071\n",
      "Total Loss: 0.05489523755386472\n",
      "------------------------------------ epoch 7119 (42708 steps) ------------------------------------\n",
      "Max loss: 0.04675224423408508\n",
      "Min loss: 0.004353261552751064\n",
      "Mean loss: 0.01275873277336359\n",
      "Std loss: 0.015251173817578739\n",
      "Total Loss: 0.07655239664018154\n",
      "------------------------------------ epoch 7120 (42714 steps) ------------------------------------\n",
      "Max loss: 0.014440460130572319\n",
      "Min loss: 0.00495653972029686\n",
      "Mean loss: 0.009460920079921683\n",
      "Std loss: 0.0036152335875399557\n",
      "Total Loss: 0.056765520479530096\n",
      "------------------------------------ epoch 7121 (42720 steps) ------------------------------------\n",
      "Max loss: 0.015103228390216827\n",
      "Min loss: 0.0042586131021380424\n",
      "Mean loss: 0.008174015985180935\n",
      "Std loss: 0.00419318996756948\n",
      "Total Loss: 0.049044095911085606\n",
      "------------------------------------ epoch 7122 (42726 steps) ------------------------------------\n",
      "Max loss: 0.012907132506370544\n",
      "Min loss: 0.005762956105172634\n",
      "Mean loss: 0.008450112460801998\n",
      "Std loss: 0.0022583065572555493\n",
      "Total Loss: 0.05070067476481199\n",
      "------------------------------------ epoch 7123 (42732 steps) ------------------------------------\n",
      "Max loss: 0.022511890158057213\n",
      "Min loss: 0.00405573844909668\n",
      "Mean loss: 0.01115716970525682\n",
      "Std loss: 0.006654731352667642\n",
      "Total Loss: 0.06694301823154092\n",
      "------------------------------------ epoch 7124 (42738 steps) ------------------------------------\n",
      "Max loss: 0.008161856792867184\n",
      "Min loss: 0.004215113818645477\n",
      "Mean loss: 0.005960229939470689\n",
      "Std loss: 0.001553168489691409\n",
      "Total Loss: 0.03576137963682413\n",
      "------------------------------------ epoch 7125 (42744 steps) ------------------------------------\n",
      "Max loss: 0.034056633710861206\n",
      "Min loss: 0.004615784622728825\n",
      "Mean loss: 0.011628839963426193\n",
      "Std loss: 0.010205171757395807\n",
      "Total Loss: 0.06977303978055716\n",
      "------------------------------------ epoch 7126 (42750 steps) ------------------------------------\n",
      "Max loss: 0.008389717899262905\n",
      "Min loss: 0.004263656679540873\n",
      "Mean loss: 0.005841887167965372\n",
      "Std loss: 0.00149728334383185\n",
      "Total Loss: 0.035051323007792234\n",
      "------------------------------------ epoch 7127 (42756 steps) ------------------------------------\n",
      "Max loss: 0.06014290452003479\n",
      "Min loss: 0.0045808544382452965\n",
      "Mean loss: 0.017366872479518253\n",
      "Std loss: 0.019596687303513462\n",
      "Total Loss: 0.10420123487710953\n",
      "------------------------------------ epoch 7128 (42762 steps) ------------------------------------\n",
      "Max loss: 0.009524881839752197\n",
      "Min loss: 0.004547208547592163\n",
      "Mean loss: 0.006584776332601905\n",
      "Std loss: 0.0017430021456196184\n",
      "Total Loss: 0.03950865799561143\n",
      "------------------------------------ epoch 7129 (42768 steps) ------------------------------------\n",
      "Max loss: 0.014310058206319809\n",
      "Min loss: 0.004687291570007801\n",
      "Mean loss: 0.008199913892894983\n",
      "Std loss: 0.00402418021089394\n",
      "Total Loss: 0.0491994833573699\n",
      "------------------------------------ epoch 7130 (42774 steps) ------------------------------------\n",
      "Max loss: 0.03227414935827255\n",
      "Min loss: 0.0037413444370031357\n",
      "Mean loss: 0.013263403127590815\n",
      "Std loss: 0.011595703543097888\n",
      "Total Loss: 0.07958041876554489\n",
      "------------------------------------ epoch 7131 (42780 steps) ------------------------------------\n",
      "Max loss: 0.021588444709777832\n",
      "Min loss: 0.005037791561335325\n",
      "Mean loss: 0.00961935450322926\n",
      "Std loss: 0.005653381151226446\n",
      "Total Loss: 0.05771612701937556\n",
      "------------------------------------ epoch 7132 (42786 steps) ------------------------------------\n",
      "Max loss: 0.010787403210997581\n",
      "Min loss: 0.007128437515348196\n",
      "Mean loss: 0.008129013624663154\n",
      "Std loss: 0.0012181966824489794\n",
      "Total Loss: 0.048774081747978926\n",
      "------------------------------------ epoch 7133 (42792 steps) ------------------------------------\n",
      "Max loss: 0.028136882930994034\n",
      "Min loss: 0.004170354921370745\n",
      "Mean loss: 0.013394436721379558\n",
      "Std loss: 0.008570788958161345\n",
      "Total Loss: 0.08036662032827735\n",
      "------------------------------------ epoch 7134 (42798 steps) ------------------------------------\n",
      "Max loss: 0.013673201203346252\n",
      "Min loss: 0.004726634826511145\n",
      "Mean loss: 0.007956403540447354\n",
      "Std loss: 0.0029578135878329226\n",
      "Total Loss: 0.047738421242684126\n",
      "------------------------------------ epoch 7135 (42804 steps) ------------------------------------\n",
      "Max loss: 0.015785083174705505\n",
      "Min loss: 0.004705511964857578\n",
      "Mean loss: 0.007605186818788449\n",
      "Std loss: 0.0037904236798937596\n",
      "Total Loss: 0.045631120912730694\n",
      "------------------------------------ epoch 7136 (42810 steps) ------------------------------------\n",
      "Max loss: 0.01340343989431858\n",
      "Min loss: 0.004034465178847313\n",
      "Mean loss: 0.006719580696274837\n",
      "Std loss: 0.0031742875820844883\n",
      "Total Loss: 0.04031748417764902\n",
      "------------------------------------ epoch 7137 (42816 steps) ------------------------------------\n",
      "Max loss: 0.014872397296130657\n",
      "Min loss: 0.005738798528909683\n",
      "Mean loss: 0.009798391566922268\n",
      "Std loss: 0.0036156482990101608\n",
      "Total Loss: 0.058790349401533604\n",
      "------------------------------------ epoch 7138 (42822 steps) ------------------------------------\n",
      "Max loss: 0.01134934276342392\n",
      "Min loss: 0.005852167494595051\n",
      "Mean loss: 0.008042679789165655\n",
      "Std loss: 0.002263243205636366\n",
      "Total Loss: 0.048256078734993935\n",
      "------------------------------------ epoch 7139 (42828 steps) ------------------------------------\n",
      "Max loss: 0.03106217086315155\n",
      "Min loss: 0.0045321350917220116\n",
      "Mean loss: 0.012630222598090768\n",
      "Std loss: 0.008631163280636745\n",
      "Total Loss: 0.07578133558854461\n",
      "------------------------------------ epoch 7140 (42834 steps) ------------------------------------\n",
      "Max loss: 0.01537871640175581\n",
      "Min loss: 0.0046973321586847305\n",
      "Mean loss: 0.009841560541341702\n",
      "Std loss: 0.004116407755746347\n",
      "Total Loss: 0.05904936324805021\n",
      "------------------------------------ epoch 7141 (42840 steps) ------------------------------------\n",
      "Max loss: 0.029475554823875427\n",
      "Min loss: 0.0055311741307377815\n",
      "Mean loss: 0.012665941147133708\n",
      "Std loss: 0.008151393006928211\n",
      "Total Loss: 0.07599564688280225\n",
      "------------------------------------ epoch 7142 (42846 steps) ------------------------------------\n",
      "Max loss: 0.018398893997073174\n",
      "Min loss: 0.004633978940546513\n",
      "Mean loss: 0.008501217467710376\n",
      "Std loss: 0.00478685607085563\n",
      "Total Loss: 0.051007304806262255\n",
      "------------------------------------ epoch 7143 (42852 steps) ------------------------------------\n",
      "Max loss: 0.010963348671793938\n",
      "Min loss: 0.004823978524655104\n",
      "Mean loss: 0.00812192812251548\n",
      "Std loss: 0.0019179027123613383\n",
      "Total Loss: 0.04873156873509288\n",
      "------------------------------------ epoch 7144 (42858 steps) ------------------------------------\n",
      "Max loss: 0.021619407460093498\n",
      "Min loss: 0.0039040311239659786\n",
      "Mean loss: 0.009162677529578408\n",
      "Std loss: 0.006810098589176359\n",
      "Total Loss: 0.054976065177470446\n",
      "------------------------------------ epoch 7145 (42864 steps) ------------------------------------\n",
      "Max loss: 0.022830035537481308\n",
      "Min loss: 0.005655247252434492\n",
      "Mean loss: 0.011660731009518107\n",
      "Std loss: 0.005506973013406491\n",
      "Total Loss: 0.06996438605710864\n",
      "------------------------------------ epoch 7146 (42870 steps) ------------------------------------\n",
      "Max loss: 0.010684223845601082\n",
      "Min loss: 0.004373474977910519\n",
      "Mean loss: 0.006182031007483602\n",
      "Std loss: 0.002122809005492188\n",
      "Total Loss: 0.03709218604490161\n",
      "------------------------------------ epoch 7147 (42876 steps) ------------------------------------\n",
      "Max loss: 0.015372306108474731\n",
      "Min loss: 0.005691138561815023\n",
      "Mean loss: 0.008706298191100359\n",
      "Std loss: 0.003857087406199198\n",
      "Total Loss: 0.052237789146602154\n",
      "------------------------------------ epoch 7148 (42882 steps) ------------------------------------\n",
      "Max loss: 0.012117783538997173\n",
      "Min loss: 0.0042950063943862915\n",
      "Mean loss: 0.0077714041496316595\n",
      "Std loss: 0.0029688386577958353\n",
      "Total Loss: 0.046628424897789955\n",
      "------------------------------------ epoch 7149 (42888 steps) ------------------------------------\n",
      "Max loss: 0.024075891822576523\n",
      "Min loss: 0.005161975044757128\n",
      "Mean loss: 0.010720847019304832\n",
      "Std loss: 0.00677155982840977\n",
      "Total Loss: 0.06432508211582899\n",
      "------------------------------------ epoch 7150 (42894 steps) ------------------------------------\n",
      "Max loss: 0.01569996401667595\n",
      "Min loss: 0.003844378050416708\n",
      "Mean loss: 0.0070628162162999315\n",
      "Std loss: 0.004038914150982702\n",
      "Total Loss: 0.04237689729779959\n",
      "------------------------------------ epoch 7151 (42900 steps) ------------------------------------\n",
      "Max loss: 0.01636681705713272\n",
      "Min loss: 0.004323917906731367\n",
      "Mean loss: 0.008791571405405799\n",
      "Std loss: 0.004916079798093967\n",
      "Total Loss: 0.0527494284324348\n",
      "------------------------------------ epoch 7152 (42906 steps) ------------------------------------\n",
      "Max loss: 0.02051696926355362\n",
      "Min loss: 0.005987446289509535\n",
      "Mean loss: 0.011905683282141885\n",
      "Std loss: 0.005278390020632299\n",
      "Total Loss: 0.0714340996928513\n",
      "------------------------------------ epoch 7153 (42912 steps) ------------------------------------\n",
      "Max loss: 0.026559924706816673\n",
      "Min loss: 0.003744395449757576\n",
      "Mean loss: 0.010078847408294678\n",
      "Std loss: 0.008232065967370469\n",
      "Total Loss: 0.060473084449768066\n",
      "------------------------------------ epoch 7154 (42918 steps) ------------------------------------\n",
      "Max loss: 0.014575989916920662\n",
      "Min loss: 0.005125987343490124\n",
      "Mean loss: 0.0077255455156167345\n",
      "Std loss: 0.003220320804418439\n",
      "Total Loss: 0.04635327309370041\n",
      "------------------------------------ epoch 7155 (42924 steps) ------------------------------------\n",
      "Max loss: 0.018100915476679802\n",
      "Min loss: 0.005294750444591045\n",
      "Mean loss: 0.009560386495043835\n",
      "Std loss: 0.0044537273643380886\n",
      "Total Loss: 0.057362318970263004\n",
      "------------------------------------ epoch 7156 (42930 steps) ------------------------------------\n",
      "Max loss: 0.031401630491018295\n",
      "Min loss: 0.005378559231758118\n",
      "Mean loss: 0.013464827400942644\n",
      "Std loss: 0.008592740579120585\n",
      "Total Loss: 0.08078896440565586\n",
      "------------------------------------ epoch 7157 (42936 steps) ------------------------------------\n",
      "Max loss: 0.011358795687556267\n",
      "Min loss: 0.007296498864889145\n",
      "Mean loss: 0.008692664637540778\n",
      "Std loss: 0.0015058048846273036\n",
      "Total Loss: 0.052155987825244665\n",
      "------------------------------------ epoch 7158 (42942 steps) ------------------------------------\n",
      "Max loss: 0.030743569135665894\n",
      "Min loss: 0.006135576404631138\n",
      "Mean loss: 0.011786126376440128\n",
      "Std loss: 0.008569014470638164\n",
      "Total Loss: 0.07071675825864077\n",
      "------------------------------------ epoch 7159 (42948 steps) ------------------------------------\n",
      "Max loss: 0.01427510380744934\n",
      "Min loss: 0.00588255375623703\n",
      "Mean loss: 0.009937800622234741\n",
      "Std loss: 0.0029222905730838466\n",
      "Total Loss: 0.05962680373340845\n",
      "------------------------------------ epoch 7160 (42954 steps) ------------------------------------\n",
      "Max loss: 0.01254001073539257\n",
      "Min loss: 0.004639297723770142\n",
      "Mean loss: 0.007536080898717046\n",
      "Std loss: 0.0025266402228843778\n",
      "Total Loss: 0.045216485392302275\n",
      "------------------------------------ epoch 7161 (42960 steps) ------------------------------------\n",
      "Max loss: 0.029931053519248962\n",
      "Min loss: 0.004660927690565586\n",
      "Mean loss: 0.011863428944100937\n",
      "Std loss: 0.008450751592155456\n",
      "Total Loss: 0.07118057366460562\n",
      "------------------------------------ epoch 7162 (42966 steps) ------------------------------------\n",
      "Max loss: 0.009421861730515957\n",
      "Min loss: 0.005355270579457283\n",
      "Mean loss: 0.007380000858878096\n",
      "Std loss: 0.001691573460035382\n",
      "Total Loss: 0.044280005153268576\n",
      "------------------------------------ epoch 7163 (42972 steps) ------------------------------------\n",
      "Max loss: 0.01797867938876152\n",
      "Min loss: 0.005341974552720785\n",
      "Mean loss: 0.009873886204635104\n",
      "Std loss: 0.004414125203862085\n",
      "Total Loss: 0.05924331722781062\n",
      "------------------------------------ epoch 7164 (42978 steps) ------------------------------------\n",
      "Max loss: 0.05087914317846298\n",
      "Min loss: 0.005048451945185661\n",
      "Mean loss: 0.014021856089433035\n",
      "Std loss: 0.016542133698791074\n",
      "Total Loss: 0.0841311365365982\n",
      "------------------------------------ epoch 7165 (42984 steps) ------------------------------------\n",
      "Max loss: 0.01833999902009964\n",
      "Min loss: 0.005273721180856228\n",
      "Mean loss: 0.010029646257559458\n",
      "Std loss: 0.004481726294333054\n",
      "Total Loss: 0.06017787754535675\n",
      "------------------------------------ epoch 7166 (42990 steps) ------------------------------------\n",
      "Max loss: 0.009556076489388943\n",
      "Min loss: 0.00597401475533843\n",
      "Mean loss: 0.007884478584552804\n",
      "Std loss: 0.0012852386239953707\n",
      "Total Loss: 0.04730687150731683\n",
      "------------------------------------ epoch 7167 (42996 steps) ------------------------------------\n",
      "Max loss: 0.020940978080034256\n",
      "Min loss: 0.0051765404641628265\n",
      "Mean loss: 0.012568824148426453\n",
      "Std loss: 0.005196022725781938\n",
      "Total Loss: 0.07541294489055872\n",
      "------------------------------------ epoch 7168 (43002 steps) ------------------------------------\n",
      "Max loss: 0.018200034275650978\n",
      "Min loss: 0.004447941668331623\n",
      "Mean loss: 0.00886177698460718\n",
      "Std loss: 0.00446533690040885\n",
      "Total Loss: 0.05317066190764308\n",
      "------------------------------------ epoch 7169 (43008 steps) ------------------------------------\n",
      "Max loss: 0.026410959661006927\n",
      "Min loss: 0.006250809878110886\n",
      "Mean loss: 0.013755201827734709\n",
      "Std loss: 0.006850102684441949\n",
      "Total Loss: 0.08253121096640825\n",
      "------------------------------------ epoch 7170 (43014 steps) ------------------------------------\n",
      "Max loss: 0.01376570574939251\n",
      "Min loss: 0.0060293893329799175\n",
      "Mean loss: 0.010505726949001351\n",
      "Std loss: 0.0027478056355283993\n",
      "Total Loss: 0.06303436169400811\n",
      "------------------------------------ epoch 7171 (43020 steps) ------------------------------------\n",
      "Max loss: 0.03266407549381256\n",
      "Min loss: 0.006078491918742657\n",
      "Mean loss: 0.01655091190089782\n",
      "Std loss: 0.009864373826384306\n",
      "Total Loss: 0.09930547140538692\n",
      "------------------------------------ epoch 7172 (43026 steps) ------------------------------------\n",
      "Max loss: 0.027727141976356506\n",
      "Min loss: 0.0052408492192626\n",
      "Mean loss: 0.01882834934319059\n",
      "Std loss: 0.00798015123531368\n",
      "Total Loss: 0.11297009605914354\n",
      "------------------------------------ epoch 7173 (43032 steps) ------------------------------------\n",
      "Max loss: 0.019204234704375267\n",
      "Min loss: 0.006405200343579054\n",
      "Mean loss: 0.01279794603275756\n",
      "Std loss: 0.004420711221706751\n",
      "Total Loss: 0.07678767619654536\n",
      "------------------------------------ epoch 7174 (43038 steps) ------------------------------------\n",
      "Max loss: 0.03267886862158775\n",
      "Min loss: 0.006008354481309652\n",
      "Mean loss: 0.014375359052792192\n",
      "Std loss: 0.009433959828913391\n",
      "Total Loss: 0.08625215431675315\n",
      "------------------------------------ epoch 7175 (43044 steps) ------------------------------------\n",
      "Max loss: 0.010832606814801693\n",
      "Min loss: 0.005516940262168646\n",
      "Mean loss: 0.008518567541614175\n",
      "Std loss: 0.00219543336353445\n",
      "Total Loss: 0.05111140524968505\n",
      "------------------------------------ epoch 7176 (43050 steps) ------------------------------------\n",
      "Max loss: 0.028670061379671097\n",
      "Min loss: 0.006613118574023247\n",
      "Mean loss: 0.014947159526248774\n",
      "Std loss: 0.008476102865822791\n",
      "Total Loss: 0.08968295715749264\n",
      "------------------------------------ epoch 7177 (43056 steps) ------------------------------------\n",
      "Max loss: 0.014632209204137325\n",
      "Min loss: 0.008227823302149773\n",
      "Mean loss: 0.011806422534088293\n",
      "Std loss: 0.00239771330079822\n",
      "Total Loss: 0.07083853520452976\n",
      "------------------------------------ epoch 7178 (43062 steps) ------------------------------------\n",
      "Max loss: 0.010039173997938633\n",
      "Min loss: 0.004636115860193968\n",
      "Mean loss: 0.007065650075674057\n",
      "Std loss: 0.0018812580655257543\n",
      "Total Loss: 0.04239390045404434\n",
      "------------------------------------ epoch 7179 (43068 steps) ------------------------------------\n",
      "Max loss: 0.018921442329883575\n",
      "Min loss: 0.006161797791719437\n",
      "Mean loss: 0.01038143535455068\n",
      "Std loss: 0.0040267473732541175\n",
      "Total Loss: 0.06228861212730408\n",
      "------------------------------------ epoch 7180 (43074 steps) ------------------------------------\n",
      "Max loss: 0.017572671175003052\n",
      "Min loss: 0.005328377243131399\n",
      "Mean loss: 0.01150112870770196\n",
      "Std loss: 0.004595586400410056\n",
      "Total Loss: 0.06900677224621177\n",
      "------------------------------------ epoch 7181 (43080 steps) ------------------------------------\n",
      "Max loss: 0.018186843022704124\n",
      "Min loss: 0.004890485666692257\n",
      "Mean loss: 0.01034194960569342\n",
      "Std loss: 0.004422376383228752\n",
      "Total Loss: 0.06205169763416052\n",
      "------------------------------------ epoch 7182 (43086 steps) ------------------------------------\n",
      "Max loss: 0.011807577684521675\n",
      "Min loss: 0.005087144672870636\n",
      "Mean loss: 0.00877246051095426\n",
      "Std loss: 0.002154995787892815\n",
      "Total Loss: 0.052634763065725565\n",
      "------------------------------------ epoch 7183 (43092 steps) ------------------------------------\n",
      "Max loss: 0.022360634058713913\n",
      "Min loss: 0.004745140206068754\n",
      "Mean loss: 0.011016953038051724\n",
      "Std loss: 0.005476569593127101\n",
      "Total Loss: 0.06610171822831035\n",
      "------------------------------------ epoch 7184 (43098 steps) ------------------------------------\n",
      "Max loss: 0.011814641766250134\n",
      "Min loss: 0.004080352373421192\n",
      "Mean loss: 0.0068042134710898\n",
      "Std loss: 0.002637391158656657\n",
      "Total Loss: 0.0408252808265388\n",
      "------------------------------------ epoch 7185 (43104 steps) ------------------------------------\n",
      "Max loss: 0.032838672399520874\n",
      "Min loss: 0.005552574992179871\n",
      "Mean loss: 0.013454468765606483\n",
      "Std loss: 0.009428263924933449\n",
      "Total Loss: 0.0807268125936389\n",
      "------------------------------------ epoch 7186 (43110 steps) ------------------------------------\n",
      "Max loss: 0.04826323315501213\n",
      "Min loss: 0.005203150678426027\n",
      "Mean loss: 0.01602761393102507\n",
      "Std loss: 0.01483399047155003\n",
      "Total Loss: 0.09616568358615041\n",
      "------------------------------------ epoch 7187 (43116 steps) ------------------------------------\n",
      "Max loss: 0.014593467116355896\n",
      "Min loss: 0.005907858721911907\n",
      "Mean loss: 0.008218891297777494\n",
      "Std loss: 0.0029230646553374024\n",
      "Total Loss: 0.04931334778666496\n",
      "------------------------------------ epoch 7188 (43122 steps) ------------------------------------\n",
      "Max loss: 0.017892077565193176\n",
      "Min loss: 0.006315227597951889\n",
      "Mean loss: 0.010706972641249498\n",
      "Std loss: 0.004248764838109474\n",
      "Total Loss: 0.06424183584749699\n",
      "------------------------------------ epoch 7189 (43128 steps) ------------------------------------\n",
      "Max loss: 0.03597302362322807\n",
      "Min loss: 0.0061417496763169765\n",
      "Mean loss: 0.01293436026511093\n",
      "Std loss: 0.010451060468540906\n",
      "Total Loss: 0.07760616159066558\n",
      "------------------------------------ epoch 7190 (43134 steps) ------------------------------------\n",
      "Max loss: 0.036149851977825165\n",
      "Min loss: 0.006454297807067633\n",
      "Mean loss: 0.013887440708155433\n",
      "Std loss: 0.010226165449863567\n",
      "Total Loss: 0.0833246442489326\n",
      "------------------------------------ epoch 7191 (43140 steps) ------------------------------------\n",
      "Max loss: 0.0229436494410038\n",
      "Min loss: 0.006006486248224974\n",
      "Mean loss: 0.01131925149820745\n",
      "Std loss: 0.005426562162115272\n",
      "Total Loss: 0.0679155089892447\n",
      "------------------------------------ epoch 7192 (43146 steps) ------------------------------------\n",
      "Max loss: 0.02085096202790737\n",
      "Min loss: 0.005183624103665352\n",
      "Mean loss: 0.012422424741089344\n",
      "Std loss: 0.004859912849556161\n",
      "Total Loss: 0.07453454844653606\n",
      "------------------------------------ epoch 7193 (43152 steps) ------------------------------------\n",
      "Max loss: 0.014470560476183891\n",
      "Min loss: 0.005237491335719824\n",
      "Mean loss: 0.008408496156334877\n",
      "Std loss: 0.003044612298825077\n",
      "Total Loss: 0.05045097693800926\n",
      "------------------------------------ epoch 7194 (43158 steps) ------------------------------------\n",
      "Max loss: 0.018729805946350098\n",
      "Min loss: 0.005143517628312111\n",
      "Mean loss: 0.00964344727496306\n",
      "Std loss: 0.004566992726075838\n",
      "Total Loss: 0.057860683649778366\n",
      "------------------------------------ epoch 7195 (43164 steps) ------------------------------------\n",
      "Max loss: 0.01880384050309658\n",
      "Min loss: 0.005520229693502188\n",
      "Mean loss: 0.011208909563720226\n",
      "Std loss: 0.004944801132889286\n",
      "Total Loss: 0.06725345738232136\n",
      "------------------------------------ epoch 7196 (43170 steps) ------------------------------------\n",
      "Max loss: 0.01878168433904648\n",
      "Min loss: 0.0043061766773462296\n",
      "Mean loss: 0.00890735963669916\n",
      "Std loss: 0.0049395259964246496\n",
      "Total Loss: 0.05344415782019496\n",
      "------------------------------------ epoch 7197 (43176 steps) ------------------------------------\n",
      "Max loss: 0.02995816245675087\n",
      "Min loss: 0.004246160853654146\n",
      "Mean loss: 0.01034212353018423\n",
      "Std loss: 0.008867767573485534\n",
      "Total Loss: 0.062052741181105375\n",
      "------------------------------------ epoch 7198 (43182 steps) ------------------------------------\n",
      "Max loss: 0.03458769991993904\n",
      "Min loss: 0.003818349912762642\n",
      "Mean loss: 0.011408675462007523\n",
      "Std loss: 0.010561438656015389\n",
      "Total Loss: 0.06845205277204514\n",
      "------------------------------------ epoch 7199 (43188 steps) ------------------------------------\n",
      "Max loss: 0.02889293245971203\n",
      "Min loss: 0.006227094680070877\n",
      "Mean loss: 0.014282807863006989\n",
      "Std loss: 0.008615743407253417\n",
      "Total Loss: 0.08569684717804193\n",
      "------------------------------------ epoch 7200 (43194 steps) ------------------------------------\n",
      "Max loss: 0.011930878274142742\n",
      "Min loss: 0.0054840645752847195\n",
      "Mean loss: 0.00832695133673648\n",
      "Std loss: 0.002207969184156237\n",
      "Total Loss: 0.04996170802041888\n",
      "------------------------------------ epoch 7201 (43200 steps) ------------------------------------\n",
      "Max loss: 0.010967141948640347\n",
      "Min loss: 0.005792379379272461\n",
      "Mean loss: 0.008071758861963948\n",
      "Std loss: 0.0021116225579674397\n",
      "Total Loss: 0.048430553171783686\n",
      "saved model at ./weights/model_7201.pth\n",
      "------------------------------------ epoch 7202 (43206 steps) ------------------------------------\n",
      "Max loss: 0.011452137492597103\n",
      "Min loss: 0.00491978507488966\n",
      "Mean loss: 0.007554138312116265\n",
      "Std loss: 0.002081001606075712\n",
      "Total Loss: 0.04532482987269759\n",
      "------------------------------------ epoch 7203 (43212 steps) ------------------------------------\n",
      "Max loss: 0.01229456439614296\n",
      "Min loss: 0.0038682855665683746\n",
      "Mean loss: 0.0064365620880077285\n",
      "Std loss: 0.002713836856662375\n",
      "Total Loss: 0.03861937252804637\n",
      "------------------------------------ epoch 7204 (43218 steps) ------------------------------------\n",
      "Max loss: 0.018411992117762566\n",
      "Min loss: 0.0038798265159130096\n",
      "Mean loss: 0.008939613200103244\n",
      "Std loss: 0.0049525826482935235\n",
      "Total Loss: 0.05363767920061946\n",
      "------------------------------------ epoch 7205 (43224 steps) ------------------------------------\n",
      "Max loss: 0.007457698695361614\n",
      "Min loss: 0.004230533726513386\n",
      "Mean loss: 0.005750882206484675\n",
      "Std loss: 0.0012927175704202198\n",
      "Total Loss: 0.03450529323890805\n",
      "------------------------------------ epoch 7206 (43230 steps) ------------------------------------\n",
      "Max loss: 0.015709079802036285\n",
      "Min loss: 0.003909042105078697\n",
      "Mean loss: 0.007907263313730558\n",
      "Std loss: 0.003934061723626404\n",
      "Total Loss: 0.04744357988238335\n",
      "------------------------------------ epoch 7207 (43236 steps) ------------------------------------\n",
      "Max loss: 0.012451454065740108\n",
      "Min loss: 0.004150068387389183\n",
      "Mean loss: 0.007662739604711533\n",
      "Std loss: 0.0032000300495543107\n",
      "Total Loss: 0.045976437628269196\n",
      "------------------------------------ epoch 7208 (43242 steps) ------------------------------------\n",
      "Max loss: 0.01636067032814026\n",
      "Min loss: 0.003961185924708843\n",
      "Mean loss: 0.00821197615005076\n",
      "Std loss: 0.004128724411723588\n",
      "Total Loss: 0.049271856900304556\n",
      "------------------------------------ epoch 7209 (43248 steps) ------------------------------------\n",
      "Max loss: 0.007818439975380898\n",
      "Min loss: 0.003917535301297903\n",
      "Mean loss: 0.006009467023735245\n",
      "Std loss: 0.0014871227672373316\n",
      "Total Loss: 0.03605680214241147\n",
      "------------------------------------ epoch 7210 (43254 steps) ------------------------------------\n",
      "Max loss: 0.011602300219237804\n",
      "Min loss: 0.003998062573373318\n",
      "Mean loss: 0.00733958735751609\n",
      "Std loss: 0.0028133336126374676\n",
      "Total Loss: 0.04403752414509654\n",
      "------------------------------------ epoch 7211 (43260 steps) ------------------------------------\n",
      "Max loss: 0.027908464893698692\n",
      "Min loss: 0.003810684662312269\n",
      "Mean loss: 0.010601935054485997\n",
      "Std loss: 0.008419328027686636\n",
      "Total Loss: 0.06361161032691598\n",
      "------------------------------------ epoch 7212 (43266 steps) ------------------------------------\n",
      "Max loss: 0.015261493623256683\n",
      "Min loss: 0.004294681362807751\n",
      "Mean loss: 0.00828273594379425\n",
      "Std loss: 0.003429143159113035\n",
      "Total Loss: 0.0496964156627655\n",
      "------------------------------------ epoch 7213 (43272 steps) ------------------------------------\n",
      "Max loss: 0.03189444914460182\n",
      "Min loss: 0.004827427677810192\n",
      "Mean loss: 0.011327679579456648\n",
      "Std loss: 0.00942305934328489\n",
      "Total Loss: 0.06796607747673988\n",
      "------------------------------------ epoch 7214 (43278 steps) ------------------------------------\n",
      "Max loss: 0.028591018170118332\n",
      "Min loss: 0.007421877235174179\n",
      "Mean loss: 0.013027503620833158\n",
      "Std loss: 0.007374270920064314\n",
      "Total Loss: 0.07816502172499895\n",
      "------------------------------------ epoch 7215 (43284 steps) ------------------------------------\n",
      "Max loss: 0.012679360806941986\n",
      "Min loss: 0.006941701285541058\n",
      "Mean loss: 0.009626013537247976\n",
      "Std loss: 0.0023413990002812872\n",
      "Total Loss: 0.057756081223487854\n",
      "------------------------------------ epoch 7216 (43290 steps) ------------------------------------\n",
      "Max loss: 0.017191294580698013\n",
      "Min loss: 0.004383211024105549\n",
      "Mean loss: 0.008404252429803213\n",
      "Std loss: 0.004267258901163813\n",
      "Total Loss: 0.050425514578819275\n",
      "------------------------------------ epoch 7217 (43296 steps) ------------------------------------\n",
      "Max loss: 0.038531020283699036\n",
      "Min loss: 0.00444256654009223\n",
      "Mean loss: 0.012042676641916236\n",
      "Std loss: 0.01200312369398514\n",
      "Total Loss: 0.07225605985149741\n",
      "------------------------------------ epoch 7218 (43302 steps) ------------------------------------\n",
      "Max loss: 0.028089705854654312\n",
      "Min loss: 0.006524298340082169\n",
      "Mean loss: 0.015483256739874681\n",
      "Std loss: 0.008157463300310554\n",
      "Total Loss: 0.09289954043924809\n",
      "------------------------------------ epoch 7219 (43308 steps) ------------------------------------\n",
      "Max loss: 0.048089783638715744\n",
      "Min loss: 0.007145244628190994\n",
      "Mean loss: 0.01599910172323386\n",
      "Std loss: 0.014438122050560694\n",
      "Total Loss: 0.09599461033940315\n",
      "------------------------------------ epoch 7220 (43314 steps) ------------------------------------\n",
      "Max loss: 0.017843451350927353\n",
      "Min loss: 0.00542685529217124\n",
      "Mean loss: 0.009257595442856351\n",
      "Std loss: 0.004127517110996052\n",
      "Total Loss: 0.05554557265713811\n",
      "------------------------------------ epoch 7221 (43320 steps) ------------------------------------\n",
      "Max loss: 0.009686727076768875\n",
      "Min loss: 0.005158494226634502\n",
      "Mean loss: 0.006859929688895742\n",
      "Std loss: 0.0014574873014481186\n",
      "Total Loss: 0.04115957813337445\n",
      "------------------------------------ epoch 7222 (43326 steps) ------------------------------------\n",
      "Max loss: 0.010398623533546925\n",
      "Min loss: 0.00469916732981801\n",
      "Mean loss: 0.0066914070242395\n",
      "Std loss: 0.0018201089350421377\n",
      "Total Loss: 0.040148442145437\n",
      "------------------------------------ epoch 7223 (43332 steps) ------------------------------------\n",
      "Max loss: 0.011668182909488678\n",
      "Min loss: 0.004292251542210579\n",
      "Mean loss: 0.006878782141332825\n",
      "Std loss: 0.0022947326167823364\n",
      "Total Loss: 0.04127269284799695\n",
      "------------------------------------ epoch 7224 (43338 steps) ------------------------------------\n",
      "Max loss: 0.012824246659874916\n",
      "Min loss: 0.0038952927570790052\n",
      "Mean loss: 0.007035714923404157\n",
      "Std loss: 0.0031481418610565326\n",
      "Total Loss: 0.04221428954042494\n",
      "------------------------------------ epoch 7225 (43344 steps) ------------------------------------\n",
      "Max loss: 0.01576099917292595\n",
      "Min loss: 0.004572697915136814\n",
      "Mean loss: 0.008700023405253887\n",
      "Std loss: 0.0039059238601433835\n",
      "Total Loss: 0.05220014043152332\n",
      "------------------------------------ epoch 7226 (43350 steps) ------------------------------------\n",
      "Max loss: 0.01127132959663868\n",
      "Min loss: 0.0034459230955690145\n",
      "Mean loss: 0.005931084897990028\n",
      "Std loss: 0.0026685076300078546\n",
      "Total Loss: 0.03558650938794017\n",
      "------------------------------------ epoch 7227 (43356 steps) ------------------------------------\n",
      "Max loss: 0.009207072667777538\n",
      "Min loss: 0.004167051520198584\n",
      "Mean loss: 0.0063908440837015705\n",
      "Std loss: 0.0016934518017446415\n",
      "Total Loss: 0.038345064502209425\n",
      "------------------------------------ epoch 7228 (43362 steps) ------------------------------------\n",
      "Max loss: 0.018640924245119095\n",
      "Min loss: 0.005099287256598473\n",
      "Mean loss: 0.01133090661217769\n",
      "Std loss: 0.005829967989161769\n",
      "Total Loss: 0.06798543967306614\n",
      "------------------------------------ epoch 7229 (43368 steps) ------------------------------------\n",
      "Max loss: 0.017070908099412918\n",
      "Min loss: 0.0057776570320129395\n",
      "Mean loss: 0.009570275821412602\n",
      "Std loss: 0.0037670592119013638\n",
      "Total Loss: 0.05742165492847562\n",
      "------------------------------------ epoch 7230 (43374 steps) ------------------------------------\n",
      "Max loss: 0.01262787263840437\n",
      "Min loss: 0.005042547360062599\n",
      "Mean loss: 0.007407755979026358\n",
      "Std loss: 0.0030386200028220166\n",
      "Total Loss: 0.044446535874158144\n",
      "------------------------------------ epoch 7231 (43380 steps) ------------------------------------\n",
      "Max loss: 0.012330235913395882\n",
      "Min loss: 0.005315215792506933\n",
      "Mean loss: 0.008978937830155095\n",
      "Std loss: 0.0023877951886363377\n",
      "Total Loss: 0.05387362698093057\n",
      "------------------------------------ epoch 7232 (43386 steps) ------------------------------------\n",
      "Max loss: 0.022762037813663483\n",
      "Min loss: 0.0049674902111291885\n",
      "Mean loss: 0.009257631143555045\n",
      "Std loss: 0.006119629965309993\n",
      "Total Loss: 0.05554578686133027\n",
      "------------------------------------ epoch 7233 (43392 steps) ------------------------------------\n",
      "Max loss: 0.013605104759335518\n",
      "Min loss: 0.004336779471486807\n",
      "Mean loss: 0.00818357391593357\n",
      "Std loss: 0.0033829693971014654\n",
      "Total Loss: 0.049101443495601416\n",
      "------------------------------------ epoch 7234 (43398 steps) ------------------------------------\n",
      "Max loss: 0.0160786472260952\n",
      "Min loss: 0.0039908392354846\n",
      "Mean loss: 0.006656487317134936\n",
      "Std loss: 0.0042404660722580815\n",
      "Total Loss: 0.03993892390280962\n",
      "------------------------------------ epoch 7235 (43404 steps) ------------------------------------\n",
      "Max loss: 0.011572646908462048\n",
      "Min loss: 0.003949155565351248\n",
      "Mean loss: 0.007224101573228836\n",
      "Std loss: 0.00278116887244781\n",
      "Total Loss: 0.043344609439373016\n",
      "------------------------------------ epoch 7236 (43410 steps) ------------------------------------\n",
      "Max loss: 0.021798770874738693\n",
      "Min loss: 0.005008602514863014\n",
      "Mean loss: 0.009288018336519599\n",
      "Std loss: 0.005891028584321904\n",
      "Total Loss: 0.055728110019117594\n",
      "------------------------------------ epoch 7237 (43416 steps) ------------------------------------\n",
      "Max loss: 0.00895226001739502\n",
      "Min loss: 0.0037675320636481047\n",
      "Mean loss: 0.005906013655476272\n",
      "Std loss: 0.0016785461987003272\n",
      "Total Loss: 0.03543608193285763\n",
      "------------------------------------ epoch 7238 (43422 steps) ------------------------------------\n",
      "Max loss: 0.0484335832297802\n",
      "Min loss: 0.003994856029748917\n",
      "Mean loss: 0.015062014572322369\n",
      "Std loss: 0.015397718056605137\n",
      "Total Loss: 0.09037208743393421\n",
      "------------------------------------ epoch 7239 (43428 steps) ------------------------------------\n",
      "Max loss: 0.011436625383794308\n",
      "Min loss: 0.004179742187261581\n",
      "Mean loss: 0.00751284269305567\n",
      "Std loss: 0.0023855499909569444\n",
      "Total Loss: 0.04507705615833402\n",
      "------------------------------------ epoch 7240 (43434 steps) ------------------------------------\n",
      "Max loss: 0.022978823632001877\n",
      "Min loss: 0.005617175251245499\n",
      "Mean loss: 0.009903732066353163\n",
      "Std loss: 0.006105121979688516\n",
      "Total Loss: 0.05942239239811897\n",
      "------------------------------------ epoch 7241 (43440 steps) ------------------------------------\n",
      "Max loss: 0.0197700634598732\n",
      "Min loss: 0.004413655027747154\n",
      "Mean loss: 0.008154807379469275\n",
      "Std loss: 0.0053277775305281974\n",
      "Total Loss: 0.04892884427681565\n",
      "------------------------------------ epoch 7242 (43446 steps) ------------------------------------\n",
      "Max loss: 0.01566627435386181\n",
      "Min loss: 0.005373120307922363\n",
      "Mean loss: 0.009247993429501852\n",
      "Std loss: 0.004345848571163178\n",
      "Total Loss: 0.05548796057701111\n",
      "------------------------------------ epoch 7243 (43452 steps) ------------------------------------\n",
      "Max loss: 0.027084721252322197\n",
      "Min loss: 0.0037998741026967764\n",
      "Mean loss: 0.011087243328802288\n",
      "Std loss: 0.008299732116204571\n",
      "Total Loss: 0.06652345997281373\n",
      "------------------------------------ epoch 7244 (43458 steps) ------------------------------------\n",
      "Max loss: 0.014793535694479942\n",
      "Min loss: 0.004306068643927574\n",
      "Mean loss: 0.007889251535137495\n",
      "Std loss: 0.0034166093912252893\n",
      "Total Loss: 0.047335509210824966\n",
      "------------------------------------ epoch 7245 (43464 steps) ------------------------------------\n",
      "Max loss: 0.016748039051890373\n",
      "Min loss: 0.004489263519644737\n",
      "Mean loss: 0.009646809737508496\n",
      "Std loss: 0.00475548184604397\n",
      "Total Loss: 0.057880858425050974\n",
      "------------------------------------ epoch 7246 (43470 steps) ------------------------------------\n",
      "Max loss: 0.021701214835047722\n",
      "Min loss: 0.004883958492428064\n",
      "Mean loss: 0.013010030301908651\n",
      "Std loss: 0.006943469258692496\n",
      "Total Loss: 0.07806018181145191\n",
      "------------------------------------ epoch 7247 (43476 steps) ------------------------------------\n",
      "Max loss: 0.008365526795387268\n",
      "Min loss: 0.0045065986923873425\n",
      "Mean loss: 0.005400776086995999\n",
      "Std loss: 0.0013562689395143633\n",
      "Total Loss: 0.032404656521975994\n",
      "------------------------------------ epoch 7248 (43482 steps) ------------------------------------\n",
      "Max loss: 0.010786185041069984\n",
      "Min loss: 0.005945013370364904\n",
      "Mean loss: 0.008237468466783563\n",
      "Std loss: 0.001785458247612202\n",
      "Total Loss: 0.04942481080070138\n",
      "------------------------------------ epoch 7249 (43488 steps) ------------------------------------\n",
      "Max loss: 0.019709499552845955\n",
      "Min loss: 0.005805882625281811\n",
      "Mean loss: 0.009157850407063961\n",
      "Std loss: 0.0048208512443593775\n",
      "Total Loss: 0.054947102442383766\n",
      "------------------------------------ epoch 7250 (43494 steps) ------------------------------------\n",
      "Max loss: 0.014785481616854668\n",
      "Min loss: 0.0043838052079081535\n",
      "Mean loss: 0.009520492128406962\n",
      "Std loss: 0.003629173135090989\n",
      "Total Loss: 0.05712295277044177\n",
      "------------------------------------ epoch 7251 (43500 steps) ------------------------------------\n",
      "Max loss: 0.027614939957857132\n",
      "Min loss: 0.004271047189831734\n",
      "Mean loss: 0.009810879593715072\n",
      "Std loss: 0.008094756015409148\n",
      "Total Loss: 0.05886527756229043\n",
      "------------------------------------ epoch 7252 (43506 steps) ------------------------------------\n",
      "Max loss: 0.013265309855341911\n",
      "Min loss: 0.00440931087359786\n",
      "Mean loss: 0.00802160807264348\n",
      "Std loss: 0.0028873318149590385\n",
      "Total Loss: 0.04812964843586087\n",
      "------------------------------------ epoch 7253 (43512 steps) ------------------------------------\n",
      "Max loss: 0.016911525279283524\n",
      "Min loss: 0.005681456532329321\n",
      "Mean loss: 0.00957097865951558\n",
      "Std loss: 0.003643132890763568\n",
      "Total Loss: 0.05742587195709348\n",
      "------------------------------------ epoch 7254 (43518 steps) ------------------------------------\n",
      "Max loss: 0.0099819740280509\n",
      "Min loss: 0.0042672352865338326\n",
      "Mean loss: 0.0060798716731369495\n",
      "Std loss: 0.002008148000215267\n",
      "Total Loss: 0.0364792300388217\n",
      "------------------------------------ epoch 7255 (43524 steps) ------------------------------------\n",
      "Max loss: 0.01907254382967949\n",
      "Min loss: 0.004177772905677557\n",
      "Mean loss: 0.008130675259356698\n",
      "Std loss: 0.005074616684016052\n",
      "Total Loss: 0.048784051556140184\n",
      "------------------------------------ epoch 7256 (43530 steps) ------------------------------------\n",
      "Max loss: 0.032266899943351746\n",
      "Min loss: 0.008353056386113167\n",
      "Mean loss: 0.016188017558306456\n",
      "Std loss: 0.008553226734780952\n",
      "Total Loss: 0.09712810534983873\n",
      "------------------------------------ epoch 7257 (43536 steps) ------------------------------------\n",
      "Max loss: 0.01726386696100235\n",
      "Min loss: 0.005802846979349852\n",
      "Mean loss: 0.009620764292776585\n",
      "Std loss: 0.003655841366488813\n",
      "Total Loss: 0.05772458575665951\n",
      "------------------------------------ epoch 7258 (43542 steps) ------------------------------------\n",
      "Max loss: 0.010556365363299847\n",
      "Min loss: 0.0055409143678843975\n",
      "Mean loss: 0.008822399269168576\n",
      "Std loss: 0.0016430203956219306\n",
      "Total Loss: 0.052934395615011454\n",
      "------------------------------------ epoch 7259 (43548 steps) ------------------------------------\n",
      "Max loss: 0.019701220095157623\n",
      "Min loss: 0.008163685910403728\n",
      "Mean loss: 0.0132209452179571\n",
      "Std loss: 0.003965749588538548\n",
      "Total Loss: 0.0793256713077426\n",
      "------------------------------------ epoch 7260 (43554 steps) ------------------------------------\n",
      "Max loss: 0.029425134882330894\n",
      "Min loss: 0.00423587579280138\n",
      "Mean loss: 0.010508695229267081\n",
      "Std loss: 0.00863243456210208\n",
      "Total Loss: 0.06305217137560248\n",
      "------------------------------------ epoch 7261 (43560 steps) ------------------------------------\n",
      "Max loss: 0.01584622636437416\n",
      "Min loss: 0.004455205984413624\n",
      "Mean loss: 0.008368245170762142\n",
      "Std loss: 0.00403330979525769\n",
      "Total Loss: 0.05020947102457285\n",
      "------------------------------------ epoch 7262 (43566 steps) ------------------------------------\n",
      "Max loss: 0.011471232399344444\n",
      "Min loss: 0.006183871533721685\n",
      "Mean loss: 0.00857770179087917\n",
      "Std loss: 0.0020880292606055865\n",
      "Total Loss: 0.05146621074527502\n",
      "------------------------------------ epoch 7263 (43572 steps) ------------------------------------\n",
      "Max loss: 0.013482717797160149\n",
      "Min loss: 0.005241424776613712\n",
      "Mean loss: 0.008223923932140073\n",
      "Std loss: 0.0032291810405009032\n",
      "Total Loss: 0.04934354359284043\n",
      "------------------------------------ epoch 7264 (43578 steps) ------------------------------------\n",
      "Max loss: 0.010881548747420311\n",
      "Min loss: 0.004738593474030495\n",
      "Mean loss: 0.007369557473187645\n",
      "Std loss: 0.002299137830038919\n",
      "Total Loss: 0.04421734483912587\n",
      "------------------------------------ epoch 7265 (43584 steps) ------------------------------------\n",
      "Max loss: 0.012650996446609497\n",
      "Min loss: 0.004169875755906105\n",
      "Mean loss: 0.007694084818164508\n",
      "Std loss: 0.0026686167198608367\n",
      "Total Loss: 0.046164508908987045\n",
      "------------------------------------ epoch 7266 (43590 steps) ------------------------------------\n",
      "Max loss: 0.018366575241088867\n",
      "Min loss: 0.004911809228360653\n",
      "Mean loss: 0.009522746860360106\n",
      "Std loss: 0.004586345359689213\n",
      "Total Loss: 0.057136481162160635\n",
      "------------------------------------ epoch 7267 (43596 steps) ------------------------------------\n",
      "Max loss: 0.01660279743373394\n",
      "Min loss: 0.0038078208453953266\n",
      "Mean loss: 0.008618182735517621\n",
      "Std loss: 0.004137591636116683\n",
      "Total Loss: 0.051709096413105726\n",
      "------------------------------------ epoch 7268 (43602 steps) ------------------------------------\n",
      "Max loss: 0.009757827036082745\n",
      "Min loss: 0.004740522708743811\n",
      "Mean loss: 0.006387004706387718\n",
      "Std loss: 0.0016764897039039002\n",
      "Total Loss: 0.03832202823832631\n",
      "------------------------------------ epoch 7269 (43608 steps) ------------------------------------\n",
      "Max loss: 0.026014402508735657\n",
      "Min loss: 0.004794420208781958\n",
      "Mean loss: 0.009479710677017769\n",
      "Std loss: 0.007451871298733918\n",
      "Total Loss: 0.05687826406210661\n",
      "------------------------------------ epoch 7270 (43614 steps) ------------------------------------\n",
      "Max loss: 0.024487262591719627\n",
      "Min loss: 0.0056008342653512955\n",
      "Mean loss: 0.01153083440537254\n",
      "Std loss: 0.006140948723669441\n",
      "Total Loss: 0.06918500643223524\n",
      "------------------------------------ epoch 7271 (43620 steps) ------------------------------------\n",
      "Max loss: 0.011220021173357964\n",
      "Min loss: 0.005368082784116268\n",
      "Mean loss: 0.007440775788078706\n",
      "Std loss: 0.0019016537356067941\n",
      "Total Loss: 0.04464465472847223\n",
      "------------------------------------ epoch 7272 (43626 steps) ------------------------------------\n",
      "Max loss: 0.015344809740781784\n",
      "Min loss: 0.004775513894855976\n",
      "Mean loss: 0.00943676095145444\n",
      "Std loss: 0.003948114049298626\n",
      "Total Loss: 0.056620565708726645\n",
      "------------------------------------ epoch 7273 (43632 steps) ------------------------------------\n",
      "Max loss: 0.07202659547328949\n",
      "Min loss: 0.004939184058457613\n",
      "Mean loss: 0.020584238149846595\n",
      "Std loss: 0.02338667316329792\n",
      "Total Loss: 0.12350542889907956\n",
      "------------------------------------ epoch 7274 (43638 steps) ------------------------------------\n",
      "Max loss: 0.029586978256702423\n",
      "Min loss: 0.005325719248503447\n",
      "Mean loss: 0.013492152327671647\n",
      "Std loss: 0.008002130090097544\n",
      "Total Loss: 0.08095291396602988\n",
      "------------------------------------ epoch 7275 (43644 steps) ------------------------------------\n",
      "Max loss: 0.01956474781036377\n",
      "Min loss: 0.006860812660306692\n",
      "Mean loss: 0.01119787486580511\n",
      "Std loss: 0.004202608878529859\n",
      "Total Loss: 0.06718724919483066\n",
      "------------------------------------ epoch 7276 (43650 steps) ------------------------------------\n",
      "Max loss: 0.012339371256530285\n",
      "Min loss: 0.005485029425472021\n",
      "Mean loss: 0.008533678948879242\n",
      "Std loss: 0.0023431695897682464\n",
      "Total Loss: 0.05120207369327545\n",
      "------------------------------------ epoch 7277 (43656 steps) ------------------------------------\n",
      "Max loss: 0.01404164545238018\n",
      "Min loss: 0.00459130248054862\n",
      "Mean loss: 0.008193832589313388\n",
      "Std loss: 0.003158533950576306\n",
      "Total Loss: 0.04916299553588033\n",
      "------------------------------------ epoch 7278 (43662 steps) ------------------------------------\n",
      "Max loss: 0.013046800158917904\n",
      "Min loss: 0.004530193284153938\n",
      "Mean loss: 0.009035533759742975\n",
      "Std loss: 0.0029912099111773205\n",
      "Total Loss: 0.05421320255845785\n",
      "------------------------------------ epoch 7279 (43668 steps) ------------------------------------\n",
      "Max loss: 0.015146039426326752\n",
      "Min loss: 0.005193549674004316\n",
      "Mean loss: 0.009387535275891423\n",
      "Std loss: 0.003731878841021691\n",
      "Total Loss: 0.05632521165534854\n",
      "------------------------------------ epoch 7280 (43674 steps) ------------------------------------\n",
      "Max loss: 0.02379070781171322\n",
      "Min loss: 0.004554868675768375\n",
      "Mean loss: 0.008834353803346554\n",
      "Std loss: 0.006794582185325895\n",
      "Total Loss: 0.05300612282007933\n",
      "------------------------------------ epoch 7281 (43680 steps) ------------------------------------\n",
      "Max loss: 0.019213205203413963\n",
      "Min loss: 0.0043735443614423275\n",
      "Mean loss: 0.010323969724898538\n",
      "Std loss: 0.005205657086904357\n",
      "Total Loss: 0.06194381834939122\n",
      "------------------------------------ epoch 7282 (43686 steps) ------------------------------------\n",
      "Max loss: 0.011753066442906857\n",
      "Min loss: 0.0033791891764849424\n",
      "Mean loss: 0.006655306671746075\n",
      "Std loss: 0.0026022926845024994\n",
      "Total Loss: 0.03993184003047645\n",
      "------------------------------------ epoch 7283 (43692 steps) ------------------------------------\n",
      "Max loss: 0.01838032715022564\n",
      "Min loss: 0.004703394137322903\n",
      "Mean loss: 0.009152001701295376\n",
      "Std loss: 0.004706045418418208\n",
      "Total Loss: 0.054912010207772255\n",
      "------------------------------------ epoch 7284 (43698 steps) ------------------------------------\n",
      "Max loss: 0.012545841746032238\n",
      "Min loss: 0.005732541438192129\n",
      "Mean loss: 0.008335156521449486\n",
      "Std loss: 0.0027860199334071674\n",
      "Total Loss: 0.05001093912869692\n",
      "------------------------------------ epoch 7285 (43704 steps) ------------------------------------\n",
      "Max loss: 0.011408853344619274\n",
      "Min loss: 0.004312700126320124\n",
      "Mean loss: 0.007335763657465577\n",
      "Std loss: 0.002155149288848402\n",
      "Total Loss: 0.04401458194479346\n",
      "------------------------------------ epoch 7286 (43710 steps) ------------------------------------\n",
      "Max loss: 0.01182809378951788\n",
      "Min loss: 0.0040453518740832806\n",
      "Mean loss: 0.00831039878539741\n",
      "Std loss: 0.0026444437110383504\n",
      "Total Loss: 0.04986239271238446\n",
      "------------------------------------ epoch 7287 (43716 steps) ------------------------------------\n",
      "Max loss: 0.013694520108401775\n",
      "Min loss: 0.004297795705497265\n",
      "Mean loss: 0.0071912215401728945\n",
      "Std loss: 0.0032933928526542792\n",
      "Total Loss: 0.04314732924103737\n",
      "------------------------------------ epoch 7288 (43722 steps) ------------------------------------\n",
      "Max loss: 0.013190554454922676\n",
      "Min loss: 0.0041407630778849125\n",
      "Mean loss: 0.006947705211738746\n",
      "Std loss: 0.0031157210060788437\n",
      "Total Loss: 0.04168623127043247\n",
      "------------------------------------ epoch 7289 (43728 steps) ------------------------------------\n",
      "Max loss: 0.018622105941176414\n",
      "Min loss: 0.004610493313521147\n",
      "Mean loss: 0.010229829077919325\n",
      "Std loss: 0.004676969595232\n",
      "Total Loss: 0.061378974467515945\n",
      "------------------------------------ epoch 7290 (43734 steps) ------------------------------------\n",
      "Max loss: 0.009133093990385532\n",
      "Min loss: 0.004377653822302818\n",
      "Mean loss: 0.006173692566032211\n",
      "Std loss: 0.0017686266385704029\n",
      "Total Loss: 0.037042155396193266\n",
      "------------------------------------ epoch 7291 (43740 steps) ------------------------------------\n",
      "Max loss: 0.011255628429353237\n",
      "Min loss: 0.004529153928160667\n",
      "Mean loss: 0.0069689407634238405\n",
      "Std loss: 0.0021602686957259585\n",
      "Total Loss: 0.04181364458054304\n",
      "------------------------------------ epoch 7292 (43746 steps) ------------------------------------\n",
      "Max loss: 0.008470764383673668\n",
      "Min loss: 0.0033752487506717443\n",
      "Mean loss: 0.004883217819345494\n",
      "Std loss: 0.0017074974656464351\n",
      "Total Loss: 0.029299306916072965\n",
      "------------------------------------ epoch 7293 (43752 steps) ------------------------------------\n",
      "Max loss: 0.007258754223585129\n",
      "Min loss: 0.003199320985004306\n",
      "Mean loss: 0.0046251251284653945\n",
      "Std loss: 0.0014345143736655059\n",
      "Total Loss: 0.027750750770792365\n",
      "------------------------------------ epoch 7294 (43758 steps) ------------------------------------\n",
      "Max loss: 0.011632556095719337\n",
      "Min loss: 0.0034645567648112774\n",
      "Mean loss: 0.006012418384974201\n",
      "Std loss: 0.002652378090731613\n",
      "Total Loss: 0.03607451030984521\n",
      "------------------------------------ epoch 7295 (43764 steps) ------------------------------------\n",
      "Max loss: 0.019788000732660294\n",
      "Min loss: 0.005073011852800846\n",
      "Mean loss: 0.011323331079135338\n",
      "Std loss: 0.004798044933602198\n",
      "Total Loss: 0.06793998647481203\n",
      "------------------------------------ epoch 7296 (43770 steps) ------------------------------------\n",
      "Max loss: 0.009887926280498505\n",
      "Min loss: 0.005144987255334854\n",
      "Mean loss: 0.007801720717300971\n",
      "Std loss: 0.0018110190917931801\n",
      "Total Loss: 0.04681032430380583\n",
      "------------------------------------ epoch 7297 (43776 steps) ------------------------------------\n",
      "Max loss: 0.009496103972196579\n",
      "Min loss: 0.004280331078916788\n",
      "Mean loss: 0.0070793408279617625\n",
      "Std loss: 0.002175871385995026\n",
      "Total Loss: 0.042476044967770576\n",
      "------------------------------------ epoch 7298 (43782 steps) ------------------------------------\n",
      "Max loss: 0.012821463868021965\n",
      "Min loss: 0.004015871323645115\n",
      "Mean loss: 0.008467560478796562\n",
      "Std loss: 0.003374076865705728\n",
      "Total Loss: 0.05080536287277937\n",
      "------------------------------------ epoch 7299 (43788 steps) ------------------------------------\n",
      "Max loss: 0.016592584550380707\n",
      "Min loss: 0.00430904608219862\n",
      "Mean loss: 0.009250501015534004\n",
      "Std loss: 0.004326610908113873\n",
      "Total Loss: 0.05550300609320402\n",
      "------------------------------------ epoch 7300 (43794 steps) ------------------------------------\n",
      "Max loss: 0.030845463275909424\n",
      "Min loss: 0.0053506940603256226\n",
      "Mean loss: 0.013433658517897129\n",
      "Std loss: 0.009133405521950588\n",
      "Total Loss: 0.08060195110738277\n",
      "------------------------------------ epoch 7301 (43800 steps) ------------------------------------\n",
      "Max loss: 0.016004495322704315\n",
      "Min loss: 0.005135716870427132\n",
      "Mean loss: 0.010323467353979746\n",
      "Std loss: 0.004387755335831453\n",
      "Total Loss: 0.06194080412387848\n",
      "saved model at ./weights/model_7301.pth\n",
      "------------------------------------ epoch 7302 (43806 steps) ------------------------------------\n",
      "Max loss: 0.007792291697114706\n",
      "Min loss: 0.004723695572465658\n",
      "Mean loss: 0.006229290661091606\n",
      "Std loss: 0.0013573743814348418\n",
      "Total Loss: 0.037375743966549635\n",
      "------------------------------------ epoch 7303 (43812 steps) ------------------------------------\n",
      "Max loss: 0.023664122447371483\n",
      "Min loss: 0.003844736609607935\n",
      "Mean loss: 0.011882084344203273\n",
      "Std loss: 0.006027342073046448\n",
      "Total Loss: 0.07129250606521964\n",
      "------------------------------------ epoch 7304 (43818 steps) ------------------------------------\n",
      "Max loss: 0.021478518843650818\n",
      "Min loss: 0.0050330935046076775\n",
      "Mean loss: 0.012609883366773525\n",
      "Std loss: 0.0059311987293886185\n",
      "Total Loss: 0.07565930020064116\n",
      "------------------------------------ epoch 7305 (43824 steps) ------------------------------------\n",
      "Max loss: 0.016339018940925598\n",
      "Min loss: 0.008091202937066555\n",
      "Mean loss: 0.011597331147640944\n",
      "Std loss: 0.002744629043717381\n",
      "Total Loss: 0.06958398688584566\n",
      "------------------------------------ epoch 7306 (43830 steps) ------------------------------------\n",
      "Max loss: 0.0085755605250597\n",
      "Min loss: 0.005260619334876537\n",
      "Mean loss: 0.00654501646446685\n",
      "Std loss: 0.0012709231934042154\n",
      "Total Loss: 0.0392700987868011\n",
      "------------------------------------ epoch 7307 (43836 steps) ------------------------------------\n",
      "Max loss: 0.01610499620437622\n",
      "Min loss: 0.00463454332202673\n",
      "Mean loss: 0.006935617808873455\n",
      "Std loss: 0.004120583693915032\n",
      "Total Loss: 0.04161370685324073\n",
      "------------------------------------ epoch 7308 (43842 steps) ------------------------------------\n",
      "Max loss: 0.015581781975924969\n",
      "Min loss: 0.004285180475562811\n",
      "Mean loss: 0.007953214459121227\n",
      "Std loss: 0.0037745464401791875\n",
      "Total Loss: 0.047719286754727364\n",
      "------------------------------------ epoch 7309 (43848 steps) ------------------------------------\n",
      "Max loss: 0.011401979252696037\n",
      "Min loss: 0.003435262246057391\n",
      "Mean loss: 0.008187308868703743\n",
      "Std loss: 0.0028769664888345695\n",
      "Total Loss: 0.04912385321222246\n",
      "------------------------------------ epoch 7310 (43854 steps) ------------------------------------\n",
      "Max loss: 0.014454284682869911\n",
      "Min loss: 0.007907991297543049\n",
      "Mean loss: 0.011141875137885412\n",
      "Std loss: 0.0025270547339199543\n",
      "Total Loss: 0.06685125082731247\n",
      "------------------------------------ epoch 7311 (43860 steps) ------------------------------------\n",
      "Max loss: 0.024303346872329712\n",
      "Min loss: 0.004818955902010202\n",
      "Mean loss: 0.011618756145859757\n",
      "Std loss: 0.006808790069203273\n",
      "Total Loss: 0.06971253687515855\n",
      "------------------------------------ epoch 7312 (43866 steps) ------------------------------------\n",
      "Max loss: 0.014398688450455666\n",
      "Min loss: 0.004144792445003986\n",
      "Mean loss: 0.008007586312790712\n",
      "Std loss: 0.003246517933126504\n",
      "Total Loss: 0.04804551787674427\n",
      "------------------------------------ epoch 7313 (43872 steps) ------------------------------------\n",
      "Max loss: 0.007593216374516487\n",
      "Min loss: 0.004532262217253447\n",
      "Mean loss: 0.005660422922422488\n",
      "Std loss: 0.001056560382615646\n",
      "Total Loss: 0.03396253753453493\n",
      "------------------------------------ epoch 7314 (43878 steps) ------------------------------------\n",
      "Max loss: 0.04718665033578873\n",
      "Min loss: 0.007640811614692211\n",
      "Mean loss: 0.017526485801984865\n",
      "Std loss: 0.013604746485736669\n",
      "Total Loss: 0.1051589148119092\n",
      "------------------------------------ epoch 7315 (43884 steps) ------------------------------------\n",
      "Max loss: 0.021224655210971832\n",
      "Min loss: 0.00436007883399725\n",
      "Mean loss: 0.010201661381870508\n",
      "Std loss: 0.005643983927957825\n",
      "Total Loss: 0.06120996829122305\n",
      "------------------------------------ epoch 7316 (43890 steps) ------------------------------------\n",
      "Max loss: 0.02790982648730278\n",
      "Min loss: 0.006482873111963272\n",
      "Mean loss: 0.012796855065971613\n",
      "Std loss: 0.007132964138572463\n",
      "Total Loss: 0.07678113039582968\n",
      "------------------------------------ epoch 7317 (43896 steps) ------------------------------------\n",
      "Max loss: 0.02957514487206936\n",
      "Min loss: 0.004675011616200209\n",
      "Mean loss: 0.01085343211889267\n",
      "Std loss: 0.008584731952916912\n",
      "Total Loss: 0.06512059271335602\n",
      "------------------------------------ epoch 7318 (43902 steps) ------------------------------------\n",
      "Max loss: 0.014026871882379055\n",
      "Min loss: 0.004741828888654709\n",
      "Mean loss: 0.009205061942338943\n",
      "Std loss: 0.0032282227865797946\n",
      "Total Loss: 0.05523037165403366\n",
      "------------------------------------ epoch 7319 (43908 steps) ------------------------------------\n",
      "Max loss: 0.016459167003631592\n",
      "Min loss: 0.005830077454447746\n",
      "Mean loss: 0.010621591005474329\n",
      "Std loss: 0.004364041686596325\n",
      "Total Loss: 0.06372954603284597\n",
      "------------------------------------ epoch 7320 (43914 steps) ------------------------------------\n",
      "Max loss: 0.0238190870732069\n",
      "Min loss: 0.005238770507276058\n",
      "Mean loss: 0.01262450156112512\n",
      "Std loss: 0.006972090005095948\n",
      "Total Loss: 0.07574700936675072\n",
      "------------------------------------ epoch 7321 (43920 steps) ------------------------------------\n",
      "Max loss: 0.0107035581022501\n",
      "Min loss: 0.006620383821427822\n",
      "Mean loss: 0.008438048729052147\n",
      "Std loss: 0.0015366503904248197\n",
      "Total Loss: 0.05062829237431288\n",
      "------------------------------------ epoch 7322 (43926 steps) ------------------------------------\n",
      "Max loss: 0.007315070368349552\n",
      "Min loss: 0.004062396939843893\n",
      "Mean loss: 0.00641118820446233\n",
      "Std loss: 0.0010948507779559391\n",
      "Total Loss: 0.03846712922677398\n",
      "------------------------------------ epoch 7323 (43932 steps) ------------------------------------\n",
      "Max loss: 0.013653233647346497\n",
      "Min loss: 0.00389303220435977\n",
      "Mean loss: 0.007577178999781609\n",
      "Std loss: 0.003241401015991151\n",
      "Total Loss: 0.04546307399868965\n",
      "------------------------------------ epoch 7324 (43938 steps) ------------------------------------\n",
      "Max loss: 0.013980652205646038\n",
      "Min loss: 0.003675650805234909\n",
      "Mean loss: 0.0075205072450141115\n",
      "Std loss: 0.0033367651691968846\n",
      "Total Loss: 0.04512304347008467\n",
      "------------------------------------ epoch 7325 (43944 steps) ------------------------------------\n",
      "Max loss: 0.024071842432022095\n",
      "Min loss: 0.004525403026491404\n",
      "Mean loss: 0.010041024768725038\n",
      "Std loss: 0.006792133412861254\n",
      "Total Loss: 0.060246148612350225\n",
      "------------------------------------ epoch 7326 (43950 steps) ------------------------------------\n",
      "Max loss: 0.009890426881611347\n",
      "Min loss: 0.00512711564078927\n",
      "Mean loss: 0.006962594498569767\n",
      "Std loss: 0.0016909988757625271\n",
      "Total Loss: 0.0417755669914186\n",
      "------------------------------------ epoch 7327 (43956 steps) ------------------------------------\n",
      "Max loss: 0.008433492854237556\n",
      "Min loss: 0.004492014180868864\n",
      "Mean loss: 0.006731582417463263\n",
      "Std loss: 0.00153201335016521\n",
      "Total Loss: 0.04038949450477958\n",
      "------------------------------------ epoch 7328 (43962 steps) ------------------------------------\n",
      "Max loss: 0.014337398111820221\n",
      "Min loss: 0.005573272239416838\n",
      "Mean loss: 0.008596111787483096\n",
      "Std loss: 0.0030502975891981555\n",
      "Total Loss: 0.05157667072489858\n",
      "------------------------------------ epoch 7329 (43968 steps) ------------------------------------\n",
      "Max loss: 0.010309169068932533\n",
      "Min loss: 0.005309074651449919\n",
      "Mean loss: 0.00813624495640397\n",
      "Std loss: 0.0019223374847439987\n",
      "Total Loss: 0.048817469738423824\n",
      "------------------------------------ epoch 7330 (43974 steps) ------------------------------------\n",
      "Max loss: 0.01496818196028471\n",
      "Min loss: 0.005686478223651648\n",
      "Mean loss: 0.008545829836900035\n",
      "Std loss: 0.003004749125777156\n",
      "Total Loss: 0.05127497902140021\n",
      "------------------------------------ epoch 7331 (43980 steps) ------------------------------------\n",
      "Max loss: 0.00919312797486782\n",
      "Min loss: 0.0037579487543553114\n",
      "Mean loss: 0.005432755414706965\n",
      "Std loss: 0.001790705770831693\n",
      "Total Loss: 0.03259653248824179\n",
      "------------------------------------ epoch 7332 (43986 steps) ------------------------------------\n",
      "Max loss: 0.01374298520386219\n",
      "Min loss: 0.0045759957283735275\n",
      "Mean loss: 0.00701748626306653\n",
      "Std loss: 0.0031499051694142805\n",
      "Total Loss: 0.04210491757839918\n",
      "------------------------------------ epoch 7333 (43992 steps) ------------------------------------\n",
      "Max loss: 0.017683299258351326\n",
      "Min loss: 0.004559380002319813\n",
      "Mean loss: 0.009001926674197117\n",
      "Std loss: 0.00451950482645036\n",
      "Total Loss: 0.054011560045182705\n",
      "------------------------------------ epoch 7334 (43998 steps) ------------------------------------\n",
      "Max loss: 0.010909277014434338\n",
      "Min loss: 0.004972353577613831\n",
      "Mean loss: 0.00712802610360086\n",
      "Std loss: 0.0024134377943963474\n",
      "Total Loss: 0.04276815662160516\n",
      "------------------------------------ epoch 7335 (44004 steps) ------------------------------------\n",
      "Max loss: 0.013371137902140617\n",
      "Min loss: 0.003474762663245201\n",
      "Mean loss: 0.00627269238854448\n",
      "Std loss: 0.0032584965402206686\n",
      "Total Loss: 0.03763615433126688\n",
      "------------------------------------ epoch 7336 (44010 steps) ------------------------------------\n",
      "Max loss: 0.015704788267612457\n",
      "Min loss: 0.003757125698029995\n",
      "Mean loss: 0.00669316528365016\n",
      "Std loss: 0.004221190480965779\n",
      "Total Loss: 0.04015899170190096\n",
      "------------------------------------ epoch 7337 (44016 steps) ------------------------------------\n",
      "Max loss: 0.016461065039038658\n",
      "Min loss: 0.004574727267026901\n",
      "Mean loss: 0.010734712782626351\n",
      "Std loss: 0.00462130547827306\n",
      "Total Loss: 0.0644082766957581\n",
      "------------------------------------ epoch 7338 (44022 steps) ------------------------------------\n",
      "Max loss: 0.008796878159046173\n",
      "Min loss: 0.006244295742362738\n",
      "Mean loss: 0.007302968141933282\n",
      "Std loss: 0.0009898833441722107\n",
      "Total Loss: 0.04381780885159969\n",
      "------------------------------------ epoch 7339 (44028 steps) ------------------------------------\n",
      "Max loss: 0.013663249090313911\n",
      "Min loss: 0.00459400424733758\n",
      "Mean loss: 0.007483465829864144\n",
      "Std loss: 0.0029212971024576624\n",
      "Total Loss: 0.044900794979184866\n",
      "------------------------------------ epoch 7340 (44034 steps) ------------------------------------\n",
      "Max loss: 0.022494345903396606\n",
      "Min loss: 0.0041496302001178265\n",
      "Mean loss: 0.009796069391692678\n",
      "Std loss: 0.006517698718744864\n",
      "Total Loss: 0.05877641635015607\n",
      "------------------------------------ epoch 7341 (44040 steps) ------------------------------------\n",
      "Max loss: 0.016306325793266296\n",
      "Min loss: 0.004461830016225576\n",
      "Mean loss: 0.0075010210275650024\n",
      "Std loss: 0.004126130422548378\n",
      "Total Loss: 0.045006126165390015\n",
      "------------------------------------ epoch 7342 (44046 steps) ------------------------------------\n",
      "Max loss: 0.017065083608031273\n",
      "Min loss: 0.005775805562734604\n",
      "Mean loss: 0.008390197840829691\n",
      "Std loss: 0.003916769313950584\n",
      "Total Loss: 0.05034118704497814\n",
      "------------------------------------ epoch 7343 (44052 steps) ------------------------------------\n",
      "Max loss: 0.012017503380775452\n",
      "Min loss: 0.004764151759445667\n",
      "Mean loss: 0.006952974867696564\n",
      "Std loss: 0.002611426520646163\n",
      "Total Loss: 0.04171784920617938\n",
      "------------------------------------ epoch 7344 (44058 steps) ------------------------------------\n",
      "Max loss: 0.04042843356728554\n",
      "Min loss: 0.005050066392868757\n",
      "Mean loss: 0.014466203283518553\n",
      "Std loss: 0.012498171816811591\n",
      "Total Loss: 0.08679721970111132\n",
      "------------------------------------ epoch 7345 (44064 steps) ------------------------------------\n",
      "Max loss: 0.011427052319049835\n",
      "Min loss: 0.004686848726123571\n",
      "Mean loss: 0.006904931428531806\n",
      "Std loss: 0.002552757960061347\n",
      "Total Loss: 0.041429588571190834\n",
      "------------------------------------ epoch 7346 (44070 steps) ------------------------------------\n",
      "Max loss: 0.020310085266828537\n",
      "Min loss: 0.004292814992368221\n",
      "Mean loss: 0.008914954261854291\n",
      "Std loss: 0.005522214915544448\n",
      "Total Loss: 0.053489725571125746\n",
      "------------------------------------ epoch 7347 (44076 steps) ------------------------------------\n",
      "Max loss: 0.02913212776184082\n",
      "Min loss: 0.0047932155430316925\n",
      "Mean loss: 0.016782861358175676\n",
      "Std loss: 0.008097453648715964\n",
      "Total Loss: 0.10069716814905405\n",
      "------------------------------------ epoch 7348 (44082 steps) ------------------------------------\n",
      "Max loss: 0.01819385215640068\n",
      "Min loss: 0.005175565369427204\n",
      "Mean loss: 0.008415260895465812\n",
      "Std loss: 0.004560256979112779\n",
      "Total Loss: 0.05049156537279487\n",
      "------------------------------------ epoch 7349 (44088 steps) ------------------------------------\n",
      "Max loss: 0.025864705443382263\n",
      "Min loss: 0.005150993820279837\n",
      "Mean loss: 0.013411169794077674\n",
      "Std loss: 0.007156539759782774\n",
      "Total Loss: 0.08046701876446605\n",
      "------------------------------------ epoch 7350 (44094 steps) ------------------------------------\n",
      "Max loss: 0.017101315781474113\n",
      "Min loss: 0.0057402728125452995\n",
      "Mean loss: 0.010936991001168886\n",
      "Std loss: 0.003355087880615591\n",
      "Total Loss: 0.06562194600701332\n",
      "------------------------------------ epoch 7351 (44100 steps) ------------------------------------\n",
      "Max loss: 0.01473438274115324\n",
      "Min loss: 0.00811518169939518\n",
      "Mean loss: 0.010685122106224298\n",
      "Std loss: 0.002384694576111973\n",
      "Total Loss: 0.06411073263734579\n",
      "------------------------------------ epoch 7352 (44106 steps) ------------------------------------\n",
      "Max loss: 0.018749762326478958\n",
      "Min loss: 0.00474916398525238\n",
      "Mean loss: 0.010448505325863758\n",
      "Std loss: 0.0054043147390960685\n",
      "Total Loss: 0.06269103195518255\n",
      "------------------------------------ epoch 7353 (44112 steps) ------------------------------------\n",
      "Max loss: 0.009951189160346985\n",
      "Min loss: 0.004322548396885395\n",
      "Mean loss: 0.0073093749427547055\n",
      "Std loss: 0.0020143104611289346\n",
      "Total Loss: 0.043856249656528234\n",
      "------------------------------------ epoch 7354 (44118 steps) ------------------------------------\n",
      "Max loss: 0.023729344829916954\n",
      "Min loss: 0.006163110490888357\n",
      "Mean loss: 0.011367821367457509\n",
      "Std loss: 0.006980241117933388\n",
      "Total Loss: 0.06820692820474505\n",
      "------------------------------------ epoch 7355 (44124 steps) ------------------------------------\n",
      "Max loss: 0.0122501440346241\n",
      "Min loss: 0.004936844576150179\n",
      "Mean loss: 0.007984074919174114\n",
      "Std loss: 0.0024880791341285666\n",
      "Total Loss: 0.04790444951504469\n",
      "------------------------------------ epoch 7356 (44130 steps) ------------------------------------\n",
      "Max loss: 0.01169527880847454\n",
      "Min loss: 0.003654284169897437\n",
      "Mean loss: 0.00739276350941509\n",
      "Std loss: 0.0031614607128448596\n",
      "Total Loss: 0.04435658105649054\n",
      "------------------------------------ epoch 7357 (44136 steps) ------------------------------------\n",
      "Max loss: 0.006898742169141769\n",
      "Min loss: 0.00417736591771245\n",
      "Mean loss: 0.005166805116459727\n",
      "Std loss: 0.0008767515727627163\n",
      "Total Loss: 0.031000830698758364\n",
      "------------------------------------ epoch 7358 (44142 steps) ------------------------------------\n",
      "Max loss: 0.0062086088582873344\n",
      "Min loss: 0.0033551803790032864\n",
      "Mean loss: 0.005226847094794114\n",
      "Std loss: 0.0009612071932161579\n",
      "Total Loss: 0.03136108256876469\n",
      "------------------------------------ epoch 7359 (44148 steps) ------------------------------------\n",
      "Max loss: 0.02090468257665634\n",
      "Min loss: 0.0035706963390111923\n",
      "Mean loss: 0.009975611620272199\n",
      "Std loss: 0.005561569207622403\n",
      "Total Loss: 0.059853669721633196\n",
      "------------------------------------ epoch 7360 (44154 steps) ------------------------------------\n",
      "Max loss: 0.00944557785987854\n",
      "Min loss: 0.005827184766530991\n",
      "Mean loss: 0.0070785948385794955\n",
      "Std loss: 0.001309086409899621\n",
      "Total Loss: 0.042471569031476974\n",
      "------------------------------------ epoch 7361 (44160 steps) ------------------------------------\n",
      "Max loss: 0.03360367938876152\n",
      "Min loss: 0.004833916202187538\n",
      "Mean loss: 0.01399841501067082\n",
      "Std loss: 0.009833191298308168\n",
      "Total Loss: 0.08399049006402493\n",
      "------------------------------------ epoch 7362 (44166 steps) ------------------------------------\n",
      "Max loss: 0.016759317368268967\n",
      "Min loss: 0.004095065873116255\n",
      "Mean loss: 0.008911472745239735\n",
      "Std loss: 0.004795186919124277\n",
      "Total Loss: 0.05346883647143841\n",
      "------------------------------------ epoch 7363 (44172 steps) ------------------------------------\n",
      "Max loss: 0.02532312646508217\n",
      "Min loss: 0.00509982043877244\n",
      "Mean loss: 0.011825607856735587\n",
      "Std loss: 0.006448521937024891\n",
      "Total Loss: 0.07095364714041352\n",
      "------------------------------------ epoch 7364 (44178 steps) ------------------------------------\n",
      "Max loss: 0.013221151195466518\n",
      "Min loss: 0.0042521897703409195\n",
      "Mean loss: 0.008226328374197086\n",
      "Std loss: 0.0031788448074118163\n",
      "Total Loss: 0.049357970245182514\n",
      "------------------------------------ epoch 7365 (44184 steps) ------------------------------------\n",
      "Max loss: 0.01771949604153633\n",
      "Min loss: 0.004745694808661938\n",
      "Mean loss: 0.009636244348560771\n",
      "Std loss: 0.004649674454078951\n",
      "Total Loss: 0.05781746609136462\n",
      "------------------------------------ epoch 7366 (44190 steps) ------------------------------------\n",
      "Max loss: 0.011570718139410019\n",
      "Min loss: 0.005803348496556282\n",
      "Mean loss: 0.007990020327270031\n",
      "Std loss: 0.0019115014138041723\n",
      "Total Loss: 0.047940121963620186\n",
      "------------------------------------ epoch 7367 (44196 steps) ------------------------------------\n",
      "Max loss: 0.01393936574459076\n",
      "Min loss: 0.00505093252286315\n",
      "Mean loss: 0.007612579269334674\n",
      "Std loss: 0.0031116258756817344\n",
      "Total Loss: 0.04567547561600804\n",
      "------------------------------------ epoch 7368 (44202 steps) ------------------------------------\n",
      "Max loss: 0.012739129364490509\n",
      "Min loss: 0.004382184240967035\n",
      "Mean loss: 0.006605776259675622\n",
      "Std loss: 0.002891487029920928\n",
      "Total Loss: 0.03963465755805373\n",
      "------------------------------------ epoch 7369 (44208 steps) ------------------------------------\n",
      "Max loss: 0.012413451448082924\n",
      "Min loss: 0.006048382725566626\n",
      "Mean loss: 0.007382850938787063\n",
      "Std loss: 0.002258327191851985\n",
      "Total Loss: 0.04429710563272238\n",
      "------------------------------------ epoch 7370 (44214 steps) ------------------------------------\n",
      "Max loss: 0.009980977512896061\n",
      "Min loss: 0.005198488011956215\n",
      "Mean loss: 0.007586910156533122\n",
      "Std loss: 0.0014932795302765587\n",
      "Total Loss: 0.04552146093919873\n",
      "------------------------------------ epoch 7371 (44220 steps) ------------------------------------\n",
      "Max loss: 0.04997692629694939\n",
      "Min loss: 0.006012320052832365\n",
      "Mean loss: 0.01688554968374471\n",
      "Std loss: 0.015132503970671602\n",
      "Total Loss: 0.10131329810246825\n",
      "------------------------------------ epoch 7372 (44226 steps) ------------------------------------\n",
      "Max loss: 0.017703719437122345\n",
      "Min loss: 0.005284136161208153\n",
      "Mean loss: 0.00940132001414895\n",
      "Std loss: 0.003964401057480145\n",
      "Total Loss: 0.0564079200848937\n",
      "------------------------------------ epoch 7373 (44232 steps) ------------------------------------\n",
      "Max loss: 0.017199169844388962\n",
      "Min loss: 0.0054933237843215466\n",
      "Mean loss: 0.01004092845444878\n",
      "Std loss: 0.003900195733984549\n",
      "Total Loss: 0.06024557072669268\n",
      "------------------------------------ epoch 7374 (44238 steps) ------------------------------------\n",
      "Max loss: 0.011779038235545158\n",
      "Min loss: 0.005309011787176132\n",
      "Mean loss: 0.007997944873447219\n",
      "Std loss: 0.002426679559323917\n",
      "Total Loss: 0.04798766924068332\n",
      "------------------------------------ epoch 7375 (44244 steps) ------------------------------------\n",
      "Max loss: 0.022359199821949005\n",
      "Min loss: 0.003851780202239752\n",
      "Mean loss: 0.009438327280804515\n",
      "Std loss: 0.006814283304111433\n",
      "Total Loss: 0.05662996368482709\n",
      "------------------------------------ epoch 7376 (44250 steps) ------------------------------------\n",
      "Max loss: 0.014347819611430168\n",
      "Min loss: 0.00459007965400815\n",
      "Mean loss: 0.008498041269679865\n",
      "Std loss: 0.003670481938274886\n",
      "Total Loss: 0.050988247618079185\n",
      "------------------------------------ epoch 7377 (44256 steps) ------------------------------------\n",
      "Max loss: 0.016994234174489975\n",
      "Min loss: 0.006102409213781357\n",
      "Mean loss: 0.010364181905363997\n",
      "Std loss: 0.0038502516222237754\n",
      "Total Loss: 0.06218509143218398\n",
      "------------------------------------ epoch 7378 (44262 steps) ------------------------------------\n",
      "Max loss: 0.013774700462818146\n",
      "Min loss: 0.004796968773007393\n",
      "Mean loss: 0.009933772186438242\n",
      "Std loss: 0.0031126242877302745\n",
      "Total Loss: 0.059602633118629456\n",
      "------------------------------------ epoch 7379 (44268 steps) ------------------------------------\n",
      "Max loss: 0.04254576563835144\n",
      "Min loss: 0.004538329318165779\n",
      "Mean loss: 0.01388463742720584\n",
      "Std loss: 0.013138002701389002\n",
      "Total Loss: 0.08330782456323504\n",
      "------------------------------------ epoch 7380 (44274 steps) ------------------------------------\n",
      "Max loss: 0.036979906260967255\n",
      "Min loss: 0.008570277132093906\n",
      "Mean loss: 0.014200158106784025\n",
      "Std loss: 0.010286814798926278\n",
      "Total Loss: 0.08520094864070415\n",
      "------------------------------------ epoch 7381 (44280 steps) ------------------------------------\n",
      "Max loss: 0.016822438687086105\n",
      "Min loss: 0.004982339683920145\n",
      "Mean loss: 0.007981797757868966\n",
      "Std loss: 0.004014118998239921\n",
      "Total Loss: 0.04789078654721379\n",
      "------------------------------------ epoch 7382 (44286 steps) ------------------------------------\n",
      "Max loss: 0.016441239044070244\n",
      "Min loss: 0.006251792423427105\n",
      "Mean loss: 0.01091035750384132\n",
      "Std loss: 0.0035705814409215967\n",
      "Total Loss: 0.06546214502304792\n",
      "------------------------------------ epoch 7383 (44292 steps) ------------------------------------\n",
      "Max loss: 0.011116250418126583\n",
      "Min loss: 0.004693580791354179\n",
      "Mean loss: 0.006842688812563817\n",
      "Std loss: 0.0022752402723853478\n",
      "Total Loss: 0.0410561328753829\n",
      "------------------------------------ epoch 7384 (44298 steps) ------------------------------------\n",
      "Max loss: 0.03324092924594879\n",
      "Min loss: 0.005408962722867727\n",
      "Mean loss: 0.014030524917567769\n",
      "Std loss: 0.009607740250029678\n",
      "Total Loss: 0.08418314950540662\n",
      "------------------------------------ epoch 7385 (44304 steps) ------------------------------------\n",
      "Max loss: 0.03671100735664368\n",
      "Min loss: 0.00482749380171299\n",
      "Mean loss: 0.01703250703091423\n",
      "Std loss: 0.012257934541930039\n",
      "Total Loss: 0.10219504218548536\n",
      "------------------------------------ epoch 7386 (44310 steps) ------------------------------------\n",
      "Max loss: 0.017094215378165245\n",
      "Min loss: 0.005432700272649527\n",
      "Mean loss: 0.009916210624699792\n",
      "Std loss: 0.003705331616023404\n",
      "Total Loss: 0.05949726374819875\n",
      "------------------------------------ epoch 7387 (44316 steps) ------------------------------------\n",
      "Max loss: 0.00933590903878212\n",
      "Min loss: 0.0056622507981956005\n",
      "Mean loss: 0.006996220753838618\n",
      "Std loss: 0.0011999003729168478\n",
      "Total Loss: 0.04197732452303171\n",
      "------------------------------------ epoch 7388 (44322 steps) ------------------------------------\n",
      "Max loss: 0.02903447113931179\n",
      "Min loss: 0.006105600856244564\n",
      "Mean loss: 0.013989568610365192\n",
      "Std loss: 0.00914140931562317\n",
      "Total Loss: 0.08393741166219115\n",
      "------------------------------------ epoch 7389 (44328 steps) ------------------------------------\n",
      "Max loss: 0.009649014100432396\n",
      "Min loss: 0.005377429537475109\n",
      "Mean loss: 0.006837362889200449\n",
      "Std loss: 0.0014450990296199459\n",
      "Total Loss: 0.041024177335202694\n",
      "------------------------------------ epoch 7390 (44334 steps) ------------------------------------\n",
      "Max loss: 0.010553760454058647\n",
      "Min loss: 0.0055525596253573895\n",
      "Mean loss: 0.0073357559740543365\n",
      "Std loss: 0.001547422838218085\n",
      "Total Loss: 0.04401453584432602\n",
      "------------------------------------ epoch 7391 (44340 steps) ------------------------------------\n",
      "Max loss: 0.01378646306693554\n",
      "Min loss: 0.00554734468460083\n",
      "Mean loss: 0.00919099384918809\n",
      "Std loss: 0.0029433274066974\n",
      "Total Loss: 0.055145963095128536\n",
      "------------------------------------ epoch 7392 (44346 steps) ------------------------------------\n",
      "Max loss: 0.015373359434306622\n",
      "Min loss: 0.0054806992411613464\n",
      "Mean loss: 0.0083895706726859\n",
      "Std loss: 0.003396829315895057\n",
      "Total Loss: 0.05033742403611541\n",
      "------------------------------------ epoch 7393 (44352 steps) ------------------------------------\n",
      "Max loss: 0.030985265970230103\n",
      "Min loss: 0.00610970426350832\n",
      "Mean loss: 0.012907218420878053\n",
      "Std loss: 0.009071901633187813\n",
      "Total Loss: 0.07744331052526832\n",
      "------------------------------------ epoch 7394 (44358 steps) ------------------------------------\n",
      "Max loss: 0.011835127137601376\n",
      "Min loss: 0.004895299673080444\n",
      "Mean loss: 0.008432654431089759\n",
      "Std loss: 0.0025844772245934805\n",
      "Total Loss: 0.05059592658653855\n",
      "------------------------------------ epoch 7395 (44364 steps) ------------------------------------\n",
      "Max loss: 0.01843067817389965\n",
      "Min loss: 0.005056068301200867\n",
      "Mean loss: 0.009703762518862883\n",
      "Std loss: 0.005144905191486699\n",
      "Total Loss: 0.0582225751131773\n",
      "------------------------------------ epoch 7396 (44370 steps) ------------------------------------\n",
      "Max loss: 0.022371914237737656\n",
      "Min loss: 0.004595362581312656\n",
      "Mean loss: 0.0110564223335435\n",
      "Std loss: 0.006527195502056206\n",
      "Total Loss: 0.066338534001261\n",
      "------------------------------------ epoch 7397 (44376 steps) ------------------------------------\n",
      "Max loss: 0.013007275760173798\n",
      "Min loss: 0.005321910139173269\n",
      "Mean loss: 0.008632994179303447\n",
      "Std loss: 0.00254471362798133\n",
      "Total Loss: 0.051797965075820684\n",
      "------------------------------------ epoch 7398 (44382 steps) ------------------------------------\n",
      "Max loss: 0.019425995647907257\n",
      "Min loss: 0.0074008069932460785\n",
      "Mean loss: 0.011325776887436708\n",
      "Std loss: 0.004074426064373433\n",
      "Total Loss: 0.06795466132462025\n",
      "------------------------------------ epoch 7399 (44388 steps) ------------------------------------\n",
      "Max loss: 0.022258739918470383\n",
      "Min loss: 0.005782347172498703\n",
      "Mean loss: 0.010847910307347775\n",
      "Std loss: 0.0063617023247717055\n",
      "Total Loss: 0.06508746184408665\n",
      "------------------------------------ epoch 7400 (44394 steps) ------------------------------------\n",
      "Max loss: 0.03127081319689751\n",
      "Min loss: 0.0062812804244458675\n",
      "Mean loss: 0.013822564001505574\n",
      "Std loss: 0.008763288439401117\n",
      "Total Loss: 0.08293538400903344\n",
      "------------------------------------ epoch 7401 (44400 steps) ------------------------------------\n",
      "Max loss: 0.02357511967420578\n",
      "Min loss: 0.004905274603515863\n",
      "Mean loss: 0.010705253264556328\n",
      "Std loss: 0.007009005051112119\n",
      "Total Loss: 0.06423151958733797\n",
      "saved model at ./weights/model_7401.pth\n",
      "------------------------------------ epoch 7402 (44406 steps) ------------------------------------\n",
      "Max loss: 0.018499694764614105\n",
      "Min loss: 0.005063406191766262\n",
      "Mean loss: 0.010217236199726662\n",
      "Std loss: 0.004174214843825196\n",
      "Total Loss: 0.061303417198359966\n",
      "------------------------------------ epoch 7403 (44412 steps) ------------------------------------\n",
      "Max loss: 0.04338308796286583\n",
      "Min loss: 0.004070121794939041\n",
      "Mean loss: 0.015260088878373304\n",
      "Std loss: 0.013799466076971285\n",
      "Total Loss: 0.09156053327023983\n",
      "------------------------------------ epoch 7404 (44418 steps) ------------------------------------\n",
      "Max loss: 0.0241874810308218\n",
      "Min loss: 0.004816349130123854\n",
      "Mean loss: 0.010057111425946156\n",
      "Std loss: 0.006708883542864545\n",
      "Total Loss: 0.06034266855567694\n",
      "------------------------------------ epoch 7405 (44424 steps) ------------------------------------\n",
      "Max loss: 0.014280500821769238\n",
      "Min loss: 0.005390805657953024\n",
      "Mean loss: 0.010771044297143817\n",
      "Std loss: 0.0034180802611760005\n",
      "Total Loss: 0.0646262657828629\n",
      "------------------------------------ epoch 7406 (44430 steps) ------------------------------------\n",
      "Max loss: 0.018550164997577667\n",
      "Min loss: 0.004903365857899189\n",
      "Mean loss: 0.010438370828827223\n",
      "Std loss: 0.004540422311649632\n",
      "Total Loss: 0.06263022497296333\n",
      "------------------------------------ epoch 7407 (44436 steps) ------------------------------------\n",
      "Max loss: 0.01739881932735443\n",
      "Min loss: 0.00428392831236124\n",
      "Mean loss: 0.0076570639697213965\n",
      "Std loss: 0.004523947623119156\n",
      "Total Loss: 0.04594238381832838\n",
      "------------------------------------ epoch 7408 (44442 steps) ------------------------------------\n",
      "Max loss: 0.019383439794182777\n",
      "Min loss: 0.007281487341970205\n",
      "Mean loss: 0.011150017225493988\n",
      "Std loss: 0.004079944029375354\n",
      "Total Loss: 0.06690010335296392\n",
      "------------------------------------ epoch 7409 (44448 steps) ------------------------------------\n",
      "Max loss: 0.01690387725830078\n",
      "Min loss: 0.004046540707349777\n",
      "Mean loss: 0.009210141530881325\n",
      "Std loss: 0.004595411450072296\n",
      "Total Loss: 0.05526084918528795\n",
      "------------------------------------ epoch 7410 (44454 steps) ------------------------------------\n",
      "Max loss: 0.013814177364110947\n",
      "Min loss: 0.004066058434545994\n",
      "Mean loss: 0.008203710662201047\n",
      "Std loss: 0.003988349463445498\n",
      "Total Loss: 0.04922226397320628\n",
      "------------------------------------ epoch 7411 (44460 steps) ------------------------------------\n",
      "Max loss: 0.013785707764327526\n",
      "Min loss: 0.0053170546889305115\n",
      "Mean loss: 0.008019334015746912\n",
      "Std loss: 0.002858953341004263\n",
      "Total Loss: 0.04811600409448147\n",
      "------------------------------------ epoch 7412 (44466 steps) ------------------------------------\n",
      "Max loss: 0.014146050438284874\n",
      "Min loss: 0.004062626976519823\n",
      "Mean loss: 0.007137504794324438\n",
      "Std loss: 0.0034481966708171177\n",
      "Total Loss: 0.04282502876594663\n",
      "------------------------------------ epoch 7413 (44472 steps) ------------------------------------\n",
      "Max loss: 0.012638313695788383\n",
      "Min loss: 0.004021892324090004\n",
      "Mean loss: 0.007728364939490954\n",
      "Std loss: 0.0036780806143284182\n",
      "Total Loss: 0.046370189636945724\n",
      "------------------------------------ epoch 7414 (44478 steps) ------------------------------------\n",
      "Max loss: 0.010708026587963104\n",
      "Min loss: 0.005176329519599676\n",
      "Mean loss: 0.006744867733990152\n",
      "Std loss: 0.0018277934918911748\n",
      "Total Loss: 0.040469206403940916\n",
      "------------------------------------ epoch 7415 (44484 steps) ------------------------------------\n",
      "Max loss: 0.05404628440737724\n",
      "Min loss: 0.005056992173194885\n",
      "Mean loss: 0.014535010249043504\n",
      "Std loss: 0.017695352896586688\n",
      "Total Loss: 0.08721006149426103\n",
      "------------------------------------ epoch 7416 (44490 steps) ------------------------------------\n",
      "Max loss: 0.02382887527346611\n",
      "Min loss: 0.004583143163472414\n",
      "Mean loss: 0.009429864818230271\n",
      "Std loss: 0.006672223809496653\n",
      "Total Loss: 0.05657918890938163\n",
      "------------------------------------ epoch 7417 (44496 steps) ------------------------------------\n",
      "Max loss: 0.018535617738962173\n",
      "Min loss: 0.005003714002668858\n",
      "Mean loss: 0.010927489260211587\n",
      "Std loss: 0.0055993565426464475\n",
      "Total Loss: 0.06556493556126952\n",
      "------------------------------------ epoch 7418 (44502 steps) ------------------------------------\n",
      "Max loss: 0.02168317697942257\n",
      "Min loss: 0.006345855072140694\n",
      "Mean loss: 0.010632641846314073\n",
      "Std loss: 0.005185751135278464\n",
      "Total Loss: 0.06379585107788444\n",
      "------------------------------------ epoch 7419 (44508 steps) ------------------------------------\n",
      "Max loss: 0.050998128950595856\n",
      "Min loss: 0.009484811685979366\n",
      "Mean loss: 0.020415447031458218\n",
      "Std loss: 0.014455970072390707\n",
      "Total Loss: 0.12249268218874931\n",
      "------------------------------------ epoch 7420 (44514 steps) ------------------------------------\n",
      "Max loss: 0.021278386935591698\n",
      "Min loss: 0.004585339222103357\n",
      "Mean loss: 0.01195626170374453\n",
      "Std loss: 0.006234589973180942\n",
      "Total Loss: 0.07173757022246718\n",
      "------------------------------------ epoch 7421 (44520 steps) ------------------------------------\n",
      "Max loss: 0.03081246092915535\n",
      "Min loss: 0.005070221610367298\n",
      "Mean loss: 0.010768715757876635\n",
      "Std loss: 0.009002024478660373\n",
      "Total Loss: 0.06461229454725981\n",
      "------------------------------------ epoch 7422 (44526 steps) ------------------------------------\n",
      "Max loss: 0.02284221351146698\n",
      "Min loss: 0.005708552896976471\n",
      "Mean loss: 0.012665186543017626\n",
      "Std loss: 0.005379189276957732\n",
      "Total Loss: 0.07599111925810575\n",
      "------------------------------------ epoch 7423 (44532 steps) ------------------------------------\n",
      "Max loss: 0.008937663398683071\n",
      "Min loss: 0.004807493183761835\n",
      "Mean loss: 0.007267606211826205\n",
      "Std loss: 0.0014817502183343636\n",
      "Total Loss: 0.04360563727095723\n",
      "------------------------------------ epoch 7424 (44538 steps) ------------------------------------\n",
      "Max loss: 0.007878832519054413\n",
      "Min loss: 0.004799584858119488\n",
      "Mean loss: 0.006177560891956091\n",
      "Std loss: 0.0010122008812484683\n",
      "Total Loss: 0.037065365351736546\n",
      "------------------------------------ epoch 7425 (44544 steps) ------------------------------------\n",
      "Max loss: 0.044598113745450974\n",
      "Min loss: 0.0054062847048044205\n",
      "Mean loss: 0.018359908368438482\n",
      "Std loss: 0.01309942595903043\n",
      "Total Loss: 0.1101594502106309\n",
      "------------------------------------ epoch 7426 (44550 steps) ------------------------------------\n",
      "Max loss: 0.009384183213114738\n",
      "Min loss: 0.0049129389226436615\n",
      "Mean loss: 0.0073356167413294315\n",
      "Std loss: 0.0018818604525835641\n",
      "Total Loss: 0.04401370044797659\n",
      "------------------------------------ epoch 7427 (44556 steps) ------------------------------------\n",
      "Max loss: 0.02191416546702385\n",
      "Min loss: 0.005354568362236023\n",
      "Mean loss: 0.009498631426443657\n",
      "Std loss: 0.005743353623240341\n",
      "Total Loss: 0.05699178855866194\n",
      "------------------------------------ epoch 7428 (44562 steps) ------------------------------------\n",
      "Max loss: 0.01829710230231285\n",
      "Min loss: 0.00569568295031786\n",
      "Mean loss: 0.010206444344172875\n",
      "Std loss: 0.004641602826519695\n",
      "Total Loss: 0.06123866606503725\n",
      "------------------------------------ epoch 7429 (44568 steps) ------------------------------------\n",
      "Max loss: 0.01778554543852806\n",
      "Min loss: 0.0046402402222156525\n",
      "Mean loss: 0.008958391224344572\n",
      "Std loss: 0.005127602510346599\n",
      "Total Loss: 0.05375034734606743\n",
      "------------------------------------ epoch 7430 (44574 steps) ------------------------------------\n",
      "Max loss: 0.011439831927418709\n",
      "Min loss: 0.005059091839939356\n",
      "Mean loss: 0.007544988999143243\n",
      "Std loss: 0.0023500250075382404\n",
      "Total Loss: 0.04526993399485946\n",
      "------------------------------------ epoch 7431 (44580 steps) ------------------------------------\n",
      "Max loss: 0.028576401993632317\n",
      "Min loss: 0.0037892998661845922\n",
      "Mean loss: 0.01042748890661945\n",
      "Std loss: 0.008436754329073459\n",
      "Total Loss: 0.0625649334397167\n",
      "------------------------------------ epoch 7432 (44586 steps) ------------------------------------\n",
      "Max loss: 0.015284320339560509\n",
      "Min loss: 0.00522904796525836\n",
      "Mean loss: 0.010195507279907664\n",
      "Std loss: 0.003433602261955378\n",
      "Total Loss: 0.06117304367944598\n",
      "------------------------------------ epoch 7433 (44592 steps) ------------------------------------\n",
      "Max loss: 0.02629357948899269\n",
      "Min loss: 0.005547236185520887\n",
      "Mean loss: 0.012401026984055838\n",
      "Std loss: 0.007049282294243653\n",
      "Total Loss: 0.07440616190433502\n",
      "------------------------------------ epoch 7434 (44598 steps) ------------------------------------\n",
      "Max loss: 0.019883571192622185\n",
      "Min loss: 0.005825500935316086\n",
      "Mean loss: 0.009682786030073961\n",
      "Std loss: 0.004794316617925896\n",
      "Total Loss: 0.058096716180443764\n",
      "------------------------------------ epoch 7435 (44604 steps) ------------------------------------\n",
      "Max loss: 0.012948878109455109\n",
      "Min loss: 0.0058742607943713665\n",
      "Mean loss: 0.009088095044717193\n",
      "Std loss: 0.0027697860629415185\n",
      "Total Loss: 0.054528570268303156\n",
      "------------------------------------ epoch 7436 (44610 steps) ------------------------------------\n",
      "Max loss: 0.013346443884074688\n",
      "Min loss: 0.004511328414082527\n",
      "Mean loss: 0.007399695071702202\n",
      "Std loss: 0.0029830343754417912\n",
      "Total Loss: 0.04439817043021321\n",
      "------------------------------------ epoch 7437 (44616 steps) ------------------------------------\n",
      "Max loss: 0.02975943312048912\n",
      "Min loss: 0.00519060343503952\n",
      "Mean loss: 0.012886918382719159\n",
      "Std loss: 0.008524840735717935\n",
      "Total Loss: 0.07732151029631495\n",
      "------------------------------------ epoch 7438 (44622 steps) ------------------------------------\n",
      "Max loss: 0.006918627768754959\n",
      "Min loss: 0.0042190528474748135\n",
      "Mean loss: 0.005569314040864508\n",
      "Std loss: 0.0010276987038385056\n",
      "Total Loss: 0.033415884245187044\n",
      "------------------------------------ epoch 7439 (44628 steps) ------------------------------------\n",
      "Max loss: 0.015916841104626656\n",
      "Min loss: 0.004611419513821602\n",
      "Mean loss: 0.009474278427660465\n",
      "Std loss: 0.004120105107049301\n",
      "Total Loss: 0.05684567056596279\n",
      "------------------------------------ epoch 7440 (44634 steps) ------------------------------------\n",
      "Max loss: 0.02948404848575592\n",
      "Min loss: 0.005696525797247887\n",
      "Mean loss: 0.014629597309976816\n",
      "Std loss: 0.007789755139475671\n",
      "Total Loss: 0.0877775838598609\n",
      "------------------------------------ epoch 7441 (44640 steps) ------------------------------------\n",
      "Max loss: 0.031612589955329895\n",
      "Min loss: 0.005747830029577017\n",
      "Mean loss: 0.012952232034876943\n",
      "Std loss: 0.0088913808534574\n",
      "Total Loss: 0.07771339220926166\n",
      "------------------------------------ epoch 7442 (44646 steps) ------------------------------------\n",
      "Max loss: 0.010624043643474579\n",
      "Min loss: 0.00715153431519866\n",
      "Mean loss: 0.008932837362711629\n",
      "Std loss: 0.0012638183644914033\n",
      "Total Loss: 0.05359702417626977\n",
      "------------------------------------ epoch 7443 (44652 steps) ------------------------------------\n",
      "Max loss: 0.016667749732732773\n",
      "Min loss: 0.005099060945212841\n",
      "Mean loss: 0.00955803234440585\n",
      "Std loss: 0.004523178703547411\n",
      "Total Loss: 0.0573481940664351\n",
      "------------------------------------ epoch 7444 (44658 steps) ------------------------------------\n",
      "Max loss: 0.016441959887742996\n",
      "Min loss: 0.006209079176187515\n",
      "Mean loss: 0.010067553957924247\n",
      "Std loss: 0.003558123578943129\n",
      "Total Loss: 0.06040532374754548\n",
      "------------------------------------ epoch 7445 (44664 steps) ------------------------------------\n",
      "Max loss: 0.03456485643982887\n",
      "Min loss: 0.008087385445833206\n",
      "Mean loss: 0.015496437593052784\n",
      "Std loss: 0.00896868299851513\n",
      "Total Loss: 0.09297862555831671\n",
      "------------------------------------ epoch 7446 (44670 steps) ------------------------------------\n",
      "Max loss: 0.02279936894774437\n",
      "Min loss: 0.004859148524701595\n",
      "Mean loss: 0.012769922070826093\n",
      "Std loss: 0.007287124655773072\n",
      "Total Loss: 0.07661953242495656\n",
      "------------------------------------ epoch 7447 (44676 steps) ------------------------------------\n",
      "Max loss: 0.04578213393688202\n",
      "Min loss: 0.004409099463373423\n",
      "Mean loss: 0.013342675364886722\n",
      "Std loss: 0.014558028131694201\n",
      "Total Loss: 0.08005605218932033\n",
      "------------------------------------ epoch 7448 (44682 steps) ------------------------------------\n",
      "Max loss: 0.010641731321811676\n",
      "Min loss: 0.0057386658154428005\n",
      "Mean loss: 0.00889348603474597\n",
      "Std loss: 0.001697228323791058\n",
      "Total Loss: 0.05336091620847583\n",
      "------------------------------------ epoch 7449 (44688 steps) ------------------------------------\n",
      "Max loss: 0.055415716022253036\n",
      "Min loss: 0.005675342865288258\n",
      "Mean loss: 0.015055160271003842\n",
      "Std loss: 0.01808537475334031\n",
      "Total Loss: 0.09033096162602305\n",
      "------------------------------------ epoch 7450 (44694 steps) ------------------------------------\n",
      "Max loss: 0.008989734575152397\n",
      "Min loss: 0.006121724843978882\n",
      "Mean loss: 0.007534700135389964\n",
      "Std loss: 0.0010447267416938842\n",
      "Total Loss: 0.04520820081233978\n",
      "------------------------------------ epoch 7451 (44700 steps) ------------------------------------\n",
      "Max loss: 0.017699265852570534\n",
      "Min loss: 0.00632284302264452\n",
      "Mean loss: 0.010512853351732096\n",
      "Std loss: 0.0038508692332372154\n",
      "Total Loss: 0.06307712011039257\n",
      "------------------------------------ epoch 7452 (44706 steps) ------------------------------------\n",
      "Max loss: 0.013063011690974236\n",
      "Min loss: 0.005316238850355148\n",
      "Mean loss: 0.008309160669644674\n",
      "Std loss: 0.0029693015359685227\n",
      "Total Loss: 0.04985496401786804\n",
      "------------------------------------ epoch 7453 (44712 steps) ------------------------------------\n",
      "Max loss: 0.016648154705762863\n",
      "Min loss: 0.005590036045759916\n",
      "Mean loss: 0.011240317563836774\n",
      "Std loss: 0.0037760812768338654\n",
      "Total Loss: 0.06744190538302064\n",
      "------------------------------------ epoch 7454 (44718 steps) ------------------------------------\n",
      "Max loss: 0.030984215438365936\n",
      "Min loss: 0.004865560214966536\n",
      "Mean loss: 0.01648399932309985\n",
      "Std loss: 0.008880262032270766\n",
      "Total Loss: 0.09890399593859911\n",
      "------------------------------------ epoch 7455 (44724 steps) ------------------------------------\n",
      "Max loss: 0.03988233208656311\n",
      "Min loss: 0.0043087429367005825\n",
      "Mean loss: 0.011550675068671504\n",
      "Std loss: 0.01272659634156817\n",
      "Total Loss: 0.06930405041202903\n",
      "------------------------------------ epoch 7456 (44730 steps) ------------------------------------\n",
      "Max loss: 0.03621934726834297\n",
      "Min loss: 0.004883505869656801\n",
      "Mean loss: 0.014599094244961938\n",
      "Std loss: 0.010984777104653563\n",
      "Total Loss: 0.08759456546977162\n",
      "------------------------------------ epoch 7457 (44736 steps) ------------------------------------\n",
      "Max loss: 0.02098231576383114\n",
      "Min loss: 0.004945279099047184\n",
      "Mean loss: 0.009745432141547402\n",
      "Std loss: 0.0054386859449280225\n",
      "Total Loss: 0.05847259284928441\n",
      "------------------------------------ epoch 7458 (44742 steps) ------------------------------------\n",
      "Max loss: 0.02053452841937542\n",
      "Min loss: 0.0061446684412658215\n",
      "Mean loss: 0.011033790108437339\n",
      "Std loss: 0.004644029119732494\n",
      "Total Loss: 0.06620274065062404\n",
      "------------------------------------ epoch 7459 (44748 steps) ------------------------------------\n",
      "Max loss: 0.02502705156803131\n",
      "Min loss: 0.00612269714474678\n",
      "Mean loss: 0.014949333155527711\n",
      "Std loss: 0.007056590037401655\n",
      "Total Loss: 0.08969599893316627\n",
      "------------------------------------ epoch 7460 (44754 steps) ------------------------------------\n",
      "Max loss: 0.015618957579135895\n",
      "Min loss: 0.00522390753030777\n",
      "Mean loss: 0.008547673855597774\n",
      "Std loss: 0.0036030353449104315\n",
      "Total Loss: 0.051286043133586645\n",
      "------------------------------------ epoch 7461 (44760 steps) ------------------------------------\n",
      "Max loss: 0.01740894839167595\n",
      "Min loss: 0.004684225656092167\n",
      "Mean loss: 0.009412202518433332\n",
      "Std loss: 0.0042745553888542225\n",
      "Total Loss: 0.056473215110599995\n",
      "------------------------------------ epoch 7462 (44766 steps) ------------------------------------\n",
      "Max loss: 0.009487409144639969\n",
      "Min loss: 0.005647899582982063\n",
      "Mean loss: 0.007605200866237283\n",
      "Std loss: 0.0014373634840319164\n",
      "Total Loss: 0.045631205197423697\n",
      "------------------------------------ epoch 7463 (44772 steps) ------------------------------------\n",
      "Max loss: 0.009191997349262238\n",
      "Min loss: 0.004660679958760738\n",
      "Mean loss: 0.00744303905715545\n",
      "Std loss: 0.0015101567945095405\n",
      "Total Loss: 0.0446582343429327\n",
      "------------------------------------ epoch 7464 (44778 steps) ------------------------------------\n",
      "Max loss: 0.01774764060974121\n",
      "Min loss: 0.004784655757248402\n",
      "Mean loss: 0.01069730861733357\n",
      "Std loss: 0.004679786174223432\n",
      "Total Loss: 0.06418385170400143\n",
      "------------------------------------ epoch 7465 (44784 steps) ------------------------------------\n",
      "Max loss: 0.013534849509596825\n",
      "Min loss: 0.004371952265501022\n",
      "Mean loss: 0.006931353515634934\n",
      "Std loss: 0.003198050404964388\n",
      "Total Loss: 0.041588121093809605\n",
      "------------------------------------ epoch 7466 (44790 steps) ------------------------------------\n",
      "Max loss: 0.011554654687643051\n",
      "Min loss: 0.004655543714761734\n",
      "Mean loss: 0.007207711925730109\n",
      "Std loss: 0.0021450548312312596\n",
      "Total Loss: 0.043246271554380655\n",
      "------------------------------------ epoch 7467 (44796 steps) ------------------------------------\n",
      "Max loss: 0.010756115429103374\n",
      "Min loss: 0.004308061674237251\n",
      "Mean loss: 0.006765257644777496\n",
      "Std loss: 0.002037412280371768\n",
      "Total Loss: 0.04059154586866498\n",
      "------------------------------------ epoch 7468 (44802 steps) ------------------------------------\n",
      "Max loss: 0.014292657375335693\n",
      "Min loss: 0.004469818435609341\n",
      "Mean loss: 0.008958124555647373\n",
      "Std loss: 0.0037651812511547136\n",
      "Total Loss: 0.05374874733388424\n",
      "------------------------------------ epoch 7469 (44808 steps) ------------------------------------\n",
      "Max loss: 0.01719587668776512\n",
      "Min loss: 0.004558181390166283\n",
      "Mean loss: 0.009340323663006226\n",
      "Std loss: 0.004787781407285173\n",
      "Total Loss: 0.05604194197803736\n",
      "------------------------------------ epoch 7470 (44814 steps) ------------------------------------\n",
      "Max loss: 0.008955367840826511\n",
      "Min loss: 0.005730093456804752\n",
      "Mean loss: 0.007556604376683633\n",
      "Std loss: 0.0011954717789835468\n",
      "Total Loss: 0.045339626260101795\n",
      "------------------------------------ epoch 7471 (44820 steps) ------------------------------------\n",
      "Max loss: 0.012989453971385956\n",
      "Min loss: 0.0043006581254303455\n",
      "Mean loss: 0.007547053198019664\n",
      "Std loss: 0.002931548666277168\n",
      "Total Loss: 0.04528231918811798\n",
      "------------------------------------ epoch 7472 (44826 steps) ------------------------------------\n",
      "Max loss: 0.007244482636451721\n",
      "Min loss: 0.00424490962177515\n",
      "Mean loss: 0.005737006974716981\n",
      "Std loss: 0.0011703715440759778\n",
      "Total Loss: 0.03442204184830189\n",
      "------------------------------------ epoch 7473 (44832 steps) ------------------------------------\n",
      "Max loss: 0.01664375327527523\n",
      "Min loss: 0.004810097627341747\n",
      "Mean loss: 0.009481612903376421\n",
      "Std loss: 0.004042604684442\n",
      "Total Loss: 0.05688967742025852\n",
      "------------------------------------ epoch 7474 (44838 steps) ------------------------------------\n",
      "Max loss: 0.033574093133211136\n",
      "Min loss: 0.003876996226608753\n",
      "Mean loss: 0.011095284329106411\n",
      "Std loss: 0.01020465124485471\n",
      "Total Loss: 0.06657170597463846\n",
      "------------------------------------ epoch 7475 (44844 steps) ------------------------------------\n",
      "Max loss: 0.025402486324310303\n",
      "Min loss: 0.004320803564041853\n",
      "Mean loss: 0.009011272884284457\n",
      "Std loss: 0.007366107115781132\n",
      "Total Loss: 0.05406763730570674\n",
      "------------------------------------ epoch 7476 (44850 steps) ------------------------------------\n",
      "Max loss: 0.011510100215673447\n",
      "Min loss: 0.005718315951526165\n",
      "Mean loss: 0.008732995484024286\n",
      "Std loss: 0.002409182498574953\n",
      "Total Loss: 0.05239797290414572\n",
      "------------------------------------ epoch 7477 (44856 steps) ------------------------------------\n",
      "Max loss: 0.016004124656319618\n",
      "Min loss: 0.005757191684097052\n",
      "Mean loss: 0.010614357190206647\n",
      "Std loss: 0.003187178326357552\n",
      "Total Loss: 0.06368614314123988\n",
      "------------------------------------ epoch 7478 (44862 steps) ------------------------------------\n",
      "Max loss: 0.010070309042930603\n",
      "Min loss: 0.005421481095254421\n",
      "Mean loss: 0.0077190271113067865\n",
      "Std loss: 0.0017457698281436098\n",
      "Total Loss: 0.04631416266784072\n",
      "------------------------------------ epoch 7479 (44868 steps) ------------------------------------\n",
      "Max loss: 0.040136273950338364\n",
      "Min loss: 0.005032275803387165\n",
      "Mean loss: 0.012385470404600104\n",
      "Std loss: 0.012550362978689528\n",
      "Total Loss: 0.07431282242760062\n",
      "------------------------------------ epoch 7480 (44874 steps) ------------------------------------\n",
      "Max loss: 0.015438321977853775\n",
      "Min loss: 0.0059510162100195885\n",
      "Mean loss: 0.010515302419662476\n",
      "Std loss: 0.003431855942543247\n",
      "Total Loss: 0.06309181451797485\n",
      "------------------------------------ epoch 7481 (44880 steps) ------------------------------------\n",
      "Max loss: 0.014717066660523415\n",
      "Min loss: 0.006816288456320763\n",
      "Mean loss: 0.010253590842088064\n",
      "Std loss: 0.002826717383359528\n",
      "Total Loss: 0.06152154505252838\n",
      "------------------------------------ epoch 7482 (44886 steps) ------------------------------------\n",
      "Max loss: 0.03524807095527649\n",
      "Min loss: 0.008079740218818188\n",
      "Mean loss: 0.015395778076102337\n",
      "Std loss: 0.00921670648801399\n",
      "Total Loss: 0.09237466845661402\n",
      "------------------------------------ epoch 7483 (44892 steps) ------------------------------------\n",
      "Max loss: 0.015995891764760017\n",
      "Min loss: 0.005995862185955048\n",
      "Mean loss: 0.00925574405118823\n",
      "Std loss: 0.0036903378785691433\n",
      "Total Loss: 0.05553446430712938\n",
      "------------------------------------ epoch 7484 (44898 steps) ------------------------------------\n",
      "Max loss: 0.010800080373883247\n",
      "Min loss: 0.004650542046874762\n",
      "Mean loss: 0.007228465052321553\n",
      "Std loss: 0.0021690597219518482\n",
      "Total Loss: 0.04337079031392932\n",
      "------------------------------------ epoch 7485 (44904 steps) ------------------------------------\n",
      "Max loss: 0.03106144815683365\n",
      "Min loss: 0.005302290432155132\n",
      "Mean loss: 0.012273881817236543\n",
      "Std loss: 0.008977221417284558\n",
      "Total Loss: 0.07364329090341926\n",
      "------------------------------------ epoch 7486 (44910 steps) ------------------------------------\n",
      "Max loss: 0.030837520956993103\n",
      "Min loss: 0.005150576122105122\n",
      "Mean loss: 0.01059540423254172\n",
      "Std loss: 0.00919212424980605\n",
      "Total Loss: 0.06357242539525032\n",
      "------------------------------------ epoch 7487 (44916 steps) ------------------------------------\n",
      "Max loss: 0.041257139295339584\n",
      "Min loss: 0.005559827201068401\n",
      "Mean loss: 0.012675522593781352\n",
      "Std loss: 0.012846494450380802\n",
      "Total Loss: 0.07605313556268811\n",
      "------------------------------------ epoch 7488 (44922 steps) ------------------------------------\n",
      "Max loss: 0.026915717869997025\n",
      "Min loss: 0.00678049772977829\n",
      "Mean loss: 0.014145860758920511\n",
      "Std loss: 0.006661982955506845\n",
      "Total Loss: 0.08487516455352306\n",
      "------------------------------------ epoch 7489 (44928 steps) ------------------------------------\n",
      "Max loss: 0.01864040270447731\n",
      "Min loss: 0.007563257589936256\n",
      "Mean loss: 0.012089556393524012\n",
      "Std loss: 0.0036667775743357608\n",
      "Total Loss: 0.07253733836114407\n",
      "------------------------------------ epoch 7490 (44934 steps) ------------------------------------\n",
      "Max loss: 0.018344949930906296\n",
      "Min loss: 0.005124663934111595\n",
      "Mean loss: 0.010350348893553019\n",
      "Std loss: 0.004964287784458988\n",
      "Total Loss: 0.06210209336131811\n",
      "------------------------------------ epoch 7491 (44940 steps) ------------------------------------\n",
      "Max loss: 0.01429670862853527\n",
      "Min loss: 0.004334172699600458\n",
      "Mean loss: 0.006959961649651329\n",
      "Std loss: 0.003357329042191673\n",
      "Total Loss: 0.04175976989790797\n",
      "------------------------------------ epoch 7492 (44946 steps) ------------------------------------\n",
      "Max loss: 0.013261175714433193\n",
      "Min loss: 0.005272526293992996\n",
      "Mean loss: 0.007928759170075258\n",
      "Std loss: 0.0027289673457764436\n",
      "Total Loss: 0.047572555020451546\n",
      "------------------------------------ epoch 7493 (44952 steps) ------------------------------------\n",
      "Max loss: 0.008273832499980927\n",
      "Min loss: 0.004005014430731535\n",
      "Mean loss: 0.006797965926428636\n",
      "Std loss: 0.0013970141949771362\n",
      "Total Loss: 0.040787795558571815\n",
      "------------------------------------ epoch 7494 (44958 steps) ------------------------------------\n",
      "Max loss: 0.028052514418959618\n",
      "Min loss: 0.004804400261491537\n",
      "Mean loss: 0.009759953012689948\n",
      "Std loss: 0.008281503882071238\n",
      "Total Loss: 0.05855971807613969\n",
      "------------------------------------ epoch 7495 (44964 steps) ------------------------------------\n",
      "Max loss: 0.020826391875743866\n",
      "Min loss: 0.00554694002494216\n",
      "Mean loss: 0.012476732876772681\n",
      "Std loss: 0.005684115346084937\n",
      "Total Loss: 0.07486039726063609\n",
      "------------------------------------ epoch 7496 (44970 steps) ------------------------------------\n",
      "Max loss: 0.021551139652729034\n",
      "Min loss: 0.00447483453899622\n",
      "Mean loss: 0.009090867902462682\n",
      "Std loss: 0.005815205229169609\n",
      "Total Loss: 0.05454520741477609\n",
      "------------------------------------ epoch 7497 (44976 steps) ------------------------------------\n",
      "Max loss: 0.01925548166036606\n",
      "Min loss: 0.003746288362890482\n",
      "Mean loss: 0.008535846369341016\n",
      "Std loss: 0.005749333632647208\n",
      "Total Loss: 0.051215078216046095\n",
      "------------------------------------ epoch 7498 (44982 steps) ------------------------------------\n",
      "Max loss: 0.04767677187919617\n",
      "Min loss: 0.00463651679456234\n",
      "Mean loss: 0.018530310519660514\n",
      "Std loss: 0.015187196006102254\n",
      "Total Loss: 0.11118186311796308\n",
      "------------------------------------ epoch 7499 (44988 steps) ------------------------------------\n",
      "Max loss: 0.01643987186253071\n",
      "Min loss: 0.004687972832471132\n",
      "Mean loss: 0.008253743095944325\n",
      "Std loss: 0.0038331903173720455\n",
      "Total Loss: 0.04952245857566595\n",
      "------------------------------------ epoch 7500 (44994 steps) ------------------------------------\n",
      "Max loss: 0.013491308316588402\n",
      "Min loss: 0.005851523019373417\n",
      "Mean loss: 0.01063957205042243\n",
      "Std loss: 0.0028137591724974767\n",
      "Total Loss: 0.06383743230253458\n",
      "------------------------------------ epoch 7501 (45000 steps) ------------------------------------\n",
      "Max loss: 0.013772882521152496\n",
      "Min loss: 0.0037590619176626205\n",
      "Mean loss: 0.008083253943671783\n",
      "Std loss: 0.0037860837484585963\n",
      "Total Loss: 0.0484995236620307\n",
      "saved model at ./weights/model_7501.pth\n",
      "------------------------------------ epoch 7502 (45006 steps) ------------------------------------\n",
      "Max loss: 0.009937562048435211\n",
      "Min loss: 0.005014741327613592\n",
      "Mean loss: 0.00720846854771177\n",
      "Std loss: 0.001757200347242394\n",
      "Total Loss: 0.04325081128627062\n",
      "------------------------------------ epoch 7503 (45012 steps) ------------------------------------\n",
      "Max loss: 0.01702870801091194\n",
      "Min loss: 0.004301006905734539\n",
      "Mean loss: 0.007934281757722298\n",
      "Std loss: 0.004475002694015192\n",
      "Total Loss: 0.04760569054633379\n",
      "------------------------------------ epoch 7504 (45018 steps) ------------------------------------\n",
      "Max loss: 0.022730108350515366\n",
      "Min loss: 0.004718323703855276\n",
      "Mean loss: 0.011894174463426074\n",
      "Std loss: 0.005595732008740533\n",
      "Total Loss: 0.07136504678055644\n",
      "------------------------------------ epoch 7505 (45024 steps) ------------------------------------\n",
      "Max loss: 0.010167054831981659\n",
      "Min loss: 0.004716208204627037\n",
      "Mean loss: 0.006707690500964721\n",
      "Std loss: 0.0018018027578906644\n",
      "Total Loss: 0.040246143005788326\n",
      "------------------------------------ epoch 7506 (45030 steps) ------------------------------------\n",
      "Max loss: 0.00829688087105751\n",
      "Min loss: 0.003853333881124854\n",
      "Mean loss: 0.005882732608976464\n",
      "Std loss: 0.0017758773430242764\n",
      "Total Loss: 0.03529639565385878\n",
      "------------------------------------ epoch 7507 (45036 steps) ------------------------------------\n",
      "Max loss: 0.015773355960845947\n",
      "Min loss: 0.003564676037058234\n",
      "Mean loss: 0.007378596385630469\n",
      "Std loss: 0.004147370870656842\n",
      "Total Loss: 0.04427157831378281\n",
      "------------------------------------ epoch 7508 (45042 steps) ------------------------------------\n",
      "Max loss: 0.010032525286078453\n",
      "Min loss: 0.003942480310797691\n",
      "Mean loss: 0.006993651390075684\n",
      "Std loss: 0.002261170513831743\n",
      "Total Loss: 0.0419619083404541\n",
      "------------------------------------ epoch 7509 (45048 steps) ------------------------------------\n",
      "Max loss: 0.030759504064917564\n",
      "Min loss: 0.007452426478266716\n",
      "Mean loss: 0.014523370501895746\n",
      "Std loss: 0.0075486894445577395\n",
      "Total Loss: 0.08714022301137447\n",
      "------------------------------------ epoch 7510 (45054 steps) ------------------------------------\n",
      "Max loss: 0.04352287948131561\n",
      "Min loss: 0.0041229017078876495\n",
      "Mean loss: 0.014459168926502267\n",
      "Std loss: 0.013742069753522886\n",
      "Total Loss: 0.0867550135590136\n",
      "------------------------------------ epoch 7511 (45060 steps) ------------------------------------\n",
      "Max loss: 0.008418369106948376\n",
      "Min loss: 0.0046149869449436665\n",
      "Mean loss: 0.0064815308433026075\n",
      "Std loss: 0.0013350251052604764\n",
      "Total Loss: 0.038889185059815645\n",
      "------------------------------------ epoch 7512 (45066 steps) ------------------------------------\n",
      "Max loss: 0.020288139581680298\n",
      "Min loss: 0.004247547127306461\n",
      "Mean loss: 0.009208850484962264\n",
      "Std loss: 0.005287654684253253\n",
      "Total Loss: 0.05525310290977359\n",
      "------------------------------------ epoch 7513 (45072 steps) ------------------------------------\n",
      "Max loss: 0.023150864988565445\n",
      "Min loss: 0.004235193599015474\n",
      "Mean loss: 0.009709591589247188\n",
      "Std loss: 0.007027817152592092\n",
      "Total Loss: 0.05825754953548312\n",
      "------------------------------------ epoch 7514 (45078 steps) ------------------------------------\n",
      "Max loss: 0.03144790977239609\n",
      "Min loss: 0.0039099836722016335\n",
      "Mean loss: 0.013499053427949548\n",
      "Std loss: 0.009573728620477348\n",
      "Total Loss: 0.08099432056769729\n",
      "------------------------------------ epoch 7515 (45084 steps) ------------------------------------\n",
      "Max loss: 0.028495609760284424\n",
      "Min loss: 0.004631787538528442\n",
      "Mean loss: 0.01214538118802011\n",
      "Std loss: 0.008151677429947945\n",
      "Total Loss: 0.07287228712812066\n",
      "------------------------------------ epoch 7516 (45090 steps) ------------------------------------\n",
      "Max loss: 0.016196276992559433\n",
      "Min loss: 0.006039408501237631\n",
      "Mean loss: 0.010464020927126208\n",
      "Std loss: 0.0035521073686794005\n",
      "Total Loss: 0.06278412556275725\n",
      "------------------------------------ epoch 7517 (45096 steps) ------------------------------------\n",
      "Max loss: 0.016484256833791733\n",
      "Min loss: 0.0063512008637189865\n",
      "Mean loss: 0.011441142298281193\n",
      "Std loss: 0.004120178827452719\n",
      "Total Loss: 0.06864685378968716\n",
      "------------------------------------ epoch 7518 (45102 steps) ------------------------------------\n",
      "Max loss: 0.018763607367873192\n",
      "Min loss: 0.005067452322691679\n",
      "Mean loss: 0.008738493934894601\n",
      "Std loss: 0.004596190552565635\n",
      "Total Loss: 0.05243096360936761\n",
      "------------------------------------ epoch 7519 (45108 steps) ------------------------------------\n",
      "Max loss: 0.01807590201497078\n",
      "Min loss: 0.0047571053728461266\n",
      "Mean loss: 0.008351911402617892\n",
      "Std loss: 0.00447420566417277\n",
      "Total Loss: 0.05011146841570735\n",
      "------------------------------------ epoch 7520 (45114 steps) ------------------------------------\n",
      "Max loss: 0.011072858236730099\n",
      "Min loss: 0.005363279953598976\n",
      "Mean loss: 0.007422516277680795\n",
      "Std loss: 0.00195838068545749\n",
      "Total Loss: 0.044535097666084766\n",
      "------------------------------------ epoch 7521 (45120 steps) ------------------------------------\n",
      "Max loss: 0.03005187027156353\n",
      "Min loss: 0.004975809715688229\n",
      "Mean loss: 0.015230109915137291\n",
      "Std loss: 0.008566757082236717\n",
      "Total Loss: 0.09138065949082375\n",
      "------------------------------------ epoch 7522 (45126 steps) ------------------------------------\n",
      "Max loss: 0.010281931608915329\n",
      "Min loss: 0.004847107920795679\n",
      "Mean loss: 0.007199228974059224\n",
      "Std loss: 0.002118031764608331\n",
      "Total Loss: 0.043195373844355345\n",
      "------------------------------------ epoch 7523 (45132 steps) ------------------------------------\n",
      "Max loss: 0.016727609559893608\n",
      "Min loss: 0.0057328445836901665\n",
      "Mean loss: 0.00909712064700822\n",
      "Std loss: 0.003687703875250351\n",
      "Total Loss: 0.05458272388204932\n",
      "------------------------------------ epoch 7524 (45138 steps) ------------------------------------\n",
      "Max loss: 0.03608214110136032\n",
      "Min loss: 0.005556306801736355\n",
      "Mean loss: 0.01494668268909057\n",
      "Std loss: 0.011804263061329857\n",
      "Total Loss: 0.08968009613454342\n",
      "------------------------------------ epoch 7525 (45144 steps) ------------------------------------\n",
      "Max loss: 0.012172723188996315\n",
      "Min loss: 0.004135172814130783\n",
      "Mean loss: 0.008504833715657393\n",
      "Std loss: 0.00284248805057848\n",
      "Total Loss: 0.05102900229394436\n",
      "------------------------------------ epoch 7526 (45150 steps) ------------------------------------\n",
      "Max loss: 0.019161492586135864\n",
      "Min loss: 0.004784773103892803\n",
      "Mean loss: 0.011432473780587316\n",
      "Std loss: 0.005817984359929959\n",
      "Total Loss: 0.0685948426835239\n",
      "------------------------------------ epoch 7527 (45156 steps) ------------------------------------\n",
      "Max loss: 0.007993039675056934\n",
      "Min loss: 0.004802169278264046\n",
      "Mean loss: 0.006999986013397574\n",
      "Std loss: 0.0010654820030052995\n",
      "Total Loss: 0.04199991608038545\n",
      "------------------------------------ epoch 7528 (45162 steps) ------------------------------------\n",
      "Max loss: 0.012181039899587631\n",
      "Min loss: 0.0047219982370734215\n",
      "Mean loss: 0.007916014020641645\n",
      "Std loss: 0.0025868379199144916\n",
      "Total Loss: 0.04749608412384987\n",
      "------------------------------------ epoch 7529 (45168 steps) ------------------------------------\n",
      "Max loss: 0.009816274046897888\n",
      "Min loss: 0.004401267506182194\n",
      "Mean loss: 0.006516169523820281\n",
      "Std loss: 0.00198264607317383\n",
      "Total Loss: 0.039097017142921686\n",
      "------------------------------------ epoch 7530 (45174 steps) ------------------------------------\n",
      "Max loss: 0.03235512971878052\n",
      "Min loss: 0.004955939948558807\n",
      "Mean loss: 0.012721648827816049\n",
      "Std loss: 0.009446180928239359\n",
      "Total Loss: 0.0763298929668963\n",
      "------------------------------------ epoch 7531 (45180 steps) ------------------------------------\n",
      "Max loss: 0.01622987538576126\n",
      "Min loss: 0.0035727284848690033\n",
      "Mean loss: 0.008045899759357175\n",
      "Std loss: 0.0039670878558503275\n",
      "Total Loss: 0.048275398556143045\n",
      "------------------------------------ epoch 7532 (45186 steps) ------------------------------------\n",
      "Max loss: 0.01615525409579277\n",
      "Min loss: 0.005179580766707659\n",
      "Mean loss: 0.009756024228408933\n",
      "Std loss: 0.004471659434332521\n",
      "Total Loss: 0.058536145370453596\n",
      "------------------------------------ epoch 7533 (45192 steps) ------------------------------------\n",
      "Max loss: 0.009398577734827995\n",
      "Min loss: 0.004753160756081343\n",
      "Mean loss: 0.006934033241122961\n",
      "Std loss: 0.0017693367141077884\n",
      "Total Loss: 0.041604199446737766\n",
      "------------------------------------ epoch 7534 (45198 steps) ------------------------------------\n",
      "Max loss: 0.03767793998122215\n",
      "Min loss: 0.004922337364405394\n",
      "Mean loss: 0.013995824381709099\n",
      "Std loss: 0.011226475968138843\n",
      "Total Loss: 0.08397494629025459\n",
      "------------------------------------ epoch 7535 (45204 steps) ------------------------------------\n",
      "Max loss: 0.014962082728743553\n",
      "Min loss: 0.006231306120753288\n",
      "Mean loss: 0.008537079828480879\n",
      "Std loss: 0.00311167450975487\n",
      "Total Loss: 0.05122247897088528\n",
      "------------------------------------ epoch 7536 (45210 steps) ------------------------------------\n",
      "Max loss: 0.010352546349167824\n",
      "Min loss: 0.004336782731115818\n",
      "Mean loss: 0.007147736769790451\n",
      "Std loss: 0.0022402351082235613\n",
      "Total Loss: 0.042886420618742704\n",
      "------------------------------------ epoch 7537 (45216 steps) ------------------------------------\n",
      "Max loss: 0.026940790936350822\n",
      "Min loss: 0.00523420562967658\n",
      "Mean loss: 0.01267374656163156\n",
      "Std loss: 0.007632146214279154\n",
      "Total Loss: 0.07604247936978936\n",
      "------------------------------------ epoch 7538 (45222 steps) ------------------------------------\n",
      "Max loss: 0.013590669259428978\n",
      "Min loss: 0.006046374328434467\n",
      "Mean loss: 0.009080412642409405\n",
      "Std loss: 0.0027581429449523758\n",
      "Total Loss: 0.054482475854456425\n",
      "------------------------------------ epoch 7539 (45228 steps) ------------------------------------\n",
      "Max loss: 0.014284998178482056\n",
      "Min loss: 0.0041103726252913475\n",
      "Mean loss: 0.009279032393048206\n",
      "Std loss: 0.004209229803263932\n",
      "Total Loss: 0.05567419435828924\n",
      "------------------------------------ epoch 7540 (45234 steps) ------------------------------------\n",
      "Max loss: 0.019427839666604996\n",
      "Min loss: 0.004306652583181858\n",
      "Mean loss: 0.01012590310225884\n",
      "Std loss: 0.0049982439102425336\n",
      "Total Loss: 0.06075541861355305\n",
      "------------------------------------ epoch 7541 (45240 steps) ------------------------------------\n",
      "Max loss: 0.016319788992404938\n",
      "Min loss: 0.0074437279254198074\n",
      "Mean loss: 0.009944455077250799\n",
      "Std loss: 0.0031761243204025554\n",
      "Total Loss: 0.05966673046350479\n",
      "------------------------------------ epoch 7542 (45246 steps) ------------------------------------\n",
      "Max loss: 0.021732043474912643\n",
      "Min loss: 0.004491161555051804\n",
      "Mean loss: 0.008304594627891978\n",
      "Std loss: 0.00602805186005384\n",
      "Total Loss: 0.049827567767351866\n",
      "------------------------------------ epoch 7543 (45252 steps) ------------------------------------\n",
      "Max loss: 0.014888828620314598\n",
      "Min loss: 0.00709632970392704\n",
      "Mean loss: 0.009757670185839137\n",
      "Std loss: 0.0027953646665664945\n",
      "Total Loss: 0.05854602111503482\n",
      "------------------------------------ epoch 7544 (45258 steps) ------------------------------------\n",
      "Max loss: 0.011260256171226501\n",
      "Min loss: 0.005017527379095554\n",
      "Mean loss: 0.00806382034594814\n",
      "Std loss: 0.00253031899174771\n",
      "Total Loss: 0.04838292207568884\n",
      "------------------------------------ epoch 7545 (45264 steps) ------------------------------------\n",
      "Max loss: 0.02245405688881874\n",
      "Min loss: 0.004611842334270477\n",
      "Mean loss: 0.009802845539525151\n",
      "Std loss: 0.006066424651977289\n",
      "Total Loss: 0.05881707323715091\n",
      "------------------------------------ epoch 7546 (45270 steps) ------------------------------------\n",
      "Max loss: 0.007996847853064537\n",
      "Min loss: 0.004563076421618462\n",
      "Mean loss: 0.005779846648996075\n",
      "Std loss: 0.0012102357169844018\n",
      "Total Loss: 0.03467907989397645\n",
      "------------------------------------ epoch 7547 (45276 steps) ------------------------------------\n",
      "Max loss: 0.02809060923755169\n",
      "Min loss: 0.0037569336127489805\n",
      "Mean loss: 0.012719820447576543\n",
      "Std loss: 0.009437723203322429\n",
      "Total Loss: 0.07631892268545926\n",
      "------------------------------------ epoch 7548 (45282 steps) ------------------------------------\n",
      "Max loss: 0.022194471210241318\n",
      "Min loss: 0.003687393618747592\n",
      "Mean loss: 0.01007089480602493\n",
      "Std loss: 0.006207266659992691\n",
      "Total Loss: 0.06042536883614957\n",
      "------------------------------------ epoch 7549 (45288 steps) ------------------------------------\n",
      "Max loss: 0.02475278079509735\n",
      "Min loss: 0.004850828554481268\n",
      "Mean loss: 0.009945942632233104\n",
      "Std loss: 0.0067670363412292754\n",
      "Total Loss: 0.05967565579339862\n",
      "------------------------------------ epoch 7550 (45294 steps) ------------------------------------\n",
      "Max loss: 0.05070369690656662\n",
      "Min loss: 0.009453150443732738\n",
      "Mean loss: 0.0190122053027153\n",
      "Std loss: 0.01431734768297434\n",
      "Total Loss: 0.11407323181629181\n",
      "------------------------------------ epoch 7551 (45300 steps) ------------------------------------\n",
      "Max loss: 0.01377008855342865\n",
      "Min loss: 0.005958750378340483\n",
      "Mean loss: 0.010186614468693733\n",
      "Std loss: 0.0034606598069347504\n",
      "Total Loss: 0.0611196868121624\n",
      "------------------------------------ epoch 7552 (45306 steps) ------------------------------------\n",
      "Max loss: 0.015260608866810799\n",
      "Min loss: 0.005659854970872402\n",
      "Mean loss: 0.008156405994668603\n",
      "Std loss: 0.0032567667969589245\n",
      "Total Loss: 0.04893843596801162\n",
      "------------------------------------ epoch 7553 (45312 steps) ------------------------------------\n",
      "Max loss: 0.026499103754758835\n",
      "Min loss: 0.006081867031753063\n",
      "Mean loss: 0.013075620556871096\n",
      "Std loss: 0.007088671310584291\n",
      "Total Loss: 0.07845372334122658\n",
      "------------------------------------ epoch 7554 (45318 steps) ------------------------------------\n",
      "Max loss: 0.03493242710828781\n",
      "Min loss: 0.006149654742330313\n",
      "Mean loss: 0.01152513533209761\n",
      "Std loss: 0.010474885236689439\n",
      "Total Loss: 0.06915081199258566\n",
      "------------------------------------ epoch 7555 (45324 steps) ------------------------------------\n",
      "Max loss: 0.01540510542690754\n",
      "Min loss: 0.0045105027966201305\n",
      "Mean loss: 0.00901004004602631\n",
      "Std loss: 0.004224345741586451\n",
      "Total Loss: 0.054060240276157856\n",
      "------------------------------------ epoch 7556 (45330 steps) ------------------------------------\n",
      "Max loss: 0.015864094719290733\n",
      "Min loss: 0.0056543052196502686\n",
      "Mean loss: 0.009656000571946302\n",
      "Std loss: 0.003349377431538576\n",
      "Total Loss: 0.05793600343167782\n",
      "------------------------------------ epoch 7557 (45336 steps) ------------------------------------\n",
      "Max loss: 0.008407281711697578\n",
      "Min loss: 0.0045913453213870525\n",
      "Mean loss: 0.005748794414103031\n",
      "Std loss: 0.00136682171397617\n",
      "Total Loss: 0.03449276648461819\n",
      "------------------------------------ epoch 7558 (45342 steps) ------------------------------------\n",
      "Max loss: 0.014050696045160294\n",
      "Min loss: 0.004397337790578604\n",
      "Mean loss: 0.007779363077133894\n",
      "Std loss: 0.003324008073942906\n",
      "Total Loss: 0.046676178462803364\n",
      "------------------------------------ epoch 7559 (45348 steps) ------------------------------------\n",
      "Max loss: 0.01420043408870697\n",
      "Min loss: 0.0036587659269571304\n",
      "Mean loss: 0.007867091956237951\n",
      "Std loss: 0.004209514551086818\n",
      "Total Loss: 0.04720255173742771\n",
      "------------------------------------ epoch 7560 (45354 steps) ------------------------------------\n",
      "Max loss: 0.017566556110978127\n",
      "Min loss: 0.005774743389338255\n",
      "Mean loss: 0.009271578397601843\n",
      "Std loss: 0.003891541437366059\n",
      "Total Loss: 0.05562947038561106\n",
      "------------------------------------ epoch 7561 (45360 steps) ------------------------------------\n",
      "Max loss: 0.011960679665207863\n",
      "Min loss: 0.0062192827463150024\n",
      "Mean loss: 0.00902827518681685\n",
      "Std loss: 0.0020225062155869705\n",
      "Total Loss: 0.05416965112090111\n",
      "------------------------------------ epoch 7562 (45366 steps) ------------------------------------\n",
      "Max loss: 0.010091682896018028\n",
      "Min loss: 0.004129066132009029\n",
      "Mean loss: 0.006605572843303283\n",
      "Std loss: 0.0020498497698224418\n",
      "Total Loss: 0.0396334370598197\n",
      "------------------------------------ epoch 7563 (45372 steps) ------------------------------------\n",
      "Max loss: 0.01948593184351921\n",
      "Min loss: 0.00417594239115715\n",
      "Mean loss: 0.009277934596563378\n",
      "Std loss: 0.005326894538575075\n",
      "Total Loss: 0.055667607579380274\n",
      "------------------------------------ epoch 7564 (45378 steps) ------------------------------------\n",
      "Max loss: 0.01686488464474678\n",
      "Min loss: 0.0037457142025232315\n",
      "Mean loss: 0.008626317605376244\n",
      "Std loss: 0.004994387483330061\n",
      "Total Loss: 0.05175790563225746\n",
      "------------------------------------ epoch 7565 (45384 steps) ------------------------------------\n",
      "Max loss: 0.010754488408565521\n",
      "Min loss: 0.003922969102859497\n",
      "Mean loss: 0.007292820140719414\n",
      "Std loss: 0.002491001582256201\n",
      "Total Loss: 0.04375692084431648\n",
      "------------------------------------ epoch 7566 (45390 steps) ------------------------------------\n",
      "Max loss: 0.011508597992360592\n",
      "Min loss: 0.004091307520866394\n",
      "Mean loss: 0.007419759330029289\n",
      "Std loss: 0.0025691658790378985\n",
      "Total Loss: 0.044518555980175734\n",
      "------------------------------------ epoch 7567 (45396 steps) ------------------------------------\n",
      "Max loss: 0.01074947603046894\n",
      "Min loss: 0.004572371952235699\n",
      "Mean loss: 0.0075033622949073715\n",
      "Std loss: 0.002067709267312985\n",
      "Total Loss: 0.04502017376944423\n",
      "------------------------------------ epoch 7568 (45402 steps) ------------------------------------\n",
      "Max loss: 0.011684492230415344\n",
      "Min loss: 0.004751367028802633\n",
      "Mean loss: 0.00823000903862218\n",
      "Std loss: 0.0025727214901779616\n",
      "Total Loss: 0.049380054231733084\n",
      "------------------------------------ epoch 7569 (45408 steps) ------------------------------------\n",
      "Max loss: 0.02644025906920433\n",
      "Min loss: 0.004226156044751406\n",
      "Mean loss: 0.009764729145293435\n",
      "Std loss: 0.007983195053672771\n",
      "Total Loss: 0.05858837487176061\n",
      "------------------------------------ epoch 7570 (45414 steps) ------------------------------------\n",
      "Max loss: 0.012965312227606773\n",
      "Min loss: 0.0042503755539655685\n",
      "Mean loss: 0.00814650859683752\n",
      "Std loss: 0.0029415074547210164\n",
      "Total Loss: 0.048879051581025124\n",
      "------------------------------------ epoch 7571 (45420 steps) ------------------------------------\n",
      "Max loss: 0.01772940531373024\n",
      "Min loss: 0.004479150753468275\n",
      "Mean loss: 0.009511476149782538\n",
      "Std loss: 0.00512568589568228\n",
      "Total Loss: 0.05706885689869523\n",
      "------------------------------------ epoch 7572 (45426 steps) ------------------------------------\n",
      "Max loss: 0.01363170426338911\n",
      "Min loss: 0.004101342987269163\n",
      "Mean loss: 0.008216815069317818\n",
      "Std loss: 0.0033901953716296523\n",
      "Total Loss: 0.049300890415906906\n",
      "------------------------------------ epoch 7573 (45432 steps) ------------------------------------\n",
      "Max loss: 0.024037322029471397\n",
      "Min loss: 0.004581380635499954\n",
      "Mean loss: 0.00950930337421596\n",
      "Std loss: 0.006684974937329705\n",
      "Total Loss: 0.05705582024529576\n",
      "------------------------------------ epoch 7574 (45438 steps) ------------------------------------\n",
      "Max loss: 0.01645076647400856\n",
      "Min loss: 0.005095038563013077\n",
      "Mean loss: 0.010230752173811197\n",
      "Std loss: 0.00422002725858481\n",
      "Total Loss: 0.061384513042867184\n",
      "------------------------------------ epoch 7575 (45444 steps) ------------------------------------\n",
      "Max loss: 0.016840185970067978\n",
      "Min loss: 0.0043991743586957455\n",
      "Mean loss: 0.009786370831231276\n",
      "Std loss: 0.005054504050571502\n",
      "Total Loss: 0.05871822498738766\n",
      "------------------------------------ epoch 7576 (45450 steps) ------------------------------------\n",
      "Max loss: 0.01196459960192442\n",
      "Min loss: 0.0041878437623381615\n",
      "Mean loss: 0.007385053283845385\n",
      "Std loss: 0.002579634122032442\n",
      "Total Loss: 0.04431031970307231\n",
      "------------------------------------ epoch 7577 (45456 steps) ------------------------------------\n",
      "Max loss: 0.009260255843400955\n",
      "Min loss: 0.0057366108521819115\n",
      "Mean loss: 0.006992983166128397\n",
      "Std loss: 0.0012472363036167187\n",
      "Total Loss: 0.04195789899677038\n",
      "------------------------------------ epoch 7578 (45462 steps) ------------------------------------\n",
      "Max loss: 0.012117918580770493\n",
      "Min loss: 0.005213405471295118\n",
      "Mean loss: 0.00777679697299997\n",
      "Std loss: 0.0030728116257223038\n",
      "Total Loss: 0.04666078183799982\n",
      "------------------------------------ epoch 7579 (45468 steps) ------------------------------------\n",
      "Max loss: 0.014952379278838634\n",
      "Min loss: 0.004792593419551849\n",
      "Mean loss: 0.007690947037190199\n",
      "Std loss: 0.003979733135487631\n",
      "Total Loss: 0.04614568222314119\n",
      "------------------------------------ epoch 7580 (45474 steps) ------------------------------------\n",
      "Max loss: 0.01261188369244337\n",
      "Min loss: 0.004580480512231588\n",
      "Mean loss: 0.007996466709300876\n",
      "Std loss: 0.0026606951898418096\n",
      "Total Loss: 0.047978800255805254\n",
      "------------------------------------ epoch 7581 (45480 steps) ------------------------------------\n",
      "Max loss: 0.010504807345569134\n",
      "Min loss: 0.004631700925529003\n",
      "Mean loss: 0.007452974251161019\n",
      "Std loss: 0.002083542394741062\n",
      "Total Loss: 0.044717845506966114\n",
      "------------------------------------ epoch 7582 (45486 steps) ------------------------------------\n",
      "Max loss: 0.012165948748588562\n",
      "Min loss: 0.004168903920799494\n",
      "Mean loss: 0.007668481984486182\n",
      "Std loss: 0.0031143062480725025\n",
      "Total Loss: 0.046010891906917095\n",
      "------------------------------------ epoch 7583 (45492 steps) ------------------------------------\n",
      "Max loss: 0.007238039281219244\n",
      "Min loss: 0.0044950381852686405\n",
      "Mean loss: 0.005969660511861245\n",
      "Std loss: 0.0008513092629942456\n",
      "Total Loss: 0.03581796307116747\n",
      "------------------------------------ epoch 7584 (45498 steps) ------------------------------------\n",
      "Max loss: 0.02141585759818554\n",
      "Min loss: 0.004074912518262863\n",
      "Mean loss: 0.00897775536092619\n",
      "Std loss: 0.006065342603906445\n",
      "Total Loss: 0.053866532165557146\n",
      "------------------------------------ epoch 7585 (45504 steps) ------------------------------------\n",
      "Max loss: 0.016244884580373764\n",
      "Min loss: 0.0035624969750642776\n",
      "Mean loss: 0.006785598428299029\n",
      "Std loss: 0.0043790584288227975\n",
      "Total Loss: 0.04071359056979418\n",
      "------------------------------------ epoch 7586 (45510 steps) ------------------------------------\n",
      "Max loss: 0.019350506365299225\n",
      "Min loss: 0.0056034475564956665\n",
      "Mean loss: 0.01101026792700092\n",
      "Std loss: 0.005989706764405521\n",
      "Total Loss: 0.06606160756200552\n",
      "------------------------------------ epoch 7587 (45516 steps) ------------------------------------\n",
      "Max loss: 0.012189151719212532\n",
      "Min loss: 0.0052643632516264915\n",
      "Mean loss: 0.008693578730647763\n",
      "Std loss: 0.00216281924324564\n",
      "Total Loss: 0.052161472383886576\n",
      "------------------------------------ epoch 7588 (45522 steps) ------------------------------------\n",
      "Max loss: 0.011407388374209404\n",
      "Min loss: 0.005058960989117622\n",
      "Mean loss: 0.00755668703156213\n",
      "Std loss: 0.002050423555567439\n",
      "Total Loss: 0.04534012218937278\n",
      "------------------------------------ epoch 7589 (45528 steps) ------------------------------------\n",
      "Max loss: 0.018684349954128265\n",
      "Min loss: 0.004329985938966274\n",
      "Mean loss: 0.011429630452767015\n",
      "Std loss: 0.004922231495469224\n",
      "Total Loss: 0.06857778271660209\n",
      "------------------------------------ epoch 7590 (45534 steps) ------------------------------------\n",
      "Max loss: 0.010516166687011719\n",
      "Min loss: 0.004210931248962879\n",
      "Mean loss: 0.006527029676362872\n",
      "Std loss: 0.0020148417777771026\n",
      "Total Loss: 0.03916217805817723\n",
      "------------------------------------ epoch 7591 (45540 steps) ------------------------------------\n",
      "Max loss: 0.03851545602083206\n",
      "Min loss: 0.004492581821978092\n",
      "Mean loss: 0.015085593486825625\n",
      "Std loss: 0.012190936060389056\n",
      "Total Loss: 0.09051356092095375\n",
      "------------------------------------ epoch 7592 (45546 steps) ------------------------------------\n",
      "Max loss: 0.010396938771009445\n",
      "Min loss: 0.005673940293490887\n",
      "Mean loss: 0.007563800318166614\n",
      "Std loss: 0.0015732815453110283\n",
      "Total Loss: 0.04538280190899968\n",
      "------------------------------------ epoch 7593 (45552 steps) ------------------------------------\n",
      "Max loss: 0.026625795289874077\n",
      "Min loss: 0.0038486942648887634\n",
      "Mean loss: 0.009370747798432907\n",
      "Std loss: 0.008074798400443743\n",
      "Total Loss: 0.05622448679059744\n",
      "------------------------------------ epoch 7594 (45558 steps) ------------------------------------\n",
      "Max loss: 0.016262128949165344\n",
      "Min loss: 0.005071786232292652\n",
      "Mean loss: 0.00932874740101397\n",
      "Std loss: 0.004810032823817267\n",
      "Total Loss: 0.05597248440608382\n",
      "------------------------------------ epoch 7595 (45564 steps) ------------------------------------\n",
      "Max loss: 0.03822069615125656\n",
      "Min loss: 0.006570726633071899\n",
      "Mean loss: 0.015646598457048338\n",
      "Std loss: 0.010774215050747356\n",
      "Total Loss: 0.09387959074229002\n",
      "------------------------------------ epoch 7596 (45570 steps) ------------------------------------\n",
      "Max loss: 0.015316802076995373\n",
      "Min loss: 0.0050958432257175446\n",
      "Mean loss: 0.008594284299761057\n",
      "Std loss: 0.00340010237014279\n",
      "Total Loss: 0.05156570579856634\n",
      "------------------------------------ epoch 7597 (45576 steps) ------------------------------------\n",
      "Max loss: 0.012130486778914928\n",
      "Min loss: 0.004217265173792839\n",
      "Mean loss: 0.007299566408619285\n",
      "Std loss: 0.002544892484049893\n",
      "Total Loss: 0.04379739845171571\n",
      "------------------------------------ epoch 7598 (45582 steps) ------------------------------------\n",
      "Max loss: 0.025396142154932022\n",
      "Min loss: 0.004281659610569477\n",
      "Mean loss: 0.010561648290604353\n",
      "Std loss: 0.007301991457746193\n",
      "Total Loss: 0.06336988974362612\n",
      "------------------------------------ epoch 7599 (45588 steps) ------------------------------------\n",
      "Max loss: 0.01639382913708687\n",
      "Min loss: 0.0068467361852526665\n",
      "Mean loss: 0.010102106491103768\n",
      "Std loss: 0.0032997446811050372\n",
      "Total Loss: 0.06061263894662261\n",
      "------------------------------------ epoch 7600 (45594 steps) ------------------------------------\n",
      "Max loss: 0.010122565552592278\n",
      "Min loss: 0.003967117052525282\n",
      "Mean loss: 0.006006137700751424\n",
      "Std loss: 0.0021591601195105\n",
      "Total Loss: 0.03603682620450854\n",
      "------------------------------------ epoch 7601 (45600 steps) ------------------------------------\n",
      "Max loss: 0.02792973443865776\n",
      "Min loss: 0.004022825509309769\n",
      "Mean loss: 0.01164689000385503\n",
      "Std loss: 0.00954994883908628\n",
      "Total Loss: 0.06988134002313018\n",
      "saved model at ./weights/model_7601.pth\n",
      "------------------------------------ epoch 7602 (45606 steps) ------------------------------------\n",
      "Max loss: 0.03459274768829346\n",
      "Min loss: 0.005516683217138052\n",
      "Mean loss: 0.013468469570701322\n",
      "Std loss: 0.010150360811516322\n",
      "Total Loss: 0.08081081742420793\n",
      "------------------------------------ epoch 7603 (45612 steps) ------------------------------------\n",
      "Max loss: 0.014376583509147167\n",
      "Min loss: 0.005604074336588383\n",
      "Mean loss: 0.008701184609284004\n",
      "Std loss: 0.0030651173391261196\n",
      "Total Loss: 0.05220710765570402\n",
      "------------------------------------ epoch 7604 (45618 steps) ------------------------------------\n",
      "Max loss: 0.021831348538398743\n",
      "Min loss: 0.004772471729665995\n",
      "Mean loss: 0.00917393791799744\n",
      "Std loss: 0.005833553369866299\n",
      "Total Loss: 0.05504362750798464\n",
      "------------------------------------ epoch 7605 (45624 steps) ------------------------------------\n",
      "Max loss: 0.013508947566151619\n",
      "Min loss: 0.004781158175319433\n",
      "Mean loss: 0.00909818452782929\n",
      "Std loss: 0.003203271993516484\n",
      "Total Loss: 0.05458910716697574\n",
      "------------------------------------ epoch 7606 (45630 steps) ------------------------------------\n",
      "Max loss: 0.016770603135228157\n",
      "Min loss: 0.005992530845105648\n",
      "Mean loss: 0.009355412097647786\n",
      "Std loss: 0.0037239290191023967\n",
      "Total Loss: 0.05613247258588672\n",
      "------------------------------------ epoch 7607 (45636 steps) ------------------------------------\n",
      "Max loss: 0.015573808923363686\n",
      "Min loss: 0.004442007280886173\n",
      "Mean loss: 0.010249478897700707\n",
      "Std loss: 0.003561979621114829\n",
      "Total Loss: 0.06149687338620424\n",
      "------------------------------------ epoch 7608 (45642 steps) ------------------------------------\n",
      "Max loss: 0.008905967697501183\n",
      "Min loss: 0.004345070570707321\n",
      "Mean loss: 0.006602176930755377\n",
      "Std loss: 0.0018539929455189013\n",
      "Total Loss: 0.03961306158453226\n",
      "------------------------------------ epoch 7609 (45648 steps) ------------------------------------\n",
      "Max loss: 0.007117796689271927\n",
      "Min loss: 0.0033582025207579136\n",
      "Mean loss: 0.0054846083124478655\n",
      "Std loss: 0.0012455640487017438\n",
      "Total Loss: 0.032907649874687195\n",
      "------------------------------------ epoch 7610 (45654 steps) ------------------------------------\n",
      "Max loss: 0.020384948700666428\n",
      "Min loss: 0.004995359573513269\n",
      "Mean loss: 0.0084229475663354\n",
      "Std loss: 0.005382351768746021\n",
      "Total Loss: 0.0505376853980124\n",
      "------------------------------------ epoch 7611 (45660 steps) ------------------------------------\n",
      "Max loss: 0.01027503702789545\n",
      "Min loss: 0.005840307101607323\n",
      "Mean loss: 0.007714057806879282\n",
      "Std loss: 0.0017291770133461726\n",
      "Total Loss: 0.04628434684127569\n",
      "------------------------------------ epoch 7612 (45666 steps) ------------------------------------\n",
      "Max loss: 0.026544859632849693\n",
      "Min loss: 0.004869794938713312\n",
      "Mean loss: 0.009198313967014352\n",
      "Std loss: 0.007827624660923927\n",
      "Total Loss: 0.055189883802086115\n",
      "------------------------------------ epoch 7613 (45672 steps) ------------------------------------\n",
      "Max loss: 0.01143306028097868\n",
      "Min loss: 0.004426530562341213\n",
      "Mean loss: 0.007120915300523241\n",
      "Std loss: 0.002163146496868047\n",
      "Total Loss: 0.04272549180313945\n",
      "------------------------------------ epoch 7614 (45678 steps) ------------------------------------\n",
      "Max loss: 0.015266953967511654\n",
      "Min loss: 0.005503594875335693\n",
      "Mean loss: 0.00969765242189169\n",
      "Std loss: 0.0038903355467587108\n",
      "Total Loss: 0.058185914531350136\n",
      "------------------------------------ epoch 7615 (45684 steps) ------------------------------------\n",
      "Max loss: 0.012553555890917778\n",
      "Min loss: 0.0052068233489990234\n",
      "Mean loss: 0.008071106548110643\n",
      "Std loss: 0.0030017063237081725\n",
      "Total Loss: 0.048426639288663864\n",
      "------------------------------------ epoch 7616 (45690 steps) ------------------------------------\n",
      "Max loss: 0.016601137816905975\n",
      "Min loss: 0.004793793894350529\n",
      "Mean loss: 0.007679159054532647\n",
      "Std loss: 0.0042054968730629244\n",
      "Total Loss: 0.04607495432719588\n",
      "------------------------------------ epoch 7617 (45696 steps) ------------------------------------\n",
      "Max loss: 0.007807117886841297\n",
      "Min loss: 0.00646045058965683\n",
      "Mean loss: 0.0067763143839935465\n",
      "Std loss: 0.0004715727660000963\n",
      "Total Loss: 0.04065788630396128\n",
      "------------------------------------ epoch 7618 (45702 steps) ------------------------------------\n",
      "Max loss: 0.01068576704710722\n",
      "Min loss: 0.00470763212069869\n",
      "Mean loss: 0.007028818440934022\n",
      "Std loss: 0.0019767939705715404\n",
      "Total Loss: 0.042172910645604134\n",
      "------------------------------------ epoch 7619 (45708 steps) ------------------------------------\n",
      "Max loss: 0.010678892955183983\n",
      "Min loss: 0.004583557602018118\n",
      "Mean loss: 0.006723078743865092\n",
      "Std loss: 0.001891065855922895\n",
      "Total Loss: 0.040338472463190556\n",
      "------------------------------------ epoch 7620 (45714 steps) ------------------------------------\n",
      "Max loss: 0.01889105886220932\n",
      "Min loss: 0.00437540840357542\n",
      "Mean loss: 0.008715692209079862\n",
      "Std loss: 0.004747237177416791\n",
      "Total Loss: 0.05229415325447917\n",
      "------------------------------------ epoch 7621 (45720 steps) ------------------------------------\n",
      "Max loss: 0.008247476071119308\n",
      "Min loss: 0.005221630446612835\n",
      "Mean loss: 0.006921248200039069\n",
      "Std loss: 0.0010924453317943123\n",
      "Total Loss: 0.04152748920023441\n",
      "------------------------------------ epoch 7622 (45726 steps) ------------------------------------\n",
      "Max loss: 0.015021330676972866\n",
      "Min loss: 0.004546071402728558\n",
      "Mean loss: 0.008225883822888136\n",
      "Std loss: 0.00333203966908251\n",
      "Total Loss: 0.049355302937328815\n",
      "------------------------------------ epoch 7623 (45732 steps) ------------------------------------\n",
      "Max loss: 0.011401704512536526\n",
      "Min loss: 0.005555323325097561\n",
      "Mean loss: 0.008692582293103138\n",
      "Std loss: 0.0017204496430162086\n",
      "Total Loss: 0.05215549375861883\n",
      "------------------------------------ epoch 7624 (45738 steps) ------------------------------------\n",
      "Max loss: 0.011196427047252655\n",
      "Min loss: 0.004126390442252159\n",
      "Mean loss: 0.007205990453561147\n",
      "Std loss: 0.0021958886802707304\n",
      "Total Loss: 0.04323594272136688\n",
      "------------------------------------ epoch 7625 (45744 steps) ------------------------------------\n",
      "Max loss: 0.02284969948232174\n",
      "Min loss: 0.004219530150294304\n",
      "Mean loss: 0.012765046674758196\n",
      "Std loss: 0.0072000371614582125\n",
      "Total Loss: 0.07659028004854918\n",
      "------------------------------------ epoch 7626 (45750 steps) ------------------------------------\n",
      "Max loss: 0.010077055543661118\n",
      "Min loss: 0.0038758739829063416\n",
      "Mean loss: 0.00614772760309279\n",
      "Std loss: 0.001890534924537209\n",
      "Total Loss: 0.03688636561855674\n",
      "------------------------------------ epoch 7627 (45756 steps) ------------------------------------\n",
      "Max loss: 0.010127771645784378\n",
      "Min loss: 0.004422580357640982\n",
      "Mean loss: 0.006356929739316304\n",
      "Std loss: 0.00187601523999278\n",
      "Total Loss: 0.03814157843589783\n",
      "------------------------------------ epoch 7628 (45762 steps) ------------------------------------\n",
      "Max loss: 0.0220847949385643\n",
      "Min loss: 0.004821101203560829\n",
      "Mean loss: 0.009137954019630948\n",
      "Std loss: 0.00600128283875141\n",
      "Total Loss: 0.05482772411778569\n",
      "------------------------------------ epoch 7629 (45768 steps) ------------------------------------\n",
      "Max loss: 0.012291917577385902\n",
      "Min loss: 0.004332968033850193\n",
      "Mean loss: 0.007035576505586505\n",
      "Std loss: 0.0028056380235165507\n",
      "Total Loss: 0.04221345903351903\n",
      "------------------------------------ epoch 7630 (45774 steps) ------------------------------------\n",
      "Max loss: 0.021571971476078033\n",
      "Min loss: 0.0035352155100554228\n",
      "Mean loss: 0.008246001748678585\n",
      "Std loss: 0.0062037948949786766\n",
      "Total Loss: 0.04947601049207151\n",
      "------------------------------------ epoch 7631 (45780 steps) ------------------------------------\n",
      "Max loss: 0.011821924708783627\n",
      "Min loss: 0.003933175932615995\n",
      "Mean loss: 0.006960389747594793\n",
      "Std loss: 0.0027033685042233088\n",
      "Total Loss: 0.04176233848556876\n",
      "------------------------------------ epoch 7632 (45786 steps) ------------------------------------\n",
      "Max loss: 0.00797533243894577\n",
      "Min loss: 0.004673569928854704\n",
      "Mean loss: 0.005663535480077068\n",
      "Std loss: 0.0011471300606768079\n",
      "Total Loss: 0.03398121288046241\n",
      "------------------------------------ epoch 7633 (45792 steps) ------------------------------------\n",
      "Max loss: 0.007145391311496496\n",
      "Min loss: 0.004063081927597523\n",
      "Mean loss: 0.0057864992413669825\n",
      "Std loss: 0.000997760142260557\n",
      "Total Loss: 0.034718995448201895\n",
      "------------------------------------ epoch 7634 (45798 steps) ------------------------------------\n",
      "Max loss: 0.013181909918785095\n",
      "Min loss: 0.004308975767344236\n",
      "Mean loss: 0.00799554578649501\n",
      "Std loss: 0.002919804446268418\n",
      "Total Loss: 0.04797327471897006\n",
      "------------------------------------ epoch 7635 (45804 steps) ------------------------------------\n",
      "Max loss: 0.017643041908740997\n",
      "Min loss: 0.0034643698018044233\n",
      "Mean loss: 0.00964058528188616\n",
      "Std loss: 0.005433288354282972\n",
      "Total Loss: 0.05784351169131696\n",
      "------------------------------------ epoch 7636 (45810 steps) ------------------------------------\n",
      "Max loss: 0.011822765693068504\n",
      "Min loss: 0.004640178754925728\n",
      "Mean loss: 0.007565008321156104\n",
      "Std loss: 0.0024254706135456478\n",
      "Total Loss: 0.045390049926936626\n",
      "------------------------------------ epoch 7637 (45816 steps) ------------------------------------\n",
      "Max loss: 0.029552355408668518\n",
      "Min loss: 0.0038159694522619247\n",
      "Mean loss: 0.01137162217249473\n",
      "Std loss: 0.008698348616899257\n",
      "Total Loss: 0.06822973303496838\n",
      "------------------------------------ epoch 7638 (45822 steps) ------------------------------------\n",
      "Max loss: 0.02141522616147995\n",
      "Min loss: 0.005066261626780033\n",
      "Mean loss: 0.009361015710358819\n",
      "Std loss: 0.005733841478245582\n",
      "Total Loss: 0.05616609426215291\n",
      "------------------------------------ epoch 7639 (45828 steps) ------------------------------------\n",
      "Max loss: 0.011853676289319992\n",
      "Min loss: 0.005461202934384346\n",
      "Mean loss: 0.007553555459404985\n",
      "Std loss: 0.002349352973407859\n",
      "Total Loss: 0.04532133275642991\n",
      "------------------------------------ epoch 7640 (45834 steps) ------------------------------------\n",
      "Max loss: 0.018282227218151093\n",
      "Min loss: 0.005098365712910891\n",
      "Mean loss: 0.009057162096723914\n",
      "Std loss: 0.0043828438779320985\n",
      "Total Loss: 0.054342972580343485\n",
      "------------------------------------ epoch 7641 (45840 steps) ------------------------------------\n",
      "Max loss: 0.03389454632997513\n",
      "Min loss: 0.00783734954893589\n",
      "Mean loss: 0.013238983228802681\n",
      "Std loss: 0.009353458153694228\n",
      "Total Loss: 0.07943389937281609\n",
      "------------------------------------ epoch 7642 (45846 steps) ------------------------------------\n",
      "Max loss: 0.031770009547472\n",
      "Min loss: 0.006035942584276199\n",
      "Mean loss: 0.0145916942662249\n",
      "Std loss: 0.009198365336273686\n",
      "Total Loss: 0.0875501655973494\n",
      "------------------------------------ epoch 7643 (45852 steps) ------------------------------------\n",
      "Max loss: 0.023867737501859665\n",
      "Min loss: 0.0047052353620529175\n",
      "Mean loss: 0.012104195697853962\n",
      "Std loss: 0.006966400789294936\n",
      "Total Loss: 0.07262517418712378\n",
      "------------------------------------ epoch 7644 (45858 steps) ------------------------------------\n",
      "Max loss: 0.013748097233474255\n",
      "Min loss: 0.005414359271526337\n",
      "Mean loss: 0.007914704115440449\n",
      "Std loss: 0.0027590781781686273\n",
      "Total Loss: 0.04748822469264269\n",
      "------------------------------------ epoch 7645 (45864 steps) ------------------------------------\n",
      "Max loss: 0.008408967405557632\n",
      "Min loss: 0.005097729153931141\n",
      "Mean loss: 0.006701173726469278\n",
      "Std loss: 0.0012104866624265262\n",
      "Total Loss: 0.04020704235881567\n",
      "------------------------------------ epoch 7646 (45870 steps) ------------------------------------\n",
      "Max loss: 0.02212795615196228\n",
      "Min loss: 0.004368138499557972\n",
      "Mean loss: 0.010830382661273083\n",
      "Std loss: 0.005996921704387298\n",
      "Total Loss: 0.06498229596763849\n",
      "------------------------------------ epoch 7647 (45876 steps) ------------------------------------\n",
      "Max loss: 0.02982906810939312\n",
      "Min loss: 0.004589769057929516\n",
      "Mean loss: 0.011487365855524937\n",
      "Std loss: 0.008653412297941072\n",
      "Total Loss: 0.06892419513314962\n",
      "------------------------------------ epoch 7648 (45882 steps) ------------------------------------\n",
      "Max loss: 0.01726348139345646\n",
      "Min loss: 0.004012677818536758\n",
      "Mean loss: 0.007978727497781316\n",
      "Std loss: 0.0044974678428436445\n",
      "Total Loss: 0.0478723649866879\n",
      "------------------------------------ epoch 7649 (45888 steps) ------------------------------------\n",
      "Max loss: 0.009664865210652351\n",
      "Min loss: 0.005067291669547558\n",
      "Mean loss: 0.007221968146041036\n",
      "Std loss: 0.0014931446316129608\n",
      "Total Loss: 0.043331808876246214\n",
      "------------------------------------ epoch 7650 (45894 steps) ------------------------------------\n",
      "Max loss: 0.035453397780656815\n",
      "Min loss: 0.004521822556853294\n",
      "Mean loss: 0.01294176218410333\n",
      "Std loss: 0.010776144849354895\n",
      "Total Loss: 0.07765057310461998\n",
      "------------------------------------ epoch 7651 (45900 steps) ------------------------------------\n",
      "Max loss: 0.020961416885256767\n",
      "Min loss: 0.004810609854757786\n",
      "Mean loss: 0.010037623811513186\n",
      "Std loss: 0.005233190485831318\n",
      "Total Loss: 0.06022574286907911\n",
      "------------------------------------ epoch 7652 (45906 steps) ------------------------------------\n",
      "Max loss: 0.029149694368243217\n",
      "Min loss: 0.004805242642760277\n",
      "Mean loss: 0.01385734936532875\n",
      "Std loss: 0.007920405705872283\n",
      "Total Loss: 0.0831440961919725\n",
      "------------------------------------ epoch 7653 (45912 steps) ------------------------------------\n",
      "Max loss: 0.014003823511302471\n",
      "Min loss: 0.005641465540975332\n",
      "Mean loss: 0.008705423368761936\n",
      "Std loss: 0.0029313839456641087\n",
      "Total Loss: 0.05223254021257162\n",
      "------------------------------------ epoch 7654 (45918 steps) ------------------------------------\n",
      "Max loss: 0.0073749469593167305\n",
      "Min loss: 0.004947412759065628\n",
      "Mean loss: 0.006508086419974764\n",
      "Std loss: 0.0007634869051119907\n",
      "Total Loss: 0.039048518519848585\n",
      "------------------------------------ epoch 7655 (45924 steps) ------------------------------------\n",
      "Max loss: 0.04427725076675415\n",
      "Min loss: 0.0053590089082717896\n",
      "Mean loss: 0.014092302337909738\n",
      "Std loss: 0.013588355393563456\n",
      "Total Loss: 0.08455381402745843\n",
      "------------------------------------ epoch 7656 (45930 steps) ------------------------------------\n",
      "Max loss: 0.019109833985567093\n",
      "Min loss: 0.005272882990539074\n",
      "Mean loss: 0.009778129402548075\n",
      "Std loss: 0.005485589712066108\n",
      "Total Loss: 0.05866877641528845\n",
      "------------------------------------ epoch 7657 (45936 steps) ------------------------------------\n",
      "Max loss: 0.013777881860733032\n",
      "Min loss: 0.005509650334715843\n",
      "Mean loss: 0.008696631217996279\n",
      "Std loss: 0.002865483682235231\n",
      "Total Loss: 0.052179787307977676\n",
      "------------------------------------ epoch 7658 (45942 steps) ------------------------------------\n",
      "Max loss: 0.010059303604066372\n",
      "Min loss: 0.004332250915467739\n",
      "Mean loss: 0.0064733619801700115\n",
      "Std loss: 0.002003531554896242\n",
      "Total Loss: 0.03884017188102007\n",
      "------------------------------------ epoch 7659 (45948 steps) ------------------------------------\n",
      "Max loss: 0.008963607251644135\n",
      "Min loss: 0.005183550529181957\n",
      "Mean loss: 0.006794619218756755\n",
      "Std loss: 0.001377874205705555\n",
      "Total Loss: 0.04076771531254053\n",
      "------------------------------------ epoch 7660 (45954 steps) ------------------------------------\n",
      "Max loss: 0.023579850792884827\n",
      "Min loss: 0.00504674157127738\n",
      "Mean loss: 0.00989299244247377\n",
      "Std loss: 0.0067731997338588345\n",
      "Total Loss: 0.059357954654842615\n",
      "------------------------------------ epoch 7661 (45960 steps) ------------------------------------\n",
      "Max loss: 0.010795186273753643\n",
      "Min loss: 0.004867753945291042\n",
      "Mean loss: 0.006985090517749389\n",
      "Std loss: 0.002138364075114031\n",
      "Total Loss: 0.041910543106496334\n",
      "------------------------------------ epoch 7662 (45966 steps) ------------------------------------\n",
      "Max loss: 0.019254587590694427\n",
      "Min loss: 0.0042670415714383125\n",
      "Mean loss: 0.009674927995850643\n",
      "Std loss: 0.004744773509059738\n",
      "Total Loss: 0.058049567975103855\n",
      "------------------------------------ epoch 7663 (45972 steps) ------------------------------------\n",
      "Max loss: 0.020524442195892334\n",
      "Min loss: 0.005717639811336994\n",
      "Mean loss: 0.010186205773303906\n",
      "Std loss: 0.005353529201633963\n",
      "Total Loss: 0.06111723463982344\n",
      "------------------------------------ epoch 7664 (45978 steps) ------------------------------------\n",
      "Max loss: 0.014555669389665127\n",
      "Min loss: 0.004598645959049463\n",
      "Mean loss: 0.008503372858588895\n",
      "Std loss: 0.0035434137574487617\n",
      "Total Loss: 0.051020237151533365\n",
      "------------------------------------ epoch 7665 (45984 steps) ------------------------------------\n",
      "Max loss: 0.032288193702697754\n",
      "Min loss: 0.0042913565412163734\n",
      "Mean loss: 0.0105336745424817\n",
      "Std loss: 0.009842273098508155\n",
      "Total Loss: 0.0632020472548902\n",
      "------------------------------------ epoch 7666 (45990 steps) ------------------------------------\n",
      "Max loss: 0.01681157387793064\n",
      "Min loss: 0.005783346947282553\n",
      "Mean loss: 0.01102120941504836\n",
      "Std loss: 0.004242611058463927\n",
      "Total Loss: 0.06612725649029016\n",
      "------------------------------------ epoch 7667 (45996 steps) ------------------------------------\n",
      "Max loss: 0.021710500121116638\n",
      "Min loss: 0.005274507217109203\n",
      "Mean loss: 0.013455016848941645\n",
      "Std loss: 0.006839648609007023\n",
      "Total Loss: 0.08073010109364986\n",
      "------------------------------------ epoch 7668 (46002 steps) ------------------------------------\n",
      "Max loss: 0.020059892907738686\n",
      "Min loss: 0.004533811472356319\n",
      "Mean loss: 0.010430400725454092\n",
      "Std loss: 0.005614284410206154\n",
      "Total Loss: 0.06258240435272455\n",
      "------------------------------------ epoch 7669 (46008 steps) ------------------------------------\n",
      "Max loss: 0.021295038983225822\n",
      "Min loss: 0.005413203500211239\n",
      "Mean loss: 0.010791806581740579\n",
      "Std loss: 0.005345815983394194\n",
      "Total Loss: 0.06475083949044347\n",
      "------------------------------------ epoch 7670 (46014 steps) ------------------------------------\n",
      "Max loss: 0.014490747824311256\n",
      "Min loss: 0.0047372495755553246\n",
      "Mean loss: 0.010123258301367363\n",
      "Std loss: 0.0033041878251377815\n",
      "Total Loss: 0.060739549808204174\n",
      "------------------------------------ epoch 7671 (46020 steps) ------------------------------------\n",
      "Max loss: 0.019337352365255356\n",
      "Min loss: 0.003616366535425186\n",
      "Mean loss: 0.00735229067504406\n",
      "Std loss: 0.00554680821522674\n",
      "Total Loss: 0.04411374405026436\n",
      "------------------------------------ epoch 7672 (46026 steps) ------------------------------------\n",
      "Max loss: 0.04786160588264465\n",
      "Min loss: 0.004523680545389652\n",
      "Mean loss: 0.020143468398600817\n",
      "Std loss: 0.01327211540831055\n",
      "Total Loss: 0.1208608103916049\n",
      "------------------------------------ epoch 7673 (46032 steps) ------------------------------------\n",
      "Max loss: 0.023651834577322006\n",
      "Min loss: 0.006632419303059578\n",
      "Mean loss: 0.013900332308063904\n",
      "Std loss: 0.005882789196876488\n",
      "Total Loss: 0.08340199384838343\n",
      "------------------------------------ epoch 7674 (46038 steps) ------------------------------------\n",
      "Max loss: 0.014398242346942425\n",
      "Min loss: 0.005012921057641506\n",
      "Mean loss: 0.010039909975603223\n",
      "Std loss: 0.003741037329216574\n",
      "Total Loss: 0.06023945985361934\n",
      "------------------------------------ epoch 7675 (46044 steps) ------------------------------------\n",
      "Max loss: 0.013820136897265911\n",
      "Min loss: 0.006418860517442226\n",
      "Mean loss: 0.010332544955114523\n",
      "Std loss: 0.0024330114371794785\n",
      "Total Loss: 0.06199526973068714\n",
      "------------------------------------ epoch 7676 (46050 steps) ------------------------------------\n",
      "Max loss: 0.018832862377166748\n",
      "Min loss: 0.004471957683563232\n",
      "Mean loss: 0.010735192879413566\n",
      "Std loss: 0.005840655987439637\n",
      "Total Loss: 0.06441115727648139\n",
      "------------------------------------ epoch 7677 (46056 steps) ------------------------------------\n",
      "Max loss: 0.016898101195693016\n",
      "Min loss: 0.004524581599980593\n",
      "Mean loss: 0.008730003610253334\n",
      "Std loss: 0.004310852156738661\n",
      "Total Loss: 0.052380021661520004\n",
      "------------------------------------ epoch 7678 (46062 steps) ------------------------------------\n",
      "Max loss: 0.026288948953151703\n",
      "Min loss: 0.006201558746397495\n",
      "Mean loss: 0.012415533031647405\n",
      "Std loss: 0.007052600322149109\n",
      "Total Loss: 0.07449319818988442\n",
      "------------------------------------ epoch 7679 (46068 steps) ------------------------------------\n",
      "Max loss: 0.011920107528567314\n",
      "Min loss: 0.0047273035161197186\n",
      "Mean loss: 0.006810869478310148\n",
      "Std loss: 0.0024273097208700038\n",
      "Total Loss: 0.04086521686986089\n",
      "------------------------------------ epoch 7680 (46074 steps) ------------------------------------\n",
      "Max loss: 0.014283004216849804\n",
      "Min loss: 0.004322875291109085\n",
      "Mean loss: 0.008054516511037946\n",
      "Std loss: 0.003986372682985577\n",
      "Total Loss: 0.048327099066227674\n",
      "------------------------------------ epoch 7681 (46080 steps) ------------------------------------\n",
      "Max loss: 0.016830328851938248\n",
      "Min loss: 0.006398329511284828\n",
      "Mean loss: 0.009256589381645123\n",
      "Std loss: 0.0035340225437302275\n",
      "Total Loss: 0.05553953628987074\n",
      "------------------------------------ epoch 7682 (46086 steps) ------------------------------------\n",
      "Max loss: 0.016546709463000298\n",
      "Min loss: 0.004742068238556385\n",
      "Mean loss: 0.009034796928366026\n",
      "Std loss: 0.00417703032503804\n",
      "Total Loss: 0.05420878157019615\n",
      "------------------------------------ epoch 7683 (46092 steps) ------------------------------------\n",
      "Max loss: 0.02202942781150341\n",
      "Min loss: 0.005336811300367117\n",
      "Mean loss: 0.013653711803878347\n",
      "Std loss: 0.005317361593208632\n",
      "Total Loss: 0.08192227082327008\n",
      "------------------------------------ epoch 7684 (46098 steps) ------------------------------------\n",
      "Max loss: 0.023288821801543236\n",
      "Min loss: 0.0047559174709022045\n",
      "Mean loss: 0.010966309113427997\n",
      "Std loss: 0.007095412969299414\n",
      "Total Loss: 0.06579785468056798\n",
      "------------------------------------ epoch 7685 (46104 steps) ------------------------------------\n",
      "Max loss: 0.014827681705355644\n",
      "Min loss: 0.003983271308243275\n",
      "Mean loss: 0.008549473791693648\n",
      "Std loss: 0.003848778612216504\n",
      "Total Loss: 0.051296842750161886\n",
      "------------------------------------ epoch 7686 (46110 steps) ------------------------------------\n",
      "Max loss: 0.015443988144397736\n",
      "Min loss: 0.004799510817974806\n",
      "Mean loss: 0.009018055939426025\n",
      "Std loss: 0.003764534872987807\n",
      "Total Loss: 0.05410833563655615\n",
      "------------------------------------ epoch 7687 (46116 steps) ------------------------------------\n",
      "Max loss: 0.01678132265806198\n",
      "Min loss: 0.006510774604976177\n",
      "Mean loss: 0.010712834540754557\n",
      "Std loss: 0.0031480689866769667\n",
      "Total Loss: 0.06427700724452734\n",
      "------------------------------------ epoch 7688 (46122 steps) ------------------------------------\n",
      "Max loss: 0.02001337520778179\n",
      "Min loss: 0.00519193708896637\n",
      "Mean loss: 0.01064284099265933\n",
      "Std loss: 0.005021028673420411\n",
      "Total Loss: 0.06385704595595598\n",
      "------------------------------------ epoch 7689 (46128 steps) ------------------------------------\n",
      "Max loss: 0.01440910529345274\n",
      "Min loss: 0.005211784970015287\n",
      "Mean loss: 0.00837684174378713\n",
      "Std loss: 0.0030882260923381652\n",
      "Total Loss: 0.05026105046272278\n",
      "------------------------------------ epoch 7690 (46134 steps) ------------------------------------\n",
      "Max loss: 0.009811971336603165\n",
      "Min loss: 0.004019215237349272\n",
      "Mean loss: 0.006736452225595713\n",
      "Std loss: 0.002263907714220786\n",
      "Total Loss: 0.040418713353574276\n",
      "------------------------------------ epoch 7691 (46140 steps) ------------------------------------\n",
      "Max loss: 0.01747044548392296\n",
      "Min loss: 0.005350341089069843\n",
      "Mean loss: 0.011004263069480658\n",
      "Std loss: 0.0045721975747253325\n",
      "Total Loss: 0.06602557841688395\n",
      "------------------------------------ epoch 7692 (46146 steps) ------------------------------------\n",
      "Max loss: 0.013724517077207565\n",
      "Min loss: 0.005488581955432892\n",
      "Mean loss: 0.008853420925637087\n",
      "Std loss: 0.0027710657080957774\n",
      "Total Loss: 0.05312052555382252\n",
      "------------------------------------ epoch 7693 (46152 steps) ------------------------------------\n",
      "Max loss: 0.009283573366701603\n",
      "Min loss: 0.005593105219304562\n",
      "Mean loss: 0.007156563612322013\n",
      "Std loss: 0.0012719201077716976\n",
      "Total Loss: 0.042939381673932076\n",
      "------------------------------------ epoch 7694 (46158 steps) ------------------------------------\n",
      "Max loss: 0.024279307574033737\n",
      "Min loss: 0.005973295774310827\n",
      "Mean loss: 0.010733772612487277\n",
      "Std loss: 0.00622467604849073\n",
      "Total Loss: 0.06440263567492366\n",
      "------------------------------------ epoch 7695 (46164 steps) ------------------------------------\n",
      "Max loss: 0.01264497172087431\n",
      "Min loss: 0.004130388610064983\n",
      "Mean loss: 0.007025450002402067\n",
      "Std loss: 0.003010744815317608\n",
      "Total Loss: 0.0421527000144124\n",
      "------------------------------------ epoch 7696 (46170 steps) ------------------------------------\n",
      "Max loss: 0.007138247601687908\n",
      "Min loss: 0.003693066071718931\n",
      "Mean loss: 0.0049843979844202595\n",
      "Std loss: 0.0012593651310179462\n",
      "Total Loss: 0.02990638790652156\n",
      "------------------------------------ epoch 7697 (46176 steps) ------------------------------------\n",
      "Max loss: 0.008893181569874287\n",
      "Min loss: 0.0037159889470785856\n",
      "Mean loss: 0.0060970328049734235\n",
      "Std loss: 0.0017594556749614032\n",
      "Total Loss: 0.03658219682984054\n",
      "------------------------------------ epoch 7698 (46182 steps) ------------------------------------\n",
      "Max loss: 0.012626334093511105\n",
      "Min loss: 0.005156048573553562\n",
      "Mean loss: 0.007830314493427673\n",
      "Std loss: 0.0026275705434984846\n",
      "Total Loss: 0.046981886960566044\n",
      "------------------------------------ epoch 7699 (46188 steps) ------------------------------------\n",
      "Max loss: 0.02086137793958187\n",
      "Min loss: 0.004781822208315134\n",
      "Mean loss: 0.010920249624177814\n",
      "Std loss: 0.006601562401743441\n",
      "Total Loss: 0.06552149774506688\n",
      "------------------------------------ epoch 7700 (46194 steps) ------------------------------------\n",
      "Max loss: 0.010823974385857582\n",
      "Min loss: 0.00554403942078352\n",
      "Mean loss: 0.007131521745274465\n",
      "Std loss: 0.0018124909774293017\n",
      "Total Loss: 0.042789130471646786\n",
      "------------------------------------ epoch 7701 (46200 steps) ------------------------------------\n",
      "Max loss: 0.0134293083101511\n",
      "Min loss: 0.007255941163748503\n",
      "Mean loss: 0.008911943606411418\n",
      "Std loss: 0.0021666274053100077\n",
      "Total Loss: 0.053471661638468504\n",
      "saved model at ./weights/model_7701.pth\n",
      "------------------------------------ epoch 7702 (46206 steps) ------------------------------------\n",
      "Max loss: 0.02148173749446869\n",
      "Min loss: 0.004460569005459547\n",
      "Mean loss: 0.009474805323407054\n",
      "Std loss: 0.005771330053376855\n",
      "Total Loss: 0.056848831940442324\n",
      "------------------------------------ epoch 7703 (46212 steps) ------------------------------------\n",
      "Max loss: 0.007736804895102978\n",
      "Min loss: 0.004437064286321402\n",
      "Mean loss: 0.005417357819775741\n",
      "Std loss: 0.0011723841947100964\n",
      "Total Loss: 0.03250414691865444\n",
      "------------------------------------ epoch 7704 (46218 steps) ------------------------------------\n",
      "Max loss: 0.00981678906828165\n",
      "Min loss: 0.0039712428115308285\n",
      "Mean loss: 0.006875270356734593\n",
      "Std loss: 0.0020697911848343554\n",
      "Total Loss: 0.04125162214040756\n",
      "------------------------------------ epoch 7705 (46224 steps) ------------------------------------\n",
      "Max loss: 0.012245310470461845\n",
      "Min loss: 0.004125394858419895\n",
      "Mean loss: 0.007179043255746365\n",
      "Std loss: 0.002475031634014515\n",
      "Total Loss: 0.04307425953447819\n",
      "------------------------------------ epoch 7706 (46230 steps) ------------------------------------\n",
      "Max loss: 0.03064386174082756\n",
      "Min loss: 0.004343011416494846\n",
      "Mean loss: 0.010489581773678461\n",
      "Std loss: 0.009247813762303629\n",
      "Total Loss: 0.06293749064207077\n",
      "------------------------------------ epoch 7707 (46236 steps) ------------------------------------\n",
      "Max loss: 0.007370071019977331\n",
      "Min loss: 0.003966280724853277\n",
      "Mean loss: 0.006036860790724556\n",
      "Std loss: 0.0011664310788713195\n",
      "Total Loss: 0.036221164744347334\n",
      "------------------------------------ epoch 7708 (46242 steps) ------------------------------------\n",
      "Max loss: 0.009671545587480068\n",
      "Min loss: 0.003977140411734581\n",
      "Mean loss: 0.006250197145467003\n",
      "Std loss: 0.0017704892383098743\n",
      "Total Loss: 0.03750118287280202\n",
      "------------------------------------ epoch 7709 (46248 steps) ------------------------------------\n",
      "Max loss: 0.007554649841040373\n",
      "Min loss: 0.0037273732014000416\n",
      "Mean loss: 0.00575287997101744\n",
      "Std loss: 0.0014331641189671714\n",
      "Total Loss: 0.03451727982610464\n",
      "------------------------------------ epoch 7710 (46254 steps) ------------------------------------\n",
      "Max loss: 0.00829185638576746\n",
      "Min loss: 0.003933777566999197\n",
      "Mean loss: 0.00567992761110266\n",
      "Std loss: 0.0016633978972854146\n",
      "Total Loss: 0.03407956566661596\n",
      "------------------------------------ epoch 7711 (46260 steps) ------------------------------------\n",
      "Max loss: 0.009119361639022827\n",
      "Min loss: 0.003683198941871524\n",
      "Mean loss: 0.006411892478354275\n",
      "Std loss: 0.0019248945377173942\n",
      "Total Loss: 0.03847135487012565\n",
      "------------------------------------ epoch 7712 (46266 steps) ------------------------------------\n",
      "Max loss: 0.007201513275504112\n",
      "Min loss: 0.0031650103628635406\n",
      "Mean loss: 0.004970928421244025\n",
      "Std loss: 0.001663177585405808\n",
      "Total Loss: 0.02982557052746415\n",
      "------------------------------------ epoch 7713 (46272 steps) ------------------------------------\n",
      "Max loss: 0.015432553365826607\n",
      "Min loss: 0.004405067767947912\n",
      "Mean loss: 0.00797476495305697\n",
      "Std loss: 0.0038926208605392044\n",
      "Total Loss: 0.04784858971834183\n",
      "------------------------------------ epoch 7714 (46278 steps) ------------------------------------\n",
      "Max loss: 0.023198086768388748\n",
      "Min loss: 0.004649925045669079\n",
      "Mean loss: 0.010260066405559579\n",
      "Std loss: 0.00651609852748309\n",
      "Total Loss: 0.06156039843335748\n",
      "------------------------------------ epoch 7715 (46284 steps) ------------------------------------\n",
      "Max loss: 0.024149935692548752\n",
      "Min loss: 0.005461480002850294\n",
      "Mean loss: 0.012664261584480604\n",
      "Std loss: 0.0079842944036785\n",
      "Total Loss: 0.07598556950688362\n",
      "------------------------------------ epoch 7716 (46290 steps) ------------------------------------\n",
      "Max loss: 0.01881197653710842\n",
      "Min loss: 0.005426162853837013\n",
      "Mean loss: 0.010227525684361657\n",
      "Std loss: 0.004675361460861004\n",
      "Total Loss: 0.06136515410616994\n",
      "------------------------------------ epoch 7717 (46296 steps) ------------------------------------\n",
      "Max loss: 0.031434450298547745\n",
      "Min loss: 0.005348899867385626\n",
      "Mean loss: 0.015490501963843903\n",
      "Std loss: 0.009564064328226578\n",
      "Total Loss: 0.09294301178306341\n",
      "------------------------------------ epoch 7718 (46302 steps) ------------------------------------\n",
      "Max loss: 0.01933456026017666\n",
      "Min loss: 0.004945910535752773\n",
      "Mean loss: 0.008347957860678434\n",
      "Std loss: 0.005129709793262678\n",
      "Total Loss: 0.050087747164070606\n",
      "------------------------------------ epoch 7719 (46308 steps) ------------------------------------\n",
      "Max loss: 0.01771536096930504\n",
      "Min loss: 0.004848498851060867\n",
      "Mean loss: 0.008301288665582737\n",
      "Std loss: 0.00441609577524782\n",
      "Total Loss: 0.04980773199349642\n",
      "------------------------------------ epoch 7720 (46314 steps) ------------------------------------\n",
      "Max loss: 0.01599365472793579\n",
      "Min loss: 0.005282131489366293\n",
      "Mean loss: 0.008964039773369828\n",
      "Std loss: 0.003645401179845619\n",
      "Total Loss: 0.05378423864021897\n",
      "------------------------------------ epoch 7721 (46320 steps) ------------------------------------\n",
      "Max loss: 0.008744931779801846\n",
      "Min loss: 0.004794483073055744\n",
      "Mean loss: 0.006937193373839061\n",
      "Std loss: 0.0014014190734632293\n",
      "Total Loss: 0.04162316024303436\n",
      "------------------------------------ epoch 7722 (46326 steps) ------------------------------------\n",
      "Max loss: 0.017835207283496857\n",
      "Min loss: 0.006078807637095451\n",
      "Mean loss: 0.010972636130948862\n",
      "Std loss: 0.00358482357182404\n",
      "Total Loss: 0.06583581678569317\n",
      "------------------------------------ epoch 7723 (46332 steps) ------------------------------------\n",
      "Max loss: 0.009091652929782867\n",
      "Min loss: 0.0036628185771405697\n",
      "Mean loss: 0.0059708403423428535\n",
      "Std loss: 0.0019051302094411178\n",
      "Total Loss: 0.03582504205405712\n",
      "------------------------------------ epoch 7724 (46338 steps) ------------------------------------\n",
      "Max loss: 0.021037673577666283\n",
      "Min loss: 0.006014029495418072\n",
      "Mean loss: 0.010203188440452019\n",
      "Std loss: 0.005204256050068646\n",
      "Total Loss: 0.061219130642712116\n",
      "------------------------------------ epoch 7725 (46344 steps) ------------------------------------\n",
      "Max loss: 0.011068746447563171\n",
      "Min loss: 0.004488503094762564\n",
      "Mean loss: 0.006526184966787696\n",
      "Std loss: 0.002330799854188792\n",
      "Total Loss: 0.039157109800726175\n",
      "------------------------------------ epoch 7726 (46350 steps) ------------------------------------\n",
      "Max loss: 0.03113473579287529\n",
      "Min loss: 0.004385804757475853\n",
      "Mean loss: 0.010632681893184781\n",
      "Std loss: 0.009273707720266047\n",
      "Total Loss: 0.06379609135910869\n",
      "------------------------------------ epoch 7727 (46356 steps) ------------------------------------\n",
      "Max loss: 0.013506896793842316\n",
      "Min loss: 0.004195161163806915\n",
      "Mean loss: 0.008273587329313159\n",
      "Std loss: 0.0036870417087072702\n",
      "Total Loss: 0.049641523975878954\n",
      "------------------------------------ epoch 7728 (46362 steps) ------------------------------------\n",
      "Max loss: 0.010414011776447296\n",
      "Min loss: 0.0044944435358047485\n",
      "Mean loss: 0.007869604819764694\n",
      "Std loss: 0.0018312571182477553\n",
      "Total Loss: 0.04721762891858816\n",
      "------------------------------------ epoch 7729 (46368 steps) ------------------------------------\n",
      "Max loss: 0.014972835779190063\n",
      "Min loss: 0.003907530568540096\n",
      "Mean loss: 0.0072472623238960905\n",
      "Std loss: 0.003785325919715525\n",
      "Total Loss: 0.04348357394337654\n",
      "------------------------------------ epoch 7730 (46374 steps) ------------------------------------\n",
      "Max loss: 0.016694050282239914\n",
      "Min loss: 0.004043736029416323\n",
      "Mean loss: 0.008901643954838315\n",
      "Std loss: 0.003925869233335369\n",
      "Total Loss: 0.053409863729029894\n",
      "------------------------------------ epoch 7731 (46380 steps) ------------------------------------\n",
      "Max loss: 0.009726188145577908\n",
      "Min loss: 0.0045329499989748\n",
      "Mean loss: 0.006035849063967665\n",
      "Std loss: 0.0017478194428151323\n",
      "Total Loss: 0.03621509438380599\n",
      "------------------------------------ epoch 7732 (46386 steps) ------------------------------------\n",
      "Max loss: 0.011625631712377071\n",
      "Min loss: 0.003031546948477626\n",
      "Mean loss: 0.0071548694201434655\n",
      "Std loss: 0.0031427126433845173\n",
      "Total Loss: 0.04292921652086079\n",
      "------------------------------------ epoch 7733 (46392 steps) ------------------------------------\n",
      "Max loss: 0.04523881524801254\n",
      "Min loss: 0.003111420664936304\n",
      "Mean loss: 0.01288313379821678\n",
      "Std loss: 0.014815558706176905\n",
      "Total Loss: 0.07729880278930068\n",
      "------------------------------------ epoch 7734 (46398 steps) ------------------------------------\n",
      "Max loss: 0.03222958743572235\n",
      "Min loss: 0.005368111189454794\n",
      "Mean loss: 0.017392928168798488\n",
      "Std loss: 0.009907406116753675\n",
      "Total Loss: 0.10435756901279092\n",
      "------------------------------------ epoch 7735 (46404 steps) ------------------------------------\n",
      "Max loss: 0.012345883995294571\n",
      "Min loss: 0.0059499493800103664\n",
      "Mean loss: 0.007876638090237975\n",
      "Std loss: 0.002259473602342512\n",
      "Total Loss: 0.04725982854142785\n",
      "------------------------------------ epoch 7736 (46410 steps) ------------------------------------\n",
      "Max loss: 0.016474327072501183\n",
      "Min loss: 0.005784377455711365\n",
      "Mean loss: 0.010659249499440193\n",
      "Std loss: 0.004331813478462825\n",
      "Total Loss: 0.06395549699664116\n",
      "------------------------------------ epoch 7737 (46416 steps) ------------------------------------\n",
      "Max loss: 0.034181803464889526\n",
      "Min loss: 0.006213991902768612\n",
      "Mean loss: 0.01395996722082297\n",
      "Std loss: 0.009406449121460464\n",
      "Total Loss: 0.08375980332493782\n",
      "------------------------------------ epoch 7738 (46422 steps) ------------------------------------\n",
      "Max loss: 0.04460334777832031\n",
      "Min loss: 0.004454717040061951\n",
      "Mean loss: 0.013464942031229535\n",
      "Std loss: 0.014057358661785634\n",
      "Total Loss: 0.08078965218737721\n",
      "------------------------------------ epoch 7739 (46428 steps) ------------------------------------\n",
      "Max loss: 0.0214934553951025\n",
      "Min loss: 0.006248871795833111\n",
      "Mean loss: 0.01053499880557259\n",
      "Std loss: 0.005054514535028806\n",
      "Total Loss: 0.06320999283343554\n",
      "------------------------------------ epoch 7740 (46434 steps) ------------------------------------\n",
      "Max loss: 0.011796774342656136\n",
      "Min loss: 0.005220636259764433\n",
      "Mean loss: 0.0075082840242733555\n",
      "Std loss: 0.0020531801243163198\n",
      "Total Loss: 0.045049704145640135\n",
      "------------------------------------ epoch 7741 (46440 steps) ------------------------------------\n",
      "Max loss: 0.012464595958590508\n",
      "Min loss: 0.005844810511916876\n",
      "Mean loss: 0.00862325083774825\n",
      "Std loss: 0.00231919311783514\n",
      "Total Loss: 0.051739505026489496\n",
      "------------------------------------ epoch 7742 (46446 steps) ------------------------------------\n",
      "Max loss: 0.014217846095561981\n",
      "Min loss: 0.004333538003265858\n",
      "Mean loss: 0.008957555672774712\n",
      "Std loss: 0.003463961936309775\n",
      "Total Loss: 0.05374533403664827\n",
      "------------------------------------ epoch 7743 (46452 steps) ------------------------------------\n",
      "Max loss: 0.0146790174767375\n",
      "Min loss: 0.0057354397140443325\n",
      "Mean loss: 0.009800784289836884\n",
      "Std loss: 0.003706601890733067\n",
      "Total Loss: 0.0588047057390213\n",
      "------------------------------------ epoch 7744 (46458 steps) ------------------------------------\n",
      "Max loss: 0.03861723095178604\n",
      "Min loss: 0.005274253897368908\n",
      "Mean loss: 0.012757769708211223\n",
      "Std loss: 0.011797747669197639\n",
      "Total Loss: 0.07654661824926734\n",
      "------------------------------------ epoch 7745 (46464 steps) ------------------------------------\n",
      "Max loss: 0.013834680430591106\n",
      "Min loss: 0.00585824903100729\n",
      "Mean loss: 0.0096094257508715\n",
      "Std loss: 0.0033106975262324504\n",
      "Total Loss: 0.057656554505228996\n",
      "------------------------------------ epoch 7746 (46470 steps) ------------------------------------\n",
      "Max loss: 0.01958439312875271\n",
      "Min loss: 0.004893233999609947\n",
      "Mean loss: 0.009352771410097679\n",
      "Std loss: 0.005225445330549776\n",
      "Total Loss: 0.05611662846058607\n",
      "------------------------------------ epoch 7747 (46476 steps) ------------------------------------\n",
      "Max loss: 0.013444507494568825\n",
      "Min loss: 0.0065353037789464\n",
      "Mean loss: 0.010380230688800415\n",
      "Std loss: 0.002393571417676475\n",
      "Total Loss: 0.062281384132802486\n",
      "------------------------------------ epoch 7748 (46482 steps) ------------------------------------\n",
      "Max loss: 0.013123082928359509\n",
      "Min loss: 0.004620749037712812\n",
      "Mean loss: 0.006690603603298466\n",
      "Std loss: 0.00291025223172939\n",
      "Total Loss: 0.04014362161979079\n",
      "------------------------------------ epoch 7749 (46488 steps) ------------------------------------\n",
      "Max loss: 0.011186334304511547\n",
      "Min loss: 0.0038352063857018948\n",
      "Mean loss: 0.00752690271474421\n",
      "Std loss: 0.003040112063139085\n",
      "Total Loss: 0.04516141628846526\n",
      "------------------------------------ epoch 7750 (46494 steps) ------------------------------------\n",
      "Max loss: 0.0224925484508276\n",
      "Min loss: 0.007589404936879873\n",
      "Mean loss: 0.014298457729940614\n",
      "Std loss: 0.004924688746197664\n",
      "Total Loss: 0.08579074637964368\n",
      "------------------------------------ epoch 7751 (46500 steps) ------------------------------------\n",
      "Max loss: 0.017142636701464653\n",
      "Min loss: 0.004191014915704727\n",
      "Mean loss: 0.008428656030446291\n",
      "Std loss: 0.004176387827712636\n",
      "Total Loss: 0.050571936182677746\n",
      "------------------------------------ epoch 7752 (46506 steps) ------------------------------------\n",
      "Max loss: 0.008159514516592026\n",
      "Min loss: 0.004729064647108316\n",
      "Mean loss: 0.005936949669073026\n",
      "Std loss: 0.0013318517367151507\n",
      "Total Loss: 0.03562169801443815\n",
      "------------------------------------ epoch 7753 (46512 steps) ------------------------------------\n",
      "Max loss: 0.011551141738891602\n",
      "Min loss: 0.00400589732453227\n",
      "Mean loss: 0.0069675378035753965\n",
      "Std loss: 0.00289080097292085\n",
      "Total Loss: 0.04180522682145238\n",
      "------------------------------------ epoch 7754 (46518 steps) ------------------------------------\n",
      "Max loss: 0.012824169360101223\n",
      "Min loss: 0.00334713002666831\n",
      "Mean loss: 0.00841440485479931\n",
      "Std loss: 0.003415376382294556\n",
      "Total Loss: 0.05048642912879586\n",
      "------------------------------------ epoch 7755 (46524 steps) ------------------------------------\n",
      "Max loss: 0.01282702200114727\n",
      "Min loss: 0.007039938122034073\n",
      "Mean loss: 0.009826381069918474\n",
      "Std loss: 0.0020990215619661468\n",
      "Total Loss: 0.05895828641951084\n",
      "------------------------------------ epoch 7756 (46530 steps) ------------------------------------\n",
      "Max loss: 0.015745295211672783\n",
      "Min loss: 0.004046529531478882\n",
      "Mean loss: 0.007736097477997343\n",
      "Std loss: 0.003799254590412882\n",
      "Total Loss: 0.046416584867984056\n",
      "------------------------------------ epoch 7757 (46536 steps) ------------------------------------\n",
      "Max loss: 0.015653109177947044\n",
      "Min loss: 0.0037226658314466476\n",
      "Mean loss: 0.008829255898793539\n",
      "Std loss: 0.004434484548608835\n",
      "Total Loss: 0.05297553539276123\n",
      "------------------------------------ epoch 7758 (46542 steps) ------------------------------------\n",
      "Max loss: 0.007948790676891804\n",
      "Min loss: 0.004222028888761997\n",
      "Mean loss: 0.005422390221307675\n",
      "Std loss: 0.001373339693673766\n",
      "Total Loss: 0.03253434132784605\n",
      "------------------------------------ epoch 7759 (46548 steps) ------------------------------------\n",
      "Max loss: 0.03157496452331543\n",
      "Min loss: 0.0046933237463235855\n",
      "Mean loss: 0.013502445770427585\n",
      "Std loss: 0.009936664121550465\n",
      "Total Loss: 0.08101467462256551\n",
      "------------------------------------ epoch 7760 (46554 steps) ------------------------------------\n",
      "Max loss: 0.03903292119503021\n",
      "Min loss: 0.0049247052520513535\n",
      "Mean loss: 0.014222603291273117\n",
      "Std loss: 0.011482037979837306\n",
      "Total Loss: 0.0853356197476387\n",
      "------------------------------------ epoch 7761 (46560 steps) ------------------------------------\n",
      "Max loss: 0.01697409898042679\n",
      "Min loss: 0.00610993430018425\n",
      "Mean loss: 0.00942077930085361\n",
      "Std loss: 0.003969062277998067\n",
      "Total Loss: 0.05652467580512166\n",
      "------------------------------------ epoch 7762 (46566 steps) ------------------------------------\n",
      "Max loss: 0.015583652071654797\n",
      "Min loss: 0.004862401634454727\n",
      "Mean loss: 0.009173621501152715\n",
      "Std loss: 0.0037627051295553464\n",
      "Total Loss: 0.055041729006916285\n",
      "------------------------------------ epoch 7763 (46572 steps) ------------------------------------\n",
      "Max loss: 0.05444180220365524\n",
      "Min loss: 0.004880298860371113\n",
      "Mean loss: 0.021149329220255215\n",
      "Std loss: 0.0181425729742051\n",
      "Total Loss: 0.1268959753215313\n",
      "------------------------------------ epoch 7764 (46578 steps) ------------------------------------\n",
      "Max loss: 0.015438256785273552\n",
      "Min loss: 0.0068252356722950935\n",
      "Mean loss: 0.011290634516626596\n",
      "Std loss: 0.0030173009818248867\n",
      "Total Loss: 0.06774380709975958\n",
      "------------------------------------ epoch 7765 (46584 steps) ------------------------------------\n",
      "Max loss: 0.025764649733901024\n",
      "Min loss: 0.005049857776612043\n",
      "Mean loss: 0.01260905844780306\n",
      "Std loss: 0.00738887409338203\n",
      "Total Loss: 0.07565435068681836\n",
      "------------------------------------ epoch 7766 (46590 steps) ------------------------------------\n",
      "Max loss: 0.03970605134963989\n",
      "Min loss: 0.006832027342170477\n",
      "Mean loss: 0.01422873690413932\n",
      "Std loss: 0.011524572497491236\n",
      "Total Loss: 0.08537242142483592\n",
      "------------------------------------ epoch 7767 (46596 steps) ------------------------------------\n",
      "Max loss: 0.029410719871520996\n",
      "Min loss: 0.005437835585325956\n",
      "Mean loss: 0.01219660866384705\n",
      "Std loss: 0.008244232710411933\n",
      "Total Loss: 0.0731796519830823\n",
      "------------------------------------ epoch 7768 (46602 steps) ------------------------------------\n",
      "Max loss: 0.01633201539516449\n",
      "Min loss: 0.006240442395210266\n",
      "Mean loss: 0.01070593964929382\n",
      "Std loss: 0.0033821656339417944\n",
      "Total Loss: 0.06423563789576292\n",
      "------------------------------------ epoch 7769 (46608 steps) ------------------------------------\n",
      "Max loss: 0.017424747347831726\n",
      "Min loss: 0.0049753738567233086\n",
      "Mean loss: 0.011560420195261637\n",
      "Std loss: 0.005407393527903085\n",
      "Total Loss: 0.06936252117156982\n",
      "------------------------------------ epoch 7770 (46614 steps) ------------------------------------\n",
      "Max loss: 0.020293310284614563\n",
      "Min loss: 0.005091162398457527\n",
      "Mean loss: 0.008870851714164019\n",
      "Std loss: 0.005246123581484374\n",
      "Total Loss: 0.05322511028498411\n",
      "------------------------------------ epoch 7771 (46620 steps) ------------------------------------\n",
      "Max loss: 0.015979312360286713\n",
      "Min loss: 0.007248210720717907\n",
      "Mean loss: 0.010280426746855179\n",
      "Std loss: 0.002814364597409898\n",
      "Total Loss: 0.06168256048113108\n",
      "------------------------------------ epoch 7772 (46626 steps) ------------------------------------\n",
      "Max loss: 0.010822176001966\n",
      "Min loss: 0.004251742735505104\n",
      "Mean loss: 0.007211335701867938\n",
      "Std loss: 0.002002229335697728\n",
      "Total Loss: 0.04326801421120763\n",
      "------------------------------------ epoch 7773 (46632 steps) ------------------------------------\n",
      "Max loss: 0.05664079636335373\n",
      "Min loss: 0.004133324138820171\n",
      "Mean loss: 0.013975028259058794\n",
      "Std loss: 0.019107265129133025\n",
      "Total Loss: 0.08385016955435276\n",
      "------------------------------------ epoch 7774 (46638 steps) ------------------------------------\n",
      "Max loss: 0.019180871546268463\n",
      "Min loss: 0.0048555731773376465\n",
      "Mean loss: 0.01161855785176158\n",
      "Std loss: 0.006072837414094905\n",
      "Total Loss: 0.06971134711056948\n",
      "------------------------------------ epoch 7775 (46644 steps) ------------------------------------\n",
      "Max loss: 0.021892188116908073\n",
      "Min loss: 0.0045749954879283905\n",
      "Mean loss: 0.012388680906345447\n",
      "Std loss: 0.006789028997343387\n",
      "Total Loss: 0.07433208543807268\n",
      "------------------------------------ epoch 7776 (46650 steps) ------------------------------------\n",
      "Max loss: 0.024119935929775238\n",
      "Min loss: 0.0058142878115177155\n",
      "Mean loss: 0.010934100641558567\n",
      "Std loss: 0.006093959446430277\n",
      "Total Loss: 0.0656046038493514\n",
      "------------------------------------ epoch 7777 (46656 steps) ------------------------------------\n",
      "Max loss: 0.011036279611289501\n",
      "Min loss: 0.005574938841164112\n",
      "Mean loss: 0.00786799603762726\n",
      "Std loss: 0.0016883961729336479\n",
      "Total Loss: 0.04720797622576356\n",
      "------------------------------------ epoch 7778 (46662 steps) ------------------------------------\n",
      "Max loss: 0.024194341152906418\n",
      "Min loss: 0.0044290535151958466\n",
      "Mean loss: 0.010218994847188393\n",
      "Std loss: 0.006460898460967266\n",
      "Total Loss: 0.06131396908313036\n",
      "------------------------------------ epoch 7779 (46668 steps) ------------------------------------\n",
      "Max loss: 0.016238749027252197\n",
      "Min loss: 0.0047083888202905655\n",
      "Mean loss: 0.008809786522760987\n",
      "Std loss: 0.004104700238098397\n",
      "Total Loss: 0.052858719136565924\n",
      "------------------------------------ epoch 7780 (46674 steps) ------------------------------------\n",
      "Max loss: 0.02283906564116478\n",
      "Min loss: 0.006514041684567928\n",
      "Mean loss: 0.013050040074934563\n",
      "Std loss: 0.005237359375668803\n",
      "Total Loss: 0.07830024044960737\n",
      "------------------------------------ epoch 7781 (46680 steps) ------------------------------------\n",
      "Max loss: 0.017415950074791908\n",
      "Min loss: 0.005142091773450375\n",
      "Mean loss: 0.008241054291526476\n",
      "Std loss: 0.004301227551241927\n",
      "Total Loss: 0.04944632574915886\n",
      "------------------------------------ epoch 7782 (46686 steps) ------------------------------------\n",
      "Max loss: 0.01268797554075718\n",
      "Min loss: 0.003998701926320791\n",
      "Mean loss: 0.006868659285828471\n",
      "Std loss: 0.0027851231080575586\n",
      "Total Loss: 0.04121195571497083\n",
      "------------------------------------ epoch 7783 (46692 steps) ------------------------------------\n",
      "Max loss: 0.025150485336780548\n",
      "Min loss: 0.004183873068541288\n",
      "Mean loss: 0.009267420507967472\n",
      "Std loss: 0.007197082889513353\n",
      "Total Loss: 0.05560452304780483\n",
      "------------------------------------ epoch 7784 (46698 steps) ------------------------------------\n",
      "Max loss: 0.02951066941022873\n",
      "Min loss: 0.005237925797700882\n",
      "Mean loss: 0.010130838180581728\n",
      "Std loss: 0.00871556437112662\n",
      "Total Loss: 0.06078502908349037\n",
      "------------------------------------ epoch 7785 (46704 steps) ------------------------------------\n",
      "Max loss: 0.016160806640982628\n",
      "Min loss: 0.005865695886313915\n",
      "Mean loss: 0.009732981600488225\n",
      "Std loss: 0.0037509587981455154\n",
      "Total Loss: 0.058397889602929354\n",
      "------------------------------------ epoch 7786 (46710 steps) ------------------------------------\n",
      "Max loss: 0.015256291255354881\n",
      "Min loss: 0.005198551341891289\n",
      "Mean loss: 0.008788099512457848\n",
      "Std loss: 0.003129456425151342\n",
      "Total Loss: 0.052728597074747086\n",
      "------------------------------------ epoch 7787 (46716 steps) ------------------------------------\n",
      "Max loss: 0.02815563790500164\n",
      "Min loss: 0.0038848768454045057\n",
      "Mean loss: 0.012708490753235916\n",
      "Std loss: 0.009068929634133404\n",
      "Total Loss: 0.0762509445194155\n",
      "------------------------------------ epoch 7788 (46722 steps) ------------------------------------\n",
      "Max loss: 0.023902809247374535\n",
      "Min loss: 0.004692726768553257\n",
      "Mean loss: 0.010261699091643095\n",
      "Std loss: 0.0068804015298604675\n",
      "Total Loss: 0.06157019454985857\n",
      "------------------------------------ epoch 7789 (46728 steps) ------------------------------------\n",
      "Max loss: 0.019085276871919632\n",
      "Min loss: 0.004205846227705479\n",
      "Mean loss: 0.010090328752994537\n",
      "Std loss: 0.004572340191825096\n",
      "Total Loss: 0.060541972517967224\n",
      "------------------------------------ epoch 7790 (46734 steps) ------------------------------------\n",
      "Max loss: 0.010707734152674675\n",
      "Min loss: 0.004386259708553553\n",
      "Mean loss: 0.00744988505418102\n",
      "Std loss: 0.0022178966074278533\n",
      "Total Loss: 0.04469931032508612\n",
      "------------------------------------ epoch 7791 (46740 steps) ------------------------------------\n",
      "Max loss: 0.006970006041228771\n",
      "Min loss: 0.004191239830106497\n",
      "Mean loss: 0.005739398843919237\n",
      "Std loss: 0.0008421646557450249\n",
      "Total Loss: 0.034436393063515425\n",
      "------------------------------------ epoch 7792 (46746 steps) ------------------------------------\n",
      "Max loss: 0.014569847844541073\n",
      "Min loss: 0.0035320166498422623\n",
      "Mean loss: 0.007854239161436757\n",
      "Std loss: 0.0044250708078836095\n",
      "Total Loss: 0.04712543496862054\n",
      "------------------------------------ epoch 7793 (46752 steps) ------------------------------------\n",
      "Max loss: 0.011047723703086376\n",
      "Min loss: 0.004042495042085648\n",
      "Mean loss: 0.0066623032713929815\n",
      "Std loss: 0.0022104481032991876\n",
      "Total Loss: 0.03997381962835789\n",
      "------------------------------------ epoch 7794 (46758 steps) ------------------------------------\n",
      "Max loss: 0.011035250499844551\n",
      "Min loss: 0.005694281309843063\n",
      "Mean loss: 0.008158025021354357\n",
      "Std loss: 0.0018253044869630624\n",
      "Total Loss: 0.048948150128126144\n",
      "------------------------------------ epoch 7795 (46764 steps) ------------------------------------\n",
      "Max loss: 0.014162289910018444\n",
      "Min loss: 0.004364629276096821\n",
      "Mean loss: 0.007152815659840901\n",
      "Std loss: 0.0035277556883262593\n",
      "Total Loss: 0.04291689395904541\n",
      "------------------------------------ epoch 7796 (46770 steps) ------------------------------------\n",
      "Max loss: 0.016772225499153137\n",
      "Min loss: 0.004345510620623827\n",
      "Mean loss: 0.007904005469754338\n",
      "Std loss: 0.004273276787474138\n",
      "Total Loss: 0.04742403281852603\n",
      "------------------------------------ epoch 7797 (46776 steps) ------------------------------------\n",
      "Max loss: 0.013758191838860512\n",
      "Min loss: 0.005297516472637653\n",
      "Mean loss: 0.008919955852131048\n",
      "Std loss: 0.0032534624814567255\n",
      "Total Loss: 0.05351973511278629\n",
      "------------------------------------ epoch 7798 (46782 steps) ------------------------------------\n",
      "Max loss: 0.012239180505275726\n",
      "Min loss: 0.004175879061222076\n",
      "Mean loss: 0.0072612374400099116\n",
      "Std loss: 0.002819725101247381\n",
      "Total Loss: 0.04356742464005947\n",
      "------------------------------------ epoch 7799 (46788 steps) ------------------------------------\n",
      "Max loss: 0.015912149101495743\n",
      "Min loss: 0.004465506877750158\n",
      "Mean loss: 0.007557860653226574\n",
      "Std loss: 0.003900847357575746\n",
      "Total Loss: 0.045347163919359446\n",
      "------------------------------------ epoch 7800 (46794 steps) ------------------------------------\n",
      "Max loss: 0.022126352414488792\n",
      "Min loss: 0.003814001102000475\n",
      "Mean loss: 0.008372990026449164\n",
      "Std loss: 0.006298573133156958\n",
      "Total Loss: 0.05023794015869498\n",
      "------------------------------------ epoch 7801 (46800 steps) ------------------------------------\n",
      "Max loss: 0.027178868651390076\n",
      "Min loss: 0.004445833154022694\n",
      "Mean loss: 0.010359732046102485\n",
      "Std loss: 0.007689564746633845\n",
      "Total Loss: 0.062158392276614904\n",
      "saved model at ./weights/model_7801.pth\n",
      "------------------------------------ epoch 7802 (46806 steps) ------------------------------------\n",
      "Max loss: 0.016053631901741028\n",
      "Min loss: 0.004109212663024664\n",
      "Mean loss: 0.007855283096432686\n",
      "Std loss: 0.004626608642039653\n",
      "Total Loss: 0.047131698578596115\n",
      "------------------------------------ epoch 7803 (46812 steps) ------------------------------------\n",
      "Max loss: 0.02280634641647339\n",
      "Min loss: 0.0053739300929009914\n",
      "Mean loss: 0.011113885402058562\n",
      "Std loss: 0.00563512139661216\n",
      "Total Loss: 0.06668331241235137\n",
      "------------------------------------ epoch 7804 (46818 steps) ------------------------------------\n",
      "Max loss: 0.022032005712389946\n",
      "Min loss: 0.007441113702952862\n",
      "Mean loss: 0.010465553651253382\n",
      "Std loss: 0.005195588503018732\n",
      "Total Loss: 0.0627933219075203\n",
      "------------------------------------ epoch 7805 (46824 steps) ------------------------------------\n",
      "Max loss: 0.021542813628911972\n",
      "Min loss: 0.0036795507185161114\n",
      "Mean loss: 0.010491983344157537\n",
      "Std loss: 0.00642355111449162\n",
      "Total Loss: 0.06295190006494522\n",
      "------------------------------------ epoch 7806 (46830 steps) ------------------------------------\n",
      "Max loss: 0.009997199289500713\n",
      "Min loss: 0.004750804975628853\n",
      "Mean loss: 0.007294498694439729\n",
      "Std loss: 0.0018202222177949134\n",
      "Total Loss: 0.043766992166638374\n",
      "------------------------------------ epoch 7807 (46836 steps) ------------------------------------\n",
      "Max loss: 0.03842926770448685\n",
      "Min loss: 0.004699655342847109\n",
      "Mean loss: 0.012191293140252432\n",
      "Std loss: 0.011905097993283818\n",
      "Total Loss: 0.07314775884151459\n",
      "------------------------------------ epoch 7808 (46842 steps) ------------------------------------\n",
      "Max loss: 0.018760744482278824\n",
      "Min loss: 0.004903439432382584\n",
      "Mean loss: 0.009778562001883984\n",
      "Std loss: 0.00486538241452389\n",
      "Total Loss: 0.0586713720113039\n",
      "------------------------------------ epoch 7809 (46848 steps) ------------------------------------\n",
      "Max loss: 0.010382883250713348\n",
      "Min loss: 0.006325569003820419\n",
      "Mean loss: 0.00796563100690643\n",
      "Std loss: 0.001442290076145236\n",
      "Total Loss: 0.04779378604143858\n",
      "------------------------------------ epoch 7810 (46854 steps) ------------------------------------\n",
      "Max loss: 0.014720837585628033\n",
      "Min loss: 0.0045604524202644825\n",
      "Mean loss: 0.00853890886840721\n",
      "Std loss: 0.0036706245358399375\n",
      "Total Loss: 0.05123345321044326\n",
      "------------------------------------ epoch 7811 (46860 steps) ------------------------------------\n",
      "Max loss: 0.02208799682557583\n",
      "Min loss: 0.004305886570364237\n",
      "Mean loss: 0.012565839570015669\n",
      "Std loss: 0.007616183797629349\n",
      "Total Loss: 0.07539503742009401\n",
      "------------------------------------ epoch 7812 (46866 steps) ------------------------------------\n",
      "Max loss: 0.013592151924967766\n",
      "Min loss: 0.0050690253265202045\n",
      "Mean loss: 0.00834675474713246\n",
      "Std loss: 0.0028939057166220204\n",
      "Total Loss: 0.05008052848279476\n",
      "------------------------------------ epoch 7813 (46872 steps) ------------------------------------\n",
      "Max loss: 0.016676709055900574\n",
      "Min loss: 0.005571320187300444\n",
      "Mean loss: 0.009798110540335378\n",
      "Std loss: 0.003406183066983365\n",
      "Total Loss: 0.05878866324201226\n",
      "------------------------------------ epoch 7814 (46878 steps) ------------------------------------\n",
      "Max loss: 0.01272908877581358\n",
      "Min loss: 0.005203964654356241\n",
      "Mean loss: 0.007262237059573333\n",
      "Std loss: 0.0025603114582118075\n",
      "Total Loss: 0.043573422357439995\n",
      "------------------------------------ epoch 7815 (46884 steps) ------------------------------------\n",
      "Max loss: 0.012457691133022308\n",
      "Min loss: 0.005929077975451946\n",
      "Mean loss: 0.00874186617632707\n",
      "Std loss: 0.002174038123170225\n",
      "Total Loss: 0.05245119705796242\n",
      "------------------------------------ epoch 7816 (46890 steps) ------------------------------------\n",
      "Max loss: 0.02502175234258175\n",
      "Min loss: 0.0036458063405007124\n",
      "Mean loss: 0.008309214880379537\n",
      "Std loss: 0.0075608462481966425\n",
      "Total Loss: 0.049855289282277226\n",
      "------------------------------------ epoch 7817 (46896 steps) ------------------------------------\n",
      "Max loss: 0.007832635194063187\n",
      "Min loss: 0.005309673957526684\n",
      "Mean loss: 0.006790117283041279\n",
      "Std loss: 0.0007742012933009581\n",
      "Total Loss: 0.04074070369824767\n",
      "------------------------------------ epoch 7818 (46902 steps) ------------------------------------\n",
      "Max loss: 0.008590142242610455\n",
      "Min loss: 0.004566340707242489\n",
      "Mean loss: 0.00591831118799746\n",
      "Std loss: 0.0015100580432447646\n",
      "Total Loss: 0.03550986712798476\n",
      "------------------------------------ epoch 7819 (46908 steps) ------------------------------------\n",
      "Max loss: 0.02812546119093895\n",
      "Min loss: 0.00411176634952426\n",
      "Mean loss: 0.009545124601572752\n",
      "Std loss: 0.008402479651611085\n",
      "Total Loss: 0.05727074760943651\n",
      "------------------------------------ epoch 7820 (46914 steps) ------------------------------------\n",
      "Max loss: 0.011345570906996727\n",
      "Min loss: 0.0037372137885540724\n",
      "Mean loss: 0.007647892110981047\n",
      "Std loss: 0.0026986044384439863\n",
      "Total Loss: 0.04588735266588628\n",
      "------------------------------------ epoch 7821 (46920 steps) ------------------------------------\n",
      "Max loss: 0.02304910682141781\n",
      "Min loss: 0.004521570634096861\n",
      "Mean loss: 0.009677341052641472\n",
      "Std loss: 0.006207727979978438\n",
      "Total Loss: 0.05806404631584883\n",
      "------------------------------------ epoch 7822 (46926 steps) ------------------------------------\n",
      "Max loss: 0.014947003684937954\n",
      "Min loss: 0.0053268722258508205\n",
      "Mean loss: 0.01009322376921773\n",
      "Std loss: 0.003881464805417214\n",
      "Total Loss: 0.06055934261530638\n",
      "------------------------------------ epoch 7823 (46932 steps) ------------------------------------\n",
      "Max loss: 0.018021825700998306\n",
      "Min loss: 0.0048662275075912476\n",
      "Mean loss: 0.008832888444885612\n",
      "Std loss: 0.004647477030419984\n",
      "Total Loss: 0.05299733066931367\n",
      "------------------------------------ epoch 7824 (46938 steps) ------------------------------------\n",
      "Max loss: 0.02170301042497158\n",
      "Min loss: 0.004490571096539497\n",
      "Mean loss: 0.009672865737229586\n",
      "Std loss: 0.005853646451080186\n",
      "Total Loss: 0.058037194423377514\n",
      "------------------------------------ epoch 7825 (46944 steps) ------------------------------------\n",
      "Max loss: 0.012004557996988297\n",
      "Min loss: 0.004749549552798271\n",
      "Mean loss: 0.008143394875029722\n",
      "Std loss: 0.0025866245532145194\n",
      "Total Loss: 0.04886036925017834\n",
      "------------------------------------ epoch 7826 (46950 steps) ------------------------------------\n",
      "Max loss: 0.01715785264968872\n",
      "Min loss: 0.004784644581377506\n",
      "Mean loss: 0.008077926390493909\n",
      "Std loss: 0.004191644201320041\n",
      "Total Loss: 0.04846755834296346\n",
      "------------------------------------ epoch 7827 (46956 steps) ------------------------------------\n",
      "Max loss: 0.02472861111164093\n",
      "Min loss: 0.004906178452074528\n",
      "Mean loss: 0.011624771325538555\n",
      "Std loss: 0.007277377982510569\n",
      "Total Loss: 0.06974862795323133\n",
      "------------------------------------ epoch 7828 (46962 steps) ------------------------------------\n",
      "Max loss: 0.010293874889612198\n",
      "Min loss: 0.004861478693783283\n",
      "Mean loss: 0.006589421071112156\n",
      "Std loss: 0.002017064440772441\n",
      "Total Loss: 0.039536526426672935\n",
      "------------------------------------ epoch 7829 (46968 steps) ------------------------------------\n",
      "Max loss: 0.03460460901260376\n",
      "Min loss: 0.00449743028730154\n",
      "Mean loss: 0.011915568184728423\n",
      "Std loss: 0.010341318896271523\n",
      "Total Loss: 0.07149340910837054\n",
      "------------------------------------ epoch 7830 (46974 steps) ------------------------------------\n",
      "Max loss: 0.028987359255552292\n",
      "Min loss: 0.006247177254408598\n",
      "Mean loss: 0.0161812217750897\n",
      "Std loss: 0.008141959449481852\n",
      "Total Loss: 0.0970873306505382\n",
      "------------------------------------ epoch 7831 (46980 steps) ------------------------------------\n",
      "Max loss: 0.013306958600878716\n",
      "Min loss: 0.007423648610711098\n",
      "Mean loss: 0.010515595165391764\n",
      "Std loss: 0.0017774328834899969\n",
      "Total Loss: 0.06309357099235058\n",
      "------------------------------------ epoch 7832 (46986 steps) ------------------------------------\n",
      "Max loss: 0.016941435635089874\n",
      "Min loss: 0.005512986332178116\n",
      "Mean loss: 0.009255284365887443\n",
      "Std loss: 0.003761715581710298\n",
      "Total Loss: 0.05553170619532466\n",
      "------------------------------------ epoch 7833 (46992 steps) ------------------------------------\n",
      "Max loss: 0.018018562346696854\n",
      "Min loss: 0.005810992792248726\n",
      "Mean loss: 0.010454154418160519\n",
      "Std loss: 0.0040006833760764604\n",
      "Total Loss: 0.06272492650896311\n",
      "------------------------------------ epoch 7834 (46998 steps) ------------------------------------\n",
      "Max loss: 0.015359757468104362\n",
      "Min loss: 0.004849856719374657\n",
      "Mean loss: 0.007906398891160885\n",
      "Std loss: 0.0036993877876981616\n",
      "Total Loss: 0.04743839334696531\n",
      "------------------------------------ epoch 7835 (47004 steps) ------------------------------------\n",
      "Max loss: 0.012302828952670097\n",
      "Min loss: 0.004880126100033522\n",
      "Mean loss: 0.009328278402487436\n",
      "Std loss: 0.002669585667979779\n",
      "Total Loss: 0.05596967041492462\n",
      "------------------------------------ epoch 7836 (47010 steps) ------------------------------------\n",
      "Max loss: 0.010883248411118984\n",
      "Min loss: 0.004375707358121872\n",
      "Mean loss: 0.007159738801419735\n",
      "Std loss: 0.0021148594096704228\n",
      "Total Loss: 0.04295843280851841\n",
      "------------------------------------ epoch 7837 (47016 steps) ------------------------------------\n",
      "Max loss: 0.00904766283929348\n",
      "Min loss: 0.00424539390951395\n",
      "Mean loss: 0.0058660972863435745\n",
      "Std loss: 0.0016283277488761837\n",
      "Total Loss: 0.03519658371806145\n",
      "------------------------------------ epoch 7838 (47022 steps) ------------------------------------\n",
      "Max loss: 0.012384315952658653\n",
      "Min loss: 0.0034561199136078358\n",
      "Mean loss: 0.007490683114156127\n",
      "Std loss: 0.0028916467254582333\n",
      "Total Loss: 0.04494409868493676\n",
      "------------------------------------ epoch 7839 (47028 steps) ------------------------------------\n",
      "Max loss: 0.017028208822011948\n",
      "Min loss: 0.004962136968970299\n",
      "Mean loss: 0.009542359970510006\n",
      "Std loss: 0.0045370115762887325\n",
      "Total Loss: 0.057254159823060036\n",
      "------------------------------------ epoch 7840 (47034 steps) ------------------------------------\n",
      "Max loss: 0.020614415407180786\n",
      "Min loss: 0.004377232398837805\n",
      "Mean loss: 0.009120781905949116\n",
      "Std loss: 0.005518141631364287\n",
      "Total Loss: 0.054724691435694695\n",
      "------------------------------------ epoch 7841 (47040 steps) ------------------------------------\n",
      "Max loss: 0.010193461552262306\n",
      "Min loss: 0.004962587729096413\n",
      "Mean loss: 0.007241763019313415\n",
      "Std loss: 0.0020954261783451334\n",
      "Total Loss: 0.04345057811588049\n",
      "------------------------------------ epoch 7842 (47046 steps) ------------------------------------\n",
      "Max loss: 0.022216102108359337\n",
      "Min loss: 0.005154055077582598\n",
      "Mean loss: 0.010614115511998534\n",
      "Std loss: 0.005468236688866634\n",
      "Total Loss: 0.0636846930719912\n",
      "------------------------------------ epoch 7843 (47052 steps) ------------------------------------\n",
      "Max loss: 0.014333642087876797\n",
      "Min loss: 0.006116549484431744\n",
      "Mean loss: 0.009261213010177016\n",
      "Std loss: 0.00292549407748146\n",
      "Total Loss: 0.0555672780610621\n",
      "------------------------------------ epoch 7844 (47058 steps) ------------------------------------\n",
      "Max loss: 0.02362826094031334\n",
      "Min loss: 0.005267023574560881\n",
      "Mean loss: 0.013050248147919774\n",
      "Std loss: 0.005463756102717297\n",
      "Total Loss: 0.07830148888751864\n",
      "------------------------------------ epoch 7845 (47064 steps) ------------------------------------\n",
      "Max loss: 0.012403110042214394\n",
      "Min loss: 0.004175457172095776\n",
      "Mean loss: 0.007049673702567816\n",
      "Std loss: 0.002864662192180455\n",
      "Total Loss: 0.042298042215406895\n",
      "------------------------------------ epoch 7846 (47070 steps) ------------------------------------\n",
      "Max loss: 0.013999124988913536\n",
      "Min loss: 0.004640655592083931\n",
      "Mean loss: 0.007861084615190824\n",
      "Std loss: 0.0033404593903265042\n",
      "Total Loss: 0.04716650769114494\n",
      "------------------------------------ epoch 7847 (47076 steps) ------------------------------------\n",
      "Max loss: 0.011998854577541351\n",
      "Min loss: 0.0039285654202103615\n",
      "Mean loss: 0.006729073589667678\n",
      "Std loss: 0.0027074349505659496\n",
      "Total Loss: 0.04037444153800607\n",
      "------------------------------------ epoch 7848 (47082 steps) ------------------------------------\n",
      "Max loss: 0.016737952828407288\n",
      "Min loss: 0.0035185529850423336\n",
      "Mean loss: 0.009117022855207324\n",
      "Std loss: 0.0052613940722747755\n",
      "Total Loss: 0.054702137131243944\n",
      "------------------------------------ epoch 7849 (47088 steps) ------------------------------------\n",
      "Max loss: 0.014693754725158215\n",
      "Min loss: 0.0048721060156822205\n",
      "Mean loss: 0.009893145567427078\n",
      "Std loss: 0.0029042650715148037\n",
      "Total Loss: 0.05935887340456247\n",
      "------------------------------------ epoch 7850 (47094 steps) ------------------------------------\n",
      "Max loss: 0.0157974511384964\n",
      "Min loss: 0.004661282524466515\n",
      "Mean loss: 0.00843901427773138\n",
      "Std loss: 0.00394626444699841\n",
      "Total Loss: 0.05063408566638827\n",
      "------------------------------------ epoch 7851 (47100 steps) ------------------------------------\n",
      "Max loss: 0.013057654723525047\n",
      "Min loss: 0.0043565984815359116\n",
      "Mean loss: 0.008132126337538162\n",
      "Std loss: 0.0034060494430624626\n",
      "Total Loss: 0.04879275802522898\n",
      "------------------------------------ epoch 7852 (47106 steps) ------------------------------------\n",
      "Max loss: 0.011046215891838074\n",
      "Min loss: 0.004881690256297588\n",
      "Mean loss: 0.006903704100598891\n",
      "Std loss: 0.0019962293751937155\n",
      "Total Loss: 0.04142222460359335\n",
      "------------------------------------ epoch 7853 (47112 steps) ------------------------------------\n",
      "Max loss: 0.023918379098176956\n",
      "Min loss: 0.004037758335471153\n",
      "Mean loss: 0.010025600281854471\n",
      "Std loss: 0.006576812497879267\n",
      "Total Loss: 0.06015360169112682\n",
      "------------------------------------ epoch 7854 (47118 steps) ------------------------------------\n",
      "Max loss: 0.040823161602020264\n",
      "Min loss: 0.004632638767361641\n",
      "Mean loss: 0.013747392998387417\n",
      "Std loss: 0.012463720669062364\n",
      "Total Loss: 0.0824843579903245\n",
      "------------------------------------ epoch 7855 (47124 steps) ------------------------------------\n",
      "Max loss: 0.012870884500443935\n",
      "Min loss: 0.0047502340748906136\n",
      "Mean loss: 0.007522811957945426\n",
      "Std loss: 0.002746806999841356\n",
      "Total Loss: 0.04513687174767256\n",
      "------------------------------------ epoch 7856 (47130 steps) ------------------------------------\n",
      "Max loss: 0.015860535204410553\n",
      "Min loss: 0.006708370056003332\n",
      "Mean loss: 0.011169939224297801\n",
      "Std loss: 0.0032345950387567056\n",
      "Total Loss: 0.06701963534578681\n",
      "------------------------------------ epoch 7857 (47136 steps) ------------------------------------\n",
      "Max loss: 0.01692410558462143\n",
      "Min loss: 0.004499200731515884\n",
      "Mean loss: 0.007264373358339071\n",
      "Std loss: 0.004418878510482866\n",
      "Total Loss: 0.04358624015003443\n",
      "------------------------------------ epoch 7858 (47142 steps) ------------------------------------\n",
      "Max loss: 0.015963081270456314\n",
      "Min loss: 0.003956451080739498\n",
      "Mean loss: 0.007310079177841544\n",
      "Std loss: 0.004051363450300277\n",
      "Total Loss: 0.043860475067049265\n",
      "------------------------------------ epoch 7859 (47148 steps) ------------------------------------\n",
      "Max loss: 0.008221755735576153\n",
      "Min loss: 0.005983197595924139\n",
      "Mean loss: 0.00691372932245334\n",
      "Std loss: 0.0007855062128266207\n",
      "Total Loss: 0.04148237593472004\n",
      "------------------------------------ epoch 7860 (47154 steps) ------------------------------------\n",
      "Max loss: 0.010372985154390335\n",
      "Min loss: 0.00402779970318079\n",
      "Mean loss: 0.006264458876103163\n",
      "Std loss: 0.002124261543832309\n",
      "Total Loss: 0.03758675325661898\n",
      "------------------------------------ epoch 7861 (47160 steps) ------------------------------------\n",
      "Max loss: 0.0173424631357193\n",
      "Min loss: 0.006240222603082657\n",
      "Mean loss: 0.010215333430096507\n",
      "Std loss: 0.004241808399412799\n",
      "Total Loss: 0.06129200058057904\n",
      "------------------------------------ epoch 7862 (47166 steps) ------------------------------------\n",
      "Max loss: 0.05724705010652542\n",
      "Min loss: 0.005182682536542416\n",
      "Mean loss: 0.020094261039048433\n",
      "Std loss: 0.017887253784597087\n",
      "Total Loss: 0.1205655662342906\n",
      "------------------------------------ epoch 7863 (47172 steps) ------------------------------------\n",
      "Max loss: 0.03413799777626991\n",
      "Min loss: 0.006700337864458561\n",
      "Mean loss: 0.019797164481133223\n",
      "Std loss: 0.010481512297387093\n",
      "Total Loss: 0.11878298688679934\n",
      "------------------------------------ epoch 7864 (47178 steps) ------------------------------------\n",
      "Max loss: 0.018386051058769226\n",
      "Min loss: 0.009088565595448017\n",
      "Mean loss: 0.012760892355193695\n",
      "Std loss: 0.0028606728189978755\n",
      "Total Loss: 0.07656535413116217\n",
      "------------------------------------ epoch 7865 (47184 steps) ------------------------------------\n",
      "Max loss: 0.027294009923934937\n",
      "Min loss: 0.0074712373316287994\n",
      "Mean loss: 0.014730477860818306\n",
      "Std loss: 0.006504606079329038\n",
      "Total Loss: 0.08838286716490984\n",
      "------------------------------------ epoch 7866 (47190 steps) ------------------------------------\n",
      "Max loss: 0.02413281612098217\n",
      "Min loss: 0.005350037477910519\n",
      "Mean loss: 0.013021313662951192\n",
      "Std loss: 0.006923376830978135\n",
      "Total Loss: 0.07812788197770715\n",
      "------------------------------------ epoch 7867 (47196 steps) ------------------------------------\n",
      "Max loss: 0.014872054569423199\n",
      "Min loss: 0.00454840250313282\n",
      "Mean loss: 0.008897840588664016\n",
      "Std loss: 0.004047518954326544\n",
      "Total Loss: 0.05338704353198409\n",
      "------------------------------------ epoch 7868 (47202 steps) ------------------------------------\n",
      "Max loss: 0.043519165366888046\n",
      "Min loss: 0.00804750807583332\n",
      "Mean loss: 0.016850014682859182\n",
      "Std loss: 0.012103315316037718\n",
      "Total Loss: 0.1011000880971551\n",
      "------------------------------------ epoch 7869 (47208 steps) ------------------------------------\n",
      "Max loss: 0.015683364123106003\n",
      "Min loss: 0.005572894122451544\n",
      "Mean loss: 0.009018408367410302\n",
      "Std loss: 0.003192900309530303\n",
      "Total Loss: 0.05411045020446181\n",
      "------------------------------------ epoch 7870 (47214 steps) ------------------------------------\n",
      "Max loss: 0.022424522787332535\n",
      "Min loss: 0.00593480234965682\n",
      "Mean loss: 0.012028027015427748\n",
      "Std loss: 0.006916906836292292\n",
      "Total Loss: 0.07216816209256649\n",
      "------------------------------------ epoch 7871 (47220 steps) ------------------------------------\n",
      "Max loss: 0.015397006645798683\n",
      "Min loss: 0.006109254434704781\n",
      "Mean loss: 0.010745485158016285\n",
      "Std loss: 0.003544069765725588\n",
      "Total Loss: 0.0644729109480977\n",
      "------------------------------------ epoch 7872 (47226 steps) ------------------------------------\n",
      "Max loss: 0.020569687709212303\n",
      "Min loss: 0.0066927592270076275\n",
      "Mean loss: 0.010544762713834643\n",
      "Std loss: 0.004721463614421086\n",
      "Total Loss: 0.06326857628300786\n",
      "------------------------------------ epoch 7873 (47232 steps) ------------------------------------\n",
      "Max loss: 0.009213199838995934\n",
      "Min loss: 0.004740593954920769\n",
      "Mean loss: 0.006834183897202213\n",
      "Std loss: 0.0014169951343985041\n",
      "Total Loss: 0.04100510338321328\n",
      "------------------------------------ epoch 7874 (47238 steps) ------------------------------------\n",
      "Max loss: 0.03629700839519501\n",
      "Min loss: 0.008112326264381409\n",
      "Mean loss: 0.015510113599399725\n",
      "Std loss: 0.00961666727878052\n",
      "Total Loss: 0.09306068159639835\n",
      "------------------------------------ epoch 7875 (47244 steps) ------------------------------------\n",
      "Max loss: 0.03712617978453636\n",
      "Min loss: 0.004865442402660847\n",
      "Mean loss: 0.01670518595104416\n",
      "Std loss: 0.010240151497779662\n",
      "Total Loss: 0.10023111570626497\n",
      "------------------------------------ epoch 7876 (47250 steps) ------------------------------------\n",
      "Max loss: 0.015877842903137207\n",
      "Min loss: 0.005202178843319416\n",
      "Mean loss: 0.009930969293539723\n",
      "Std loss: 0.0038725423111393947\n",
      "Total Loss: 0.05958581576123834\n",
      "------------------------------------ epoch 7877 (47256 steps) ------------------------------------\n",
      "Max loss: 0.037092454731464386\n",
      "Min loss: 0.005334767978638411\n",
      "Mean loss: 0.016663649041826527\n",
      "Std loss: 0.010651808803476952\n",
      "Total Loss: 0.09998189425095916\n",
      "------------------------------------ epoch 7878 (47262 steps) ------------------------------------\n",
      "Max loss: 0.011347088031470776\n",
      "Min loss: 0.006008881609886885\n",
      "Mean loss: 0.007690513273701072\n",
      "Std loss: 0.0020060257089050703\n",
      "Total Loss: 0.04614307964220643\n",
      "------------------------------------ epoch 7879 (47268 steps) ------------------------------------\n",
      "Max loss: 0.017748596146702766\n",
      "Min loss: 0.007498617749661207\n",
      "Mean loss: 0.012445913394913077\n",
      "Std loss: 0.003959481859230367\n",
      "Total Loss: 0.07467548036947846\n",
      "------------------------------------ epoch 7880 (47274 steps) ------------------------------------\n",
      "Max loss: 0.024931220337748528\n",
      "Min loss: 0.005293907597661018\n",
      "Mean loss: 0.013149376958608627\n",
      "Std loss: 0.00637001426290819\n",
      "Total Loss: 0.07889626175165176\n",
      "------------------------------------ epoch 7881 (47280 steps) ------------------------------------\n",
      "Max loss: 0.01053608488291502\n",
      "Min loss: 0.005598712712526321\n",
      "Mean loss: 0.007720391498878598\n",
      "Std loss: 0.001801168595760086\n",
      "Total Loss: 0.04632234899327159\n",
      "------------------------------------ epoch 7882 (47286 steps) ------------------------------------\n",
      "Max loss: 0.010876890271902084\n",
      "Min loss: 0.005146560724824667\n",
      "Mean loss: 0.007197942507142822\n",
      "Std loss: 0.0018840415865846625\n",
      "Total Loss: 0.04318765504285693\n",
      "------------------------------------ epoch 7883 (47292 steps) ------------------------------------\n",
      "Max loss: 0.016287051141262054\n",
      "Min loss: 0.006528682075440884\n",
      "Mean loss: 0.010798847458014885\n",
      "Std loss: 0.0036245556542544187\n",
      "Total Loss: 0.06479308474808931\n",
      "------------------------------------ epoch 7884 (47298 steps) ------------------------------------\n",
      "Max loss: 0.00897145550698042\n",
      "Min loss: 0.004227164201438427\n",
      "Mean loss: 0.006555438662568728\n",
      "Std loss: 0.0013806232250893344\n",
      "Total Loss: 0.03933263197541237\n",
      "------------------------------------ epoch 7885 (47304 steps) ------------------------------------\n",
      "Max loss: 0.007162702735513449\n",
      "Min loss: 0.003418153151869774\n",
      "Mean loss: 0.005278112056354682\n",
      "Std loss: 0.0011851032367169755\n",
      "Total Loss: 0.03166867233812809\n",
      "------------------------------------ epoch 7886 (47310 steps) ------------------------------------\n",
      "Max loss: 0.01907331310212612\n",
      "Min loss: 0.0033059976994991302\n",
      "Mean loss: 0.007445086957886815\n",
      "Std loss: 0.005361594651301574\n",
      "Total Loss: 0.04467052174732089\n",
      "------------------------------------ epoch 7887 (47316 steps) ------------------------------------\n",
      "Max loss: 0.03303655982017517\n",
      "Min loss: 0.003547444473952055\n",
      "Mean loss: 0.00958503340370953\n",
      "Std loss: 0.010530391381800345\n",
      "Total Loss: 0.057510200422257185\n",
      "------------------------------------ epoch 7888 (47322 steps) ------------------------------------\n",
      "Max loss: 0.025250714272260666\n",
      "Min loss: 0.005167079158127308\n",
      "Mean loss: 0.013042493490502238\n",
      "Std loss: 0.006991861480427855\n",
      "Total Loss: 0.07825496094301343\n",
      "------------------------------------ epoch 7889 (47328 steps) ------------------------------------\n",
      "Max loss: 0.009872457012534142\n",
      "Min loss: 0.003702507819980383\n",
      "Mean loss: 0.00592982613792022\n",
      "Std loss: 0.0021509207922491102\n",
      "Total Loss: 0.035578956827521324\n",
      "------------------------------------ epoch 7890 (47334 steps) ------------------------------------\n",
      "Max loss: 0.009201875887811184\n",
      "Min loss: 0.0036510764621198177\n",
      "Mean loss: 0.00638202391564846\n",
      "Std loss: 0.0016998611462805068\n",
      "Total Loss: 0.03829214349389076\n",
      "------------------------------------ epoch 7891 (47340 steps) ------------------------------------\n",
      "Max loss: 0.010451003909111023\n",
      "Min loss: 0.005168110132217407\n",
      "Mean loss: 0.007590878754854202\n",
      "Std loss: 0.001605532914606632\n",
      "Total Loss: 0.045545272529125214\n",
      "------------------------------------ epoch 7892 (47346 steps) ------------------------------------\n",
      "Max loss: 0.011752281337976456\n",
      "Min loss: 0.005258922930806875\n",
      "Mean loss: 0.007870556476215521\n",
      "Std loss: 0.002763943224877437\n",
      "Total Loss: 0.04722333885729313\n",
      "------------------------------------ epoch 7893 (47352 steps) ------------------------------------\n",
      "Max loss: 0.01380123756825924\n",
      "Min loss: 0.004719879478216171\n",
      "Mean loss: 0.010142994889368614\n",
      "Std loss: 0.003167729106805663\n",
      "Total Loss: 0.06085796933621168\n",
      "------------------------------------ epoch 7894 (47358 steps) ------------------------------------\n",
      "Max loss: 0.014582378789782524\n",
      "Min loss: 0.0035226873587816954\n",
      "Mean loss: 0.00738214193067203\n",
      "Std loss: 0.0037973592241401706\n",
      "Total Loss: 0.04429285158403218\n",
      "------------------------------------ epoch 7895 (47364 steps) ------------------------------------\n",
      "Max loss: 0.008554241620004177\n",
      "Min loss: 0.0036460403352975845\n",
      "Mean loss: 0.006713910571609934\n",
      "Std loss: 0.001895247079112195\n",
      "Total Loss: 0.040283463429659605\n",
      "------------------------------------ epoch 7896 (47370 steps) ------------------------------------\n",
      "Max loss: 0.014345175586640835\n",
      "Min loss: 0.003917322494089603\n",
      "Mean loss: 0.008711151623477539\n",
      "Std loss: 0.0038464043583186763\n",
      "Total Loss: 0.05226690974086523\n",
      "------------------------------------ epoch 7897 (47376 steps) ------------------------------------\n",
      "Max loss: 0.015099585056304932\n",
      "Min loss: 0.004954980686306953\n",
      "Mean loss: 0.007988612866029143\n",
      "Std loss: 0.0033965102232861124\n",
      "Total Loss: 0.04793167719617486\n",
      "------------------------------------ epoch 7898 (47382 steps) ------------------------------------\n",
      "Max loss: 0.019043689593672752\n",
      "Min loss: 0.004882466048002243\n",
      "Mean loss: 0.011106937425211072\n",
      "Std loss: 0.005249292078292554\n",
      "Total Loss: 0.06664162455126643\n",
      "------------------------------------ epoch 7899 (47388 steps) ------------------------------------\n",
      "Max loss: 0.009749404154717922\n",
      "Min loss: 0.003974080551415682\n",
      "Mean loss: 0.0064203696480641765\n",
      "Std loss: 0.0020489372974811255\n",
      "Total Loss: 0.03852221788838506\n",
      "------------------------------------ epoch 7900 (47394 steps) ------------------------------------\n",
      "Max loss: 0.009717261418700218\n",
      "Min loss: 0.003786165965721011\n",
      "Mean loss: 0.006740378177103897\n",
      "Std loss: 0.0022154054255089243\n",
      "Total Loss: 0.04044226906262338\n",
      "------------------------------------ epoch 7901 (47400 steps) ------------------------------------\n",
      "Max loss: 0.028417076915502548\n",
      "Min loss: 0.005411466117948294\n",
      "Mean loss: 0.012277126545086503\n",
      "Std loss: 0.007623940414962004\n",
      "Total Loss: 0.07366275927051902\n",
      "saved model at ./weights/model_7901.pth\n",
      "------------------------------------ epoch 7902 (47406 steps) ------------------------------------\n",
      "Max loss: 0.008263197727501392\n",
      "Min loss: 0.0036149495281279087\n",
      "Mean loss: 0.004805850175519784\n",
      "Std loss: 0.0016178470175133063\n",
      "Total Loss: 0.028835101053118706\n",
      "------------------------------------ epoch 7903 (47412 steps) ------------------------------------\n",
      "Max loss: 0.009507003240287304\n",
      "Min loss: 0.003954705782234669\n",
      "Mean loss: 0.006224574909235041\n",
      "Std loss: 0.001878977842897347\n",
      "Total Loss: 0.03734744945541024\n",
      "------------------------------------ epoch 7904 (47418 steps) ------------------------------------\n",
      "Max loss: 0.011220661923289299\n",
      "Min loss: 0.0035087361466139555\n",
      "Mean loss: 0.0060600145952776074\n",
      "Std loss: 0.0028481675904528272\n",
      "Total Loss: 0.036360087571665645\n",
      "------------------------------------ epoch 7905 (47424 steps) ------------------------------------\n",
      "Max loss: 0.011554970405995846\n",
      "Min loss: 0.0032338432502001524\n",
      "Mean loss: 0.00629638135433197\n",
      "Std loss: 0.0029982338838894494\n",
      "Total Loss: 0.03777828812599182\n",
      "------------------------------------ epoch 7906 (47430 steps) ------------------------------------\n",
      "Max loss: 0.005901222582906485\n",
      "Min loss: 0.0034224395640194416\n",
      "Mean loss: 0.004682018033539255\n",
      "Std loss: 0.0009563301575875904\n",
      "Total Loss: 0.028092108201235533\n",
      "------------------------------------ epoch 7907 (47436 steps) ------------------------------------\n",
      "Max loss: 0.01839582622051239\n",
      "Min loss: 0.003961814101785421\n",
      "Mean loss: 0.00783944510233899\n",
      "Std loss: 0.004924750011761919\n",
      "Total Loss: 0.04703667061403394\n",
      "------------------------------------ epoch 7908 (47442 steps) ------------------------------------\n",
      "Max loss: 0.026353923603892326\n",
      "Min loss: 0.004230002406984568\n",
      "Mean loss: 0.014707850680376092\n",
      "Std loss: 0.009318769596525191\n",
      "Total Loss: 0.08824710408225656\n",
      "------------------------------------ epoch 7909 (47448 steps) ------------------------------------\n",
      "Max loss: 0.010783342644572258\n",
      "Min loss: 0.005191775504499674\n",
      "Mean loss: 0.007512576878070831\n",
      "Std loss: 0.0018518105484379129\n",
      "Total Loss: 0.04507546126842499\n",
      "------------------------------------ epoch 7910 (47454 steps) ------------------------------------\n",
      "Max loss: 0.00982891395688057\n",
      "Min loss: 0.003952953033149242\n",
      "Mean loss: 0.0063480610648791\n",
      "Std loss: 0.002163568440897067\n",
      "Total Loss: 0.0380883663892746\n",
      "------------------------------------ epoch 7911 (47460 steps) ------------------------------------\n",
      "Max loss: 0.015824228525161743\n",
      "Min loss: 0.004843283444643021\n",
      "Mean loss: 0.008709479899456104\n",
      "Std loss: 0.0036532052948423928\n",
      "Total Loss: 0.05225687939673662\n",
      "------------------------------------ epoch 7912 (47466 steps) ------------------------------------\n",
      "Max loss: 0.008089851588010788\n",
      "Min loss: 0.004662202205508947\n",
      "Mean loss: 0.006674329362188776\n",
      "Std loss: 0.001192321474141004\n",
      "Total Loss: 0.04004597617313266\n",
      "------------------------------------ epoch 7913 (47472 steps) ------------------------------------\n",
      "Max loss: 0.013181490823626518\n",
      "Min loss: 0.0038426569662988186\n",
      "Mean loss: 0.007308924415459235\n",
      "Std loss: 0.0035655082857606003\n",
      "Total Loss: 0.04385354649275541\n",
      "------------------------------------ epoch 7914 (47478 steps) ------------------------------------\n",
      "Max loss: 0.012725338339805603\n",
      "Min loss: 0.004227630328387022\n",
      "Mean loss: 0.007769252406433225\n",
      "Std loss: 0.003068479848343664\n",
      "Total Loss: 0.04661551443859935\n",
      "------------------------------------ epoch 7915 (47484 steps) ------------------------------------\n",
      "Max loss: 0.012344736605882645\n",
      "Min loss: 0.003394066821783781\n",
      "Mean loss: 0.007579474089046319\n",
      "Std loss: 0.0032235889730003633\n",
      "Total Loss: 0.045476844534277916\n",
      "------------------------------------ epoch 7916 (47490 steps) ------------------------------------\n",
      "Max loss: 0.026527350768446922\n",
      "Min loss: 0.004127927124500275\n",
      "Mean loss: 0.012442158535122871\n",
      "Std loss: 0.007228943336865228\n",
      "Total Loss: 0.07465295121073723\n",
      "------------------------------------ epoch 7917 (47496 steps) ------------------------------------\n",
      "Max loss: 0.016957437619566917\n",
      "Min loss: 0.006335643585771322\n",
      "Mean loss: 0.011458506730074683\n",
      "Std loss: 0.003571373906568473\n",
      "Total Loss: 0.0687510403804481\n",
      "------------------------------------ epoch 7918 (47502 steps) ------------------------------------\n",
      "Max loss: 0.013498026877641678\n",
      "Min loss: 0.00638214498758316\n",
      "Mean loss: 0.009172524480770031\n",
      "Std loss: 0.0025920359825288277\n",
      "Total Loss: 0.05503514688462019\n",
      "------------------------------------ epoch 7919 (47508 steps) ------------------------------------\n",
      "Max loss: 0.014066360890865326\n",
      "Min loss: 0.0062439702451229095\n",
      "Mean loss: 0.009318896879752478\n",
      "Std loss: 0.003046380353065777\n",
      "Total Loss: 0.05591338127851486\n",
      "------------------------------------ epoch 7920 (47514 steps) ------------------------------------\n",
      "Max loss: 0.01920909807085991\n",
      "Min loss: 0.004712756723165512\n",
      "Mean loss: 0.009839831152930856\n",
      "Std loss: 0.005625176509559422\n",
      "Total Loss: 0.059038986917585135\n",
      "------------------------------------ epoch 7921 (47520 steps) ------------------------------------\n",
      "Max loss: 0.024411175400018692\n",
      "Min loss: 0.004426385276019573\n",
      "Mean loss: 0.010169126093387604\n",
      "Std loss: 0.006895962339232158\n",
      "Total Loss: 0.06101475656032562\n",
      "------------------------------------ epoch 7922 (47526 steps) ------------------------------------\n",
      "Max loss: 0.026645097881555557\n",
      "Min loss: 0.004323994740843773\n",
      "Mean loss: 0.009590651995191971\n",
      "Std loss: 0.007862041660814185\n",
      "Total Loss: 0.05754391197115183\n",
      "------------------------------------ epoch 7923 (47532 steps) ------------------------------------\n",
      "Max loss: 0.0167593564838171\n",
      "Min loss: 0.00584430992603302\n",
      "Mean loss: 0.01014102545256416\n",
      "Std loss: 0.003761055483365453\n",
      "Total Loss: 0.06084615271538496\n",
      "------------------------------------ epoch 7924 (47538 steps) ------------------------------------\n",
      "Max loss: 0.012930309399962425\n",
      "Min loss: 0.00368280871771276\n",
      "Mean loss: 0.009095205032887558\n",
      "Std loss: 0.0030650890498686066\n",
      "Total Loss: 0.05457123019732535\n",
      "------------------------------------ epoch 7925 (47544 steps) ------------------------------------\n",
      "Max loss: 0.024904971942305565\n",
      "Min loss: 0.005818940699100494\n",
      "Mean loss: 0.012259469367563725\n",
      "Std loss: 0.007428950986763859\n",
      "Total Loss: 0.07355681620538235\n",
      "------------------------------------ epoch 7926 (47550 steps) ------------------------------------\n",
      "Max loss: 0.010312138125300407\n",
      "Min loss: 0.0049789827316999435\n",
      "Mean loss: 0.007879038030902544\n",
      "Std loss: 0.0019102149671323821\n",
      "Total Loss: 0.04727422818541527\n",
      "------------------------------------ epoch 7927 (47556 steps) ------------------------------------\n",
      "Max loss: 0.015430334024131298\n",
      "Min loss: 0.005359949544072151\n",
      "Mean loss: 0.007855834517007073\n",
      "Std loss: 0.0034935037560561632\n",
      "Total Loss: 0.04713500710204244\n",
      "------------------------------------ epoch 7928 (47562 steps) ------------------------------------\n",
      "Max loss: 0.01256688591092825\n",
      "Min loss: 0.004257815424352884\n",
      "Mean loss: 0.007194180740043521\n",
      "Std loss: 0.0028495069953901704\n",
      "Total Loss: 0.043165084440261126\n",
      "------------------------------------ epoch 7929 (47568 steps) ------------------------------------\n",
      "Max loss: 0.014764509163796902\n",
      "Min loss: 0.0036758771166205406\n",
      "Mean loss: 0.008797610023369392\n",
      "Std loss: 0.0037795797654274683\n",
      "Total Loss: 0.05278566014021635\n",
      "------------------------------------ epoch 7930 (47574 steps) ------------------------------------\n",
      "Max loss: 0.015517586842179298\n",
      "Min loss: 0.0032153353095054626\n",
      "Mean loss: 0.00791166431736201\n",
      "Std loss: 0.005039786801551802\n",
      "Total Loss: 0.04746998590417206\n",
      "------------------------------------ epoch 7931 (47580 steps) ------------------------------------\n",
      "Max loss: 0.008603373542428017\n",
      "Min loss: 0.004305418115109205\n",
      "Mean loss: 0.005867602226013939\n",
      "Std loss: 0.0014610277206277985\n",
      "Total Loss: 0.03520561335608363\n",
      "------------------------------------ epoch 7932 (47586 steps) ------------------------------------\n",
      "Max loss: 0.013826976530253887\n",
      "Min loss: 0.003914012107998133\n",
      "Mean loss: 0.007632032114391525\n",
      "Std loss: 0.00374817495135981\n",
      "Total Loss: 0.045792192686349154\n",
      "------------------------------------ epoch 7933 (47592 steps) ------------------------------------\n",
      "Max loss: 0.011686772108078003\n",
      "Min loss: 0.005201851017773151\n",
      "Mean loss: 0.007061050661529104\n",
      "Std loss: 0.00217688961442001\n",
      "Total Loss: 0.042366303969174623\n",
      "------------------------------------ epoch 7934 (47598 steps) ------------------------------------\n",
      "Max loss: 0.02935083955526352\n",
      "Min loss: 0.003749314695596695\n",
      "Mean loss: 0.011984111120303472\n",
      "Std loss: 0.008210635548446045\n",
      "Total Loss: 0.07190466672182083\n",
      "------------------------------------ epoch 7935 (47604 steps) ------------------------------------\n",
      "Max loss: 0.02616158500313759\n",
      "Min loss: 0.004800494760274887\n",
      "Mean loss: 0.010722203801075617\n",
      "Std loss: 0.007563502193909731\n",
      "Total Loss: 0.0643332228064537\n",
      "------------------------------------ epoch 7936 (47610 steps) ------------------------------------\n",
      "Max loss: 0.018221966922283173\n",
      "Min loss: 0.005331274121999741\n",
      "Mean loss: 0.011214016160617271\n",
      "Std loss: 0.004958025875327377\n",
      "Total Loss: 0.06728409696370363\n",
      "------------------------------------ epoch 7937 (47616 steps) ------------------------------------\n",
      "Max loss: 0.012993646785616875\n",
      "Min loss: 0.00554546108469367\n",
      "Mean loss: 0.008497734709332386\n",
      "Std loss: 0.0023936348251751382\n",
      "Total Loss: 0.05098640825599432\n",
      "------------------------------------ epoch 7938 (47622 steps) ------------------------------------\n",
      "Max loss: 0.06670652329921722\n",
      "Min loss: 0.005679411813616753\n",
      "Mean loss: 0.018819370695079368\n",
      "Std loss: 0.021646077908502415\n",
      "Total Loss: 0.1129162241704762\n",
      "------------------------------------ epoch 7939 (47628 steps) ------------------------------------\n",
      "Max loss: 0.028127342462539673\n",
      "Min loss: 0.0074195838533341885\n",
      "Mean loss: 0.014042181195691228\n",
      "Std loss: 0.00739360849350072\n",
      "Total Loss: 0.08425308717414737\n",
      "------------------------------------ epoch 7940 (47634 steps) ------------------------------------\n",
      "Max loss: 0.01442690845578909\n",
      "Min loss: 0.006339814513921738\n",
      "Mean loss: 0.010220216276745001\n",
      "Std loss: 0.0028260893180892693\n",
      "Total Loss: 0.06132129766047001\n",
      "------------------------------------ epoch 7941 (47640 steps) ------------------------------------\n",
      "Max loss: 0.017272423952817917\n",
      "Min loss: 0.007420678623020649\n",
      "Mean loss: 0.010932598263025284\n",
      "Std loss: 0.0030757963961087096\n",
      "Total Loss: 0.0655955895781517\n",
      "------------------------------------ epoch 7942 (47646 steps) ------------------------------------\n",
      "Max loss: 0.01165367104113102\n",
      "Min loss: 0.0061007956974208355\n",
      "Mean loss: 0.008846250595524907\n",
      "Std loss: 0.0019497334324580597\n",
      "Total Loss: 0.05307750357314944\n",
      "------------------------------------ epoch 7943 (47652 steps) ------------------------------------\n",
      "Max loss: 0.018344568088650703\n",
      "Min loss: 0.005514996126294136\n",
      "Mean loss: 0.011092723269636432\n",
      "Std loss: 0.00450858329338943\n",
      "Total Loss: 0.0665563396178186\n",
      "------------------------------------ epoch 7944 (47658 steps) ------------------------------------\n",
      "Max loss: 0.013982521370053291\n",
      "Min loss: 0.005047065205872059\n",
      "Mean loss: 0.008521185955032706\n",
      "Std loss: 0.0031177177039393615\n",
      "Total Loss: 0.05112711573019624\n",
      "------------------------------------ epoch 7945 (47664 steps) ------------------------------------\n",
      "Max loss: 0.010486617684364319\n",
      "Min loss: 0.004770239815115929\n",
      "Mean loss: 0.006983006605878472\n",
      "Std loss: 0.0022866812702296616\n",
      "Total Loss: 0.041898039635270834\n",
      "------------------------------------ epoch 7946 (47670 steps) ------------------------------------\n",
      "Max loss: 0.03696521371603012\n",
      "Min loss: 0.006710323970764875\n",
      "Mean loss: 0.01650288941649099\n",
      "Std loss: 0.010749523400375775\n",
      "Total Loss: 0.09901733649894595\n",
      "------------------------------------ epoch 7947 (47676 steps) ------------------------------------\n",
      "Max loss: 0.019057922065258026\n",
      "Min loss: 0.004293493926525116\n",
      "Mean loss: 0.008404072762156526\n",
      "Std loss: 0.004941862951681818\n",
      "Total Loss: 0.05042443657293916\n",
      "------------------------------------ epoch 7948 (47682 steps) ------------------------------------\n",
      "Max loss: 0.016194753348827362\n",
      "Min loss: 0.006282161455601454\n",
      "Mean loss: 0.009091896393025914\n",
      "Std loss: 0.003355619651137073\n",
      "Total Loss: 0.05455137835815549\n",
      "------------------------------------ epoch 7949 (47688 steps) ------------------------------------\n",
      "Max loss: 0.012278913520276546\n",
      "Min loss: 0.004991061054170132\n",
      "Mean loss: 0.00816685085495313\n",
      "Std loss: 0.0031009583929744937\n",
      "Total Loss: 0.04900110512971878\n",
      "------------------------------------ epoch 7950 (47694 steps) ------------------------------------\n",
      "Max loss: 0.009508226066827774\n",
      "Min loss: 0.005284588318318129\n",
      "Mean loss: 0.007241583584497373\n",
      "Std loss: 0.0015489181907019924\n",
      "Total Loss: 0.043449501506984234\n",
      "------------------------------------ epoch 7951 (47700 steps) ------------------------------------\n",
      "Max loss: 0.026395462453365326\n",
      "Min loss: 0.004693762399256229\n",
      "Mean loss: 0.009486733082061013\n",
      "Std loss: 0.007678976400511992\n",
      "Total Loss: 0.056920398492366076\n",
      "------------------------------------ epoch 7952 (47706 steps) ------------------------------------\n",
      "Max loss: 0.011820191517472267\n",
      "Min loss: 0.0039879316464066505\n",
      "Mean loss: 0.008208345156162977\n",
      "Std loss: 0.0033374923822307137\n",
      "Total Loss: 0.04925007093697786\n",
      "------------------------------------ epoch 7953 (47712 steps) ------------------------------------\n",
      "Max loss: 0.01072816364467144\n",
      "Min loss: 0.003574298694729805\n",
      "Mean loss: 0.006351190308729808\n",
      "Std loss: 0.0022580572239146524\n",
      "Total Loss: 0.038107141852378845\n",
      "------------------------------------ epoch 7954 (47718 steps) ------------------------------------\n",
      "Max loss: 0.026419319212436676\n",
      "Min loss: 0.0045592039823532104\n",
      "Mean loss: 0.010787724750116467\n",
      "Std loss: 0.008257941319129592\n",
      "Total Loss: 0.0647263485006988\n",
      "------------------------------------ epoch 7955 (47724 steps) ------------------------------------\n",
      "Max loss: 0.014286123216152191\n",
      "Min loss: 0.004619710147380829\n",
      "Mean loss: 0.008083258755505085\n",
      "Std loss: 0.003223500323227925\n",
      "Total Loss: 0.04849955253303051\n",
      "------------------------------------ epoch 7956 (47730 steps) ------------------------------------\n",
      "Max loss: 0.008045842871069908\n",
      "Min loss: 0.0039328113198280334\n",
      "Mean loss: 0.005882068459565441\n",
      "Std loss: 0.0014869077586269798\n",
      "Total Loss: 0.035292410757392645\n",
      "------------------------------------ epoch 7957 (47736 steps) ------------------------------------\n",
      "Max loss: 0.014499848708510399\n",
      "Min loss: 0.003990973811596632\n",
      "Mean loss: 0.008799380933245024\n",
      "Std loss: 0.004165323322103308\n",
      "Total Loss: 0.05279628559947014\n",
      "------------------------------------ epoch 7958 (47742 steps) ------------------------------------\n",
      "Max loss: 0.02362409047782421\n",
      "Min loss: 0.004885008558630943\n",
      "Mean loss: 0.009583706501871347\n",
      "Std loss: 0.00645130199500021\n",
      "Total Loss: 0.057502239011228085\n",
      "------------------------------------ epoch 7959 (47748 steps) ------------------------------------\n",
      "Max loss: 0.015698935836553574\n",
      "Min loss: 0.00561834080144763\n",
      "Mean loss: 0.009745911229401827\n",
      "Std loss: 0.0033679480670786633\n",
      "Total Loss: 0.05847546737641096\n",
      "------------------------------------ epoch 7960 (47754 steps) ------------------------------------\n",
      "Max loss: 0.020151756703853607\n",
      "Min loss: 0.010806847363710403\n",
      "Mean loss: 0.015083288618673881\n",
      "Std loss: 0.003396482726406443\n",
      "Total Loss: 0.09049973171204329\n",
      "------------------------------------ epoch 7961 (47760 steps) ------------------------------------\n",
      "Max loss: 0.053002264350652695\n",
      "Min loss: 0.0049848477356135845\n",
      "Mean loss: 0.016682070136691134\n",
      "Std loss: 0.01656669603501881\n",
      "Total Loss: 0.1000924208201468\n",
      "------------------------------------ epoch 7962 (47766 steps) ------------------------------------\n",
      "Max loss: 0.16172929108142853\n",
      "Min loss: 0.012354664504528046\n",
      "Mean loss: 0.06185487161080042\n",
      "Std loss: 0.05296058941878075\n",
      "Total Loss: 0.37112922966480255\n",
      "------------------------------------ epoch 7963 (47772 steps) ------------------------------------\n",
      "Max loss: 0.1812177151441574\n",
      "Min loss: 0.07436306774616241\n",
      "Mean loss: 0.10833869874477386\n",
      "Std loss: 0.034559167239365154\n",
      "Total Loss: 0.6500321924686432\n",
      "------------------------------------ epoch 7964 (47778 steps) ------------------------------------\n",
      "Max loss: 0.2962530255317688\n",
      "Min loss: 0.1069653332233429\n",
      "Mean loss: 0.16121340046326318\n",
      "Std loss: 0.062407254604512255\n",
      "Total Loss: 0.9672804027795792\n",
      "------------------------------------ epoch 7965 (47784 steps) ------------------------------------\n",
      "Max loss: 0.10055135190486908\n",
      "Min loss: 0.07078084349632263\n",
      "Mean loss: 0.08812278509140015\n",
      "Std loss: 0.011068943768248648\n",
      "Total Loss: 0.5287367105484009\n",
      "------------------------------------ epoch 7966 (47790 steps) ------------------------------------\n",
      "Max loss: 0.6923663020133972\n",
      "Min loss: 0.1837024688720703\n",
      "Mean loss: 0.3585559204220772\n",
      "Std loss: 0.17946276540247846\n",
      "Total Loss: 2.151335522532463\n",
      "------------------------------------ epoch 7967 (47796 steps) ------------------------------------\n",
      "Max loss: 0.23195695877075195\n",
      "Min loss: 0.12863747775554657\n",
      "Mean loss: 0.17653310298919678\n",
      "Std loss: 0.030562817109420423\n",
      "Total Loss: 1.0591986179351807\n",
      "------------------------------------ epoch 7968 (47802 steps) ------------------------------------\n",
      "Max loss: 0.1659090220928192\n",
      "Min loss: 0.10282488167285919\n",
      "Mean loss: 0.1306874118745327\n",
      "Std loss: 0.0197229052543993\n",
      "Total Loss: 0.7841244712471962\n",
      "------------------------------------ epoch 7969 (47808 steps) ------------------------------------\n",
      "Max loss: 0.7296099662780762\n",
      "Min loss: 0.08043484389781952\n",
      "Mean loss: 0.27152813225984573\n",
      "Std loss: 0.2253230976056317\n",
      "Total Loss: 1.6291687935590744\n",
      "------------------------------------ epoch 7970 (47814 steps) ------------------------------------\n",
      "Max loss: 0.8711814284324646\n",
      "Min loss: 0.13514107465744019\n",
      "Mean loss: 0.29086875667174655\n",
      "Std loss: 0.2609253205871366\n",
      "Total Loss: 1.7452125400304794\n",
      "------------------------------------ epoch 7971 (47820 steps) ------------------------------------\n",
      "Max loss: 0.20116838812828064\n",
      "Min loss: 0.11366066336631775\n",
      "Mean loss: 0.16410871098438898\n",
      "Std loss: 0.028331709661074986\n",
      "Total Loss: 0.9846522659063339\n",
      "------------------------------------ epoch 7972 (47826 steps) ------------------------------------\n",
      "Max loss: 0.15454085171222687\n",
      "Min loss: 0.10619036853313446\n",
      "Mean loss: 0.1277244525651137\n",
      "Std loss: 0.014596636110388824\n",
      "Total Loss: 0.7663467153906822\n",
      "------------------------------------ epoch 7973 (47832 steps) ------------------------------------\n",
      "Max loss: 0.10691710561513901\n",
      "Min loss: 0.07163015007972717\n",
      "Mean loss: 0.08235122511784236\n",
      "Std loss: 0.011424467324788044\n",
      "Total Loss: 0.49410735070705414\n",
      "------------------------------------ epoch 7974 (47838 steps) ------------------------------------\n",
      "Max loss: 0.08864390105009079\n",
      "Min loss: 0.07002101093530655\n",
      "Mean loss: 0.07808020090063413\n",
      "Std loss: 0.008051344410600183\n",
      "Total Loss: 0.4684812054038048\n",
      "------------------------------------ epoch 7975 (47844 steps) ------------------------------------\n",
      "Max loss: 0.07595427334308624\n",
      "Min loss: 0.042829614132642746\n",
      "Mean loss: 0.053651132310430207\n",
      "Std loss: 0.011157588518079721\n",
      "Total Loss: 0.32190679386258125\n",
      "------------------------------------ epoch 7976 (47850 steps) ------------------------------------\n",
      "Max loss: 0.05325620621442795\n",
      "Min loss: 0.03508114069700241\n",
      "Mean loss: 0.04253788789113363\n",
      "Std loss: 0.00628205858606673\n",
      "Total Loss: 0.25522732734680176\n",
      "------------------------------------ epoch 7977 (47856 steps) ------------------------------------\n",
      "Max loss: 0.04589071124792099\n",
      "Min loss: 0.02998465858399868\n",
      "Mean loss: 0.03682847569386164\n",
      "Std loss: 0.006351459892138845\n",
      "Total Loss: 0.22097085416316986\n",
      "------------------------------------ epoch 7978 (47862 steps) ------------------------------------\n",
      "Max loss: 0.08218090236186981\n",
      "Min loss: 0.022494137287139893\n",
      "Mean loss: 0.043873273146649204\n",
      "Std loss: 0.01884482580223671\n",
      "Total Loss: 0.2632396388798952\n",
      "------------------------------------ epoch 7979 (47868 steps) ------------------------------------\n",
      "Max loss: 0.08270569145679474\n",
      "Min loss: 0.02051442675292492\n",
      "Mean loss: 0.03465131235619386\n",
      "Std loss: 0.021962433998856497\n",
      "Total Loss: 0.20790787413716316\n",
      "------------------------------------ epoch 7980 (47874 steps) ------------------------------------\n",
      "Max loss: 0.05449218675494194\n",
      "Min loss: 0.026477307081222534\n",
      "Mean loss: 0.03600507757316033\n",
      "Std loss: 0.009681557943284338\n",
      "Total Loss: 0.21603046543896198\n",
      "------------------------------------ epoch 7981 (47880 steps) ------------------------------------\n",
      "Max loss: 0.04560476541519165\n",
      "Min loss: 0.020393814891576767\n",
      "Mean loss: 0.0272549232468009\n",
      "Std loss: 0.009013163880546755\n",
      "Total Loss: 0.1635295394808054\n",
      "------------------------------------ epoch 7982 (47886 steps) ------------------------------------\n",
      "Max loss: 0.03666655719280243\n",
      "Min loss: 0.018936362117528915\n",
      "Mean loss: 0.02448611706495285\n",
      "Std loss: 0.006262289015569171\n",
      "Total Loss: 0.1469167023897171\n",
      "------------------------------------ epoch 7983 (47892 steps) ------------------------------------\n",
      "Max loss: 0.06854922324419022\n",
      "Min loss: 0.01671779900789261\n",
      "Mean loss: 0.030421956442296505\n",
      "Std loss: 0.01749139129510893\n",
      "Total Loss: 0.18253173865377903\n",
      "------------------------------------ epoch 7984 (47898 steps) ------------------------------------\n",
      "Max loss: 0.0380072258412838\n",
      "Min loss: 0.014747628942131996\n",
      "Mean loss: 0.024626458374162514\n",
      "Std loss: 0.008503323538439859\n",
      "Total Loss: 0.1477587502449751\n",
      "------------------------------------ epoch 7985 (47904 steps) ------------------------------------\n",
      "Max loss: 0.026290856301784515\n",
      "Min loss: 0.013423845171928406\n",
      "Mean loss: 0.018782035758097965\n",
      "Std loss: 0.004540233609185058\n",
      "Total Loss: 0.1126922145485878\n",
      "------------------------------------ epoch 7986 (47910 steps) ------------------------------------\n",
      "Max loss: 0.021534515544772148\n",
      "Min loss: 0.013233064673841\n",
      "Mean loss: 0.015559024487932524\n",
      "Std loss: 0.0028282292501954445\n",
      "Total Loss: 0.09335414692759514\n",
      "------------------------------------ epoch 7987 (47916 steps) ------------------------------------\n",
      "Max loss: 0.04902103543281555\n",
      "Min loss: 0.011117148213088512\n",
      "Mean loss: 0.02439439467464884\n",
      "Std loss: 0.011930101820450912\n",
      "Total Loss: 0.14636636804789305\n",
      "------------------------------------ epoch 7988 (47922 steps) ------------------------------------\n",
      "Max loss: 0.03175966069102287\n",
      "Min loss: 0.011181565001606941\n",
      "Mean loss: 0.019960810896009207\n",
      "Std loss: 0.007189199561776552\n",
      "Total Loss: 0.11976486537605524\n",
      "------------------------------------ epoch 7989 (47928 steps) ------------------------------------\n",
      "Max loss: 0.0553634837269783\n",
      "Min loss: 0.012210113927721977\n",
      "Mean loss: 0.025873940748473007\n",
      "Std loss: 0.015532135990949616\n",
      "Total Loss: 0.15524364449083805\n",
      "------------------------------------ epoch 7990 (47934 steps) ------------------------------------\n",
      "Max loss: 0.04648903012275696\n",
      "Min loss: 0.011153895407915115\n",
      "Mean loss: 0.022366020518044632\n",
      "Std loss: 0.0112893278509248\n",
      "Total Loss: 0.13419612310826778\n",
      "------------------------------------ epoch 7991 (47940 steps) ------------------------------------\n",
      "Max loss: 0.025864897295832634\n",
      "Min loss: 0.010169420391321182\n",
      "Mean loss: 0.017748615083595116\n",
      "Std loss: 0.005908918889994087\n",
      "Total Loss: 0.1064916905015707\n",
      "------------------------------------ epoch 7992 (47946 steps) ------------------------------------\n",
      "Max loss: 0.025712033733725548\n",
      "Min loss: 0.01082840096205473\n",
      "Mean loss: 0.015804282389581203\n",
      "Std loss: 0.00534553223716576\n",
      "Total Loss: 0.09482569433748722\n",
      "------------------------------------ epoch 7993 (47952 steps) ------------------------------------\n",
      "Max loss: 0.019602283835411072\n",
      "Min loss: 0.011404795572161674\n",
      "Mean loss: 0.016283963962147634\n",
      "Std loss: 0.002508534687070233\n",
      "Total Loss: 0.0977037837728858\n",
      "------------------------------------ epoch 7994 (47958 steps) ------------------------------------\n",
      "Max loss: 0.020427105948328972\n",
      "Min loss: 0.009414134547114372\n",
      "Mean loss: 0.014182749049117168\n",
      "Std loss: 0.004038705672945472\n",
      "Total Loss: 0.085096494294703\n",
      "------------------------------------ epoch 7995 (47964 steps) ------------------------------------\n",
      "Max loss: 0.045048318803310394\n",
      "Min loss: 0.011650416068732738\n",
      "Mean loss: 0.019688935484737158\n",
      "Std loss: 0.01150809743681776\n",
      "Total Loss: 0.11813361290842295\n",
      "------------------------------------ epoch 7996 (47970 steps) ------------------------------------\n",
      "Max loss: 0.018875598907470703\n",
      "Min loss: 0.011657144874334335\n",
      "Mean loss: 0.014310773617277542\n",
      "Std loss: 0.002558969140047607\n",
      "Total Loss: 0.08586464170366526\n",
      "------------------------------------ epoch 7997 (47976 steps) ------------------------------------\n",
      "Max loss: 0.0391036719083786\n",
      "Min loss: 0.011775709688663483\n",
      "Mean loss: 0.020423954042295616\n",
      "Std loss: 0.009632066261682518\n",
      "Total Loss: 0.12254372425377369\n",
      "------------------------------------ epoch 7998 (47982 steps) ------------------------------------\n",
      "Max loss: 0.020151101052761078\n",
      "Min loss: 0.011641423217952251\n",
      "Mean loss: 0.016124913779397804\n",
      "Std loss: 0.003005622038099593\n",
      "Total Loss: 0.09674948267638683\n",
      "------------------------------------ epoch 7999 (47988 steps) ------------------------------------\n",
      "Max loss: 0.04762415215373039\n",
      "Min loss: 0.01328740082681179\n",
      "Mean loss: 0.023950306698679924\n",
      "Std loss: 0.01151642231557477\n",
      "Total Loss: 0.14370184019207954\n",
      "------------------------------------ epoch 8000 (47994 steps) ------------------------------------\n",
      "Max loss: 0.019975779578089714\n",
      "Min loss: 0.009279532358050346\n",
      "Mean loss: 0.014862791169434786\n",
      "Std loss: 0.0038466122178547777\n",
      "Total Loss: 0.08917674701660872\n",
      "------------------------------------ epoch 8001 (48000 steps) ------------------------------------\n",
      "Max loss: 0.01693984493613243\n",
      "Min loss: 0.007696063257753849\n",
      "Mean loss: 0.012317989176760117\n",
      "Std loss: 0.0027009916529302674\n",
      "Total Loss: 0.0739079350605607\n",
      "saved model at ./weights/model_8001.pth\n",
      "------------------------------------ epoch 8002 (48006 steps) ------------------------------------\n",
      "Max loss: 0.020737111568450928\n",
      "Min loss: 0.008107580244541168\n",
      "Mean loss: 0.01235184787462155\n",
      "Std loss: 0.004277013747110143\n",
      "Total Loss: 0.0741110872477293\n",
      "------------------------------------ epoch 8003 (48012 steps) ------------------------------------\n",
      "Max loss: 0.016683900728821754\n",
      "Min loss: 0.007926966063678265\n",
      "Mean loss: 0.010712421343972286\n",
      "Std loss: 0.002860889042431569\n",
      "Total Loss: 0.06427452806383371\n",
      "------------------------------------ epoch 8004 (48018 steps) ------------------------------------\n",
      "Max loss: 0.026386808604002\n",
      "Min loss: 0.009272928349673748\n",
      "Mean loss: 0.01556721841916442\n",
      "Std loss: 0.006438248293192063\n",
      "Total Loss: 0.09340331051498652\n",
      "------------------------------------ epoch 8005 (48024 steps) ------------------------------------\n",
      "Max loss: 0.052437376230955124\n",
      "Min loss: 0.00913812406361103\n",
      "Mean loss: 0.023814573263128597\n",
      "Std loss: 0.014521841653175245\n",
      "Total Loss: 0.1428874395787716\n",
      "------------------------------------ epoch 8006 (48030 steps) ------------------------------------\n",
      "Max loss: 0.025643303990364075\n",
      "Min loss: 0.010360813699662685\n",
      "Mean loss: 0.013766054064035416\n",
      "Std loss: 0.005390358037760014\n",
      "Total Loss: 0.0825963243842125\n",
      "------------------------------------ epoch 8007 (48036 steps) ------------------------------------\n",
      "Max loss: 0.016897281631827354\n",
      "Min loss: 0.009349116124212742\n",
      "Mean loss: 0.013150971382856369\n",
      "Std loss: 0.0027536325158515895\n",
      "Total Loss: 0.07890582829713821\n",
      "------------------------------------ epoch 8008 (48042 steps) ------------------------------------\n",
      "Max loss: 0.018701931461691856\n",
      "Min loss: 0.00836733728647232\n",
      "Mean loss: 0.013968081989636024\n",
      "Std loss: 0.003947598823619919\n",
      "Total Loss: 0.08380849193781614\n",
      "------------------------------------ epoch 8009 (48048 steps) ------------------------------------\n",
      "Max loss: 0.03677785396575928\n",
      "Min loss: 0.008374510332942009\n",
      "Mean loss: 0.019594411210467417\n",
      "Std loss: 0.0108441125494766\n",
      "Total Loss: 0.11756646726280451\n",
      "------------------------------------ epoch 8010 (48054 steps) ------------------------------------\n",
      "Max loss: 0.013039077632129192\n",
      "Min loss: 0.008034450933337212\n",
      "Mean loss: 0.01081031064192454\n",
      "Std loss: 0.0017716874346980134\n",
      "Total Loss: 0.06486186385154724\n",
      "------------------------------------ epoch 8011 (48060 steps) ------------------------------------\n",
      "Max loss: 0.024503758177161217\n",
      "Min loss: 0.008587460964918137\n",
      "Mean loss: 0.013542921282351017\n",
      "Std loss: 0.005278942671548683\n",
      "Total Loss: 0.0812575276941061\n",
      "------------------------------------ epoch 8012 (48066 steps) ------------------------------------\n",
      "Max loss: 0.02386912703514099\n",
      "Min loss: 0.010011699981987476\n",
      "Mean loss: 0.01547123600418369\n",
      "Std loss: 0.005169686804565684\n",
      "Total Loss: 0.09282741602510214\n",
      "------------------------------------ epoch 8013 (48072 steps) ------------------------------------\n",
      "Max loss: 0.03688381612300873\n",
      "Min loss: 0.00772780179977417\n",
      "Mean loss: 0.016215947922319174\n",
      "Std loss: 0.009908987612590197\n",
      "Total Loss: 0.09729568753391504\n",
      "------------------------------------ epoch 8014 (48078 steps) ------------------------------------\n",
      "Max loss: 0.01363265048712492\n",
      "Min loss: 0.008745219558477402\n",
      "Mean loss: 0.011170460532108942\n",
      "Std loss: 0.0020297961306021623\n",
      "Total Loss: 0.06702276319265366\n",
      "------------------------------------ epoch 8015 (48084 steps) ------------------------------------\n",
      "Max loss: 0.02113362029194832\n",
      "Min loss: 0.006993971765041351\n",
      "Mean loss: 0.013194996553162733\n",
      "Std loss: 0.005729211231149991\n",
      "Total Loss: 0.0791699793189764\n",
      "------------------------------------ epoch 8016 (48090 steps) ------------------------------------\n",
      "Max loss: 0.033196136355400085\n",
      "Min loss: 0.00694458931684494\n",
      "Mean loss: 0.013119740799690286\n",
      "Std loss: 0.009591984122416885\n",
      "Total Loss: 0.07871844479814172\n",
      "------------------------------------ epoch 8017 (48096 steps) ------------------------------------\n",
      "Max loss: 0.06502775847911835\n",
      "Min loss: 0.0067066531628370285\n",
      "Mean loss: 0.019982154325892527\n",
      "Std loss: 0.020352149849804584\n",
      "Total Loss: 0.11989292595535517\n",
      "------------------------------------ epoch 8018 (48102 steps) ------------------------------------\n",
      "Max loss: 0.01926565170288086\n",
      "Min loss: 0.0062233684584498405\n",
      "Mean loss: 0.012803917285054922\n",
      "Std loss: 0.004299855208388264\n",
      "Total Loss: 0.07682350371032953\n",
      "------------------------------------ epoch 8019 (48108 steps) ------------------------------------\n",
      "Max loss: 0.017856057733297348\n",
      "Min loss: 0.0070263175293803215\n",
      "Mean loss: 0.010421532361457745\n",
      "Std loss: 0.003680714992909859\n",
      "Total Loss: 0.06252919416874647\n",
      "------------------------------------ epoch 8020 (48114 steps) ------------------------------------\n",
      "Max loss: 0.022664321586489677\n",
      "Min loss: 0.006436971947550774\n",
      "Mean loss: 0.00979327488069733\n",
      "Std loss: 0.005803215784127748\n",
      "Total Loss: 0.05875964928418398\n",
      "------------------------------------ epoch 8021 (48120 steps) ------------------------------------\n",
      "Max loss: 0.01643449068069458\n",
      "Min loss: 0.006294346414506435\n",
      "Mean loss: 0.009689340678354105\n",
      "Std loss: 0.00318435601869745\n",
      "Total Loss: 0.058136044070124626\n",
      "------------------------------------ epoch 8022 (48126 steps) ------------------------------------\n",
      "Max loss: 0.014475315809249878\n",
      "Min loss: 0.006453169044107199\n",
      "Mean loss: 0.009642500818396607\n",
      "Std loss: 0.003355478807945682\n",
      "Total Loss: 0.05785500491037965\n",
      "------------------------------------ epoch 8023 (48132 steps) ------------------------------------\n",
      "Max loss: 0.015217900276184082\n",
      "Min loss: 0.006458071991801262\n",
      "Mean loss: 0.010209826674933234\n",
      "Std loss: 0.0032109239138686423\n",
      "Total Loss: 0.06125896004959941\n",
      "------------------------------------ epoch 8024 (48138 steps) ------------------------------------\n",
      "Max loss: 0.019950492307543755\n",
      "Min loss: 0.006010866723954678\n",
      "Mean loss: 0.01052908164759477\n",
      "Std loss: 0.004834020624752055\n",
      "Total Loss: 0.06317448988556862\n",
      "------------------------------------ epoch 8025 (48144 steps) ------------------------------------\n",
      "Max loss: 0.01603267714381218\n",
      "Min loss: 0.007982615381479263\n",
      "Mean loss: 0.011399345006793737\n",
      "Std loss: 0.0027159188166223196\n",
      "Total Loss: 0.06839607004076242\n",
      "------------------------------------ epoch 8026 (48150 steps) ------------------------------------\n",
      "Max loss: 0.040750663727521896\n",
      "Min loss: 0.009500626474618912\n",
      "Mean loss: 0.01588618429377675\n",
      "Std loss: 0.011243318496641973\n",
      "Total Loss: 0.0953171057626605\n",
      "------------------------------------ epoch 8027 (48156 steps) ------------------------------------\n",
      "Max loss: 0.018848348408937454\n",
      "Min loss: 0.006825578864663839\n",
      "Mean loss: 0.010057331761345267\n",
      "Std loss: 0.004025496167047516\n",
      "Total Loss: 0.060343990568071604\n",
      "------------------------------------ epoch 8028 (48162 steps) ------------------------------------\n",
      "Max loss: 0.013486476615071297\n",
      "Min loss: 0.00642857700586319\n",
      "Mean loss: 0.008580661456411084\n",
      "Std loss: 0.002336042425399186\n",
      "Total Loss: 0.0514839687384665\n",
      "------------------------------------ epoch 8029 (48168 steps) ------------------------------------\n",
      "Max loss: 0.02201133593916893\n",
      "Min loss: 0.006249644793570042\n",
      "Mean loss: 0.010617376382773122\n",
      "Std loss: 0.0054185987022905245\n",
      "Total Loss: 0.06370425829663873\n",
      "------------------------------------ epoch 8030 (48174 steps) ------------------------------------\n",
      "Max loss: 0.012273568660020828\n",
      "Min loss: 0.0068420469760894775\n",
      "Mean loss: 0.008401660791908702\n",
      "Std loss: 0.0018799680446220078\n",
      "Total Loss: 0.05040996475145221\n",
      "------------------------------------ epoch 8031 (48180 steps) ------------------------------------\n",
      "Max loss: 0.017028402537107468\n",
      "Min loss: 0.006006365641951561\n",
      "Mean loss: 0.00939977557087938\n",
      "Std loss: 0.0036959524204869193\n",
      "Total Loss: 0.05639865342527628\n",
      "------------------------------------ epoch 8032 (48186 steps) ------------------------------------\n",
      "Max loss: 0.018751423805952072\n",
      "Min loss: 0.006288015749305487\n",
      "Mean loss: 0.010567297382901112\n",
      "Std loss: 0.004174033988547693\n",
      "Total Loss: 0.06340378429740667\n",
      "------------------------------------ epoch 8033 (48192 steps) ------------------------------------\n",
      "Max loss: 0.012573853135108948\n",
      "Min loss: 0.006655310746282339\n",
      "Mean loss: 0.00943470816127956\n",
      "Std loss: 0.0022957502786190554\n",
      "Total Loss: 0.056608248967677355\n",
      "------------------------------------ epoch 8034 (48198 steps) ------------------------------------\n",
      "Max loss: 0.016961179673671722\n",
      "Min loss: 0.006271929480135441\n",
      "Mean loss: 0.009235878242179751\n",
      "Std loss: 0.00356359071834627\n",
      "Total Loss: 0.05541526945307851\n",
      "------------------------------------ epoch 8035 (48204 steps) ------------------------------------\n",
      "Max loss: 0.013339035212993622\n",
      "Min loss: 0.006116930395364761\n",
      "Mean loss: 0.008846002786109844\n",
      "Std loss: 0.0024506504489609916\n",
      "Total Loss: 0.05307601671665907\n",
      "------------------------------------ epoch 8036 (48210 steps) ------------------------------------\n",
      "Max loss: 0.013874009251594543\n",
      "Min loss: 0.00674344040453434\n",
      "Mean loss: 0.010102776965747276\n",
      "Std loss: 0.00240722527543753\n",
      "Total Loss: 0.06061666179448366\n",
      "------------------------------------ epoch 8037 (48216 steps) ------------------------------------\n",
      "Max loss: 0.011709382757544518\n",
      "Min loss: 0.006057607941329479\n",
      "Mean loss: 0.007804614491760731\n",
      "Std loss: 0.0018673611560058776\n",
      "Total Loss: 0.046827686950564384\n",
      "------------------------------------ epoch 8038 (48222 steps) ------------------------------------\n",
      "Max loss: 0.019406821578741074\n",
      "Min loss: 0.00651851762086153\n",
      "Mean loss: 0.00966472364962101\n",
      "Std loss: 0.004432190570775886\n",
      "Total Loss: 0.05798834189772606\n",
      "------------------------------------ epoch 8039 (48228 steps) ------------------------------------\n",
      "Max loss: 0.010709255933761597\n",
      "Min loss: 0.005401083268225193\n",
      "Mean loss: 0.007802131042505304\n",
      "Std loss: 0.0020800695511837505\n",
      "Total Loss: 0.046812786255031824\n",
      "------------------------------------ epoch 8040 (48234 steps) ------------------------------------\n",
      "Max loss: 0.04500962793827057\n",
      "Min loss: 0.007333062589168549\n",
      "Mean loss: 0.021094184834510088\n",
      "Std loss: 0.013726343841474729\n",
      "Total Loss: 0.12656510900706053\n",
      "------------------------------------ epoch 8041 (48240 steps) ------------------------------------\n",
      "Max loss: 0.021250348538160324\n",
      "Min loss: 0.0077843740582466125\n",
      "Mean loss: 0.015243360151847204\n",
      "Std loss: 0.004813413942103085\n",
      "Total Loss: 0.09146016091108322\n",
      "------------------------------------ epoch 8042 (48246 steps) ------------------------------------\n",
      "Max loss: 0.04212387651205063\n",
      "Min loss: 0.005294270813465118\n",
      "Mean loss: 0.01686650499080618\n",
      "Std loss: 0.01377361854265356\n",
      "Total Loss: 0.1011990299448371\n",
      "------------------------------------ epoch 8043 (48252 steps) ------------------------------------\n",
      "Max loss: 0.01795808970928192\n",
      "Min loss: 0.007229312788695097\n",
      "Mean loss: 0.012244646204635501\n",
      "Std loss: 0.0038240169585514105\n",
      "Total Loss: 0.073467877227813\n",
      "------------------------------------ epoch 8044 (48258 steps) ------------------------------------\n",
      "Max loss: 0.01214686781167984\n",
      "Min loss: 0.005749986972659826\n",
      "Mean loss: 0.009206370528166493\n",
      "Std loss: 0.0021976190915232088\n",
      "Total Loss: 0.05523822316899896\n",
      "------------------------------------ epoch 8045 (48264 steps) ------------------------------------\n",
      "Max loss: 0.02943246439099312\n",
      "Min loss: 0.004989306442439556\n",
      "Mean loss: 0.012976900519182285\n",
      "Std loss: 0.008417842860202196\n",
      "Total Loss: 0.07786140311509371\n",
      "------------------------------------ epoch 8046 (48270 steps) ------------------------------------\n",
      "Max loss: 0.009970277547836304\n",
      "Min loss: 0.006258475594222546\n",
      "Mean loss: 0.007549570485328634\n",
      "Std loss: 0.0012440861790668151\n",
      "Total Loss: 0.04529742291197181\n",
      "------------------------------------ epoch 8047 (48276 steps) ------------------------------------\n",
      "Max loss: 0.014923814684152603\n",
      "Min loss: 0.0053708842024207115\n",
      "Mean loss: 0.009081848741819462\n",
      "Std loss: 0.003348675448540022\n",
      "Total Loss: 0.05449109245091677\n",
      "------------------------------------ epoch 8048 (48282 steps) ------------------------------------\n",
      "Max loss: 0.017041951417922974\n",
      "Min loss: 0.005856554955244064\n",
      "Mean loss: 0.010659237702687582\n",
      "Std loss: 0.004027637741137233\n",
      "Total Loss: 0.06395542621612549\n",
      "------------------------------------ epoch 8049 (48288 steps) ------------------------------------\n",
      "Max loss: 0.015736421570181847\n",
      "Min loss: 0.006117680575698614\n",
      "Mean loss: 0.010534225885445872\n",
      "Std loss: 0.003128372469038277\n",
      "Total Loss: 0.06320535531267524\n",
      "------------------------------------ epoch 8050 (48294 steps) ------------------------------------\n",
      "Max loss: 0.024529315531253815\n",
      "Min loss: 0.005937495734542608\n",
      "Mean loss: 0.010247026725361744\n",
      "Std loss: 0.006610770354470944\n",
      "Total Loss: 0.06148216035217047\n",
      "------------------------------------ epoch 8051 (48300 steps) ------------------------------------\n",
      "Max loss: 0.015677524730563164\n",
      "Min loss: 0.006157710216939449\n",
      "Mean loss: 0.010429919852564732\n",
      "Std loss: 0.0031380657260814925\n",
      "Total Loss: 0.0625795191153884\n",
      "------------------------------------ epoch 8052 (48306 steps) ------------------------------------\n",
      "Max loss: 0.03515608236193657\n",
      "Min loss: 0.006572164595127106\n",
      "Mean loss: 0.015498929657042027\n",
      "Std loss: 0.009245619124580802\n",
      "Total Loss: 0.09299357794225216\n",
      "------------------------------------ epoch 8053 (48312 steps) ------------------------------------\n",
      "Max loss: 0.021585416048765182\n",
      "Min loss: 0.006337649188935757\n",
      "Mean loss: 0.010992155720790228\n",
      "Std loss: 0.005127705168797047\n",
      "Total Loss: 0.06595293432474136\n",
      "------------------------------------ epoch 8054 (48318 steps) ------------------------------------\n",
      "Max loss: 0.020175721496343613\n",
      "Min loss: 0.006195208989083767\n",
      "Mean loss: 0.010483756971855959\n",
      "Std loss: 0.004569150994778137\n",
      "Total Loss: 0.06290254183113575\n",
      "------------------------------------ epoch 8055 (48324 steps) ------------------------------------\n",
      "Max loss: 0.012151199392974377\n",
      "Min loss: 0.004952264949679375\n",
      "Mean loss: 0.008069729742904505\n",
      "Std loss: 0.002756873784478941\n",
      "Total Loss: 0.048418378457427025\n",
      "------------------------------------ epoch 8056 (48330 steps) ------------------------------------\n",
      "Max loss: 0.027112744748592377\n",
      "Min loss: 0.005550005938857794\n",
      "Mean loss: 0.01655351626686752\n",
      "Std loss: 0.008175106628714771\n",
      "Total Loss: 0.09932109760120511\n",
      "------------------------------------ epoch 8057 (48336 steps) ------------------------------------\n",
      "Max loss: 0.014012498781085014\n",
      "Min loss: 0.0062452382408082485\n",
      "Mean loss: 0.008165765088051558\n",
      "Std loss: 0.0027388591274895703\n",
      "Total Loss: 0.048994590528309345\n",
      "------------------------------------ epoch 8058 (48342 steps) ------------------------------------\n",
      "Max loss: 0.020423514768481255\n",
      "Min loss: 0.005020759534090757\n",
      "Mean loss: 0.011032607173547149\n",
      "Std loss: 0.00529937926671385\n",
      "Total Loss: 0.06619564304128289\n",
      "------------------------------------ epoch 8059 (48348 steps) ------------------------------------\n",
      "Max loss: 0.018720004707574844\n",
      "Min loss: 0.0062848953530192375\n",
      "Mean loss: 0.009366639346505204\n",
      "Std loss: 0.004272417940057868\n",
      "Total Loss: 0.05619983607903123\n",
      "------------------------------------ epoch 8060 (48354 steps) ------------------------------------\n",
      "Max loss: 0.03041858598589897\n",
      "Min loss: 0.006756966467946768\n",
      "Mean loss: 0.015092153179769715\n",
      "Std loss: 0.007309556207528444\n",
      "Total Loss: 0.09055291907861829\n",
      "------------------------------------ epoch 8061 (48360 steps) ------------------------------------\n",
      "Max loss: 0.009772565215826035\n",
      "Min loss: 0.005134535487741232\n",
      "Mean loss: 0.007302356030171116\n",
      "Std loss: 0.0019611351357890837\n",
      "Total Loss: 0.0438141361810267\n",
      "------------------------------------ epoch 8062 (48366 steps) ------------------------------------\n",
      "Max loss: 0.010277997702360153\n",
      "Min loss: 0.005483604501932859\n",
      "Mean loss: 0.007424871514861782\n",
      "Std loss: 0.0020204170936760713\n",
      "Total Loss: 0.044549229089170694\n",
      "------------------------------------ epoch 8063 (48372 steps) ------------------------------------\n",
      "Max loss: 0.011317318305373192\n",
      "Min loss: 0.005701350048184395\n",
      "Mean loss: 0.007670722048108776\n",
      "Std loss: 0.0019647204769481148\n",
      "Total Loss: 0.04602433228865266\n",
      "------------------------------------ epoch 8064 (48378 steps) ------------------------------------\n",
      "Max loss: 0.012275801971554756\n",
      "Min loss: 0.00561494892463088\n",
      "Mean loss: 0.007432468002662063\n",
      "Std loss: 0.0022121606110363876\n",
      "Total Loss: 0.044594808015972376\n",
      "------------------------------------ epoch 8065 (48384 steps) ------------------------------------\n",
      "Max loss: 0.03291454538702965\n",
      "Min loss: 0.006323717534542084\n",
      "Mean loss: 0.013363303150981665\n",
      "Std loss: 0.009023830171126623\n",
      "Total Loss: 0.08017981890588999\n",
      "------------------------------------ epoch 8066 (48390 steps) ------------------------------------\n",
      "Max loss: 0.013702834956347942\n",
      "Min loss: 0.005793627351522446\n",
      "Mean loss: 0.008606046127776304\n",
      "Std loss: 0.0025561853961783\n",
      "Total Loss: 0.05163627676665783\n",
      "------------------------------------ epoch 8067 (48396 steps) ------------------------------------\n",
      "Max loss: 0.029201455414295197\n",
      "Min loss: 0.00830856803804636\n",
      "Mean loss: 0.016796070771912735\n",
      "Std loss: 0.0076021312604020565\n",
      "Total Loss: 0.1007764246314764\n",
      "------------------------------------ epoch 8068 (48402 steps) ------------------------------------\n",
      "Max loss: 0.014249890111386776\n",
      "Min loss: 0.0063480837270617485\n",
      "Mean loss: 0.010008919363220533\n",
      "Std loss: 0.0030229979760052436\n",
      "Total Loss: 0.060053516179323196\n",
      "------------------------------------ epoch 8069 (48408 steps) ------------------------------------\n",
      "Max loss: 0.022280490025877953\n",
      "Min loss: 0.005573004484176636\n",
      "Mean loss: 0.01260788730966548\n",
      "Std loss: 0.006888952425423059\n",
      "Total Loss: 0.07564732385799289\n",
      "------------------------------------ epoch 8070 (48414 steps) ------------------------------------\n",
      "Max loss: 0.009911064058542252\n",
      "Min loss: 0.00487014465034008\n",
      "Mean loss: 0.006551867118105292\n",
      "Std loss: 0.0017807054471842352\n",
      "Total Loss: 0.039311202708631754\n",
      "------------------------------------ epoch 8071 (48420 steps) ------------------------------------\n",
      "Max loss: 0.023937474936246872\n",
      "Min loss: 0.00570503156632185\n",
      "Mean loss: 0.012588596902787685\n",
      "Std loss: 0.005887148949660202\n",
      "Total Loss: 0.07553158141672611\n",
      "------------------------------------ epoch 8072 (48426 steps) ------------------------------------\n",
      "Max loss: 0.009643368422985077\n",
      "Min loss: 0.005044982302933931\n",
      "Mean loss: 0.00726415915414691\n",
      "Std loss: 0.0016114097651639403\n",
      "Total Loss: 0.04358495492488146\n",
      "------------------------------------ epoch 8073 (48432 steps) ------------------------------------\n",
      "Max loss: 0.01268274150788784\n",
      "Min loss: 0.005318055860698223\n",
      "Mean loss: 0.008886985713616014\n",
      "Std loss: 0.0023935255851379447\n",
      "Total Loss: 0.05332191428169608\n",
      "------------------------------------ epoch 8074 (48438 steps) ------------------------------------\n",
      "Max loss: 0.00780060887336731\n",
      "Min loss: 0.00473519042134285\n",
      "Mean loss: 0.005975343209380905\n",
      "Std loss: 0.0010664525074162094\n",
      "Total Loss: 0.03585205925628543\n",
      "------------------------------------ epoch 8075 (48444 steps) ------------------------------------\n",
      "Max loss: 0.016898266971111298\n",
      "Min loss: 0.0059018200263381\n",
      "Mean loss: 0.010334521376838287\n",
      "Std loss: 0.003426742885520471\n",
      "Total Loss: 0.06200712826102972\n",
      "------------------------------------ epoch 8076 (48450 steps) ------------------------------------\n",
      "Max loss: 0.017533667385578156\n",
      "Min loss: 0.008534123189747334\n",
      "Mean loss: 0.01247046065206329\n",
      "Std loss: 0.0036167701895225856\n",
      "Total Loss: 0.07482276391237974\n",
      "------------------------------------ epoch 8077 (48456 steps) ------------------------------------\n",
      "Max loss: 0.013710684143006802\n",
      "Min loss: 0.004999453201889992\n",
      "Mean loss: 0.007856060285121202\n",
      "Std loss: 0.0030805783318524262\n",
      "Total Loss: 0.047136361710727215\n",
      "------------------------------------ epoch 8078 (48462 steps) ------------------------------------\n",
      "Max loss: 0.008617251180112362\n",
      "Min loss: 0.0046783871948719025\n",
      "Mean loss: 0.006534462329000235\n",
      "Std loss: 0.0013602598899615835\n",
      "Total Loss: 0.03920677397400141\n",
      "------------------------------------ epoch 8079 (48468 steps) ------------------------------------\n",
      "Max loss: 0.015136102214455605\n",
      "Min loss: 0.005272948183119297\n",
      "Mean loss: 0.008507112429166833\n",
      "Std loss: 0.003507743308014049\n",
      "Total Loss: 0.051042674575001\n",
      "------------------------------------ epoch 8080 (48474 steps) ------------------------------------\n",
      "Max loss: 0.016246961429715157\n",
      "Min loss: 0.005683386232703924\n",
      "Mean loss: 0.00971340985658268\n",
      "Std loss: 0.003641075051311585\n",
      "Total Loss: 0.05828045913949609\n",
      "------------------------------------ epoch 8081 (48480 steps) ------------------------------------\n",
      "Max loss: 0.011970143765211105\n",
      "Min loss: 0.004566683433949947\n",
      "Mean loss: 0.008752940765892466\n",
      "Std loss: 0.0027469980242646923\n",
      "Total Loss: 0.052517644595354795\n",
      "------------------------------------ epoch 8082 (48486 steps) ------------------------------------\n",
      "Max loss: 0.01543167419731617\n",
      "Min loss: 0.006433210335671902\n",
      "Mean loss: 0.00944492065658172\n",
      "Std loss: 0.003438913202061107\n",
      "Total Loss: 0.05666952393949032\n",
      "------------------------------------ epoch 8083 (48492 steps) ------------------------------------\n",
      "Max loss: 0.00869530439376831\n",
      "Min loss: 0.005257029086351395\n",
      "Mean loss: 0.00674557825550437\n",
      "Std loss: 0.0010886474629107998\n",
      "Total Loss: 0.04047346953302622\n",
      "------------------------------------ epoch 8084 (48498 steps) ------------------------------------\n",
      "Max loss: 0.017903296276926994\n",
      "Min loss: 0.0043370421044528484\n",
      "Mean loss: 0.008856688626110554\n",
      "Std loss: 0.004868647427172055\n",
      "Total Loss: 0.05314013175666332\n",
      "------------------------------------ epoch 8085 (48504 steps) ------------------------------------\n",
      "Max loss: 0.014899520203471184\n",
      "Min loss: 0.004398066084831953\n",
      "Mean loss: 0.009243575623258948\n",
      "Std loss: 0.003473494013776207\n",
      "Total Loss: 0.05546145373955369\n",
      "------------------------------------ epoch 8086 (48510 steps) ------------------------------------\n",
      "Max loss: 0.023831402882933617\n",
      "Min loss: 0.004920812323689461\n",
      "Mean loss: 0.010700763668864965\n",
      "Std loss: 0.0063054668104015185\n",
      "Total Loss: 0.06420458201318979\n",
      "------------------------------------ epoch 8087 (48516 steps) ------------------------------------\n",
      "Max loss: 0.008827688172459602\n",
      "Min loss: 0.004575550090521574\n",
      "Mean loss: 0.006935048460339506\n",
      "Std loss: 0.001662849861041068\n",
      "Total Loss: 0.04161029076203704\n",
      "------------------------------------ epoch 8088 (48522 steps) ------------------------------------\n",
      "Max loss: 0.008521504700183868\n",
      "Min loss: 0.0053214142099022865\n",
      "Mean loss: 0.006375847849994898\n",
      "Std loss: 0.0010345757185412808\n",
      "Total Loss: 0.03825508709996939\n",
      "------------------------------------ epoch 8089 (48528 steps) ------------------------------------\n",
      "Max loss: 0.015240627340972424\n",
      "Min loss: 0.0063342805951833725\n",
      "Mean loss: 0.008444625341023007\n",
      "Std loss: 0.0031102758915865637\n",
      "Total Loss: 0.05066775204613805\n",
      "------------------------------------ epoch 8090 (48534 steps) ------------------------------------\n",
      "Max loss: 0.02200508676469326\n",
      "Min loss: 0.0049366215243935585\n",
      "Mean loss: 0.011267147570227584\n",
      "Std loss: 0.0061137345323866695\n",
      "Total Loss: 0.0676028854213655\n",
      "------------------------------------ epoch 8091 (48540 steps) ------------------------------------\n",
      "Max loss: 0.010499168187379837\n",
      "Min loss: 0.004859035834670067\n",
      "Mean loss: 0.007589785925423105\n",
      "Std loss: 0.0025262952729254197\n",
      "Total Loss: 0.04553871555253863\n",
      "------------------------------------ epoch 8092 (48546 steps) ------------------------------------\n",
      "Max loss: 0.02081836387515068\n",
      "Min loss: 0.004771105013787746\n",
      "Mean loss: 0.010555921355262399\n",
      "Std loss: 0.005548635490982552\n",
      "Total Loss: 0.06333552813157439\n",
      "------------------------------------ epoch 8093 (48552 steps) ------------------------------------\n",
      "Max loss: 0.031043952330946922\n",
      "Min loss: 0.005813281517475843\n",
      "Mean loss: 0.012550977136318883\n",
      "Std loss: 0.009138918035611364\n",
      "Total Loss: 0.0753058628179133\n",
      "------------------------------------ epoch 8094 (48558 steps) ------------------------------------\n",
      "Max loss: 0.013558637350797653\n",
      "Min loss: 0.0049157338216900826\n",
      "Mean loss: 0.008007130352780223\n",
      "Std loss: 0.002844999540500557\n",
      "Total Loss: 0.04804278211668134\n",
      "------------------------------------ epoch 8095 (48564 steps) ------------------------------------\n",
      "Max loss: 0.021870290860533714\n",
      "Min loss: 0.005222253035753965\n",
      "Mean loss: 0.010070817700276772\n",
      "Std loss: 0.005526096825501991\n",
      "Total Loss: 0.06042490620166063\n",
      "------------------------------------ epoch 8096 (48570 steps) ------------------------------------\n",
      "Max loss: 0.01124737411737442\n",
      "Min loss: 0.005054932087659836\n",
      "Mean loss: 0.007566819122682015\n",
      "Std loss: 0.002162090606477858\n",
      "Total Loss: 0.04540091473609209\n",
      "------------------------------------ epoch 8097 (48576 steps) ------------------------------------\n",
      "Max loss: 0.013521037064492702\n",
      "Min loss: 0.004843135364353657\n",
      "Mean loss: 0.00890524072262148\n",
      "Std loss: 0.0030615473231406314\n",
      "Total Loss: 0.053431444335728884\n",
      "------------------------------------ epoch 8098 (48582 steps) ------------------------------------\n",
      "Max loss: 0.026679236441850662\n",
      "Min loss: 0.004783302545547485\n",
      "Mean loss: 0.011786869804685315\n",
      "Std loss: 0.007858790586601647\n",
      "Total Loss: 0.07072121882811189\n",
      "------------------------------------ epoch 8099 (48588 steps) ------------------------------------\n",
      "Max loss: 0.011117215268313885\n",
      "Min loss: 0.0042875176295638084\n",
      "Mean loss: 0.00702994045180579\n",
      "Std loss: 0.002292850388544695\n",
      "Total Loss: 0.04217964271083474\n",
      "------------------------------------ epoch 8100 (48594 steps) ------------------------------------\n",
      "Max loss: 0.009123507887125015\n",
      "Min loss: 0.005560164339840412\n",
      "Mean loss: 0.007344201517601808\n",
      "Std loss: 0.0010946634864563206\n",
      "Total Loss: 0.04406520910561085\n",
      "------------------------------------ epoch 8101 (48600 steps) ------------------------------------\n",
      "Max loss: 0.013931738212704659\n",
      "Min loss: 0.004319294821470976\n",
      "Mean loss: 0.006929591918985049\n",
      "Std loss: 0.003187007782422398\n",
      "Total Loss: 0.041577551513910294\n",
      "saved model at ./weights/model_8101.pth\n",
      "------------------------------------ epoch 8102 (48606 steps) ------------------------------------\n",
      "Max loss: 0.01293868850916624\n",
      "Min loss: 0.004337985999882221\n",
      "Mean loss: 0.007586477246756355\n",
      "Std loss: 0.003312341487727271\n",
      "Total Loss: 0.04551886348053813\n",
      "------------------------------------ epoch 8103 (48612 steps) ------------------------------------\n",
      "Max loss: 0.01787884160876274\n",
      "Min loss: 0.004882003180682659\n",
      "Mean loss: 0.008864794004087647\n",
      "Std loss: 0.004373977013291297\n",
      "Total Loss: 0.05318876402452588\n",
      "------------------------------------ epoch 8104 (48618 steps) ------------------------------------\n",
      "Max loss: 0.011530861258506775\n",
      "Min loss: 0.004568747244775295\n",
      "Mean loss: 0.007876267501463493\n",
      "Std loss: 0.0022591777828959727\n",
      "Total Loss: 0.047257605008780956\n",
      "------------------------------------ epoch 8105 (48624 steps) ------------------------------------\n",
      "Max loss: 0.012088390998542309\n",
      "Min loss: 0.003993513993918896\n",
      "Mean loss: 0.00750436734718581\n",
      "Std loss: 0.0027411952006587427\n",
      "Total Loss: 0.04502620408311486\n",
      "------------------------------------ epoch 8106 (48630 steps) ------------------------------------\n",
      "Max loss: 0.012906305491924286\n",
      "Min loss: 0.004088607616722584\n",
      "Mean loss: 0.007247097867851456\n",
      "Std loss: 0.0030880914025284297\n",
      "Total Loss: 0.043482587207108736\n",
      "------------------------------------ epoch 8107 (48636 steps) ------------------------------------\n",
      "Max loss: 0.021559257060289383\n",
      "Min loss: 0.004931323230266571\n",
      "Mean loss: 0.008966175373643637\n",
      "Std loss: 0.0057385029304706766\n",
      "Total Loss: 0.05379705224186182\n",
      "------------------------------------ epoch 8108 (48642 steps) ------------------------------------\n",
      "Max loss: 0.02509332448244095\n",
      "Min loss: 0.0067095085978507996\n",
      "Mean loss: 0.012969319242984056\n",
      "Std loss: 0.006289746947020893\n",
      "Total Loss: 0.07781591545790434\n",
      "------------------------------------ epoch 8109 (48648 steps) ------------------------------------\n",
      "Max loss: 0.014807939529418945\n",
      "Min loss: 0.0053577544167637825\n",
      "Mean loss: 0.007458287679279844\n",
      "Std loss: 0.003416779901269627\n",
      "Total Loss: 0.044749726075679064\n",
      "------------------------------------ epoch 8110 (48654 steps) ------------------------------------\n",
      "Max loss: 0.014700271189212799\n",
      "Min loss: 0.006475219503045082\n",
      "Mean loss: 0.009109917950506011\n",
      "Std loss: 0.0029204537084574858\n",
      "Total Loss: 0.05465950770303607\n",
      "------------------------------------ epoch 8111 (48660 steps) ------------------------------------\n",
      "Max loss: 0.016448764130473137\n",
      "Min loss: 0.0045240493491292\n",
      "Mean loss: 0.00890453850540022\n",
      "Std loss: 0.004045276075853495\n",
      "Total Loss: 0.05342723103240132\n",
      "------------------------------------ epoch 8112 (48666 steps) ------------------------------------\n",
      "Max loss: 0.023104578256607056\n",
      "Min loss: 0.004802566021680832\n",
      "Mean loss: 0.010346131088833014\n",
      "Std loss: 0.006141146000778827\n",
      "Total Loss: 0.062076786532998085\n",
      "------------------------------------ epoch 8113 (48672 steps) ------------------------------------\n",
      "Max loss: 0.03964861109852791\n",
      "Min loss: 0.0046904198825359344\n",
      "Mean loss: 0.014234706371401748\n",
      "Std loss: 0.011982162919499302\n",
      "Total Loss: 0.08540823822841048\n",
      "------------------------------------ epoch 8114 (48678 steps) ------------------------------------\n",
      "Max loss: 0.012505832128226757\n",
      "Min loss: 0.005848690867424011\n",
      "Mean loss: 0.009019172905633846\n",
      "Std loss: 0.002640842612876668\n",
      "Total Loss: 0.05411503743380308\n",
      "------------------------------------ epoch 8115 (48684 steps) ------------------------------------\n",
      "Max loss: 0.03049289993941784\n",
      "Min loss: 0.008005961775779724\n",
      "Mean loss: 0.014385719783604145\n",
      "Std loss: 0.008172426585124366\n",
      "Total Loss: 0.08631431870162487\n",
      "------------------------------------ epoch 8116 (48690 steps) ------------------------------------\n",
      "Max loss: 0.019674470648169518\n",
      "Min loss: 0.005483269691467285\n",
      "Mean loss: 0.010564148736496767\n",
      "Std loss: 0.0046877176989044275\n",
      "Total Loss: 0.0633848924189806\n",
      "------------------------------------ epoch 8117 (48696 steps) ------------------------------------\n",
      "Max loss: 0.021127259358763695\n",
      "Min loss: 0.004829765297472477\n",
      "Mean loss: 0.00831567084727188\n",
      "Std loss: 0.005800966511179987\n",
      "Total Loss: 0.04989402508363128\n",
      "------------------------------------ epoch 8118 (48702 steps) ------------------------------------\n",
      "Max loss: 0.013968143612146378\n",
      "Min loss: 0.0056284451857209206\n",
      "Mean loss: 0.008662393704677621\n",
      "Std loss: 0.002642379435983682\n",
      "Total Loss: 0.05197436222806573\n",
      "------------------------------------ epoch 8119 (48708 steps) ------------------------------------\n",
      "Max loss: 0.00985303521156311\n",
      "Min loss: 0.005218617618083954\n",
      "Mean loss: 0.007557871130605538\n",
      "Std loss: 0.0018828782361804293\n",
      "Total Loss: 0.04534722678363323\n",
      "------------------------------------ epoch 8120 (48714 steps) ------------------------------------\n",
      "Max loss: 0.01940111070871353\n",
      "Min loss: 0.006360236555337906\n",
      "Mean loss: 0.009986461140215397\n",
      "Std loss: 0.004372023440242014\n",
      "Total Loss: 0.05991876684129238\n",
      "------------------------------------ epoch 8121 (48720 steps) ------------------------------------\n",
      "Max loss: 0.03132101893424988\n",
      "Min loss: 0.004343764390796423\n",
      "Mean loss: 0.010948576110725602\n",
      "Std loss: 0.009212326554318367\n",
      "Total Loss: 0.06569145666435361\n",
      "------------------------------------ epoch 8122 (48726 steps) ------------------------------------\n",
      "Max loss: 0.008846469223499298\n",
      "Min loss: 0.004477884620428085\n",
      "Mean loss: 0.007288345601409674\n",
      "Std loss: 0.001611025425697565\n",
      "Total Loss: 0.04373007360845804\n",
      "------------------------------------ epoch 8123 (48732 steps) ------------------------------------\n",
      "Max loss: 0.011165646836161613\n",
      "Min loss: 0.004736483097076416\n",
      "Mean loss: 0.006512464645008246\n",
      "Std loss: 0.002192052987028096\n",
      "Total Loss: 0.03907478787004948\n",
      "------------------------------------ epoch 8124 (48738 steps) ------------------------------------\n",
      "Max loss: 0.009241463616490364\n",
      "Min loss: 0.004174469970166683\n",
      "Mean loss: 0.006589488436778386\n",
      "Std loss: 0.0022118957146496842\n",
      "Total Loss: 0.03953693062067032\n",
      "------------------------------------ epoch 8125 (48744 steps) ------------------------------------\n",
      "Max loss: 0.011072809807956219\n",
      "Min loss: 0.005298325791954994\n",
      "Mean loss: 0.0068288651915887994\n",
      "Std loss: 0.001961011678889625\n",
      "Total Loss: 0.040973191149532795\n",
      "------------------------------------ epoch 8126 (48750 steps) ------------------------------------\n",
      "Max loss: 0.0120720025151968\n",
      "Min loss: 0.004594387020915747\n",
      "Mean loss: 0.0070056491531431675\n",
      "Std loss: 0.002712448085340833\n",
      "Total Loss: 0.042033894918859005\n",
      "------------------------------------ epoch 8127 (48756 steps) ------------------------------------\n",
      "Max loss: 0.008796975016593933\n",
      "Min loss: 0.004788187798112631\n",
      "Mean loss: 0.006254172263046105\n",
      "Std loss: 0.001488851667020656\n",
      "Total Loss: 0.037525033578276634\n",
      "------------------------------------ epoch 8128 (48762 steps) ------------------------------------\n",
      "Max loss: 0.010613957419991493\n",
      "Min loss: 0.004005080088973045\n",
      "Mean loss: 0.00576169944057862\n",
      "Std loss: 0.0022196360142397948\n",
      "Total Loss: 0.03457019664347172\n",
      "------------------------------------ epoch 8129 (48768 steps) ------------------------------------\n",
      "Max loss: 0.029495498165488243\n",
      "Min loss: 0.004923166707158089\n",
      "Mean loss: 0.011137351548920075\n",
      "Std loss: 0.008674607492627287\n",
      "Total Loss: 0.06682410929352045\n",
      "------------------------------------ epoch 8130 (48774 steps) ------------------------------------\n",
      "Max loss: 0.012967847287654877\n",
      "Min loss: 0.004060658626258373\n",
      "Mean loss: 0.007322731195017695\n",
      "Std loss: 0.0037193720244148207\n",
      "Total Loss: 0.04393638717010617\n",
      "------------------------------------ epoch 8131 (48780 steps) ------------------------------------\n",
      "Max loss: 0.015676161274313927\n",
      "Min loss: 0.00634441152215004\n",
      "Mean loss: 0.008714485059802731\n",
      "Std loss: 0.003264638582068309\n",
      "Total Loss: 0.052286910358816385\n",
      "------------------------------------ epoch 8132 (48786 steps) ------------------------------------\n",
      "Max loss: 0.02595885843038559\n",
      "Min loss: 0.005511949770152569\n",
      "Mean loss: 0.011105122277513146\n",
      "Std loss: 0.007351380376011075\n",
      "Total Loss: 0.06663073366507888\n",
      "------------------------------------ epoch 8133 (48792 steps) ------------------------------------\n",
      "Max loss: 0.014507696032524109\n",
      "Min loss: 0.005266986787319183\n",
      "Mean loss: 0.010647913441061974\n",
      "Std loss: 0.003500431881000393\n",
      "Total Loss: 0.06388748064637184\n",
      "------------------------------------ epoch 8134 (48798 steps) ------------------------------------\n",
      "Max loss: 0.014682083390653133\n",
      "Min loss: 0.005175563506782055\n",
      "Mean loss: 0.009090464484567443\n",
      "Std loss: 0.003544860917366739\n",
      "Total Loss: 0.05454278690740466\n",
      "------------------------------------ epoch 8135 (48804 steps) ------------------------------------\n",
      "Max loss: 0.008922192268073559\n",
      "Min loss: 0.004232273902744055\n",
      "Mean loss: 0.006338025365645687\n",
      "Std loss: 0.0015298677720617237\n",
      "Total Loss: 0.03802815219387412\n",
      "------------------------------------ epoch 8136 (48810 steps) ------------------------------------\n",
      "Max loss: 0.010481810197234154\n",
      "Min loss: 0.004970032721757889\n",
      "Mean loss: 0.00735779005723695\n",
      "Std loss: 0.001820941716146783\n",
      "Total Loss: 0.0441467403434217\n",
      "------------------------------------ epoch 8137 (48816 steps) ------------------------------------\n",
      "Max loss: 0.032238833606243134\n",
      "Min loss: 0.004317028913646936\n",
      "Mean loss: 0.010907725663855672\n",
      "Std loss: 0.009826591254868525\n",
      "Total Loss: 0.06544635398313403\n",
      "------------------------------------ epoch 8138 (48822 steps) ------------------------------------\n",
      "Max loss: 0.015817658975720406\n",
      "Min loss: 0.004276527091860771\n",
      "Mean loss: 0.008655608786890904\n",
      "Std loss: 0.004302261522347847\n",
      "Total Loss: 0.051933652721345425\n",
      "------------------------------------ epoch 8139 (48828 steps) ------------------------------------\n",
      "Max loss: 0.01992468163371086\n",
      "Min loss: 0.004619404673576355\n",
      "Mean loss: 0.009440058066199223\n",
      "Std loss: 0.005205223059247052\n",
      "Total Loss: 0.05664034839719534\n",
      "------------------------------------ epoch 8140 (48834 steps) ------------------------------------\n",
      "Max loss: 0.021705983206629753\n",
      "Min loss: 0.005403617396950722\n",
      "Mean loss: 0.011591817019507289\n",
      "Std loss: 0.005431948393632353\n",
      "Total Loss: 0.06955090211704373\n",
      "------------------------------------ epoch 8141 (48840 steps) ------------------------------------\n",
      "Max loss: 0.017158953472971916\n",
      "Min loss: 0.004885806702077389\n",
      "Mean loss: 0.008246255728105703\n",
      "Std loss: 0.004104363274979804\n",
      "Total Loss: 0.049477534368634224\n",
      "------------------------------------ epoch 8142 (48846 steps) ------------------------------------\n",
      "Max loss: 0.0068310704082250595\n",
      "Min loss: 0.0046979887410998344\n",
      "Mean loss: 0.006066028804828723\n",
      "Std loss: 0.0007932218473119009\n",
      "Total Loss: 0.03639617282897234\n",
      "------------------------------------ epoch 8143 (48852 steps) ------------------------------------\n",
      "Max loss: 0.007820669561624527\n",
      "Min loss: 0.004174681846052408\n",
      "Mean loss: 0.005923305327693622\n",
      "Std loss: 0.001118623253364173\n",
      "Total Loss: 0.03553983196616173\n",
      "------------------------------------ epoch 8144 (48858 steps) ------------------------------------\n",
      "Max loss: 0.014008212834596634\n",
      "Min loss: 0.004404203500598669\n",
      "Mean loss: 0.008169279589007298\n",
      "Std loss: 0.0040286020513394\n",
      "Total Loss: 0.04901567753404379\n",
      "------------------------------------ epoch 8145 (48864 steps) ------------------------------------\n",
      "Max loss: 0.008107032626867294\n",
      "Min loss: 0.004053396638482809\n",
      "Mean loss: 0.006155351875349879\n",
      "Std loss: 0.0015298840267561813\n",
      "Total Loss: 0.036932111252099276\n",
      "------------------------------------ epoch 8146 (48870 steps) ------------------------------------\n",
      "Max loss: 0.02191948890686035\n",
      "Min loss: 0.004119946621358395\n",
      "Mean loss: 0.007572671088079612\n",
      "Std loss: 0.006442209588447612\n",
      "Total Loss: 0.04543602652847767\n",
      "------------------------------------ epoch 8147 (48876 steps) ------------------------------------\n",
      "Max loss: 0.012000768445432186\n",
      "Min loss: 0.005132727790623903\n",
      "Mean loss: 0.009196813606346646\n",
      "Std loss: 0.002584570092453924\n",
      "Total Loss: 0.05518088163807988\n",
      "------------------------------------ epoch 8148 (48882 steps) ------------------------------------\n",
      "Max loss: 0.016290729865431786\n",
      "Min loss: 0.004806668497622013\n",
      "Mean loss: 0.010272579500451684\n",
      "Std loss: 0.004350644370689361\n",
      "Total Loss: 0.061635477002710104\n",
      "------------------------------------ epoch 8149 (48888 steps) ------------------------------------\n",
      "Max loss: 0.039345886558294296\n",
      "Min loss: 0.004141218028962612\n",
      "Mean loss: 0.01268549271238347\n",
      "Std loss: 0.012505746748606716\n",
      "Total Loss: 0.07611295627430081\n",
      "------------------------------------ epoch 8150 (48894 steps) ------------------------------------\n",
      "Max loss: 0.023398272693157196\n",
      "Min loss: 0.007702029775828123\n",
      "Mean loss: 0.013396929251030087\n",
      "Std loss: 0.004927877920053384\n",
      "Total Loss: 0.08038157550618052\n",
      "------------------------------------ epoch 8151 (48900 steps) ------------------------------------\n",
      "Max loss: 0.07175952941179276\n",
      "Min loss: 0.00578674953430891\n",
      "Mean loss: 0.031065851915627718\n",
      "Std loss: 0.02207720652159503\n",
      "Total Loss: 0.1863951114937663\n",
      "------------------------------------ epoch 8152 (48906 steps) ------------------------------------\n",
      "Max loss: 0.01529424637556076\n",
      "Min loss: 0.007956752553582191\n",
      "Mean loss: 0.011595237844934067\n",
      "Std loss: 0.0030573358016108883\n",
      "Total Loss: 0.0695714270696044\n",
      "------------------------------------ epoch 8153 (48912 steps) ------------------------------------\n",
      "Max loss: 0.01650187373161316\n",
      "Min loss: 0.005318382289260626\n",
      "Mean loss: 0.01010565374357005\n",
      "Std loss: 0.0037311259417800577\n",
      "Total Loss: 0.0606339224614203\n",
      "------------------------------------ epoch 8154 (48918 steps) ------------------------------------\n",
      "Max loss: 0.023498084396123886\n",
      "Min loss: 0.005929702427238226\n",
      "Mean loss: 0.012871782683456937\n",
      "Std loss: 0.006048104601783842\n",
      "Total Loss: 0.07723069610074162\n",
      "------------------------------------ epoch 8155 (48924 steps) ------------------------------------\n",
      "Max loss: 0.015323324128985405\n",
      "Min loss: 0.007171450182795525\n",
      "Mean loss: 0.011340905136118332\n",
      "Std loss: 0.002914879777666205\n",
      "Total Loss: 0.06804543081671\n",
      "------------------------------------ epoch 8156 (48930 steps) ------------------------------------\n",
      "Max loss: 0.013827498070895672\n",
      "Min loss: 0.0051835887134075165\n",
      "Mean loss: 0.008411312087749442\n",
      "Std loss: 0.0027853909249273665\n",
      "Total Loss: 0.05046787252649665\n",
      "------------------------------------ epoch 8157 (48936 steps) ------------------------------------\n",
      "Max loss: 0.008224391378462315\n",
      "Min loss: 0.006110319402068853\n",
      "Mean loss: 0.006787945671627919\n",
      "Std loss: 0.0006778802497015954\n",
      "Total Loss: 0.04072767402976751\n",
      "------------------------------------ epoch 8158 (48942 steps) ------------------------------------\n",
      "Max loss: 0.014304136857390404\n",
      "Min loss: 0.005514931865036488\n",
      "Mean loss: 0.010534512888019284\n",
      "Std loss: 0.003399290032402182\n",
      "Total Loss: 0.0632070773281157\n",
      "------------------------------------ epoch 8159 (48948 steps) ------------------------------------\n",
      "Max loss: 0.011896824464201927\n",
      "Min loss: 0.004709943663328886\n",
      "Mean loss: 0.00705527871226271\n",
      "Std loss: 0.002613161006204941\n",
      "Total Loss: 0.04233167227357626\n",
      "------------------------------------ epoch 8160 (48954 steps) ------------------------------------\n",
      "Max loss: 0.026121890172362328\n",
      "Min loss: 0.006673276424407959\n",
      "Mean loss: 0.01413438655436039\n",
      "Std loss: 0.007761566501719098\n",
      "Total Loss: 0.08480631932616234\n",
      "------------------------------------ epoch 8161 (48960 steps) ------------------------------------\n",
      "Max loss: 0.009335055947303772\n",
      "Min loss: 0.004365577362477779\n",
      "Mean loss: 0.00627214671112597\n",
      "Std loss: 0.0016582361174025328\n",
      "Total Loss: 0.03763288026675582\n",
      "------------------------------------ epoch 8162 (48966 steps) ------------------------------------\n",
      "Max loss: 0.014608537778258324\n",
      "Min loss: 0.004522770177572966\n",
      "Mean loss: 0.008138003448645273\n",
      "Std loss: 0.0036836651174772706\n",
      "Total Loss: 0.04882802069187164\n",
      "------------------------------------ epoch 8163 (48972 steps) ------------------------------------\n",
      "Max loss: 0.014164172112941742\n",
      "Min loss: 0.00446603586897254\n",
      "Mean loss: 0.007950514477367202\n",
      "Std loss: 0.003275640715748418\n",
      "Total Loss: 0.047703086864203215\n",
      "------------------------------------ epoch 8164 (48978 steps) ------------------------------------\n",
      "Max loss: 0.007980039343237877\n",
      "Min loss: 0.00441588694229722\n",
      "Mean loss: 0.0058559162231783075\n",
      "Std loss: 0.0012056260894010625\n",
      "Total Loss: 0.03513549733906984\n",
      "------------------------------------ epoch 8165 (48984 steps) ------------------------------------\n",
      "Max loss: 0.0108720064163208\n",
      "Min loss: 0.005110688507556915\n",
      "Mean loss: 0.006867740380888184\n",
      "Std loss: 0.001960020218049054\n",
      "Total Loss: 0.041206442285329103\n",
      "------------------------------------ epoch 8166 (48990 steps) ------------------------------------\n",
      "Max loss: 0.010684012435376644\n",
      "Min loss: 0.004681998863816261\n",
      "Mean loss: 0.008129636524245143\n",
      "Std loss: 0.0023312694512395604\n",
      "Total Loss: 0.04877781914547086\n",
      "------------------------------------ epoch 8167 (48996 steps) ------------------------------------\n",
      "Max loss: 0.02031744457781315\n",
      "Min loss: 0.0050813378766179085\n",
      "Mean loss: 0.010031754694258174\n",
      "Std loss: 0.0052411786597149\n",
      "Total Loss: 0.06019052816554904\n",
      "------------------------------------ epoch 8168 (49002 steps) ------------------------------------\n",
      "Max loss: 0.011168112978339195\n",
      "Min loss: 0.004840811248868704\n",
      "Mean loss: 0.007679665694013238\n",
      "Std loss: 0.0021294223672593993\n",
      "Total Loss: 0.04607799416407943\n",
      "------------------------------------ epoch 8169 (49008 steps) ------------------------------------\n",
      "Max loss: 0.007576962001621723\n",
      "Min loss: 0.004144173581153154\n",
      "Mean loss: 0.0056641857760647936\n",
      "Std loss: 0.001086731035997751\n",
      "Total Loss: 0.03398511465638876\n",
      "------------------------------------ epoch 8170 (49014 steps) ------------------------------------\n",
      "Max loss: 0.03175469487905502\n",
      "Min loss: 0.003836757969111204\n",
      "Mean loss: 0.011374164139851928\n",
      "Std loss: 0.009512377308404216\n",
      "Total Loss: 0.06824498483911157\n",
      "------------------------------------ epoch 8171 (49020 steps) ------------------------------------\n",
      "Max loss: 0.014036843553185463\n",
      "Min loss: 0.003985276445746422\n",
      "Mean loss: 0.007923500146716833\n",
      "Std loss: 0.0031233684099430527\n",
      "Total Loss: 0.047541000880301\n",
      "------------------------------------ epoch 8172 (49026 steps) ------------------------------------\n",
      "Max loss: 0.016792071983218193\n",
      "Min loss: 0.0059388792142271996\n",
      "Mean loss: 0.010174830521767339\n",
      "Std loss: 0.003443204836919636\n",
      "Total Loss: 0.06104898313060403\n",
      "------------------------------------ epoch 8173 (49032 steps) ------------------------------------\n",
      "Max loss: 0.018329618498682976\n",
      "Min loss: 0.004413561895489693\n",
      "Mean loss: 0.010516557764882842\n",
      "Std loss: 0.0051905913527905845\n",
      "Total Loss: 0.06309934658929706\n",
      "------------------------------------ epoch 8174 (49038 steps) ------------------------------------\n",
      "Max loss: 0.03582179173827171\n",
      "Min loss: 0.00433870404958725\n",
      "Mean loss: 0.014347295121600231\n",
      "Std loss: 0.010496434485252934\n",
      "Total Loss: 0.08608377072960138\n",
      "------------------------------------ epoch 8175 (49044 steps) ------------------------------------\n",
      "Max loss: 0.03234792873263359\n",
      "Min loss: 0.004837479442358017\n",
      "Mean loss: 0.011249472154304385\n",
      "Std loss: 0.009565752170609112\n",
      "Total Loss: 0.06749683292582631\n",
      "------------------------------------ epoch 8176 (49050 steps) ------------------------------------\n",
      "Max loss: 0.011385041289031506\n",
      "Min loss: 0.0047242892906069756\n",
      "Mean loss: 0.007579286660378178\n",
      "Std loss: 0.00197788666022431\n",
      "Total Loss: 0.04547571996226907\n",
      "------------------------------------ epoch 8177 (49056 steps) ------------------------------------\n",
      "Max loss: 0.01349513791501522\n",
      "Min loss: 0.005561438854783773\n",
      "Mean loss: 0.008522629582633575\n",
      "Std loss: 0.002946349158296939\n",
      "Total Loss: 0.05113577749580145\n",
      "------------------------------------ epoch 8178 (49062 steps) ------------------------------------\n",
      "Max loss: 0.017595166340470314\n",
      "Min loss: 0.004137285053730011\n",
      "Mean loss: 0.008417259979372224\n",
      "Std loss: 0.004754348728558583\n",
      "Total Loss: 0.05050355987623334\n",
      "------------------------------------ epoch 8179 (49068 steps) ------------------------------------\n",
      "Max loss: 0.023942679166793823\n",
      "Min loss: 0.005321021657437086\n",
      "Mean loss: 0.009162835932026306\n",
      "Std loss: 0.006712709445239666\n",
      "Total Loss: 0.05497701559215784\n",
      "------------------------------------ epoch 8180 (49074 steps) ------------------------------------\n",
      "Max loss: 0.03943406790494919\n",
      "Min loss: 0.005530192516744137\n",
      "Mean loss: 0.017586509541918833\n",
      "Std loss: 0.012829954536952091\n",
      "Total Loss: 0.105519057251513\n",
      "------------------------------------ epoch 8181 (49080 steps) ------------------------------------\n",
      "Max loss: 0.008403286337852478\n",
      "Min loss: 0.005129748024046421\n",
      "Mean loss: 0.006666084673876564\n",
      "Std loss: 0.0012817255765293187\n",
      "Total Loss: 0.03999650804325938\n",
      "------------------------------------ epoch 8182 (49086 steps) ------------------------------------\n",
      "Max loss: 0.019327688962221146\n",
      "Min loss: 0.005089001730084419\n",
      "Mean loss: 0.008970000082626939\n",
      "Std loss: 0.0048913064867314214\n",
      "Total Loss: 0.05382000049576163\n",
      "------------------------------------ epoch 8183 (49092 steps) ------------------------------------\n",
      "Max loss: 0.023043107241392136\n",
      "Min loss: 0.0070970384404063225\n",
      "Mean loss: 0.011346251936629415\n",
      "Std loss: 0.005540618255251507\n",
      "Total Loss: 0.06807751161977649\n",
      "------------------------------------ epoch 8184 (49098 steps) ------------------------------------\n",
      "Max loss: 0.017028339207172394\n",
      "Min loss: 0.005075840279459953\n",
      "Mean loss: 0.010185731264452139\n",
      "Std loss: 0.004065154965346452\n",
      "Total Loss: 0.06111438758671284\n",
      "------------------------------------ epoch 8185 (49104 steps) ------------------------------------\n",
      "Max loss: 0.013252492994070053\n",
      "Min loss: 0.004658770747482777\n",
      "Mean loss: 0.007604849990457296\n",
      "Std loss: 0.0030285769880824738\n",
      "Total Loss: 0.04562909994274378\n",
      "------------------------------------ epoch 8186 (49110 steps) ------------------------------------\n",
      "Max loss: 0.012652087956666946\n",
      "Min loss: 0.0050565809942781925\n",
      "Mean loss: 0.007368273877849181\n",
      "Std loss: 0.0024488329279075418\n",
      "Total Loss: 0.04420964326709509\n",
      "------------------------------------ epoch 8187 (49116 steps) ------------------------------------\n",
      "Max loss: 0.020180322229862213\n",
      "Min loss: 0.003695844439789653\n",
      "Mean loss: 0.009130863201183578\n",
      "Std loss: 0.005754451370644312\n",
      "Total Loss: 0.054785179207101464\n",
      "------------------------------------ epoch 8188 (49122 steps) ------------------------------------\n",
      "Max loss: 0.009419912472367287\n",
      "Min loss: 0.004151212517172098\n",
      "Mean loss: 0.006237884207318227\n",
      "Std loss: 0.001845358094956551\n",
      "Total Loss: 0.03742730524390936\n",
      "------------------------------------ epoch 8189 (49128 steps) ------------------------------------\n",
      "Max loss: 0.01591324806213379\n",
      "Min loss: 0.004285744391381741\n",
      "Mean loss: 0.008506365042800704\n",
      "Std loss: 0.004706237422907869\n",
      "Total Loss: 0.05103819025680423\n",
      "------------------------------------ epoch 8190 (49134 steps) ------------------------------------\n",
      "Max loss: 0.029487788677215576\n",
      "Min loss: 0.0045728497207164764\n",
      "Mean loss: 0.0093752796916912\n",
      "Std loss: 0.00902617181015097\n",
      "Total Loss: 0.0562516781501472\n",
      "------------------------------------ epoch 8191 (49140 steps) ------------------------------------\n",
      "Max loss: 0.01468552928417921\n",
      "Min loss: 0.0059311166405677795\n",
      "Mean loss: 0.00910698218892018\n",
      "Std loss: 0.003232012160054607\n",
      "Total Loss: 0.05464189313352108\n",
      "------------------------------------ epoch 8192 (49146 steps) ------------------------------------\n",
      "Max loss: 0.007802668958902359\n",
      "Min loss: 0.004259181208908558\n",
      "Mean loss: 0.005638423065344493\n",
      "Std loss: 0.0013510169988610152\n",
      "Total Loss: 0.033830538392066956\n",
      "------------------------------------ epoch 8193 (49152 steps) ------------------------------------\n",
      "Max loss: 0.009026959538459778\n",
      "Min loss: 0.004958826117217541\n",
      "Mean loss: 0.0068247495995213585\n",
      "Std loss: 0.001267321735047631\n",
      "Total Loss: 0.04094849759712815\n",
      "------------------------------------ epoch 8194 (49158 steps) ------------------------------------\n",
      "Max loss: 0.033115632832050323\n",
      "Min loss: 0.004800410941243172\n",
      "Mean loss: 0.011415889641890923\n",
      "Std loss: 0.00990114972674869\n",
      "Total Loss: 0.06849533785134554\n",
      "------------------------------------ epoch 8195 (49164 steps) ------------------------------------\n",
      "Max loss: 0.018741970881819725\n",
      "Min loss: 0.004451250657439232\n",
      "Mean loss: 0.008980068688591322\n",
      "Std loss: 0.005109160646472324\n",
      "Total Loss: 0.05388041213154793\n",
      "------------------------------------ epoch 8196 (49170 steps) ------------------------------------\n",
      "Max loss: 0.026240123435854912\n",
      "Min loss: 0.004695012234151363\n",
      "Mean loss: 0.012050932894150415\n",
      "Std loss: 0.007071945775076613\n",
      "Total Loss: 0.0723055973649025\n",
      "------------------------------------ epoch 8197 (49176 steps) ------------------------------------\n",
      "Max loss: 0.011186420917510986\n",
      "Min loss: 0.0049531059339642525\n",
      "Mean loss: 0.008417426065231362\n",
      "Std loss: 0.002151912123867382\n",
      "Total Loss: 0.05050455639138818\n",
      "------------------------------------ epoch 8198 (49182 steps) ------------------------------------\n",
      "Max loss: 0.009589465335011482\n",
      "Min loss: 0.0045447563752532005\n",
      "Mean loss: 0.006787855410948396\n",
      "Std loss: 0.0017600748857978176\n",
      "Total Loss: 0.040727132465690374\n",
      "------------------------------------ epoch 8199 (49188 steps) ------------------------------------\n",
      "Max loss: 0.007950514554977417\n",
      "Min loss: 0.0051824357360601425\n",
      "Mean loss: 0.006261220822731654\n",
      "Std loss: 0.00098546243549903\n",
      "Total Loss: 0.03756732493638992\n",
      "------------------------------------ epoch 8200 (49194 steps) ------------------------------------\n",
      "Max loss: 0.01754855364561081\n",
      "Min loss: 0.0041127558797597885\n",
      "Mean loss: 0.00855254870839417\n",
      "Std loss: 0.004459498048152677\n",
      "Total Loss: 0.05131529225036502\n",
      "------------------------------------ epoch 8201 (49200 steps) ------------------------------------\n",
      "Max loss: 0.021924342960119247\n",
      "Min loss: 0.005478655453771353\n",
      "Mean loss: 0.011481682071462274\n",
      "Std loss: 0.006862930499654613\n",
      "Total Loss: 0.06889009242877364\n",
      "saved model at ./weights/model_8201.pth\n",
      "------------------------------------ epoch 8202 (49206 steps) ------------------------------------\n",
      "Max loss: 0.017245911061763763\n",
      "Min loss: 0.0037905992940068245\n",
      "Mean loss: 0.007909537991508842\n",
      "Std loss: 0.004558915703571401\n",
      "Total Loss: 0.04745722794905305\n",
      "------------------------------------ epoch 8203 (49212 steps) ------------------------------------\n",
      "Max loss: 0.010895080864429474\n",
      "Min loss: 0.004521597642451525\n",
      "Mean loss: 0.007026491376260917\n",
      "Std loss: 0.002043031369674628\n",
      "Total Loss: 0.0421589482575655\n",
      "------------------------------------ epoch 8204 (49218 steps) ------------------------------------\n",
      "Max loss: 0.010399428196251392\n",
      "Min loss: 0.0038714990951120853\n",
      "Mean loss: 0.006667174554119508\n",
      "Std loss: 0.002045892164378802\n",
      "Total Loss: 0.040003047324717045\n",
      "------------------------------------ epoch 8205 (49224 steps) ------------------------------------\n",
      "Max loss: 0.012206563726067543\n",
      "Min loss: 0.0035196251701563597\n",
      "Mean loss: 0.0071554861109082895\n",
      "Std loss: 0.003243711334397338\n",
      "Total Loss: 0.04293291666544974\n",
      "------------------------------------ epoch 8206 (49230 steps) ------------------------------------\n",
      "Max loss: 0.020069995895028114\n",
      "Min loss: 0.0037215035408735275\n",
      "Mean loss: 0.009199139506866535\n",
      "Std loss: 0.005732133042808894\n",
      "Total Loss: 0.05519483704119921\n",
      "------------------------------------ epoch 8207 (49236 steps) ------------------------------------\n",
      "Max loss: 0.011803960427641869\n",
      "Min loss: 0.004402544349431992\n",
      "Mean loss: 0.007042877143248916\n",
      "Std loss: 0.002622286981503801\n",
      "Total Loss: 0.042257262859493494\n",
      "------------------------------------ epoch 8208 (49242 steps) ------------------------------------\n",
      "Max loss: 0.008743003942072392\n",
      "Min loss: 0.0046731214970350266\n",
      "Mean loss: 0.006133513292297721\n",
      "Std loss: 0.0017409605800088085\n",
      "Total Loss: 0.036801079753786325\n",
      "------------------------------------ epoch 8209 (49248 steps) ------------------------------------\n",
      "Max loss: 0.015571417286992073\n",
      "Min loss: 0.004315933212637901\n",
      "Mean loss: 0.007959006276602546\n",
      "Std loss: 0.0036417725866348806\n",
      "Total Loss: 0.04775403765961528\n",
      "------------------------------------ epoch 8210 (49254 steps) ------------------------------------\n",
      "Max loss: 0.023319585248827934\n",
      "Min loss: 0.0038401621859520674\n",
      "Mean loss: 0.007831787806935608\n",
      "Std loss: 0.006962175823775003\n",
      "Total Loss: 0.04699072684161365\n",
      "------------------------------------ epoch 8211 (49260 steps) ------------------------------------\n",
      "Max loss: 0.009672784246504307\n",
      "Min loss: 0.0037252288311719894\n",
      "Mean loss: 0.0059033130916456384\n",
      "Std loss: 0.002007920229463728\n",
      "Total Loss: 0.03541987854987383\n",
      "------------------------------------ epoch 8212 (49266 steps) ------------------------------------\n",
      "Max loss: 0.019502680748701096\n",
      "Min loss: 0.006571516394615173\n",
      "Mean loss: 0.010531933279708028\n",
      "Std loss: 0.00455441737609324\n",
      "Total Loss: 0.06319159967824817\n",
      "------------------------------------ epoch 8213 (49272 steps) ------------------------------------\n",
      "Max loss: 0.023671794682741165\n",
      "Min loss: 0.004330716095864773\n",
      "Mean loss: 0.00903290999121964\n",
      "Std loss: 0.0066990995972781835\n",
      "Total Loss: 0.05419745994731784\n",
      "------------------------------------ epoch 8214 (49278 steps) ------------------------------------\n",
      "Max loss: 0.011783333495259285\n",
      "Min loss: 0.004706166684627533\n",
      "Mean loss: 0.00631639229444166\n",
      "Std loss: 0.002476088724622149\n",
      "Total Loss: 0.03789835376664996\n",
      "------------------------------------ epoch 8215 (49284 steps) ------------------------------------\n",
      "Max loss: 0.012737876735627651\n",
      "Min loss: 0.0041935034096241\n",
      "Mean loss: 0.006702179554849863\n",
      "Std loss: 0.0029103800128078058\n",
      "Total Loss: 0.04021307732909918\n",
      "------------------------------------ epoch 8216 (49290 steps) ------------------------------------\n",
      "Max loss: 0.025704752653837204\n",
      "Min loss: 0.0044946689158678055\n",
      "Mean loss: 0.0130452710048606\n",
      "Std loss: 0.0080359835242912\n",
      "Total Loss: 0.0782716260291636\n",
      "------------------------------------ epoch 8217 (49296 steps) ------------------------------------\n",
      "Max loss: 0.010002704337239265\n",
      "Min loss: 0.0036230399273335934\n",
      "Mean loss: 0.006477484789987405\n",
      "Std loss: 0.0023682593834830914\n",
      "Total Loss: 0.03886490873992443\n",
      "------------------------------------ epoch 8218 (49302 steps) ------------------------------------\n",
      "Max loss: 0.010390125215053558\n",
      "Min loss: 0.00381403393112123\n",
      "Mean loss: 0.006503650336526334\n",
      "Std loss: 0.0023460262090600563\n",
      "Total Loss: 0.039021902019158006\n",
      "------------------------------------ epoch 8219 (49308 steps) ------------------------------------\n",
      "Max loss: 0.011860786937177181\n",
      "Min loss: 0.004127540625631809\n",
      "Mean loss: 0.006499408433834712\n",
      "Std loss: 0.0025632412453481463\n",
      "Total Loss: 0.03899645060300827\n",
      "------------------------------------ epoch 8220 (49314 steps) ------------------------------------\n",
      "Max loss: 0.009873654693365097\n",
      "Min loss: 0.0042151701636612415\n",
      "Mean loss: 0.006089844818537434\n",
      "Std loss: 0.0017923238753748544\n",
      "Total Loss: 0.036539068911224604\n",
      "------------------------------------ epoch 8221 (49320 steps) ------------------------------------\n",
      "Max loss: 0.0418495237827301\n",
      "Min loss: 0.004463895224034786\n",
      "Mean loss: 0.012426465982571244\n",
      "Std loss: 0.013194698048657817\n",
      "Total Loss: 0.07455879589542747\n",
      "------------------------------------ epoch 8222 (49326 steps) ------------------------------------\n",
      "Max loss: 0.021715648472309113\n",
      "Min loss: 0.005843176972121\n",
      "Mean loss: 0.012995705474168062\n",
      "Std loss: 0.005269888598101896\n",
      "Total Loss: 0.07797423284500837\n",
      "------------------------------------ epoch 8223 (49332 steps) ------------------------------------\n",
      "Max loss: 0.026861533522605896\n",
      "Min loss: 0.004221772775053978\n",
      "Mean loss: 0.011965424132843813\n",
      "Std loss: 0.0084545824312045\n",
      "Total Loss: 0.07179254479706287\n",
      "------------------------------------ epoch 8224 (49338 steps) ------------------------------------\n",
      "Max loss: 0.03755665570497513\n",
      "Min loss: 0.007286564446985722\n",
      "Mean loss: 0.016652515313277643\n",
      "Std loss: 0.010391111206356208\n",
      "Total Loss: 0.09991509187966585\n",
      "------------------------------------ epoch 8225 (49344 steps) ------------------------------------\n",
      "Max loss: 0.02655390463769436\n",
      "Min loss: 0.004523536190390587\n",
      "Mean loss: 0.013854917992527286\n",
      "Std loss: 0.007431841446803343\n",
      "Total Loss: 0.08312950795516372\n",
      "------------------------------------ epoch 8226 (49350 steps) ------------------------------------\n",
      "Max loss: 0.010681234300136566\n",
      "Min loss: 0.005425808485597372\n",
      "Mean loss: 0.008515562008445462\n",
      "Std loss: 0.0016469643330698906\n",
      "Total Loss: 0.05109337205067277\n",
      "------------------------------------ epoch 8227 (49356 steps) ------------------------------------\n",
      "Max loss: 0.007788178976625204\n",
      "Min loss: 0.005634510423988104\n",
      "Mean loss: 0.006746551875645916\n",
      "Std loss: 0.0007869926175468797\n",
      "Total Loss: 0.040479311253875494\n",
      "------------------------------------ epoch 8228 (49362 steps) ------------------------------------\n",
      "Max loss: 0.01330293994396925\n",
      "Min loss: 0.005051716696470976\n",
      "Mean loss: 0.008801675712068876\n",
      "Std loss: 0.0026278897217473355\n",
      "Total Loss: 0.052810054272413254\n",
      "------------------------------------ epoch 8229 (49368 steps) ------------------------------------\n",
      "Max loss: 0.01777360588312149\n",
      "Min loss: 0.004372777417302132\n",
      "Mean loss: 0.008561500192930302\n",
      "Std loss: 0.00460352243712497\n",
      "Total Loss: 0.051369001157581806\n",
      "------------------------------------ epoch 8230 (49374 steps) ------------------------------------\n",
      "Max loss: 0.008172910660505295\n",
      "Min loss: 0.0037633187603205442\n",
      "Mean loss: 0.006303900736384094\n",
      "Std loss: 0.0016542530189408164\n",
      "Total Loss: 0.03782340441830456\n",
      "------------------------------------ epoch 8231 (49380 steps) ------------------------------------\n",
      "Max loss: 0.010427622124552727\n",
      "Min loss: 0.0035525409039109945\n",
      "Mean loss: 0.0068475261408214765\n",
      "Std loss: 0.0024221637140114036\n",
      "Total Loss: 0.04108515684492886\n",
      "------------------------------------ epoch 8232 (49386 steps) ------------------------------------\n",
      "Max loss: 0.016240356490015984\n",
      "Min loss: 0.004286447539925575\n",
      "Mean loss: 0.008545863442122936\n",
      "Std loss: 0.004321737474916732\n",
      "Total Loss: 0.05127518065273762\n",
      "------------------------------------ epoch 8233 (49392 steps) ------------------------------------\n",
      "Max loss: 0.015774402767419815\n",
      "Min loss: 0.004007645882666111\n",
      "Mean loss: 0.009189204700912038\n",
      "Std loss: 0.004944897456381213\n",
      "Total Loss: 0.05513522820547223\n",
      "------------------------------------ epoch 8234 (49398 steps) ------------------------------------\n",
      "Max loss: 0.008356007747352123\n",
      "Min loss: 0.0040716733783483505\n",
      "Mean loss: 0.006141908389205734\n",
      "Std loss: 0.0015564823787112593\n",
      "Total Loss: 0.036851450335234404\n",
      "------------------------------------ epoch 8235 (49404 steps) ------------------------------------\n",
      "Max loss: 0.02354693040251732\n",
      "Min loss: 0.004370497539639473\n",
      "Mean loss: 0.010798496892675757\n",
      "Std loss: 0.00709805516323391\n",
      "Total Loss: 0.06479098135605454\n",
      "------------------------------------ epoch 8236 (49410 steps) ------------------------------------\n",
      "Max loss: 0.04245319962501526\n",
      "Min loss: 0.0038311283569782972\n",
      "Mean loss: 0.01274224320271363\n",
      "Std loss: 0.013488858741065297\n",
      "Total Loss: 0.07645345921628177\n",
      "------------------------------------ epoch 8237 (49416 steps) ------------------------------------\n",
      "Max loss: 0.009487602859735489\n",
      "Min loss: 0.003694671206176281\n",
      "Mean loss: 0.005636978894472122\n",
      "Std loss: 0.0019063607423636662\n",
      "Total Loss: 0.03382187336683273\n",
      "------------------------------------ epoch 8238 (49422 steps) ------------------------------------\n",
      "Max loss: 0.024478493258357048\n",
      "Min loss: 0.0043817246332764626\n",
      "Mean loss: 0.011357521793494621\n",
      "Std loss: 0.008325192911387416\n",
      "Total Loss: 0.06814513076096773\n",
      "------------------------------------ epoch 8239 (49428 steps) ------------------------------------\n",
      "Max loss: 0.007901011034846306\n",
      "Min loss: 0.0033207458909600973\n",
      "Mean loss: 0.0053032620732362075\n",
      "Std loss: 0.0019400176732709486\n",
      "Total Loss: 0.03181957243941724\n",
      "------------------------------------ epoch 8240 (49434 steps) ------------------------------------\n",
      "Max loss: 0.007772774901241064\n",
      "Min loss: 0.004031416494399309\n",
      "Mean loss: 0.0052295655477792025\n",
      "Std loss: 0.0013283338681215218\n",
      "Total Loss: 0.031377393286675215\n",
      "------------------------------------ epoch 8241 (49440 steps) ------------------------------------\n",
      "Max loss: 0.011103153228759766\n",
      "Min loss: 0.004329986404627562\n",
      "Mean loss: 0.007503352826461196\n",
      "Std loss: 0.002526294839270029\n",
      "Total Loss: 0.045020116958767176\n",
      "------------------------------------ epoch 8242 (49446 steps) ------------------------------------\n",
      "Max loss: 0.009504135698080063\n",
      "Min loss: 0.0037629050202667713\n",
      "Mean loss: 0.005767123385642965\n",
      "Std loss: 0.0020008238576023248\n",
      "Total Loss: 0.034602740313857794\n",
      "------------------------------------ epoch 8243 (49452 steps) ------------------------------------\n",
      "Max loss: 0.015182597562670708\n",
      "Min loss: 0.00461012776941061\n",
      "Mean loss: 0.008285967633128166\n",
      "Std loss: 0.003442016295151194\n",
      "Total Loss: 0.049715805798769\n",
      "------------------------------------ epoch 8244 (49458 steps) ------------------------------------\n",
      "Max loss: 0.04851193726062775\n",
      "Min loss: 0.004676745738834143\n",
      "Mean loss: 0.014688400474066535\n",
      "Std loss: 0.01563715220507951\n",
      "Total Loss: 0.08813040284439921\n",
      "------------------------------------ epoch 8245 (49464 steps) ------------------------------------\n",
      "Max loss: 0.01790526509284973\n",
      "Min loss: 0.004292760509997606\n",
      "Mean loss: 0.007494532968848944\n",
      "Std loss: 0.004770032278793976\n",
      "Total Loss: 0.04496719781309366\n",
      "------------------------------------ epoch 8246 (49470 steps) ------------------------------------\n",
      "Max loss: 0.02457754872739315\n",
      "Min loss: 0.004407799802720547\n",
      "Mean loss: 0.010005722288042307\n",
      "Std loss: 0.007015782108157946\n",
      "Total Loss: 0.06003433372825384\n",
      "------------------------------------ epoch 8247 (49476 steps) ------------------------------------\n",
      "Max loss: 0.009458871558308601\n",
      "Min loss: 0.0039165327325463295\n",
      "Mean loss: 0.0058857113278160495\n",
      "Std loss: 0.0018059561812441894\n",
      "Total Loss: 0.035314267966896296\n",
      "------------------------------------ epoch 8248 (49482 steps) ------------------------------------\n",
      "Max loss: 0.02862631529569626\n",
      "Min loss: 0.003961586393415928\n",
      "Mean loss: 0.01107502713178595\n",
      "Std loss: 0.00803596517809442\n",
      "Total Loss: 0.0664501627907157\n",
      "------------------------------------ epoch 8249 (49488 steps) ------------------------------------\n",
      "Max loss: 0.019095757976174355\n",
      "Min loss: 0.005345570854842663\n",
      "Mean loss: 0.014043532156695923\n",
      "Std loss: 0.004422610591949159\n",
      "Total Loss: 0.08426119294017553\n",
      "------------------------------------ epoch 8250 (49494 steps) ------------------------------------\n",
      "Max loss: 0.011801711283624172\n",
      "Min loss: 0.005620383657515049\n",
      "Mean loss: 0.007801807640741269\n",
      "Std loss: 0.0019881740999125237\n",
      "Total Loss: 0.04681084584444761\n",
      "------------------------------------ epoch 8251 (49500 steps) ------------------------------------\n",
      "Max loss: 0.007888857275247574\n",
      "Min loss: 0.004228965379297733\n",
      "Mean loss: 0.005507552685836951\n",
      "Std loss: 0.0012597366617935583\n",
      "Total Loss: 0.033045316115021706\n",
      "------------------------------------ epoch 8252 (49506 steps) ------------------------------------\n",
      "Max loss: 0.007683955132961273\n",
      "Min loss: 0.004146333318203688\n",
      "Mean loss: 0.005549601123978694\n",
      "Std loss: 0.0011906528917520406\n",
      "Total Loss: 0.033297606743872166\n",
      "------------------------------------ epoch 8253 (49512 steps) ------------------------------------\n",
      "Max loss: 0.009143881499767303\n",
      "Min loss: 0.003844769671559334\n",
      "Mean loss: 0.005727671319618821\n",
      "Std loss: 0.0017041158843911542\n",
      "Total Loss: 0.03436602791771293\n",
      "------------------------------------ epoch 8254 (49518 steps) ------------------------------------\n",
      "Max loss: 0.008992088958621025\n",
      "Min loss: 0.00430594477802515\n",
      "Mean loss: 0.00753563301016887\n",
      "Std loss: 0.0016302399330362664\n",
      "Total Loss: 0.04521379806101322\n",
      "------------------------------------ epoch 8255 (49524 steps) ------------------------------------\n",
      "Max loss: 0.03331278637051582\n",
      "Min loss: 0.0038338499143719673\n",
      "Mean loss: 0.011484159311900536\n",
      "Std loss: 0.010225039940757918\n",
      "Total Loss: 0.06890495587140322\n",
      "------------------------------------ epoch 8256 (49530 steps) ------------------------------------\n",
      "Max loss: 0.01508304849267006\n",
      "Min loss: 0.004287278279662132\n",
      "Mean loss: 0.008076642562324802\n",
      "Std loss: 0.0035721345446926593\n",
      "Total Loss: 0.04845985537394881\n",
      "------------------------------------ epoch 8257 (49536 steps) ------------------------------------\n",
      "Max loss: 0.01049660425633192\n",
      "Min loss: 0.004122195765376091\n",
      "Mean loss: 0.006774873162309329\n",
      "Std loss: 0.002663002521052406\n",
      "Total Loss: 0.04064923897385597\n",
      "------------------------------------ epoch 8258 (49542 steps) ------------------------------------\n",
      "Max loss: 0.009606300853192806\n",
      "Min loss: 0.0036928225308656693\n",
      "Mean loss: 0.005551853605235617\n",
      "Std loss: 0.0019359111347949673\n",
      "Total Loss: 0.0333111216314137\n",
      "------------------------------------ epoch 8259 (49548 steps) ------------------------------------\n",
      "Max loss: 0.03387133777141571\n",
      "Min loss: 0.004672949202358723\n",
      "Mean loss: 0.011187890466923514\n",
      "Std loss: 0.010260734635824673\n",
      "Total Loss: 0.06712734280154109\n",
      "------------------------------------ epoch 8260 (49554 steps) ------------------------------------\n",
      "Max loss: 0.026380717754364014\n",
      "Min loss: 0.004312531556934118\n",
      "Mean loss: 0.01163964788429439\n",
      "Std loss: 0.007516627466718563\n",
      "Total Loss: 0.06983788730576634\n",
      "------------------------------------ epoch 8261 (49560 steps) ------------------------------------\n",
      "Max loss: 0.014778995886445045\n",
      "Min loss: 0.006023520138114691\n",
      "Mean loss: 0.009827043938760957\n",
      "Std loss: 0.0028228225562841394\n",
      "Total Loss: 0.05896226363256574\n",
      "------------------------------------ epoch 8262 (49566 steps) ------------------------------------\n",
      "Max loss: 0.009766561910510063\n",
      "Min loss: 0.00471301656216383\n",
      "Mean loss: 0.006685717186580102\n",
      "Std loss: 0.0016854958116742783\n",
      "Total Loss: 0.04011430311948061\n",
      "------------------------------------ epoch 8263 (49572 steps) ------------------------------------\n",
      "Max loss: 0.023607339709997177\n",
      "Min loss: 0.005871396511793137\n",
      "Mean loss: 0.010503197942549983\n",
      "Std loss: 0.00618005735312749\n",
      "Total Loss: 0.0630191876552999\n",
      "------------------------------------ epoch 8264 (49578 steps) ------------------------------------\n",
      "Max loss: 0.02553805336356163\n",
      "Min loss: 0.004509757738560438\n",
      "Mean loss: 0.010781189892441034\n",
      "Std loss: 0.007424138727774412\n",
      "Total Loss: 0.0646871393546462\n",
      "------------------------------------ epoch 8265 (49584 steps) ------------------------------------\n",
      "Max loss: 0.02735012397170067\n",
      "Min loss: 0.004024985246360302\n",
      "Mean loss: 0.011994715469578901\n",
      "Std loss: 0.007600952768195002\n",
      "Total Loss: 0.07196829281747341\n",
      "------------------------------------ epoch 8266 (49590 steps) ------------------------------------\n",
      "Max loss: 0.012886377051472664\n",
      "Min loss: 0.004307784605771303\n",
      "Mean loss: 0.00834712308521072\n",
      "Std loss: 0.0030064681608518965\n",
      "Total Loss: 0.050082738511264324\n",
      "------------------------------------ epoch 8267 (49596 steps) ------------------------------------\n",
      "Max loss: 0.0275551900267601\n",
      "Min loss: 0.005217192694544792\n",
      "Mean loss: 0.013373020260284344\n",
      "Std loss: 0.007678839435989179\n",
      "Total Loss: 0.08023812156170607\n",
      "------------------------------------ epoch 8268 (49602 steps) ------------------------------------\n",
      "Max loss: 0.014066983014345169\n",
      "Min loss: 0.0037061385810375214\n",
      "Mean loss: 0.007476726469273369\n",
      "Std loss: 0.00322958525946293\n",
      "Total Loss: 0.04486035881564021\n",
      "------------------------------------ epoch 8269 (49608 steps) ------------------------------------\n",
      "Max loss: 0.020109666511416435\n",
      "Min loss: 0.005319432821124792\n",
      "Mean loss: 0.011663282212490836\n",
      "Std loss: 0.004937962931241852\n",
      "Total Loss: 0.06997969327494502\n",
      "------------------------------------ epoch 8270 (49614 steps) ------------------------------------\n",
      "Max loss: 0.015233350917696953\n",
      "Min loss: 0.005227487068623304\n",
      "Mean loss: 0.008615260478109121\n",
      "Std loss: 0.0035199070566126643\n",
      "Total Loss: 0.05169156286865473\n",
      "------------------------------------ epoch 8271 (49620 steps) ------------------------------------\n",
      "Max loss: 0.01939091645181179\n",
      "Min loss: 0.00469794450327754\n",
      "Mean loss: 0.00899138410265247\n",
      "Std loss: 0.004929301102135469\n",
      "Total Loss: 0.05394830461591482\n",
      "------------------------------------ epoch 8272 (49626 steps) ------------------------------------\n",
      "Max loss: 0.027063405141234398\n",
      "Min loss: 0.0046708788722753525\n",
      "Mean loss: 0.012071416170025865\n",
      "Std loss: 0.007538853465742083\n",
      "Total Loss: 0.07242849702015519\n",
      "------------------------------------ epoch 8273 (49632 steps) ------------------------------------\n",
      "Max loss: 0.022509319707751274\n",
      "Min loss: 0.00375165487639606\n",
      "Mean loss: 0.010299999228057763\n",
      "Std loss: 0.007130135317859116\n",
      "Total Loss: 0.06179999536834657\n",
      "------------------------------------ epoch 8274 (49638 steps) ------------------------------------\n",
      "Max loss: 0.01922612264752388\n",
      "Min loss: 0.004801708739250898\n",
      "Mean loss: 0.009576987940818071\n",
      "Std loss: 0.004859306555485246\n",
      "Total Loss: 0.05746192764490843\n",
      "------------------------------------ epoch 8275 (49644 steps) ------------------------------------\n",
      "Max loss: 0.018295204266905785\n",
      "Min loss: 0.004255035892128944\n",
      "Mean loss: 0.01040186733007431\n",
      "Std loss: 0.004755985270163642\n",
      "Total Loss: 0.06241120398044586\n",
      "------------------------------------ epoch 8276 (49650 steps) ------------------------------------\n",
      "Max loss: 0.01276684831827879\n",
      "Min loss: 0.004400322213768959\n",
      "Mean loss: 0.00888485616693894\n",
      "Std loss: 0.0032259563018441676\n",
      "Total Loss: 0.053309137001633644\n",
      "------------------------------------ epoch 8277 (49656 steps) ------------------------------------\n",
      "Max loss: 0.017550943419337273\n",
      "Min loss: 0.004156971350312233\n",
      "Mean loss: 0.008476554726560911\n",
      "Std loss: 0.004440698278588331\n",
      "Total Loss: 0.05085932835936546\n",
      "------------------------------------ epoch 8278 (49662 steps) ------------------------------------\n",
      "Max loss: 0.00800129771232605\n",
      "Min loss: 0.004405686166137457\n",
      "Mean loss: 0.006058286099384229\n",
      "Std loss: 0.0013101328709869823\n",
      "Total Loss: 0.03634971659630537\n",
      "------------------------------------ epoch 8279 (49668 steps) ------------------------------------\n",
      "Max loss: 0.025478478521108627\n",
      "Min loss: 0.004020837135612965\n",
      "Mean loss: 0.01108645290757219\n",
      "Std loss: 0.007370472940624306\n",
      "Total Loss: 0.06651871744543314\n",
      "------------------------------------ epoch 8280 (49674 steps) ------------------------------------\n",
      "Max loss: 0.016331197693943977\n",
      "Min loss: 0.004940877668559551\n",
      "Mean loss: 0.008096075306336084\n",
      "Std loss: 0.004010043151095441\n",
      "Total Loss: 0.04857645183801651\n",
      "------------------------------------ epoch 8281 (49680 steps) ------------------------------------\n",
      "Max loss: 0.01266934722661972\n",
      "Min loss: 0.005733809433877468\n",
      "Mean loss: 0.00963047674546639\n",
      "Std loss: 0.002723043425947223\n",
      "Total Loss: 0.05778286047279835\n",
      "------------------------------------ epoch 8282 (49686 steps) ------------------------------------\n",
      "Max loss: 0.03306419029831886\n",
      "Min loss: 0.0044306134805083275\n",
      "Mean loss: 0.012773200947170457\n",
      "Std loss: 0.009743006196102464\n",
      "Total Loss: 0.07663920568302274\n",
      "------------------------------------ epoch 8283 (49692 steps) ------------------------------------\n",
      "Max loss: 0.011393151246011257\n",
      "Min loss: 0.004124821163713932\n",
      "Mean loss: 0.007946313126012683\n",
      "Std loss: 0.002813972440397542\n",
      "Total Loss: 0.0476778787560761\n",
      "------------------------------------ epoch 8284 (49698 steps) ------------------------------------\n",
      "Max loss: 0.028884774073958397\n",
      "Min loss: 0.0046654874458909035\n",
      "Mean loss: 0.010531298816204071\n",
      "Std loss: 0.008332843904226176\n",
      "Total Loss: 0.06318779289722443\n",
      "------------------------------------ epoch 8285 (49704 steps) ------------------------------------\n",
      "Max loss: 0.013593710958957672\n",
      "Min loss: 0.004365613218396902\n",
      "Mean loss: 0.008090169867500663\n",
      "Std loss: 0.003102016081109063\n",
      "Total Loss: 0.04854101920500398\n",
      "------------------------------------ epoch 8286 (49710 steps) ------------------------------------\n",
      "Max loss: 0.04365534707903862\n",
      "Min loss: 0.003920202609151602\n",
      "Mean loss: 0.013480462366715074\n",
      "Std loss: 0.01385005179065856\n",
      "Total Loss: 0.08088277420029044\n",
      "------------------------------------ epoch 8287 (49716 steps) ------------------------------------\n",
      "Max loss: 0.0300759207457304\n",
      "Min loss: 0.005288915708661079\n",
      "Mean loss: 0.015528428213049969\n",
      "Std loss: 0.00803402691561933\n",
      "Total Loss: 0.09317056927829981\n",
      "------------------------------------ epoch 8288 (49722 steps) ------------------------------------\n",
      "Max loss: 0.01445295661687851\n",
      "Min loss: 0.005168103612959385\n",
      "Mean loss: 0.008124185415605703\n",
      "Std loss: 0.0030270352502468714\n",
      "Total Loss: 0.048745112493634224\n",
      "------------------------------------ epoch 8289 (49728 steps) ------------------------------------\n",
      "Max loss: 0.025089239701628685\n",
      "Min loss: 0.005557358264923096\n",
      "Mean loss: 0.01268890080973506\n",
      "Std loss: 0.007370579957881367\n",
      "Total Loss: 0.07613340485841036\n",
      "------------------------------------ epoch 8290 (49734 steps) ------------------------------------\n",
      "Max loss: 0.01802550069987774\n",
      "Min loss: 0.004387929569929838\n",
      "Mean loss: 0.01019577340533336\n",
      "Std loss: 0.005357404634473296\n",
      "Total Loss: 0.06117464043200016\n",
      "------------------------------------ epoch 8291 (49740 steps) ------------------------------------\n",
      "Max loss: 0.024514099583029747\n",
      "Min loss: 0.005957113578915596\n",
      "Mean loss: 0.01205543001803259\n",
      "Std loss: 0.007748708324489983\n",
      "Total Loss: 0.07233258010819554\n",
      "------------------------------------ epoch 8292 (49746 steps) ------------------------------------\n",
      "Max loss: 0.01570984721183777\n",
      "Min loss: 0.005272561218589544\n",
      "Mean loss: 0.009816668617228666\n",
      "Std loss: 0.003861937840117622\n",
      "Total Loss: 0.058900011703372\n",
      "------------------------------------ epoch 8293 (49752 steps) ------------------------------------\n",
      "Max loss: 0.01486368477344513\n",
      "Min loss: 0.009635346941649914\n",
      "Mean loss: 0.012222544134904942\n",
      "Std loss: 0.0022783881985304574\n",
      "Total Loss: 0.07333526480942965\n",
      "------------------------------------ epoch 8294 (49758 steps) ------------------------------------\n",
      "Max loss: 0.017040392383933067\n",
      "Min loss: 0.00444392766803503\n",
      "Mean loss: 0.00791689450852573\n",
      "Std loss: 0.004183481543204321\n",
      "Total Loss: 0.047501367051154375\n",
      "------------------------------------ epoch 8295 (49764 steps) ------------------------------------\n",
      "Max loss: 0.024834811687469482\n",
      "Min loss: 0.004257456865161657\n",
      "Mean loss: 0.009348495475326976\n",
      "Std loss: 0.007509173150736015\n",
      "Total Loss: 0.05609097285196185\n",
      "------------------------------------ epoch 8296 (49770 steps) ------------------------------------\n",
      "Max loss: 0.013954585418105125\n",
      "Min loss: 0.00464751897379756\n",
      "Mean loss: 0.006650433332348864\n",
      "Std loss: 0.003284711076192093\n",
      "Total Loss: 0.03990259999409318\n",
      "------------------------------------ epoch 8297 (49776 steps) ------------------------------------\n",
      "Max loss: 0.014019842259585857\n",
      "Min loss: 0.005705574061721563\n",
      "Mean loss: 0.008402170070136586\n",
      "Std loss: 0.0028722615978266364\n",
      "Total Loss: 0.05041302042081952\n",
      "------------------------------------ epoch 8298 (49782 steps) ------------------------------------\n",
      "Max loss: 0.024712374433875084\n",
      "Min loss: 0.004128701984882355\n",
      "Mean loss: 0.01088340999558568\n",
      "Std loss: 0.006963357987084245\n",
      "Total Loss: 0.06530045997351408\n",
      "------------------------------------ epoch 8299 (49788 steps) ------------------------------------\n",
      "Max loss: 0.016182899475097656\n",
      "Min loss: 0.0033164534252136946\n",
      "Mean loss: 0.008248716942034662\n",
      "Std loss: 0.004718597330302367\n",
      "Total Loss: 0.04949230165220797\n",
      "------------------------------------ epoch 8300 (49794 steps) ------------------------------------\n",
      "Max loss: 0.03292612358927727\n",
      "Min loss: 0.004261516034603119\n",
      "Mean loss: 0.013487826101481915\n",
      "Std loss: 0.009201978272071426\n",
      "Total Loss: 0.08092695660889149\n",
      "------------------------------------ epoch 8301 (49800 steps) ------------------------------------\n",
      "Max loss: 0.012435272336006165\n",
      "Min loss: 0.0050746602937579155\n",
      "Mean loss: 0.008155558491125703\n",
      "Std loss: 0.0027638686173250384\n",
      "Total Loss: 0.04893335094675422\n",
      "saved model at ./weights/model_8301.pth\n",
      "------------------------------------ epoch 8302 (49806 steps) ------------------------------------\n",
      "Max loss: 0.051427941769361496\n",
      "Min loss: 0.004542892798781395\n",
      "Mean loss: 0.019525405252352357\n",
      "Std loss: 0.017955752582143134\n",
      "Total Loss: 0.11715243151411414\n",
      "------------------------------------ epoch 8303 (49812 steps) ------------------------------------\n",
      "Max loss: 0.01594199799001217\n",
      "Min loss: 0.004084487911313772\n",
      "Mean loss: 0.009482928241292635\n",
      "Std loss: 0.004329726705183727\n",
      "Total Loss: 0.056897569447755814\n",
      "------------------------------------ epoch 8304 (49818 steps) ------------------------------------\n",
      "Max loss: 0.01754298433661461\n",
      "Min loss: 0.008537862449884415\n",
      "Mean loss: 0.010550772305577993\n",
      "Std loss: 0.0031732928919675075\n",
      "Total Loss: 0.06330463383346796\n",
      "------------------------------------ epoch 8305 (49824 steps) ------------------------------------\n",
      "Max loss: 0.011772449128329754\n",
      "Min loss: 0.005021918565034866\n",
      "Mean loss: 0.007805192610248923\n",
      "Std loss: 0.0020124971490014226\n",
      "Total Loss: 0.04683115566149354\n",
      "------------------------------------ epoch 8306 (49830 steps) ------------------------------------\n",
      "Max loss: 0.011429604142904282\n",
      "Min loss: 0.004451664164662361\n",
      "Mean loss: 0.007046883537744482\n",
      "Std loss: 0.002235790667153765\n",
      "Total Loss: 0.042281301226466894\n",
      "------------------------------------ epoch 8307 (49836 steps) ------------------------------------\n",
      "Max loss: 0.024923119693994522\n",
      "Min loss: 0.005740215536206961\n",
      "Mean loss: 0.01055676156344513\n",
      "Std loss: 0.0065444162092814315\n",
      "Total Loss: 0.06334056938067079\n",
      "------------------------------------ epoch 8308 (49842 steps) ------------------------------------\n",
      "Max loss: 0.015996165573596954\n",
      "Min loss: 0.004079503007233143\n",
      "Mean loss: 0.00851327289516727\n",
      "Std loss: 0.003684260418809949\n",
      "Total Loss: 0.05107963737100363\n",
      "------------------------------------ epoch 8309 (49848 steps) ------------------------------------\n",
      "Max loss: 0.008674178272485733\n",
      "Min loss: 0.0036213556304574013\n",
      "Mean loss: 0.006218705015877883\n",
      "Std loss: 0.001935022583742663\n",
      "Total Loss: 0.037312230095267296\n",
      "------------------------------------ epoch 8310 (49854 steps) ------------------------------------\n",
      "Max loss: 0.008936341851949692\n",
      "Min loss: 0.004863153677433729\n",
      "Mean loss: 0.006015368582059939\n",
      "Std loss: 0.0013917996003132097\n",
      "Total Loss: 0.03609221149235964\n",
      "------------------------------------ epoch 8311 (49860 steps) ------------------------------------\n",
      "Max loss: 0.015769515186548233\n",
      "Min loss: 0.003413768019527197\n",
      "Mean loss: 0.009117666088665525\n",
      "Std loss: 0.004137924689360883\n",
      "Total Loss: 0.05470599653199315\n",
      "------------------------------------ epoch 8312 (49866 steps) ------------------------------------\n",
      "Max loss: 0.007789182476699352\n",
      "Min loss: 0.0031251932960003614\n",
      "Mean loss: 0.004918585919464628\n",
      "Std loss: 0.001494396920898702\n",
      "Total Loss: 0.029511515516787767\n",
      "------------------------------------ epoch 8313 (49872 steps) ------------------------------------\n",
      "Max loss: 0.008686563931405544\n",
      "Min loss: 0.004991843365132809\n",
      "Mean loss: 0.007088821691771348\n",
      "Std loss: 0.0015227707396875893\n",
      "Total Loss: 0.04253293015062809\n",
      "------------------------------------ epoch 8314 (49878 steps) ------------------------------------\n",
      "Max loss: 0.021378442645072937\n",
      "Min loss: 0.0032553239725530148\n",
      "Mean loss: 0.008901557497059306\n",
      "Std loss: 0.005841235389488719\n",
      "Total Loss: 0.05340934498235583\n",
      "------------------------------------ epoch 8315 (49884 steps) ------------------------------------\n",
      "Max loss: 0.010896462015807629\n",
      "Min loss: 0.004920188337564468\n",
      "Mean loss: 0.00718828880538543\n",
      "Std loss: 0.0018784411428137016\n",
      "Total Loss: 0.043129732832312584\n",
      "------------------------------------ epoch 8316 (49890 steps) ------------------------------------\n",
      "Max loss: 0.00953274592757225\n",
      "Min loss: 0.004594611935317516\n",
      "Mean loss: 0.0075344057598461705\n",
      "Std loss: 0.0016764316852215615\n",
      "Total Loss: 0.045206434559077024\n",
      "------------------------------------ epoch 8317 (49896 steps) ------------------------------------\n",
      "Max loss: 0.03179476410150528\n",
      "Min loss: 0.004179920069873333\n",
      "Mean loss: 0.012414941797032952\n",
      "Std loss: 0.009764723572216968\n",
      "Total Loss: 0.07448965078219771\n",
      "------------------------------------ epoch 8318 (49902 steps) ------------------------------------\n",
      "Max loss: 0.01258019171655178\n",
      "Min loss: 0.005162512883543968\n",
      "Mean loss: 0.008067605551332235\n",
      "Std loss: 0.0028997102266973274\n",
      "Total Loss: 0.04840563330799341\n",
      "------------------------------------ epoch 8319 (49908 steps) ------------------------------------\n",
      "Max loss: 0.025747615844011307\n",
      "Min loss: 0.004399874713271856\n",
      "Mean loss: 0.011783482196430365\n",
      "Std loss: 0.008227388435394367\n",
      "Total Loss: 0.07070089317858219\n",
      "------------------------------------ epoch 8320 (49914 steps) ------------------------------------\n",
      "Max loss: 0.014342285692691803\n",
      "Min loss: 0.0038386834785342216\n",
      "Mean loss: 0.007162785390391946\n",
      "Std loss: 0.0036325810513365983\n",
      "Total Loss: 0.042976712342351675\n",
      "------------------------------------ epoch 8321 (49920 steps) ------------------------------------\n",
      "Max loss: 0.011473353952169418\n",
      "Min loss: 0.00408190069720149\n",
      "Mean loss: 0.007179081828022997\n",
      "Std loss: 0.002934179823006592\n",
      "Total Loss: 0.04307449096813798\n",
      "------------------------------------ epoch 8322 (49926 steps) ------------------------------------\n",
      "Max loss: 0.0212114155292511\n",
      "Min loss: 0.0051552848890423775\n",
      "Mean loss: 0.009324589899430672\n",
      "Std loss: 0.005484619941684359\n",
      "Total Loss: 0.055947539396584034\n",
      "------------------------------------ epoch 8323 (49932 steps) ------------------------------------\n",
      "Max loss: 0.018362391740083694\n",
      "Min loss: 0.003902002004906535\n",
      "Mean loss: 0.009061003297877809\n",
      "Std loss: 0.006213482609360602\n",
      "Total Loss: 0.05436601978726685\n",
      "------------------------------------ epoch 8324 (49938 steps) ------------------------------------\n",
      "Max loss: 0.019784219563007355\n",
      "Min loss: 0.006170453038066626\n",
      "Mean loss: 0.012601561611518264\n",
      "Std loss: 0.005012739094642373\n",
      "Total Loss: 0.07560936966910958\n",
      "------------------------------------ epoch 8325 (49944 steps) ------------------------------------\n",
      "Max loss: 0.024809442460536957\n",
      "Min loss: 0.003993833903223276\n",
      "Mean loss: 0.011083641011888782\n",
      "Std loss: 0.007020849449034204\n",
      "Total Loss: 0.0665018460713327\n",
      "------------------------------------ epoch 8326 (49950 steps) ------------------------------------\n",
      "Max loss: 0.008206658065319061\n",
      "Min loss: 0.0036080158315598965\n",
      "Mean loss: 0.005447318699831764\n",
      "Std loss: 0.0015482412265729185\n",
      "Total Loss: 0.03268391219899058\n",
      "------------------------------------ epoch 8327 (49956 steps) ------------------------------------\n",
      "Max loss: 0.006564101204276085\n",
      "Min loss: 0.003546798136085272\n",
      "Mean loss: 0.005052062838027875\n",
      "Std loss: 0.0009080960197762996\n",
      "Total Loss: 0.030312377028167248\n",
      "------------------------------------ epoch 8328 (49962 steps) ------------------------------------\n",
      "Max loss: 0.013279732316732407\n",
      "Min loss: 0.003964797593653202\n",
      "Mean loss: 0.008038811618462205\n",
      "Std loss: 0.00341117933535189\n",
      "Total Loss: 0.04823286971077323\n",
      "------------------------------------ epoch 8329 (49968 steps) ------------------------------------\n",
      "Max loss: 0.01213783212006092\n",
      "Min loss: 0.0033900844864547253\n",
      "Mean loss: 0.005679025935630004\n",
      "Std loss: 0.0029564126204529736\n",
      "Total Loss: 0.03407415561378002\n",
      "------------------------------------ epoch 8330 (49974 steps) ------------------------------------\n",
      "Max loss: 0.024886667728424072\n",
      "Min loss: 0.004458432085812092\n",
      "Mean loss: 0.009661459208776554\n",
      "Std loss: 0.007069437089458288\n",
      "Total Loss: 0.05796875525265932\n",
      "------------------------------------ epoch 8331 (49980 steps) ------------------------------------\n",
      "Max loss: 0.006493502762168646\n",
      "Min loss: 0.0038314983248710632\n",
      "Mean loss: 0.005170157256846626\n",
      "Std loss: 0.000900500429083918\n",
      "Total Loss: 0.03102094354107976\n",
      "------------------------------------ epoch 8332 (49986 steps) ------------------------------------\n",
      "Max loss: 0.022965125739574432\n",
      "Min loss: 0.004549490287899971\n",
      "Mean loss: 0.010395146751155456\n",
      "Std loss: 0.006403679516384999\n",
      "Total Loss: 0.062370880506932735\n",
      "------------------------------------ epoch 8333 (49992 steps) ------------------------------------\n",
      "Max loss: 0.017034431919455528\n",
      "Min loss: 0.005188384093344212\n",
      "Mean loss: 0.009037487674504519\n",
      "Std loss: 0.003921801392438047\n",
      "Total Loss: 0.05422492604702711\n",
      "------------------------------------ epoch 8334 (49998 steps) ------------------------------------\n",
      "Max loss: 0.0077239274978637695\n",
      "Min loss: 0.003331488464027643\n",
      "Mean loss: 0.005868050502613187\n",
      "Std loss: 0.00146419783998781\n",
      "Total Loss: 0.03520830301567912\n",
      "------------------------------------ epoch 8335 (50004 steps) ------------------------------------\n",
      "Max loss: 0.008722197264432907\n",
      "Min loss: 0.0042704432271420956\n",
      "Mean loss: 0.005954037420451641\n",
      "Std loss: 0.0015559634485032756\n",
      "Total Loss: 0.035724224522709846\n",
      "------------------------------------ epoch 8336 (50010 steps) ------------------------------------\n",
      "Max loss: 0.010653490200638771\n",
      "Min loss: 0.004065555986016989\n",
      "Mean loss: 0.0073487454404433565\n",
      "Std loss: 0.0023206310377942583\n",
      "Total Loss: 0.04409247264266014\n",
      "------------------------------------ epoch 8337 (50016 steps) ------------------------------------\n",
      "Max loss: 0.0082264244556427\n",
      "Min loss: 0.0033166161738336086\n",
      "Mean loss: 0.005222497507929802\n",
      "Std loss: 0.00160061217506202\n",
      "Total Loss: 0.03133498504757881\n",
      "------------------------------------ epoch 8338 (50022 steps) ------------------------------------\n",
      "Max loss: 0.010096325539052486\n",
      "Min loss: 0.004305054899305105\n",
      "Mean loss: 0.0067852958260724945\n",
      "Std loss: 0.002424037291997305\n",
      "Total Loss: 0.040711774956434965\n",
      "------------------------------------ epoch 8339 (50028 steps) ------------------------------------\n",
      "Max loss: 0.016215737909078598\n",
      "Min loss: 0.004202316515147686\n",
      "Mean loss: 0.00658865668810904\n",
      "Std loss: 0.004324823665291877\n",
      "Total Loss: 0.03953194012865424\n",
      "------------------------------------ epoch 8340 (50034 steps) ------------------------------------\n",
      "Max loss: 0.00902585405856371\n",
      "Min loss: 0.0035377906169742346\n",
      "Mean loss: 0.0075168436936413245\n",
      "Std loss: 0.0018730787440871305\n",
      "Total Loss: 0.04510106216184795\n",
      "------------------------------------ epoch 8341 (50040 steps) ------------------------------------\n",
      "Max loss: 0.012360652908682823\n",
      "Min loss: 0.0033277771435678005\n",
      "Mean loss: 0.007190780558933814\n",
      "Std loss: 0.0033117485687421794\n",
      "Total Loss: 0.043144683353602886\n",
      "------------------------------------ epoch 8342 (50046 steps) ------------------------------------\n",
      "Max loss: 0.016155436635017395\n",
      "Min loss: 0.0037041862960904837\n",
      "Mean loss: 0.0065930264148240285\n",
      "Std loss: 0.004344973664328065\n",
      "Total Loss: 0.03955815848894417\n",
      "------------------------------------ epoch 8343 (50052 steps) ------------------------------------\n",
      "Max loss: 0.014265313744544983\n",
      "Min loss: 0.003785067703574896\n",
      "Mean loss: 0.008170066401362419\n",
      "Std loss: 0.003592424350687347\n",
      "Total Loss: 0.049020398408174515\n",
      "------------------------------------ epoch 8344 (50058 steps) ------------------------------------\n",
      "Max loss: 0.012760784476995468\n",
      "Min loss: 0.003916362766176462\n",
      "Mean loss: 0.009057739904771248\n",
      "Std loss: 0.0033966573351235517\n",
      "Total Loss: 0.05434643942862749\n",
      "------------------------------------ epoch 8345 (50064 steps) ------------------------------------\n",
      "Max loss: 0.01615278050303459\n",
      "Min loss: 0.0034829366486519575\n",
      "Mean loss: 0.008768548568089804\n",
      "Std loss: 0.00510679899881013\n",
      "Total Loss: 0.05261129140853882\n",
      "------------------------------------ epoch 8346 (50070 steps) ------------------------------------\n",
      "Max loss: 0.017978284507989883\n",
      "Min loss: 0.00597595376893878\n",
      "Mean loss: 0.010030066128820181\n",
      "Std loss: 0.004036046199912051\n",
      "Total Loss: 0.060180396772921085\n",
      "------------------------------------ epoch 8347 (50076 steps) ------------------------------------\n",
      "Max loss: 0.013509373180568218\n",
      "Min loss: 0.004985812120139599\n",
      "Mean loss: 0.008366724320997795\n",
      "Std loss: 0.003098720948362169\n",
      "Total Loss: 0.05020034592598677\n",
      "------------------------------------ epoch 8348 (50082 steps) ------------------------------------\n",
      "Max loss: 0.010706736706197262\n",
      "Min loss: 0.004511301405727863\n",
      "Mean loss: 0.007619594223797321\n",
      "Std loss: 0.0022991687559692194\n",
      "Total Loss: 0.04571756534278393\n",
      "------------------------------------ epoch 8349 (50088 steps) ------------------------------------\n",
      "Max loss: 0.014988179318606853\n",
      "Min loss: 0.004616547375917435\n",
      "Mean loss: 0.007275632427384456\n",
      "Std loss: 0.003600943463893004\n",
      "Total Loss: 0.043653794564306736\n",
      "------------------------------------ epoch 8350 (50094 steps) ------------------------------------\n",
      "Max loss: 0.010307876393198967\n",
      "Min loss: 0.003952607046812773\n",
      "Mean loss: 0.006207683123648167\n",
      "Std loss: 0.0020222134637537096\n",
      "Total Loss: 0.037246098741889\n",
      "------------------------------------ epoch 8351 (50100 steps) ------------------------------------\n",
      "Max loss: 0.03503092750906944\n",
      "Min loss: 0.003916192799806595\n",
      "Mean loss: 0.009847778904562196\n",
      "Std loss: 0.011288147367440939\n",
      "Total Loss: 0.05908667342737317\n",
      "------------------------------------ epoch 8352 (50106 steps) ------------------------------------\n",
      "Max loss: 0.01068545039743185\n",
      "Min loss: 0.004144650883972645\n",
      "Mean loss: 0.006719819502905011\n",
      "Std loss: 0.0022322527061896308\n",
      "Total Loss: 0.04031891701743007\n",
      "------------------------------------ epoch 8353 (50112 steps) ------------------------------------\n",
      "Max loss: 0.02914389595389366\n",
      "Min loss: 0.005679295863956213\n",
      "Mean loss: 0.013318188333263\n",
      "Std loss: 0.009602074289293853\n",
      "Total Loss: 0.079909129999578\n",
      "------------------------------------ epoch 8354 (50118 steps) ------------------------------------\n",
      "Max loss: 0.02422577142715454\n",
      "Min loss: 0.003620757721364498\n",
      "Mean loss: 0.00905671420817574\n",
      "Std loss: 0.006971221955199664\n",
      "Total Loss: 0.05434028524905443\n",
      "------------------------------------ epoch 8355 (50124 steps) ------------------------------------\n",
      "Max loss: 0.010241038165986538\n",
      "Min loss: 0.004666150081902742\n",
      "Mean loss: 0.007184706473102172\n",
      "Std loss: 0.002011103485878427\n",
      "Total Loss: 0.04310823883861303\n",
      "------------------------------------ epoch 8356 (50130 steps) ------------------------------------\n",
      "Max loss: 0.013959363102912903\n",
      "Min loss: 0.007474156096577644\n",
      "Mean loss: 0.010050288401544094\n",
      "Std loss: 0.002592655834850267\n",
      "Total Loss: 0.060301730409264565\n",
      "------------------------------------ epoch 8357 (50136 steps) ------------------------------------\n",
      "Max loss: 0.024593334645032883\n",
      "Min loss: 0.004632533993571997\n",
      "Mean loss: 0.008470761124044657\n",
      "Std loss: 0.007238313346273218\n",
      "Total Loss: 0.05082456674426794\n",
      "------------------------------------ epoch 8358 (50142 steps) ------------------------------------\n",
      "Max loss: 0.007164916023612022\n",
      "Min loss: 0.0038919104263186455\n",
      "Mean loss: 0.00507409901668628\n",
      "Std loss: 0.0011172319029455963\n",
      "Total Loss: 0.030444594100117683\n",
      "------------------------------------ epoch 8359 (50148 steps) ------------------------------------\n",
      "Max loss: 0.014108717441558838\n",
      "Min loss: 0.003113566432148218\n",
      "Mean loss: 0.00784602800073723\n",
      "Std loss: 0.004317396363364204\n",
      "Total Loss: 0.04707616800442338\n",
      "------------------------------------ epoch 8360 (50154 steps) ------------------------------------\n",
      "Max loss: 0.020301222801208496\n",
      "Min loss: 0.004639376886188984\n",
      "Mean loss: 0.010650786571204662\n",
      "Std loss: 0.005116467849989885\n",
      "Total Loss: 0.06390471942722797\n",
      "------------------------------------ epoch 8361 (50160 steps) ------------------------------------\n",
      "Max loss: 0.014894897118210793\n",
      "Min loss: 0.004020859487354755\n",
      "Mean loss: 0.007531260217850407\n",
      "Std loss: 0.0035519279161257914\n",
      "Total Loss: 0.04518756130710244\n",
      "------------------------------------ epoch 8362 (50166 steps) ------------------------------------\n",
      "Max loss: 0.03306843340396881\n",
      "Min loss: 0.0033751900773495436\n",
      "Mean loss: 0.011135278075623015\n",
      "Std loss: 0.010177079183468331\n",
      "Total Loss: 0.0668116684537381\n",
      "------------------------------------ epoch 8363 (50172 steps) ------------------------------------\n",
      "Max loss: 0.016118081286549568\n",
      "Min loss: 0.005183038767427206\n",
      "Mean loss: 0.00905540732977291\n",
      "Std loss: 0.003967105886026827\n",
      "Total Loss: 0.05433244397863746\n",
      "------------------------------------ epoch 8364 (50178 steps) ------------------------------------\n",
      "Max loss: 0.013576368801295757\n",
      "Min loss: 0.0032441297080367804\n",
      "Mean loss: 0.007299956322337191\n",
      "Std loss: 0.003883749778836725\n",
      "Total Loss: 0.04379973793402314\n",
      "------------------------------------ epoch 8365 (50184 steps) ------------------------------------\n",
      "Max loss: 0.010652942582964897\n",
      "Min loss: 0.0037880984600633383\n",
      "Mean loss: 0.006690057964685063\n",
      "Std loss: 0.0028510951017478563\n",
      "Total Loss: 0.040140347788110375\n",
      "------------------------------------ epoch 8366 (50190 steps) ------------------------------------\n",
      "Max loss: 0.015229418873786926\n",
      "Min loss: 0.00392745528370142\n",
      "Mean loss: 0.007328528833265106\n",
      "Std loss: 0.004454561764791234\n",
      "Total Loss: 0.043971172999590635\n",
      "------------------------------------ epoch 8367 (50196 steps) ------------------------------------\n",
      "Max loss: 0.010541576892137527\n",
      "Min loss: 0.004653760697692633\n",
      "Mean loss: 0.0070041856573273735\n",
      "Std loss: 0.001998177075050243\n",
      "Total Loss: 0.04202511394396424\n",
      "------------------------------------ epoch 8368 (50202 steps) ------------------------------------\n",
      "Max loss: 0.012014245614409447\n",
      "Min loss: 0.003497855272144079\n",
      "Mean loss: 0.00658931346454968\n",
      "Std loss: 0.003004361827421784\n",
      "Total Loss: 0.03953588078729808\n",
      "------------------------------------ epoch 8369 (50208 steps) ------------------------------------\n",
      "Max loss: 0.013555631041526794\n",
      "Min loss: 0.004213439300656319\n",
      "Mean loss: 0.007217590231448412\n",
      "Std loss: 0.0031454747419647736\n",
      "Total Loss: 0.04330554138869047\n",
      "------------------------------------ epoch 8370 (50214 steps) ------------------------------------\n",
      "Max loss: 0.01928119547665119\n",
      "Min loss: 0.0036679625045508146\n",
      "Mean loss: 0.007609847855443756\n",
      "Std loss: 0.005466687228804041\n",
      "Total Loss: 0.045659087132662535\n",
      "------------------------------------ epoch 8371 (50220 steps) ------------------------------------\n",
      "Max loss: 0.011440704576671124\n",
      "Min loss: 0.005318876355886459\n",
      "Mean loss: 0.0066172129008919\n",
      "Std loss: 0.0021797891351616095\n",
      "Total Loss: 0.0397032774053514\n",
      "------------------------------------ epoch 8372 (50226 steps) ------------------------------------\n",
      "Max loss: 0.007524600252509117\n",
      "Min loss: 0.0036855265498161316\n",
      "Mean loss: 0.0059682405553758144\n",
      "Std loss: 0.001232772615967282\n",
      "Total Loss: 0.03580944333225489\n",
      "------------------------------------ epoch 8373 (50232 steps) ------------------------------------\n",
      "Max loss: 0.019808903336524963\n",
      "Min loss: 0.0038378245662897825\n",
      "Mean loss: 0.009315477645335099\n",
      "Std loss: 0.006253432671127015\n",
      "Total Loss: 0.05589286587201059\n",
      "------------------------------------ epoch 8374 (50238 steps) ------------------------------------\n",
      "Max loss: 0.027294766157865524\n",
      "Min loss: 0.004953705705702305\n",
      "Mean loss: 0.010111878237997493\n",
      "Std loss: 0.007881288784554479\n",
      "Total Loss: 0.06067126942798495\n",
      "------------------------------------ epoch 8375 (50244 steps) ------------------------------------\n",
      "Max loss: 0.0090554840862751\n",
      "Min loss: 0.00489561352878809\n",
      "Mean loss: 0.006181646992142002\n",
      "Std loss: 0.0014070447335930926\n",
      "Total Loss: 0.03708988195285201\n",
      "------------------------------------ epoch 8376 (50250 steps) ------------------------------------\n",
      "Max loss: 0.012874866835772991\n",
      "Min loss: 0.0036324155516922474\n",
      "Mean loss: 0.0069669609268506365\n",
      "Std loss: 0.0029587808320159386\n",
      "Total Loss: 0.04180176556110382\n",
      "------------------------------------ epoch 8377 (50256 steps) ------------------------------------\n",
      "Max loss: 0.008780783042311668\n",
      "Min loss: 0.005009838379919529\n",
      "Mean loss: 0.006189375339696805\n",
      "Std loss: 0.0014058929883797797\n",
      "Total Loss: 0.03713625203818083\n",
      "------------------------------------ epoch 8378 (50262 steps) ------------------------------------\n",
      "Max loss: 0.020290687680244446\n",
      "Min loss: 0.0034957402385771275\n",
      "Mean loss: 0.011152512626722455\n",
      "Std loss: 0.00514901236187257\n",
      "Total Loss: 0.06691507576033473\n",
      "------------------------------------ epoch 8379 (50268 steps) ------------------------------------\n",
      "Max loss: 0.008833481930196285\n",
      "Min loss: 0.004186135716736317\n",
      "Mean loss: 0.006804596865549684\n",
      "Std loss: 0.0014325448771140199\n",
      "Total Loss: 0.0408275811932981\n",
      "------------------------------------ epoch 8380 (50274 steps) ------------------------------------\n",
      "Max loss: 0.03906598687171936\n",
      "Min loss: 0.00440330570563674\n",
      "Mean loss: 0.011517556617036462\n",
      "Std loss: 0.012457126895323363\n",
      "Total Loss: 0.06910533970221877\n",
      "------------------------------------ epoch 8381 (50280 steps) ------------------------------------\n",
      "Max loss: 0.010667622089385986\n",
      "Min loss: 0.00452288007363677\n",
      "Mean loss: 0.008158274305363497\n",
      "Std loss: 0.0021641012205711276\n",
      "Total Loss: 0.04894964583218098\n",
      "------------------------------------ epoch 8382 (50286 steps) ------------------------------------\n",
      "Max loss: 0.010970186442136765\n",
      "Min loss: 0.004478896502405405\n",
      "Mean loss: 0.007209315663203597\n",
      "Std loss: 0.002027357931997649\n",
      "Total Loss: 0.04325589397922158\n",
      "------------------------------------ epoch 8383 (50292 steps) ------------------------------------\n",
      "Max loss: 0.024953290820121765\n",
      "Min loss: 0.005397572182118893\n",
      "Mean loss: 0.010742945674185952\n",
      "Std loss: 0.006526632955179122\n",
      "Total Loss: 0.06445767404511571\n",
      "------------------------------------ epoch 8384 (50298 steps) ------------------------------------\n",
      "Max loss: 0.01791238784790039\n",
      "Min loss: 0.0038495648186653852\n",
      "Mean loss: 0.007973648801756402\n",
      "Std loss: 0.005316495755853576\n",
      "Total Loss: 0.04784189281053841\n",
      "------------------------------------ epoch 8385 (50304 steps) ------------------------------------\n",
      "Max loss: 0.01026981696486473\n",
      "Min loss: 0.005540747195482254\n",
      "Mean loss: 0.007168760833640893\n",
      "Std loss: 0.0014978457946833352\n",
      "Total Loss: 0.04301256500184536\n",
      "------------------------------------ epoch 8386 (50310 steps) ------------------------------------\n",
      "Max loss: 0.006797251757234335\n",
      "Min loss: 0.004768792539834976\n",
      "Mean loss: 0.005523638256515066\n",
      "Std loss: 0.0008224391910506354\n",
      "Total Loss: 0.033141829539090395\n",
      "------------------------------------ epoch 8387 (50316 steps) ------------------------------------\n",
      "Max loss: 0.006790205370634794\n",
      "Min loss: 0.003600670490413904\n",
      "Mean loss: 0.005212369374930859\n",
      "Std loss: 0.001092467802983256\n",
      "Total Loss: 0.03127421624958515\n",
      "------------------------------------ epoch 8388 (50322 steps) ------------------------------------\n",
      "Max loss: 0.012202342972159386\n",
      "Min loss: 0.0031690672039985657\n",
      "Mean loss: 0.007154059829190373\n",
      "Std loss: 0.0033941717173126278\n",
      "Total Loss: 0.04292435897514224\n",
      "------------------------------------ epoch 8389 (50328 steps) ------------------------------------\n",
      "Max loss: 0.015627523884177208\n",
      "Min loss: 0.003268106374889612\n",
      "Mean loss: 0.00886075485808154\n",
      "Std loss: 0.00523028337205431\n",
      "Total Loss: 0.05316452914848924\n",
      "------------------------------------ epoch 8390 (50334 steps) ------------------------------------\n",
      "Max loss: 0.011934591457247734\n",
      "Min loss: 0.0031599197536706924\n",
      "Mean loss: 0.00691195127243797\n",
      "Std loss: 0.0027902344117399086\n",
      "Total Loss: 0.04147170763462782\n",
      "------------------------------------ epoch 8391 (50340 steps) ------------------------------------\n",
      "Max loss: 0.022508736699819565\n",
      "Min loss: 0.004027768969535828\n",
      "Mean loss: 0.009045504654447237\n",
      "Std loss: 0.006241124579453707\n",
      "Total Loss: 0.054273027926683426\n",
      "------------------------------------ epoch 8392 (50346 steps) ------------------------------------\n",
      "Max loss: 0.02146948128938675\n",
      "Min loss: 0.0058204857632517815\n",
      "Mean loss: 0.01181969380316635\n",
      "Std loss: 0.006625938759401544\n",
      "Total Loss: 0.0709181628189981\n",
      "------------------------------------ epoch 8393 (50352 steps) ------------------------------------\n",
      "Max loss: 0.01986275427043438\n",
      "Min loss: 0.004080749116837978\n",
      "Mean loss: 0.009768458704153696\n",
      "Std loss: 0.00552417949660822\n",
      "Total Loss: 0.05861075222492218\n",
      "------------------------------------ epoch 8394 (50358 steps) ------------------------------------\n",
      "Max loss: 0.019143812358379364\n",
      "Min loss: 0.0052560484036803246\n",
      "Mean loss: 0.009830171785627803\n",
      "Std loss: 0.004859452273498934\n",
      "Total Loss: 0.05898103071376681\n",
      "------------------------------------ epoch 8395 (50364 steps) ------------------------------------\n",
      "Max loss: 0.024379974231123924\n",
      "Min loss: 0.0052236891351640224\n",
      "Mean loss: 0.01175262057222426\n",
      "Std loss: 0.006377407638312321\n",
      "Total Loss: 0.07051572343334556\n",
      "------------------------------------ epoch 8396 (50370 steps) ------------------------------------\n",
      "Max loss: 0.010647870600223541\n",
      "Min loss: 0.0046018678694963455\n",
      "Mean loss: 0.00664696074090898\n",
      "Std loss: 0.0019548876533877326\n",
      "Total Loss: 0.03988176444545388\n",
      "------------------------------------ epoch 8397 (50376 steps) ------------------------------------\n",
      "Max loss: 0.028709586709737778\n",
      "Min loss: 0.0046608103439211845\n",
      "Mean loss: 0.0101528517746677\n",
      "Std loss: 0.008394994201732982\n",
      "Total Loss: 0.0609171106480062\n",
      "------------------------------------ epoch 8398 (50382 steps) ------------------------------------\n",
      "Max loss: 0.030402235686779022\n",
      "Min loss: 0.00495038740336895\n",
      "Mean loss: 0.012635349994525313\n",
      "Std loss: 0.00877151203898276\n",
      "Total Loss: 0.07581209996715188\n",
      "------------------------------------ epoch 8399 (50388 steps) ------------------------------------\n",
      "Max loss: 0.013634413480758667\n",
      "Min loss: 0.004427315201610327\n",
      "Mean loss: 0.007571920054033399\n",
      "Std loss: 0.002990380924644494\n",
      "Total Loss: 0.04543152032420039\n",
      "------------------------------------ epoch 8400 (50394 steps) ------------------------------------\n",
      "Max loss: 0.010104779154062271\n",
      "Min loss: 0.0043108295649290085\n",
      "Mean loss: 0.006404578220099211\n",
      "Std loss: 0.002004969069863132\n",
      "Total Loss: 0.038427469320595264\n",
      "------------------------------------ epoch 8401 (50400 steps) ------------------------------------\n",
      "Max loss: 0.012153973802924156\n",
      "Min loss: 0.003868440166115761\n",
      "Mean loss: 0.007299023137117426\n",
      "Std loss: 0.0027482453990865774\n",
      "Total Loss: 0.043794138822704554\n",
      "saved model at ./weights/model_8401.pth\n",
      "------------------------------------ epoch 8402 (50406 steps) ------------------------------------\n",
      "Max loss: 0.006114940159022808\n",
      "Min loss: 0.0034375893883407116\n",
      "Mean loss: 0.005204560235142708\n",
      "Std loss: 0.0009131550387690964\n",
      "Total Loss: 0.031227361410856247\n",
      "------------------------------------ epoch 8403 (50412 steps) ------------------------------------\n",
      "Max loss: 0.018996387720108032\n",
      "Min loss: 0.0038133999332785606\n",
      "Mean loss: 0.007443465020818015\n",
      "Std loss: 0.0055608613309969974\n",
      "Total Loss: 0.04466079012490809\n",
      "------------------------------------ epoch 8404 (50418 steps) ------------------------------------\n",
      "Max loss: 0.011324903927743435\n",
      "Min loss: 0.004389706999063492\n",
      "Mean loss: 0.006264594461147984\n",
      "Std loss: 0.002439402780426331\n",
      "Total Loss: 0.0375875667668879\n",
      "------------------------------------ epoch 8405 (50424 steps) ------------------------------------\n",
      "Max loss: 0.0172966867685318\n",
      "Min loss: 0.003297799965366721\n",
      "Mean loss: 0.007219228311441839\n",
      "Std loss: 0.0046242244709308175\n",
      "Total Loss: 0.04331536986865103\n",
      "------------------------------------ epoch 8406 (50430 steps) ------------------------------------\n",
      "Max loss: 0.043779727071523666\n",
      "Min loss: 0.004100512713193893\n",
      "Mean loss: 0.011839644284918904\n",
      "Std loss: 0.014374345147904938\n",
      "Total Loss: 0.07103786570951343\n",
      "------------------------------------ epoch 8407 (50436 steps) ------------------------------------\n",
      "Max loss: 0.02638886123895645\n",
      "Min loss: 0.005085369572043419\n",
      "Mean loss: 0.011723196133971214\n",
      "Std loss: 0.007062046349239137\n",
      "Total Loss: 0.07033917680382729\n",
      "------------------------------------ epoch 8408 (50442 steps) ------------------------------------\n",
      "Max loss: 0.011129464954137802\n",
      "Min loss: 0.0040540508925914764\n",
      "Mean loss: 0.006557739262158672\n",
      "Std loss: 0.002605001579609616\n",
      "Total Loss: 0.03934643557295203\n",
      "------------------------------------ epoch 8409 (50448 steps) ------------------------------------\n",
      "Max loss: 0.012677344493567944\n",
      "Min loss: 0.0041081253439188\n",
      "Mean loss: 0.007423256679127614\n",
      "Std loss: 0.0028401617504230396\n",
      "Total Loss: 0.04453954007476568\n",
      "------------------------------------ epoch 8410 (50454 steps) ------------------------------------\n",
      "Max loss: 0.010677541606128216\n",
      "Min loss: 0.004258052445948124\n",
      "Mean loss: 0.006223154254257679\n",
      "Std loss: 0.0021143466936757167\n",
      "Total Loss: 0.037338925525546074\n",
      "------------------------------------ epoch 8411 (50460 steps) ------------------------------------\n",
      "Max loss: 0.02951977774500847\n",
      "Min loss: 0.004095678683370352\n",
      "Mean loss: 0.009719601133838296\n",
      "Std loss: 0.008927419513893734\n",
      "Total Loss: 0.058317606803029776\n",
      "------------------------------------ epoch 8412 (50466 steps) ------------------------------------\n",
      "Max loss: 0.010185371153056622\n",
      "Min loss: 0.0035275674890726805\n",
      "Mean loss: 0.006214179913513362\n",
      "Std loss: 0.002486425002213003\n",
      "Total Loss: 0.037285079481080174\n",
      "------------------------------------ epoch 8413 (50472 steps) ------------------------------------\n",
      "Max loss: 0.009135051630437374\n",
      "Min loss: 0.004665439482778311\n",
      "Mean loss: 0.00692250292437772\n",
      "Std loss: 0.0014991125157116222\n",
      "Total Loss: 0.04153501754626632\n",
      "------------------------------------ epoch 8414 (50478 steps) ------------------------------------\n",
      "Max loss: 0.019153136759996414\n",
      "Min loss: 0.004254577215760946\n",
      "Mean loss: 0.007556152064353228\n",
      "Std loss: 0.005231306202104255\n",
      "Total Loss: 0.045336912386119366\n",
      "------------------------------------ epoch 8415 (50484 steps) ------------------------------------\n",
      "Max loss: 0.03168070688843727\n",
      "Min loss: 0.003941747825592756\n",
      "Mean loss: 0.010368326678872108\n",
      "Std loss: 0.009586910305127025\n",
      "Total Loss: 0.06220996007323265\n",
      "------------------------------------ epoch 8416 (50490 steps) ------------------------------------\n",
      "Max loss: 0.018727291375398636\n",
      "Min loss: 0.004889396484941244\n",
      "Mean loss: 0.01260219180646042\n",
      "Std loss: 0.005217744335267091\n",
      "Total Loss: 0.07561315083876252\n",
      "------------------------------------ epoch 8417 (50496 steps) ------------------------------------\n",
      "Max loss: 0.011671312153339386\n",
      "Min loss: 0.004254275467246771\n",
      "Mean loss: 0.006919388736908634\n",
      "Std loss: 0.0024554075815381954\n",
      "Total Loss: 0.04151633242145181\n",
      "------------------------------------ epoch 8418 (50502 steps) ------------------------------------\n",
      "Max loss: 0.024608178064227104\n",
      "Min loss: 0.0038598868995904922\n",
      "Mean loss: 0.012039634476726254\n",
      "Std loss: 0.00708419037343154\n",
      "Total Loss: 0.07223780686035752\n",
      "------------------------------------ epoch 8419 (50508 steps) ------------------------------------\n",
      "Max loss: 0.0166188832372427\n",
      "Min loss: 0.004139778669923544\n",
      "Mean loss: 0.008808591247846683\n",
      "Std loss: 0.004147976349345252\n",
      "Total Loss: 0.0528515474870801\n",
      "------------------------------------ epoch 8420 (50514 steps) ------------------------------------\n",
      "Max loss: 0.015053964219987392\n",
      "Min loss: 0.003233657917007804\n",
      "Mean loss: 0.007196598104201257\n",
      "Std loss: 0.003958919745967852\n",
      "Total Loss: 0.04317958862520754\n",
      "------------------------------------ epoch 8421 (50520 steps) ------------------------------------\n",
      "Max loss: 0.011083008721470833\n",
      "Min loss: 0.004286430776119232\n",
      "Mean loss: 0.007594292362531026\n",
      "Std loss: 0.002500399739301965\n",
      "Total Loss: 0.04556575417518616\n",
      "------------------------------------ epoch 8422 (50526 steps) ------------------------------------\n",
      "Max loss: 0.008184763602912426\n",
      "Min loss: 0.003935312852263451\n",
      "Mean loss: 0.006263772568975885\n",
      "Std loss: 0.0015589772345818334\n",
      "Total Loss: 0.037582635413855314\n",
      "------------------------------------ epoch 8423 (50532 steps) ------------------------------------\n",
      "Max loss: 0.010073765181005001\n",
      "Min loss: 0.004133577924221754\n",
      "Mean loss: 0.006365827051922679\n",
      "Std loss: 0.0019096357097393261\n",
      "Total Loss: 0.038194962311536074\n",
      "------------------------------------ epoch 8424 (50538 steps) ------------------------------------\n",
      "Max loss: 0.010235512629151344\n",
      "Min loss: 0.003374302526935935\n",
      "Mean loss: 0.006454252133456369\n",
      "Std loss: 0.002162078787688771\n",
      "Total Loss: 0.038725512800738215\n",
      "------------------------------------ epoch 8425 (50544 steps) ------------------------------------\n",
      "Max loss: 0.009691199287772179\n",
      "Min loss: 0.0037039704620838165\n",
      "Mean loss: 0.006139140576124191\n",
      "Std loss: 0.0018866605550226041\n",
      "Total Loss: 0.03683484345674515\n",
      "------------------------------------ epoch 8426 (50550 steps) ------------------------------------\n",
      "Max loss: 0.0332176610827446\n",
      "Min loss: 0.003454159712418914\n",
      "Mean loss: 0.011397997111392518\n",
      "Std loss: 0.009999629066985335\n",
      "Total Loss: 0.06838798266835511\n",
      "------------------------------------ epoch 8427 (50556 steps) ------------------------------------\n",
      "Max loss: 0.01243302971124649\n",
      "Min loss: 0.003816121956333518\n",
      "Mean loss: 0.007261771281870703\n",
      "Std loss: 0.003629167618614769\n",
      "Total Loss: 0.04357062769122422\n",
      "------------------------------------ epoch 8428 (50562 steps) ------------------------------------\n",
      "Max loss: 0.027895692735910416\n",
      "Min loss: 0.004145719576627016\n",
      "Mean loss: 0.01009213156066835\n",
      "Std loss: 0.008465799351973415\n",
      "Total Loss: 0.060552789364010096\n",
      "------------------------------------ epoch 8429 (50568 steps) ------------------------------------\n",
      "Max loss: 0.012825427576899529\n",
      "Min loss: 0.004215629771351814\n",
      "Mean loss: 0.0067528062500059605\n",
      "Std loss: 0.0031976559948932158\n",
      "Total Loss: 0.04051683750003576\n",
      "------------------------------------ epoch 8430 (50574 steps) ------------------------------------\n",
      "Max loss: 0.04247885197401047\n",
      "Min loss: 0.005296781193464994\n",
      "Mean loss: 0.014792742673307657\n",
      "Std loss: 0.012920148642693382\n",
      "Total Loss: 0.08875645603984594\n",
      "------------------------------------ epoch 8431 (50580 steps) ------------------------------------\n",
      "Max loss: 0.017768338322639465\n",
      "Min loss: 0.005242431070655584\n",
      "Mean loss: 0.00865632543961207\n",
      "Std loss: 0.004279149425629334\n",
      "Total Loss: 0.051937952637672424\n",
      "------------------------------------ epoch 8432 (50586 steps) ------------------------------------\n",
      "Max loss: 0.012841220945119858\n",
      "Min loss: 0.0046134330332279205\n",
      "Mean loss: 0.007223231950774789\n",
      "Std loss: 0.002833483440386791\n",
      "Total Loss: 0.04333939170464873\n",
      "------------------------------------ epoch 8433 (50592 steps) ------------------------------------\n",
      "Max loss: 0.009688746184110641\n",
      "Min loss: 0.003706543240696192\n",
      "Mean loss: 0.0058717261999845505\n",
      "Std loss: 0.0018434746694108808\n",
      "Total Loss: 0.0352303571999073\n",
      "------------------------------------ epoch 8434 (50598 steps) ------------------------------------\n",
      "Max loss: 0.008148844353854656\n",
      "Min loss: 0.003719586879014969\n",
      "Mean loss: 0.005253668874502182\n",
      "Std loss: 0.0014181820848334855\n",
      "Total Loss: 0.03152201324701309\n",
      "------------------------------------ epoch 8435 (50604 steps) ------------------------------------\n",
      "Max loss: 0.02125360630452633\n",
      "Min loss: 0.004259902518242598\n",
      "Mean loss: 0.009111013496294618\n",
      "Std loss: 0.005669946378889426\n",
      "Total Loss: 0.054666080977767706\n",
      "------------------------------------ epoch 8436 (50610 steps) ------------------------------------\n",
      "Max loss: 0.010074089281260967\n",
      "Min loss: 0.004812223836779594\n",
      "Mean loss: 0.007415740517899394\n",
      "Std loss: 0.0021596442091159464\n",
      "Total Loss: 0.044494443107396364\n",
      "------------------------------------ epoch 8437 (50616 steps) ------------------------------------\n",
      "Max loss: 0.010135386139154434\n",
      "Min loss: 0.0034727982711046934\n",
      "Mean loss: 0.005500625780162712\n",
      "Std loss: 0.002159753015544985\n",
      "Total Loss: 0.03300375468097627\n",
      "------------------------------------ epoch 8438 (50622 steps) ------------------------------------\n",
      "Max loss: 0.01342331524938345\n",
      "Min loss: 0.003985030576586723\n",
      "Mean loss: 0.008510850214709839\n",
      "Std loss: 0.0037566920561726675\n",
      "Total Loss: 0.05106510128825903\n",
      "------------------------------------ epoch 8439 (50628 steps) ------------------------------------\n",
      "Max loss: 0.020041197538375854\n",
      "Min loss: 0.004313891753554344\n",
      "Mean loss: 0.009439467064415416\n",
      "Std loss: 0.00560185642128488\n",
      "Total Loss: 0.05663680238649249\n",
      "------------------------------------ epoch 8440 (50634 steps) ------------------------------------\n",
      "Max loss: 0.04663039743900299\n",
      "Min loss: 0.004948856309056282\n",
      "Mean loss: 0.01361413400930663\n",
      "Std loss: 0.014963517409183086\n",
      "Total Loss: 0.08168480405583978\n",
      "------------------------------------ epoch 8441 (50640 steps) ------------------------------------\n",
      "Max loss: 0.014634537510573864\n",
      "Min loss: 0.006000905763357878\n",
      "Mean loss: 0.010280010212833682\n",
      "Std loss: 0.0026112388708925886\n",
      "Total Loss: 0.061680061277002096\n",
      "------------------------------------ epoch 8442 (50646 steps) ------------------------------------\n",
      "Max loss: 0.03281212970614433\n",
      "Min loss: 0.005151109304279089\n",
      "Mean loss: 0.015066931567465266\n",
      "Std loss: 0.009104111185384997\n",
      "Total Loss: 0.0904015894047916\n",
      "------------------------------------ epoch 8443 (50652 steps) ------------------------------------\n",
      "Max loss: 0.034974291920661926\n",
      "Min loss: 0.0057104164734482765\n",
      "Mean loss: 0.012849890006085237\n",
      "Std loss: 0.010448133927369534\n",
      "Total Loss: 0.07709934003651142\n",
      "------------------------------------ epoch 8444 (50658 steps) ------------------------------------\n",
      "Max loss: 0.02253664657473564\n",
      "Min loss: 0.005776581820100546\n",
      "Mean loss: 0.013193013689791163\n",
      "Std loss: 0.005128971349702124\n",
      "Total Loss: 0.07915808213874698\n",
      "------------------------------------ epoch 8445 (50664 steps) ------------------------------------\n",
      "Max loss: 0.011117423884570599\n",
      "Min loss: 0.005557445343583822\n",
      "Mean loss: 0.007923967127377788\n",
      "Std loss: 0.0017536281900005844\n",
      "Total Loss: 0.04754380276426673\n",
      "------------------------------------ epoch 8446 (50670 steps) ------------------------------------\n",
      "Max loss: 0.012749530375003815\n",
      "Min loss: 0.004830948542803526\n",
      "Mean loss: 0.007149708457291126\n",
      "Std loss: 0.0026881080811448996\n",
      "Total Loss: 0.04289825074374676\n",
      "------------------------------------ epoch 8447 (50676 steps) ------------------------------------\n",
      "Max loss: 0.013657866977155209\n",
      "Min loss: 0.0033022563438862562\n",
      "Mean loss: 0.006896081341740985\n",
      "Std loss: 0.0033966575273972903\n",
      "Total Loss: 0.041376488050445914\n",
      "------------------------------------ epoch 8448 (50682 steps) ------------------------------------\n",
      "Max loss: 0.017646046355366707\n",
      "Min loss: 0.004107275046408176\n",
      "Mean loss: 0.008893672687311968\n",
      "Std loss: 0.004496819760000825\n",
      "Total Loss: 0.0533620361238718\n",
      "------------------------------------ epoch 8449 (50688 steps) ------------------------------------\n",
      "Max loss: 0.010925818234682083\n",
      "Min loss: 0.0048179468140006065\n",
      "Mean loss: 0.006675616294766466\n",
      "Std loss: 0.002275272278034789\n",
      "Total Loss: 0.040053697768598795\n",
      "------------------------------------ epoch 8450 (50694 steps) ------------------------------------\n",
      "Max loss: 0.00836679246276617\n",
      "Min loss: 0.004633598495274782\n",
      "Mean loss: 0.005753885954618454\n",
      "Std loss: 0.0012668868382088461\n",
      "Total Loss: 0.034523315727710724\n",
      "------------------------------------ epoch 8451 (50700 steps) ------------------------------------\n",
      "Max loss: 0.017572622746229172\n",
      "Min loss: 0.004811864346265793\n",
      "Mean loss: 0.008658139811207851\n",
      "Std loss: 0.004501014909260153\n",
      "Total Loss: 0.051948838867247105\n",
      "------------------------------------ epoch 8452 (50706 steps) ------------------------------------\n",
      "Max loss: 0.00959585141390562\n",
      "Min loss: 0.004604690708220005\n",
      "Mean loss: 0.006460995879024267\n",
      "Std loss: 0.0017241509544478589\n",
      "Total Loss: 0.0387659752741456\n",
      "------------------------------------ epoch 8453 (50712 steps) ------------------------------------\n",
      "Max loss: 0.01693502813577652\n",
      "Min loss: 0.004496451932936907\n",
      "Mean loss: 0.009124849224463105\n",
      "Std loss: 0.004687624776240889\n",
      "Total Loss: 0.05474909534677863\n",
      "------------------------------------ epoch 8454 (50718 steps) ------------------------------------\n",
      "Max loss: 0.014485637657344341\n",
      "Min loss: 0.0037630139850080013\n",
      "Mean loss: 0.007399487076327205\n",
      "Std loss: 0.003603852630475463\n",
      "Total Loss: 0.04439692245796323\n",
      "------------------------------------ epoch 8455 (50724 steps) ------------------------------------\n",
      "Max loss: 0.014182749204337597\n",
      "Min loss: 0.004387248307466507\n",
      "Mean loss: 0.008080643756935993\n",
      "Std loss: 0.0029980369383585926\n",
      "Total Loss: 0.04848386254161596\n",
      "------------------------------------ epoch 8456 (50730 steps) ------------------------------------\n",
      "Max loss: 0.02352885901927948\n",
      "Min loss: 0.003632767591625452\n",
      "Mean loss: 0.00847032906798025\n",
      "Std loss: 0.0068164791308145\n",
      "Total Loss: 0.0508219744078815\n",
      "------------------------------------ epoch 8457 (50736 steps) ------------------------------------\n",
      "Max loss: 0.010526895523071289\n",
      "Min loss: 0.004001819994300604\n",
      "Mean loss: 0.006633341778069735\n",
      "Std loss: 0.002533510733574522\n",
      "Total Loss: 0.03980005066841841\n",
      "------------------------------------ epoch 8458 (50742 steps) ------------------------------------\n",
      "Max loss: 0.00988388154655695\n",
      "Min loss: 0.003158671548590064\n",
      "Mean loss: 0.006610075128264725\n",
      "Std loss: 0.0022891460639047124\n",
      "Total Loss: 0.03966045076958835\n",
      "------------------------------------ epoch 8459 (50748 steps) ------------------------------------\n",
      "Max loss: 0.00580163300037384\n",
      "Min loss: 0.0032932329922914505\n",
      "Mean loss: 0.0045767897584786015\n",
      "Std loss: 0.0007886461503525391\n",
      "Total Loss: 0.02746073855087161\n",
      "------------------------------------ epoch 8460 (50754 steps) ------------------------------------\n",
      "Max loss: 0.011943548917770386\n",
      "Min loss: 0.0032310846727341413\n",
      "Mean loss: 0.006903236033394933\n",
      "Std loss: 0.0034546480753498516\n",
      "Total Loss: 0.041419416200369596\n",
      "------------------------------------ epoch 8461 (50760 steps) ------------------------------------\n",
      "Max loss: 0.017424631863832474\n",
      "Min loss: 0.003997727297246456\n",
      "Mean loss: 0.00949434043529133\n",
      "Std loss: 0.004968835390425667\n",
      "Total Loss: 0.05696604261174798\n",
      "------------------------------------ epoch 8462 (50766 steps) ------------------------------------\n",
      "Max loss: 0.01009143702685833\n",
      "Min loss: 0.004548400640487671\n",
      "Mean loss: 0.006980395798260967\n",
      "Std loss: 0.001740122058455109\n",
      "Total Loss: 0.0418823747895658\n",
      "------------------------------------ epoch 8463 (50772 steps) ------------------------------------\n",
      "Max loss: 0.01311085931956768\n",
      "Min loss: 0.004506293218582869\n",
      "Mean loss: 0.007208394662787517\n",
      "Std loss: 0.0030300928404505337\n",
      "Total Loss: 0.0432503679767251\n",
      "------------------------------------ epoch 8464 (50778 steps) ------------------------------------\n",
      "Max loss: 0.01704709604382515\n",
      "Min loss: 0.003014035988599062\n",
      "Mean loss: 0.007940574393918117\n",
      "Std loss: 0.004473934667896674\n",
      "Total Loss: 0.0476434463635087\n",
      "------------------------------------ epoch 8465 (50784 steps) ------------------------------------\n",
      "Max loss: 0.031812094151973724\n",
      "Min loss: 0.003621916752308607\n",
      "Mean loss: 0.012409311020746827\n",
      "Std loss: 0.010694885154802769\n",
      "Total Loss: 0.07445586612448096\n",
      "------------------------------------ epoch 8466 (50790 steps) ------------------------------------\n",
      "Max loss: 0.019792091101408005\n",
      "Min loss: 0.004269808530807495\n",
      "Mean loss: 0.008056343300268054\n",
      "Std loss: 0.005339961849323025\n",
      "Total Loss: 0.048338059801608324\n",
      "------------------------------------ epoch 8467 (50796 steps) ------------------------------------\n",
      "Max loss: 0.017284056171774864\n",
      "Min loss: 0.004188546445220709\n",
      "Mean loss: 0.01000929259074231\n",
      "Std loss: 0.004229571880711901\n",
      "Total Loss: 0.06005575554445386\n",
      "------------------------------------ epoch 8468 (50802 steps) ------------------------------------\n",
      "Max loss: 0.009165599942207336\n",
      "Min loss: 0.004272710531949997\n",
      "Mean loss: 0.007208132805923621\n",
      "Std loss: 0.0020992601675852572\n",
      "Total Loss: 0.043248796835541725\n",
      "------------------------------------ epoch 8469 (50808 steps) ------------------------------------\n",
      "Max loss: 0.00715537928044796\n",
      "Min loss: 0.0038648899644613266\n",
      "Mean loss: 0.005240504319469134\n",
      "Std loss: 0.0010755851619544735\n",
      "Total Loss: 0.031443025916814804\n",
      "------------------------------------ epoch 8470 (50814 steps) ------------------------------------\n",
      "Max loss: 0.009891659021377563\n",
      "Min loss: 0.003501590806990862\n",
      "Mean loss: 0.006781739415600896\n",
      "Std loss: 0.002246851424162856\n",
      "Total Loss: 0.040690436493605375\n",
      "------------------------------------ epoch 8471 (50820 steps) ------------------------------------\n",
      "Max loss: 0.02183191478252411\n",
      "Min loss: 0.003949474543333054\n",
      "Mean loss: 0.00887998410811027\n",
      "Std loss: 0.005936836189150814\n",
      "Total Loss: 0.05327990464866161\n",
      "------------------------------------ epoch 8472 (50826 steps) ------------------------------------\n",
      "Max loss: 0.009951220825314522\n",
      "Min loss: 0.004166075028479099\n",
      "Mean loss: 0.006101327793051799\n",
      "Std loss: 0.0021554759527094448\n",
      "Total Loss: 0.036607966758310795\n",
      "------------------------------------ epoch 8473 (50832 steps) ------------------------------------\n",
      "Max loss: 0.007220055907964706\n",
      "Min loss: 0.003329560859128833\n",
      "Mean loss: 0.005221540108323097\n",
      "Std loss: 0.0014366201770362513\n",
      "Total Loss: 0.03132924064993858\n",
      "------------------------------------ epoch 8474 (50838 steps) ------------------------------------\n",
      "Max loss: 0.019159970805048943\n",
      "Min loss: 0.003670256584882736\n",
      "Mean loss: 0.0085092483398815\n",
      "Std loss: 0.005340654637838256\n",
      "Total Loss: 0.051055490039289\n",
      "------------------------------------ epoch 8475 (50844 steps) ------------------------------------\n",
      "Max loss: 0.008585283532738686\n",
      "Min loss: 0.0037225373089313507\n",
      "Mean loss: 0.006316233115891616\n",
      "Std loss: 0.0016016948739221218\n",
      "Total Loss: 0.03789739869534969\n",
      "------------------------------------ epoch 8476 (50850 steps) ------------------------------------\n",
      "Max loss: 0.01763870380818844\n",
      "Min loss: 0.0046325852163136005\n",
      "Mean loss: 0.008541485682750741\n",
      "Std loss: 0.004484930848720022\n",
      "Total Loss: 0.05124891409650445\n",
      "------------------------------------ epoch 8477 (50856 steps) ------------------------------------\n",
      "Max loss: 0.03153321519494057\n",
      "Min loss: 0.00539434514939785\n",
      "Mean loss: 0.014841906726360321\n",
      "Std loss: 0.008029641870661559\n",
      "Total Loss: 0.08905144035816193\n",
      "------------------------------------ epoch 8478 (50862 steps) ------------------------------------\n",
      "Max loss: 0.011736707761883736\n",
      "Min loss: 0.004103177227079868\n",
      "Mean loss: 0.007239643561964233\n",
      "Std loss: 0.0024736530681842167\n",
      "Total Loss: 0.0434378613717854\n",
      "------------------------------------ epoch 8479 (50868 steps) ------------------------------------\n",
      "Max loss: 0.015223483555018902\n",
      "Min loss: 0.005213630851358175\n",
      "Mean loss: 0.010245303312937418\n",
      "Std loss: 0.0032214924804598154\n",
      "Total Loss: 0.06147181987762451\n",
      "------------------------------------ epoch 8480 (50874 steps) ------------------------------------\n",
      "Max loss: 0.02065771445631981\n",
      "Min loss: 0.003961315378546715\n",
      "Mean loss: 0.009730781428515911\n",
      "Std loss: 0.005665626032652149\n",
      "Total Loss: 0.05838468857109547\n",
      "------------------------------------ epoch 8481 (50880 steps) ------------------------------------\n",
      "Max loss: 0.036984384059906006\n",
      "Min loss: 0.004870975390076637\n",
      "Mean loss: 0.013478486953924099\n",
      "Std loss: 0.011020189812969062\n",
      "Total Loss: 0.0808709217235446\n",
      "------------------------------------ epoch 8482 (50886 steps) ------------------------------------\n",
      "Max loss: 0.010474043898284435\n",
      "Min loss: 0.0038140169344842434\n",
      "Mean loss: 0.005963354914759596\n",
      "Std loss: 0.0022516726866181696\n",
      "Total Loss: 0.03578012948855758\n",
      "------------------------------------ epoch 8483 (50892 steps) ------------------------------------\n",
      "Max loss: 0.013879387639462948\n",
      "Min loss: 0.004500898532569408\n",
      "Mean loss: 0.007766942105566462\n",
      "Std loss: 0.003134087400897627\n",
      "Total Loss: 0.04660165263339877\n",
      "------------------------------------ epoch 8484 (50898 steps) ------------------------------------\n",
      "Max loss: 0.02641654573380947\n",
      "Min loss: 0.004962264560163021\n",
      "Mean loss: 0.014989730436354876\n",
      "Std loss: 0.006582193296836228\n",
      "Total Loss: 0.08993838261812925\n",
      "------------------------------------ epoch 8485 (50904 steps) ------------------------------------\n",
      "Max loss: 0.03140966594219208\n",
      "Min loss: 0.005458717234432697\n",
      "Mean loss: 0.011285332419599095\n",
      "Std loss: 0.00910529579244156\n",
      "Total Loss: 0.06771199451759458\n",
      "------------------------------------ epoch 8486 (50910 steps) ------------------------------------\n",
      "Max loss: 0.0185545664280653\n",
      "Min loss: 0.006019751541316509\n",
      "Mean loss: 0.011626624579851827\n",
      "Std loss: 0.004825582185811806\n",
      "Total Loss: 0.06975974747911096\n",
      "------------------------------------ epoch 8487 (50916 steps) ------------------------------------\n",
      "Max loss: 0.02138913981616497\n",
      "Min loss: 0.0047561535611748695\n",
      "Mean loss: 0.01096580425898234\n",
      "Std loss: 0.005560626880503676\n",
      "Total Loss: 0.06579482555389404\n",
      "------------------------------------ epoch 8488 (50922 steps) ------------------------------------\n",
      "Max loss: 0.012439437210559845\n",
      "Min loss: 0.004661131650209427\n",
      "Mean loss: 0.00833441635283331\n",
      "Std loss: 0.002287293307687536\n",
      "Total Loss: 0.050006498116999865\n",
      "------------------------------------ epoch 8489 (50928 steps) ------------------------------------\n",
      "Max loss: 0.014903604984283447\n",
      "Min loss: 0.0032632823567837477\n",
      "Mean loss: 0.010602318798191845\n",
      "Std loss: 0.004968807334020923\n",
      "Total Loss: 0.06361391278915107\n",
      "------------------------------------ epoch 8490 (50934 steps) ------------------------------------\n",
      "Max loss: 0.015621239319443703\n",
      "Min loss: 0.0040728142485022545\n",
      "Mean loss: 0.0075246306757132215\n",
      "Std loss: 0.0037880378119175893\n",
      "Total Loss: 0.04514778405427933\n",
      "------------------------------------ epoch 8491 (50940 steps) ------------------------------------\n",
      "Max loss: 0.012391234748065472\n",
      "Min loss: 0.003981455694884062\n",
      "Mean loss: 0.007512808156510194\n",
      "Std loss: 0.003335467293977927\n",
      "Total Loss: 0.045076848939061165\n",
      "------------------------------------ epoch 8492 (50946 steps) ------------------------------------\n",
      "Max loss: 0.009052615612745285\n",
      "Min loss: 0.005146537907421589\n",
      "Mean loss: 0.006861035634453098\n",
      "Std loss: 0.00158783127339214\n",
      "Total Loss: 0.04116621380671859\n",
      "------------------------------------ epoch 8493 (50952 steps) ------------------------------------\n",
      "Max loss: 0.017507150769233704\n",
      "Min loss: 0.0051728663966059685\n",
      "Mean loss: 0.009167374111711979\n",
      "Std loss: 0.004252441952403859\n",
      "Total Loss: 0.055004244670271873\n",
      "------------------------------------ epoch 8494 (50958 steps) ------------------------------------\n",
      "Max loss: 0.017020240426063538\n",
      "Min loss: 0.007526385597884655\n",
      "Mean loss: 0.009700791367019216\n",
      "Std loss: 0.0033483938154089984\n",
      "Total Loss: 0.0582047482021153\n",
      "------------------------------------ epoch 8495 (50964 steps) ------------------------------------\n",
      "Max loss: 0.009320231154561043\n",
      "Min loss: 0.00465101283043623\n",
      "Mean loss: 0.005935950670391321\n",
      "Std loss: 0.0016364769080197395\n",
      "Total Loss: 0.03561570402234793\n",
      "------------------------------------ epoch 8496 (50970 steps) ------------------------------------\n",
      "Max loss: 0.020532432943582535\n",
      "Min loss: 0.005603334866464138\n",
      "Mean loss: 0.009170830637837449\n",
      "Std loss: 0.005131322540503896\n",
      "Total Loss: 0.0550249838270247\n",
      "------------------------------------ epoch 8497 (50976 steps) ------------------------------------\n",
      "Max loss: 0.010440065525472164\n",
      "Min loss: 0.004928457085043192\n",
      "Mean loss: 0.007998020310575763\n",
      "Std loss: 0.001672913468186798\n",
      "Total Loss: 0.04798812186345458\n",
      "------------------------------------ epoch 8498 (50982 steps) ------------------------------------\n",
      "Max loss: 0.009662042371928692\n",
      "Min loss: 0.0033698794431984425\n",
      "Mean loss: 0.006323246518149972\n",
      "Std loss: 0.002019550792593474\n",
      "Total Loss: 0.03793947910889983\n",
      "------------------------------------ epoch 8499 (50988 steps) ------------------------------------\n",
      "Max loss: 0.0065013631246984005\n",
      "Min loss: 0.004130661953240633\n",
      "Mean loss: 0.005497229751199484\n",
      "Std loss: 0.0008752680615807729\n",
      "Total Loss: 0.0329833785071969\n",
      "------------------------------------ epoch 8500 (50994 steps) ------------------------------------\n",
      "Max loss: 0.006666946224868298\n",
      "Min loss: 0.003968634642660618\n",
      "Mean loss: 0.004725918484230836\n",
      "Std loss: 0.0009670054044735399\n",
      "Total Loss: 0.028355510905385017\n",
      "------------------------------------ epoch 8501 (51000 steps) ------------------------------------\n",
      "Max loss: 0.004169992171227932\n",
      "Min loss: 0.003424900583922863\n",
      "Mean loss: 0.0039005727351953587\n",
      "Std loss: 0.00026020780936161504\n",
      "Total Loss: 0.02340343641117215\n",
      "saved model at ./weights/model_8501.pth\n",
      "------------------------------------ epoch 8502 (51006 steps) ------------------------------------\n",
      "Max loss: 0.013452552258968353\n",
      "Min loss: 0.0041794669814407825\n",
      "Mean loss: 0.005956901547809442\n",
      "Std loss: 0.0033607027157163427\n",
      "Total Loss: 0.03574140928685665\n",
      "------------------------------------ epoch 8503 (51012 steps) ------------------------------------\n",
      "Max loss: 0.01391561608761549\n",
      "Min loss: 0.004166820086538792\n",
      "Mean loss: 0.007822572564085325\n",
      "Std loss: 0.003993999110166708\n",
      "Total Loss: 0.04693543538451195\n",
      "------------------------------------ epoch 8504 (51018 steps) ------------------------------------\n",
      "Max loss: 0.01163517590612173\n",
      "Min loss: 0.0044360994361341\n",
      "Mean loss: 0.006162093098585804\n",
      "Std loss: 0.0025174404092259857\n",
      "Total Loss: 0.036972558591514826\n",
      "------------------------------------ epoch 8505 (51024 steps) ------------------------------------\n",
      "Max loss: 0.008101028390228748\n",
      "Min loss: 0.003922998905181885\n",
      "Mean loss: 0.0051884755957871675\n",
      "Std loss: 0.0015792146536246104\n",
      "Total Loss: 0.031130853574723005\n",
      "------------------------------------ epoch 8506 (51030 steps) ------------------------------------\n",
      "Max loss: 0.013353108428418636\n",
      "Min loss: 0.00427484093233943\n",
      "Mean loss: 0.006985556722308199\n",
      "Std loss: 0.0030644172731804902\n",
      "Total Loss: 0.04191334033384919\n",
      "------------------------------------ epoch 8507 (51036 steps) ------------------------------------\n",
      "Max loss: 0.014806678518652916\n",
      "Min loss: 0.0035917486529797316\n",
      "Mean loss: 0.007861847795235613\n",
      "Std loss: 0.004010841006089404\n",
      "Total Loss: 0.047171086771413684\n",
      "------------------------------------ epoch 8508 (51042 steps) ------------------------------------\n",
      "Max loss: 0.009863808751106262\n",
      "Min loss: 0.0034343204461038113\n",
      "Mean loss: 0.005805449249843757\n",
      "Std loss: 0.0019731027646805966\n",
      "Total Loss: 0.03483269549906254\n",
      "------------------------------------ epoch 8509 (51048 steps) ------------------------------------\n",
      "Max loss: 0.015132920816540718\n",
      "Min loss: 0.003781875129789114\n",
      "Mean loss: 0.008086117372537652\n",
      "Std loss: 0.004660281740090796\n",
      "Total Loss: 0.048516704235225916\n",
      "------------------------------------ epoch 8510 (51054 steps) ------------------------------------\n",
      "Max loss: 0.009134089574217796\n",
      "Min loss: 0.003970107063651085\n",
      "Mean loss: 0.0056752106950928765\n",
      "Std loss: 0.001754975398677407\n",
      "Total Loss: 0.03405126417055726\n",
      "------------------------------------ epoch 8511 (51060 steps) ------------------------------------\n",
      "Max loss: 0.00877420138567686\n",
      "Min loss: 0.004974811803549528\n",
      "Mean loss: 0.0067778041896720724\n",
      "Std loss: 0.0012179128511158013\n",
      "Total Loss: 0.040666825138032436\n",
      "------------------------------------ epoch 8512 (51066 steps) ------------------------------------\n",
      "Max loss: 0.012929243966937065\n",
      "Min loss: 0.004790483042597771\n",
      "Mean loss: 0.006718692757810156\n",
      "Std loss: 0.00280893502381361\n",
      "Total Loss: 0.04031215654686093\n",
      "------------------------------------ epoch 8513 (51072 steps) ------------------------------------\n",
      "Max loss: 0.0374625064432621\n",
      "Min loss: 0.003858003532513976\n",
      "Mean loss: 0.013730256779429814\n",
      "Std loss: 0.011831066639824998\n",
      "Total Loss: 0.08238154067657888\n",
      "------------------------------------ epoch 8514 (51078 steps) ------------------------------------\n",
      "Max loss: 0.01613631844520569\n",
      "Min loss: 0.004891260527074337\n",
      "Mean loss: 0.007766212724770109\n",
      "Std loss: 0.003874590105076617\n",
      "Total Loss: 0.04659727634862065\n",
      "------------------------------------ epoch 8515 (51084 steps) ------------------------------------\n",
      "Max loss: 0.010432874783873558\n",
      "Min loss: 0.00517619214951992\n",
      "Mean loss: 0.007661859427268307\n",
      "Std loss: 0.0018653217010467815\n",
      "Total Loss: 0.04597115656360984\n",
      "------------------------------------ epoch 8516 (51090 steps) ------------------------------------\n",
      "Max loss: 0.00859970971941948\n",
      "Min loss: 0.004934729542583227\n",
      "Mean loss: 0.006039267561088006\n",
      "Std loss: 0.001213258381680678\n",
      "Total Loss: 0.036235605366528034\n",
      "------------------------------------ epoch 8517 (51096 steps) ------------------------------------\n",
      "Max loss: 0.012914517894387245\n",
      "Min loss: 0.00421238224953413\n",
      "Mean loss: 0.007758601102977991\n",
      "Std loss: 0.0032242113774694167\n",
      "Total Loss: 0.04655160661786795\n",
      "------------------------------------ epoch 8518 (51102 steps) ------------------------------------\n",
      "Max loss: 0.01830838806927204\n",
      "Min loss: 0.00458863191306591\n",
      "Mean loss: 0.00985943820948402\n",
      "Std loss: 0.0058146862026098225\n",
      "Total Loss: 0.059156629256904125\n",
      "------------------------------------ epoch 8519 (51108 steps) ------------------------------------\n",
      "Max loss: 0.04620165750384331\n",
      "Min loss: 0.0061117978766560555\n",
      "Mean loss: 0.019218659416461985\n",
      "Std loss: 0.016703274636656083\n",
      "Total Loss: 0.1153119564987719\n",
      "------------------------------------ epoch 8520 (51114 steps) ------------------------------------\n",
      "Max loss: 0.010043911635875702\n",
      "Min loss: 0.005384141579270363\n",
      "Mean loss: 0.0077378160785883665\n",
      "Std loss: 0.0019300368143773438\n",
      "Total Loss: 0.0464268964715302\n",
      "------------------------------------ epoch 8521 (51120 steps) ------------------------------------\n",
      "Max loss: 0.012877942994236946\n",
      "Min loss: 0.0072081563994288445\n",
      "Mean loss: 0.009769127471372485\n",
      "Std loss: 0.0024563536604308577\n",
      "Total Loss: 0.05861476482823491\n",
      "------------------------------------ epoch 8522 (51126 steps) ------------------------------------\n",
      "Max loss: 0.042858392000198364\n",
      "Min loss: 0.0059777251444756985\n",
      "Mean loss: 0.014042761409655213\n",
      "Std loss: 0.01306446751583196\n",
      "Total Loss: 0.08425656845793128\n",
      "------------------------------------ epoch 8523 (51132 steps) ------------------------------------\n",
      "Max loss: 0.0173601433634758\n",
      "Min loss: 0.004167700186371803\n",
      "Mean loss: 0.009221531838799516\n",
      "Std loss: 0.004554612115567883\n",
      "Total Loss: 0.0553291910327971\n",
      "------------------------------------ epoch 8524 (51138 steps) ------------------------------------\n",
      "Max loss: 0.007699651177972555\n",
      "Min loss: 0.004362531006336212\n",
      "Mean loss: 0.005894331882397334\n",
      "Std loss: 0.0013415922698143783\n",
      "Total Loss: 0.035365991294384\n",
      "------------------------------------ epoch 8525 (51144 steps) ------------------------------------\n",
      "Max loss: 0.03059072606265545\n",
      "Min loss: 0.004847940988838673\n",
      "Mean loss: 0.013010863525172075\n",
      "Std loss: 0.008807493091036282\n",
      "Total Loss: 0.07806518115103245\n",
      "------------------------------------ epoch 8526 (51150 steps) ------------------------------------\n",
      "Max loss: 0.023932166397571564\n",
      "Min loss: 0.006857330910861492\n",
      "Mean loss: 0.012983617605641484\n",
      "Std loss: 0.006365862119495478\n",
      "Total Loss: 0.0779017056338489\n",
      "------------------------------------ epoch 8527 (51156 steps) ------------------------------------\n",
      "Max loss: 0.020197059959173203\n",
      "Min loss: 0.005122028756886721\n",
      "Mean loss: 0.010194307270770272\n",
      "Std loss: 0.005313620329063352\n",
      "Total Loss: 0.06116584362462163\n",
      "------------------------------------ epoch 8528 (51162 steps) ------------------------------------\n",
      "Max loss: 0.016302334144711494\n",
      "Min loss: 0.004763549193739891\n",
      "Mean loss: 0.008173365145921707\n",
      "Std loss: 0.004040242153997811\n",
      "Total Loss: 0.04904019087553024\n",
      "------------------------------------ epoch 8529 (51168 steps) ------------------------------------\n",
      "Max loss: 0.020108457654714584\n",
      "Min loss: 0.004103540442883968\n",
      "Mean loss: 0.011245839763432741\n",
      "Std loss: 0.005339008226606956\n",
      "Total Loss: 0.06747503858059645\n",
      "------------------------------------ epoch 8530 (51174 steps) ------------------------------------\n",
      "Max loss: 0.011737036518752575\n",
      "Min loss: 0.004605429247021675\n",
      "Mean loss: 0.00727737167229255\n",
      "Std loss: 0.0027843132661390774\n",
      "Total Loss: 0.0436642300337553\n",
      "------------------------------------ epoch 8531 (51180 steps) ------------------------------------\n",
      "Max loss: 0.008800756186246872\n",
      "Min loss: 0.004878408741205931\n",
      "Mean loss: 0.006804328955089052\n",
      "Std loss: 0.0013382973756006355\n",
      "Total Loss: 0.040825973730534315\n",
      "------------------------------------ epoch 8532 (51186 steps) ------------------------------------\n",
      "Max loss: 0.009807746857404709\n",
      "Min loss: 0.004157113842666149\n",
      "Mean loss: 0.006151252659037709\n",
      "Std loss: 0.001893060129726886\n",
      "Total Loss: 0.036907515954226255\n",
      "------------------------------------ epoch 8533 (51192 steps) ------------------------------------\n",
      "Max loss: 0.017897188663482666\n",
      "Min loss: 0.003941401839256287\n",
      "Mean loss: 0.00790019721413652\n",
      "Std loss: 0.004702607349850407\n",
      "Total Loss: 0.047401183284819126\n",
      "------------------------------------ epoch 8534 (51198 steps) ------------------------------------\n",
      "Max loss: 0.007915657013654709\n",
      "Min loss: 0.003203237894922495\n",
      "Mean loss: 0.00564295817942669\n",
      "Std loss: 0.0017318388504282349\n",
      "Total Loss: 0.03385774907656014\n",
      "------------------------------------ epoch 8535 (51204 steps) ------------------------------------\n",
      "Max loss: 0.0113364327698946\n",
      "Min loss: 0.003499851329252124\n",
      "Mean loss: 0.006455624592490494\n",
      "Std loss: 0.0026064008004996076\n",
      "Total Loss: 0.038733747554942966\n",
      "------------------------------------ epoch 8536 (51210 steps) ------------------------------------\n",
      "Max loss: 0.011521685868501663\n",
      "Min loss: 0.004806391429156065\n",
      "Mean loss: 0.007303148120020826\n",
      "Std loss: 0.002385146259439216\n",
      "Total Loss: 0.04381888872012496\n",
      "------------------------------------ epoch 8537 (51216 steps) ------------------------------------\n",
      "Max loss: 0.011092054657638073\n",
      "Min loss: 0.003797295270487666\n",
      "Mean loss: 0.0066722943835581345\n",
      "Std loss: 0.0023834565973157966\n",
      "Total Loss: 0.040033766301348805\n",
      "------------------------------------ epoch 8538 (51222 steps) ------------------------------------\n",
      "Max loss: 0.008269822224974632\n",
      "Min loss: 0.004455324728041887\n",
      "Mean loss: 0.00627660130461057\n",
      "Std loss: 0.0015281437776648828\n",
      "Total Loss: 0.03765960782766342\n",
      "------------------------------------ epoch 8539 (51228 steps) ------------------------------------\n",
      "Max loss: 0.014421112835407257\n",
      "Min loss: 0.003842607606202364\n",
      "Mean loss: 0.008577928024654588\n",
      "Std loss: 0.004075682457800969\n",
      "Total Loss: 0.05146756814792752\n",
      "------------------------------------ epoch 8540 (51234 steps) ------------------------------------\n",
      "Max loss: 0.010877527296543121\n",
      "Min loss: 0.0037239985540509224\n",
      "Mean loss: 0.006825541456540425\n",
      "Std loss: 0.002706202708611899\n",
      "Total Loss: 0.040953248739242554\n",
      "------------------------------------ epoch 8541 (51240 steps) ------------------------------------\n",
      "Max loss: 0.027549047023057938\n",
      "Min loss: 0.0035648872144520283\n",
      "Mean loss: 0.010629233283301195\n",
      "Std loss: 0.008446029963415865\n",
      "Total Loss: 0.06377539969980717\n",
      "------------------------------------ epoch 8542 (51246 steps) ------------------------------------\n",
      "Max loss: 0.016705676913261414\n",
      "Min loss: 0.00355532206594944\n",
      "Mean loss: 0.006756636469314496\n",
      "Std loss: 0.004586945210404484\n",
      "Total Loss: 0.040539818815886974\n",
      "------------------------------------ epoch 8543 (51252 steps) ------------------------------------\n",
      "Max loss: 0.009303906001150608\n",
      "Min loss: 0.0036053999792784452\n",
      "Mean loss: 0.005462248499194781\n",
      "Std loss: 0.002168116014893362\n",
      "Total Loss: 0.032773490995168686\n",
      "------------------------------------ epoch 8544 (51258 steps) ------------------------------------\n",
      "Max loss: 0.03419442102313042\n",
      "Min loss: 0.004708507563918829\n",
      "Mean loss: 0.011162529854724804\n",
      "Std loss: 0.010427394542657313\n",
      "Total Loss: 0.06697517912834883\n",
      "------------------------------------ epoch 8545 (51264 steps) ------------------------------------\n",
      "Max loss: 0.009595062583684921\n",
      "Min loss: 0.003646046621724963\n",
      "Mean loss: 0.006090111215598881\n",
      "Std loss: 0.0022567691234221783\n",
      "Total Loss: 0.03654066729359329\n",
      "------------------------------------ epoch 8546 (51270 steps) ------------------------------------\n",
      "Max loss: 0.03215879946947098\n",
      "Min loss: 0.00423709861934185\n",
      "Mean loss: 0.012177407353495559\n",
      "Std loss: 0.009808969687485533\n",
      "Total Loss: 0.07306444412097335\n",
      "------------------------------------ epoch 8547 (51276 steps) ------------------------------------\n",
      "Max loss: 0.013841568492352962\n",
      "Min loss: 0.00456614512950182\n",
      "Mean loss: 0.00845470178561906\n",
      "Std loss: 0.0033676318462786646\n",
      "Total Loss: 0.05072821071371436\n",
      "------------------------------------ epoch 8548 (51282 steps) ------------------------------------\n",
      "Max loss: 0.022390348836779594\n",
      "Min loss: 0.004102025181055069\n",
      "Mean loss: 0.009812990436330438\n",
      "Std loss: 0.006159933142319089\n",
      "Total Loss: 0.058877942617982626\n",
      "------------------------------------ epoch 8549 (51288 steps) ------------------------------------\n",
      "Max loss: 0.007915867492556572\n",
      "Min loss: 0.00412312988191843\n",
      "Mean loss: 0.005647775949910283\n",
      "Std loss: 0.001457510200253027\n",
      "Total Loss: 0.0338866556994617\n",
      "------------------------------------ epoch 8550 (51294 steps) ------------------------------------\n",
      "Max loss: 0.012123147025704384\n",
      "Min loss: 0.004338131286203861\n",
      "Mean loss: 0.007884084402273098\n",
      "Std loss: 0.0029370685772739436\n",
      "Total Loss: 0.04730450641363859\n",
      "------------------------------------ epoch 8551 (51300 steps) ------------------------------------\n",
      "Max loss: 0.03594296798110008\n",
      "Min loss: 0.0044995625503361225\n",
      "Mean loss: 0.012653455215816697\n",
      "Std loss: 0.010599940098146288\n",
      "Total Loss: 0.07592073129490018\n",
      "------------------------------------ epoch 8552 (51306 steps) ------------------------------------\n",
      "Max loss: 0.019600698724389076\n",
      "Min loss: 0.00476947333663702\n",
      "Mean loss: 0.010866959734509388\n",
      "Std loss: 0.005491390848537768\n",
      "Total Loss: 0.06520175840705633\n",
      "------------------------------------ epoch 8553 (51312 steps) ------------------------------------\n",
      "Max loss: 0.03426520526409149\n",
      "Min loss: 0.00540841743350029\n",
      "Mean loss: 0.014253914821892977\n",
      "Std loss: 0.010431083744617636\n",
      "Total Loss: 0.08552348893135786\n",
      "------------------------------------ epoch 8554 (51318 steps) ------------------------------------\n",
      "Max loss: 0.01784406416118145\n",
      "Min loss: 0.0068619130179286\n",
      "Mean loss: 0.01222351457302769\n",
      "Std loss: 0.0032782456368001776\n",
      "Total Loss: 0.07334108743816614\n",
      "------------------------------------ epoch 8555 (51324 steps) ------------------------------------\n",
      "Max loss: 0.009710995480418205\n",
      "Min loss: 0.003717217594385147\n",
      "Mean loss: 0.005939936886231105\n",
      "Std loss: 0.0018583320094222708\n",
      "Total Loss: 0.03563962131738663\n",
      "------------------------------------ epoch 8556 (51330 steps) ------------------------------------\n",
      "Max loss: 0.015160594135522842\n",
      "Min loss: 0.0037244181148707867\n",
      "Mean loss: 0.00838259250546495\n",
      "Std loss: 0.003955952919304081\n",
      "Total Loss: 0.05029555503278971\n",
      "------------------------------------ epoch 8557 (51336 steps) ------------------------------------\n",
      "Max loss: 0.012759100645780563\n",
      "Min loss: 0.003759920597076416\n",
      "Mean loss: 0.006414066689709823\n",
      "Std loss: 0.0030897229167286535\n",
      "Total Loss: 0.038484400138258934\n",
      "------------------------------------ epoch 8558 (51342 steps) ------------------------------------\n",
      "Max loss: 0.020040594041347504\n",
      "Min loss: 0.0034731763880699873\n",
      "Mean loss: 0.008085732581093907\n",
      "Std loss: 0.005932107096635552\n",
      "Total Loss: 0.048514395486563444\n",
      "------------------------------------ epoch 8559 (51348 steps) ------------------------------------\n",
      "Max loss: 0.012708104215562344\n",
      "Min loss: 0.004029109608381987\n",
      "Mean loss: 0.006945710085953276\n",
      "Std loss: 0.0030545679127832654\n",
      "Total Loss: 0.04167426051571965\n",
      "------------------------------------ epoch 8560 (51354 steps) ------------------------------------\n",
      "Max loss: 0.014510794542729855\n",
      "Min loss: 0.00439132284373045\n",
      "Mean loss: 0.007825660907352964\n",
      "Std loss: 0.003289390705932584\n",
      "Total Loss: 0.046953965444117785\n",
      "------------------------------------ epoch 8561 (51360 steps) ------------------------------------\n",
      "Max loss: 0.010125268250703812\n",
      "Min loss: 0.003876883303746581\n",
      "Mean loss: 0.00628234325752904\n",
      "Std loss: 0.002409766007461538\n",
      "Total Loss: 0.03769405954517424\n",
      "------------------------------------ epoch 8562 (51366 steps) ------------------------------------\n",
      "Max loss: 0.010018767789006233\n",
      "Min loss: 0.004663511645048857\n",
      "Mean loss: 0.0068361905092994375\n",
      "Std loss: 0.0018672458206749653\n",
      "Total Loss: 0.04101714305579662\n",
      "------------------------------------ epoch 8563 (51372 steps) ------------------------------------\n",
      "Max loss: 0.019477885216474533\n",
      "Min loss: 0.00524330697953701\n",
      "Mean loss: 0.009122941798220078\n",
      "Std loss: 0.004912476080269233\n",
      "Total Loss: 0.05473765078932047\n",
      "------------------------------------ epoch 8564 (51378 steps) ------------------------------------\n",
      "Max loss: 0.0217052660882473\n",
      "Min loss: 0.003432663856074214\n",
      "Mean loss: 0.008343460853211582\n",
      "Std loss: 0.00642133326224043\n",
      "Total Loss: 0.05006076511926949\n",
      "------------------------------------ epoch 8565 (51384 steps) ------------------------------------\n",
      "Max loss: 0.01308190356940031\n",
      "Min loss: 0.003311328124254942\n",
      "Mean loss: 0.008106892618040243\n",
      "Std loss: 0.0033474757957498687\n",
      "Total Loss: 0.04864135570824146\n",
      "------------------------------------ epoch 8566 (51390 steps) ------------------------------------\n",
      "Max loss: 0.017623096704483032\n",
      "Min loss: 0.004153541289269924\n",
      "Mean loss: 0.011189954821020365\n",
      "Std loss: 0.005133344996945411\n",
      "Total Loss: 0.06713972892612219\n",
      "------------------------------------ epoch 8567 (51396 steps) ------------------------------------\n",
      "Max loss: 0.012108812108635902\n",
      "Min loss: 0.0037394692189991474\n",
      "Mean loss: 0.0068138577820112305\n",
      "Std loss: 0.0026163476233905863\n",
      "Total Loss: 0.040883146692067385\n",
      "------------------------------------ epoch 8568 (51402 steps) ------------------------------------\n",
      "Max loss: 0.025429455563426018\n",
      "Min loss: 0.004187569487839937\n",
      "Mean loss: 0.01043644865664343\n",
      "Std loss: 0.007101600687625149\n",
      "Total Loss: 0.06261869193986058\n",
      "------------------------------------ epoch 8569 (51408 steps) ------------------------------------\n",
      "Max loss: 0.016764355823397636\n",
      "Min loss: 0.004850330296903849\n",
      "Mean loss: 0.008545417338609695\n",
      "Std loss: 0.004156681939702101\n",
      "Total Loss: 0.05127250403165817\n",
      "------------------------------------ epoch 8570 (51414 steps) ------------------------------------\n",
      "Max loss: 0.01584487035870552\n",
      "Min loss: 0.005961184855550528\n",
      "Mean loss: 0.009185600948209563\n",
      "Std loss: 0.0031952189682476007\n",
      "Total Loss: 0.05511360568925738\n",
      "------------------------------------ epoch 8571 (51420 steps) ------------------------------------\n",
      "Max loss: 0.008753050118684769\n",
      "Min loss: 0.005081087350845337\n",
      "Mean loss: 0.006314377222831051\n",
      "Std loss: 0.0014171854065160845\n",
      "Total Loss: 0.0378862633369863\n",
      "------------------------------------ epoch 8572 (51426 steps) ------------------------------------\n",
      "Max loss: 0.01776774972677231\n",
      "Min loss: 0.00498651247471571\n",
      "Mean loss: 0.011648257728666067\n",
      "Std loss: 0.0047680807434080895\n",
      "Total Loss: 0.0698895463719964\n",
      "------------------------------------ epoch 8573 (51432 steps) ------------------------------------\n",
      "Max loss: 0.01148919016122818\n",
      "Min loss: 0.003836724441498518\n",
      "Mean loss: 0.00650578582038482\n",
      "Std loss: 0.002773986654902676\n",
      "Total Loss: 0.03903471492230892\n",
      "------------------------------------ epoch 8574 (51438 steps) ------------------------------------\n",
      "Max loss: 0.02122071385383606\n",
      "Min loss: 0.003611034480854869\n",
      "Mean loss: 0.010106122315240404\n",
      "Std loss: 0.0074892841848863256\n",
      "Total Loss: 0.06063673389144242\n",
      "------------------------------------ epoch 8575 (51444 steps) ------------------------------------\n",
      "Max loss: 0.02560236118733883\n",
      "Min loss: 0.004589737392961979\n",
      "Mean loss: 0.013747416281451782\n",
      "Std loss: 0.007418116845251312\n",
      "Total Loss: 0.08248449768871069\n",
      "------------------------------------ epoch 8576 (51450 steps) ------------------------------------\n",
      "Max loss: 0.01766073703765869\n",
      "Min loss: 0.004429489374160767\n",
      "Mean loss: 0.008346232352778316\n",
      "Std loss: 0.004647733227984892\n",
      "Total Loss: 0.05007739411666989\n",
      "------------------------------------ epoch 8577 (51456 steps) ------------------------------------\n",
      "Max loss: 0.02168748341500759\n",
      "Min loss: 0.005007714964449406\n",
      "Mean loss: 0.009731842825810114\n",
      "Std loss: 0.005691620148099328\n",
      "Total Loss: 0.05839105695486069\n",
      "------------------------------------ epoch 8578 (51462 steps) ------------------------------------\n",
      "Max loss: 0.00975581631064415\n",
      "Min loss: 0.00356045993976295\n",
      "Mean loss: 0.005843869061209261\n",
      "Std loss: 0.0019425406288425633\n",
      "Total Loss: 0.03506321436725557\n",
      "------------------------------------ epoch 8579 (51468 steps) ------------------------------------\n",
      "Max loss: 0.010637293569743633\n",
      "Min loss: 0.004214172717183828\n",
      "Mean loss: 0.00763734554251035\n",
      "Std loss: 0.0019656075691860866\n",
      "Total Loss: 0.0458240732550621\n",
      "------------------------------------ epoch 8580 (51474 steps) ------------------------------------\n",
      "Max loss: 0.014968408271670341\n",
      "Min loss: 0.003671928308904171\n",
      "Mean loss: 0.007940764771774411\n",
      "Std loss: 0.003733783142818037\n",
      "Total Loss: 0.04764458863064647\n",
      "------------------------------------ epoch 8581 (51480 steps) ------------------------------------\n",
      "Max loss: 0.014087190851569176\n",
      "Min loss: 0.0052994294092059135\n",
      "Mean loss: 0.009386283966402212\n",
      "Std loss: 0.0028426841830238134\n",
      "Total Loss: 0.05631770379841328\n",
      "------------------------------------ epoch 8582 (51486 steps) ------------------------------------\n",
      "Max loss: 0.006736651062965393\n",
      "Min loss: 0.0036047452595084906\n",
      "Mean loss: 0.005225704633630812\n",
      "Std loss: 0.0010656410407563989\n",
      "Total Loss: 0.03135422780178487\n",
      "------------------------------------ epoch 8583 (51492 steps) ------------------------------------\n",
      "Max loss: 0.0108809694647789\n",
      "Min loss: 0.0039393361657857895\n",
      "Mean loss: 0.006520631335054834\n",
      "Std loss: 0.002472876031807567\n",
      "Total Loss: 0.03912378801032901\n",
      "------------------------------------ epoch 8584 (51498 steps) ------------------------------------\n",
      "Max loss: 0.013716536574065685\n",
      "Min loss: 0.004148184321820736\n",
      "Mean loss: 0.008346733714764317\n",
      "Std loss: 0.003394374988234899\n",
      "Total Loss: 0.0500804022885859\n",
      "------------------------------------ epoch 8585 (51504 steps) ------------------------------------\n",
      "Max loss: 0.02689148485660553\n",
      "Min loss: 0.004450430162250996\n",
      "Mean loss: 0.009769914982219538\n",
      "Std loss: 0.007777777957015898\n",
      "Total Loss: 0.05861948989331722\n",
      "------------------------------------ epoch 8586 (51510 steps) ------------------------------------\n",
      "Max loss: 0.02659660391509533\n",
      "Min loss: 0.005938793998211622\n",
      "Mean loss: 0.012611478722343842\n",
      "Std loss: 0.006892969640006829\n",
      "Total Loss: 0.07566887233406305\n",
      "------------------------------------ epoch 8587 (51516 steps) ------------------------------------\n",
      "Max loss: 0.012648406438529491\n",
      "Min loss: 0.004871096462011337\n",
      "Mean loss: 0.006753890076652169\n",
      "Std loss: 0.0026672363468255643\n",
      "Total Loss: 0.040523340459913015\n",
      "------------------------------------ epoch 8588 (51522 steps) ------------------------------------\n",
      "Max loss: 0.013136014342308044\n",
      "Min loss: 0.005451129283756018\n",
      "Mean loss: 0.00889331599076589\n",
      "Std loss: 0.002947488208535371\n",
      "Total Loss: 0.05335989594459534\n",
      "------------------------------------ epoch 8589 (51528 steps) ------------------------------------\n",
      "Max loss: 0.06682059168815613\n",
      "Min loss: 0.004281265661120415\n",
      "Mean loss: 0.015919497624660533\n",
      "Std loss: 0.02277927071161231\n",
      "Total Loss: 0.09551698574796319\n",
      "------------------------------------ epoch 8590 (51534 steps) ------------------------------------\n",
      "Max loss: 0.038477323949337006\n",
      "Min loss: 0.00686634099110961\n",
      "Mean loss: 0.013645182441299161\n",
      "Std loss: 0.011205905849298856\n",
      "Total Loss: 0.08187109464779496\n",
      "------------------------------------ epoch 8591 (51540 steps) ------------------------------------\n",
      "Max loss: 0.010284656658768654\n",
      "Min loss: 0.0063834525644779205\n",
      "Mean loss: 0.008068231089661518\n",
      "Std loss: 0.0012163249293841538\n",
      "Total Loss: 0.04840938653796911\n",
      "------------------------------------ epoch 8592 (51546 steps) ------------------------------------\n",
      "Max loss: 0.007662435993552208\n",
      "Min loss: 0.004861656576395035\n",
      "Mean loss: 0.006558779394254088\n",
      "Std loss: 0.0008782596770906597\n",
      "Total Loss: 0.03935267636552453\n",
      "------------------------------------ epoch 8593 (51552 steps) ------------------------------------\n",
      "Max loss: 0.013323813676834106\n",
      "Min loss: 0.004809707403182983\n",
      "Mean loss: 0.007417682946349184\n",
      "Std loss: 0.002753216693489966\n",
      "Total Loss: 0.0445060976780951\n",
      "------------------------------------ epoch 8594 (51558 steps) ------------------------------------\n",
      "Max loss: 0.02716272883117199\n",
      "Min loss: 0.004521257244050503\n",
      "Mean loss: 0.01161447586491704\n",
      "Std loss: 0.007205637400459712\n",
      "Total Loss: 0.06968685518950224\n",
      "------------------------------------ epoch 8595 (51564 steps) ------------------------------------\n",
      "Max loss: 0.009797930717468262\n",
      "Min loss: 0.0062201134860515594\n",
      "Mean loss: 0.008076875315358242\n",
      "Std loss: 0.0014612309435549375\n",
      "Total Loss: 0.04846125189214945\n",
      "------------------------------------ epoch 8596 (51570 steps) ------------------------------------\n",
      "Max loss: 0.026231572031974792\n",
      "Min loss: 0.0050290413200855255\n",
      "Mean loss: 0.013593068812042475\n",
      "Std loss: 0.007961513476415882\n",
      "Total Loss: 0.08155841287225485\n",
      "------------------------------------ epoch 8597 (51576 steps) ------------------------------------\n",
      "Max loss: 0.012823931872844696\n",
      "Min loss: 0.0045791431330144405\n",
      "Mean loss: 0.009283456252887845\n",
      "Std loss: 0.002873016225011917\n",
      "Total Loss: 0.05570073751732707\n",
      "------------------------------------ epoch 8598 (51582 steps) ------------------------------------\n",
      "Max loss: 0.02382763847708702\n",
      "Min loss: 0.004291445016860962\n",
      "Mean loss: 0.010212322774653634\n",
      "Std loss: 0.006527967516733256\n",
      "Total Loss: 0.0612739366479218\n",
      "------------------------------------ epoch 8599 (51588 steps) ------------------------------------\n",
      "Max loss: 0.026268020272254944\n",
      "Min loss: 0.004689353052526712\n",
      "Mean loss: 0.010182176173354188\n",
      "Std loss: 0.0074147787497745275\n",
      "Total Loss: 0.06109305704012513\n",
      "------------------------------------ epoch 8600 (51594 steps) ------------------------------------\n",
      "Max loss: 0.016525916755199432\n",
      "Min loss: 0.005841248668730259\n",
      "Mean loss: 0.008830708296348652\n",
      "Std loss: 0.0035550237914809585\n",
      "Total Loss: 0.05298424977809191\n",
      "------------------------------------ epoch 8601 (51600 steps) ------------------------------------\n",
      "Max loss: 0.010591460391879082\n",
      "Min loss: 0.00516552897170186\n",
      "Mean loss: 0.007757177964473764\n",
      "Std loss: 0.001830249857314393\n",
      "Total Loss: 0.046543067786842585\n",
      "saved model at ./weights/model_8601.pth\n",
      "------------------------------------ epoch 8602 (51606 steps) ------------------------------------\n",
      "Max loss: 0.014772485010325909\n",
      "Min loss: 0.004235198255628347\n",
      "Mean loss: 0.00789326949355503\n",
      "Std loss: 0.0035979784566928566\n",
      "Total Loss: 0.047359616961330175\n",
      "------------------------------------ epoch 8603 (51612 steps) ------------------------------------\n",
      "Max loss: 0.012168839573860168\n",
      "Min loss: 0.004212404601275921\n",
      "Mean loss: 0.00714372416647772\n",
      "Std loss: 0.002604048928134555\n",
      "Total Loss: 0.04286234499886632\n",
      "------------------------------------ epoch 8604 (51618 steps) ------------------------------------\n",
      "Max loss: 0.006786250974982977\n",
      "Min loss: 0.003628385253250599\n",
      "Mean loss: 0.004458864956783752\n",
      "Std loss: 0.0011086313821802491\n",
      "Total Loss: 0.02675318974070251\n",
      "------------------------------------ epoch 8605 (51624 steps) ------------------------------------\n",
      "Max loss: 0.026965022087097168\n",
      "Min loss: 0.0038126609288156033\n",
      "Mean loss: 0.010876182389135161\n",
      "Std loss: 0.007746691723782272\n",
      "Total Loss: 0.06525709433481097\n",
      "------------------------------------ epoch 8606 (51630 steps) ------------------------------------\n",
      "Max loss: 0.0127573786303401\n",
      "Min loss: 0.00380721571855247\n",
      "Mean loss: 0.006634040308805804\n",
      "Std loss: 0.0029269256550031658\n",
      "Total Loss: 0.03980424185283482\n",
      "------------------------------------ epoch 8607 (51636 steps) ------------------------------------\n",
      "Max loss: 0.005719345062971115\n",
      "Min loss: 0.0036710090935230255\n",
      "Mean loss: 0.004918965821464856\n",
      "Std loss: 0.0006954947883533986\n",
      "Total Loss: 0.02951379492878914\n",
      "------------------------------------ epoch 8608 (51642 steps) ------------------------------------\n",
      "Max loss: 0.015695709735155106\n",
      "Min loss: 0.004035167396068573\n",
      "Mean loss: 0.009049443372835716\n",
      "Std loss: 0.00436716396279566\n",
      "Total Loss: 0.054296660237014294\n",
      "------------------------------------ epoch 8609 (51648 steps) ------------------------------------\n",
      "Max loss: 0.01504287775605917\n",
      "Min loss: 0.0035085384733974934\n",
      "Mean loss: 0.00736033683642745\n",
      "Std loss: 0.0038334118382358567\n",
      "Total Loss: 0.0441620210185647\n",
      "------------------------------------ epoch 8610 (51654 steps) ------------------------------------\n",
      "Max loss: 0.021640285849571228\n",
      "Min loss: 0.004727208521217108\n",
      "Mean loss: 0.010450986912474036\n",
      "Std loss: 0.005617804649277685\n",
      "Total Loss: 0.06270592147484422\n",
      "------------------------------------ epoch 8611 (51660 steps) ------------------------------------\n",
      "Max loss: 0.007036828901618719\n",
      "Min loss: 0.0034548863768577576\n",
      "Mean loss: 0.0048692428196469946\n",
      "Std loss: 0.0013123033133487128\n",
      "Total Loss: 0.029215456917881966\n",
      "------------------------------------ epoch 8612 (51666 steps) ------------------------------------\n",
      "Max loss: 0.00946276169270277\n",
      "Min loss: 0.0036680165212601423\n",
      "Mean loss: 0.0056615323992446065\n",
      "Std loss: 0.0019231780458646192\n",
      "Total Loss: 0.03396919439546764\n",
      "------------------------------------ epoch 8613 (51672 steps) ------------------------------------\n",
      "Max loss: 0.009658589027822018\n",
      "Min loss: 0.004248467739671469\n",
      "Mean loss: 0.007105521857738495\n",
      "Std loss: 0.001968199415739591\n",
      "Total Loss: 0.04263313114643097\n",
      "------------------------------------ epoch 8614 (51678 steps) ------------------------------------\n",
      "Max loss: 0.009623561054468155\n",
      "Min loss: 0.0033040232956409454\n",
      "Mean loss: 0.005947914983456333\n",
      "Std loss: 0.001980817235011831\n",
      "Total Loss: 0.035687489900738\n",
      "------------------------------------ epoch 8615 (51684 steps) ------------------------------------\n",
      "Max loss: 0.014447720721364021\n",
      "Min loss: 0.0048052468337118626\n",
      "Mean loss: 0.008230155566707253\n",
      "Std loss: 0.0030499322126289773\n",
      "Total Loss: 0.04938093340024352\n",
      "------------------------------------ epoch 8616 (51690 steps) ------------------------------------\n",
      "Max loss: 0.015144022181630135\n",
      "Min loss: 0.0037176560144871473\n",
      "Mean loss: 0.009279246092773974\n",
      "Std loss: 0.003794307182429938\n",
      "Total Loss: 0.055675476556643844\n",
      "------------------------------------ epoch 8617 (51696 steps) ------------------------------------\n",
      "Max loss: 0.012614913284778595\n",
      "Min loss: 0.004110279493033886\n",
      "Mean loss: 0.0066039483062922955\n",
      "Std loss: 0.002911419639714754\n",
      "Total Loss: 0.03962368983775377\n",
      "------------------------------------ epoch 8618 (51702 steps) ------------------------------------\n",
      "Max loss: 0.018391167744994164\n",
      "Min loss: 0.004022323526442051\n",
      "Mean loss: 0.00861727143637836\n",
      "Std loss: 0.004758549736715562\n",
      "Total Loss: 0.05170362861827016\n",
      "------------------------------------ epoch 8619 (51708 steps) ------------------------------------\n",
      "Max loss: 0.009410168044269085\n",
      "Min loss: 0.003296386916190386\n",
      "Mean loss: 0.005290382308885455\n",
      "Std loss: 0.002068378182988455\n",
      "Total Loss: 0.03174229385331273\n",
      "------------------------------------ epoch 8620 (51714 steps) ------------------------------------\n",
      "Max loss: 0.008332965895533562\n",
      "Min loss: 0.003616955829784274\n",
      "Mean loss: 0.005162529996596277\n",
      "Std loss: 0.0016264257060237108\n",
      "Total Loss: 0.03097517997957766\n",
      "------------------------------------ epoch 8621 (51720 steps) ------------------------------------\n",
      "Max loss: 0.02223186194896698\n",
      "Min loss: 0.004196476191282272\n",
      "Mean loss: 0.008759881757820645\n",
      "Std loss: 0.0061374428635902175\n",
      "Total Loss: 0.052559290546923876\n",
      "------------------------------------ epoch 8622 (51726 steps) ------------------------------------\n",
      "Max loss: 0.012068372219800949\n",
      "Min loss: 0.00424163555726409\n",
      "Mean loss: 0.006983102687324087\n",
      "Std loss: 0.0031550572993304556\n",
      "Total Loss: 0.04189861612394452\n",
      "------------------------------------ epoch 8623 (51732 steps) ------------------------------------\n",
      "Max loss: 0.02372148633003235\n",
      "Min loss: 0.0036357389762997627\n",
      "Mean loss: 0.008619647085045775\n",
      "Std loss: 0.006818487370000492\n",
      "Total Loss: 0.05171788251027465\n",
      "------------------------------------ epoch 8624 (51738 steps) ------------------------------------\n",
      "Max loss: 0.008848370984196663\n",
      "Min loss: 0.0036758575588464737\n",
      "Mean loss: 0.005721711476022999\n",
      "Std loss: 0.0016457315328904216\n",
      "Total Loss: 0.03433026885613799\n",
      "------------------------------------ epoch 8625 (51744 steps) ------------------------------------\n",
      "Max loss: 0.015852399170398712\n",
      "Min loss: 0.004523062612861395\n",
      "Mean loss: 0.009147426268706719\n",
      "Std loss: 0.003879345692614994\n",
      "Total Loss: 0.054884557612240314\n",
      "------------------------------------ epoch 8626 (51750 steps) ------------------------------------\n",
      "Max loss: 0.00973442755639553\n",
      "Min loss: 0.004205276258289814\n",
      "Mean loss: 0.005841553133601944\n",
      "Std loss: 0.0018588609821350826\n",
      "Total Loss: 0.03504931880161166\n",
      "------------------------------------ epoch 8627 (51756 steps) ------------------------------------\n",
      "Max loss: 0.011854022741317749\n",
      "Min loss: 0.0035213506780564785\n",
      "Mean loss: 0.005758271319791675\n",
      "Std loss: 0.0028696328417053526\n",
      "Total Loss: 0.03454962791875005\n",
      "------------------------------------ epoch 8628 (51762 steps) ------------------------------------\n",
      "Max loss: 0.009219858795404434\n",
      "Min loss: 0.004200217314064503\n",
      "Mean loss: 0.006715055943156282\n",
      "Std loss: 0.0019210528578517045\n",
      "Total Loss: 0.04029033565893769\n",
      "------------------------------------ epoch 8629 (51768 steps) ------------------------------------\n",
      "Max loss: 0.017662156373262405\n",
      "Min loss: 0.0043279556557536125\n",
      "Mean loss: 0.01025329390540719\n",
      "Std loss: 0.004714088330142641\n",
      "Total Loss: 0.06151976343244314\n",
      "------------------------------------ epoch 8630 (51774 steps) ------------------------------------\n",
      "Max loss: 0.02556583657860756\n",
      "Min loss: 0.0033220581244677305\n",
      "Mean loss: 0.011594137215676406\n",
      "Std loss: 0.009141886065973433\n",
      "Total Loss: 0.06956482329405844\n",
      "------------------------------------ epoch 8631 (51780 steps) ------------------------------------\n",
      "Max loss: 0.008467746898531914\n",
      "Min loss: 0.004560654517263174\n",
      "Mean loss: 0.006422868153701226\n",
      "Std loss: 0.0013779777286944284\n",
      "Total Loss: 0.038537208922207355\n",
      "------------------------------------ epoch 8632 (51786 steps) ------------------------------------\n",
      "Max loss: 0.020563917234539986\n",
      "Min loss: 0.006354336626827717\n",
      "Mean loss: 0.010163861326873302\n",
      "Std loss: 0.004723076910136489\n",
      "Total Loss: 0.060983167961239815\n",
      "------------------------------------ epoch 8633 (51792 steps) ------------------------------------\n",
      "Max loss: 0.007561352103948593\n",
      "Min loss: 0.0036680023185908794\n",
      "Mean loss: 0.005326307145878673\n",
      "Std loss: 0.0014449504037571115\n",
      "Total Loss: 0.031957842875272036\n",
      "------------------------------------ epoch 8634 (51798 steps) ------------------------------------\n",
      "Max loss: 0.034428104758262634\n",
      "Min loss: 0.004348817281424999\n",
      "Mean loss: 0.014740472504248222\n",
      "Std loss: 0.009665071495681854\n",
      "Total Loss: 0.08844283502548933\n",
      "------------------------------------ epoch 8635 (51804 steps) ------------------------------------\n",
      "Max loss: 0.015760548412799835\n",
      "Min loss: 0.003907816484570503\n",
      "Mean loss: 0.0067423666672160225\n",
      "Std loss: 0.004091406182935102\n",
      "Total Loss: 0.04045420000329614\n",
      "------------------------------------ epoch 8636 (51810 steps) ------------------------------------\n",
      "Max loss: 0.010995892807841301\n",
      "Min loss: 0.004910903051495552\n",
      "Mean loss: 0.007025697423766057\n",
      "Std loss: 0.002119982333282967\n",
      "Total Loss: 0.04215418454259634\n",
      "------------------------------------ epoch 8637 (51816 steps) ------------------------------------\n",
      "Max loss: 0.009480133652687073\n",
      "Min loss: 0.004549540579319\n",
      "Mean loss: 0.00735067599453032\n",
      "Std loss: 0.001454621587912135\n",
      "Total Loss: 0.04410405596718192\n",
      "------------------------------------ epoch 8638 (51822 steps) ------------------------------------\n",
      "Max loss: 0.005216966383159161\n",
      "Min loss: 0.003391024423763156\n",
      "Mean loss: 0.004230775909187893\n",
      "Std loss: 0.000551946410189591\n",
      "Total Loss: 0.02538465545512736\n",
      "------------------------------------ epoch 8639 (51828 steps) ------------------------------------\n",
      "Max loss: 0.010746646672487259\n",
      "Min loss: 0.003344288794323802\n",
      "Mean loss: 0.005166864915130039\n",
      "Std loss: 0.002546694811451026\n",
      "Total Loss: 0.031001189490780234\n",
      "------------------------------------ epoch 8640 (51834 steps) ------------------------------------\n",
      "Max loss: 0.012766304425895214\n",
      "Min loss: 0.0034857047721743584\n",
      "Mean loss: 0.006186462705954909\n",
      "Std loss: 0.00313702135460285\n",
      "Total Loss: 0.037118776235729456\n",
      "------------------------------------ epoch 8641 (51840 steps) ------------------------------------\n",
      "Max loss: 0.00847206637263298\n",
      "Min loss: 0.003644142998382449\n",
      "Mean loss: 0.0048681248833114905\n",
      "Std loss: 0.0016303953827967628\n",
      "Total Loss: 0.02920874929986894\n",
      "------------------------------------ epoch 8642 (51846 steps) ------------------------------------\n",
      "Max loss: 0.009214799851179123\n",
      "Min loss: 0.0030320486985147\n",
      "Mean loss: 0.005427658868332704\n",
      "Std loss: 0.0020462165701881194\n",
      "Total Loss: 0.03256595320999622\n",
      "------------------------------------ epoch 8643 (51852 steps) ------------------------------------\n",
      "Max loss: 0.009943503886461258\n",
      "Min loss: 0.004469398409128189\n",
      "Mean loss: 0.00655096722766757\n",
      "Std loss: 0.0017510810720140883\n",
      "Total Loss: 0.03930580336600542\n",
      "------------------------------------ epoch 8644 (51858 steps) ------------------------------------\n",
      "Max loss: 0.02362724393606186\n",
      "Min loss: 0.00326276826672256\n",
      "Mean loss: 0.011138796069038412\n",
      "Std loss: 0.007130968469614647\n",
      "Total Loss: 0.06683277641423047\n",
      "------------------------------------ epoch 8645 (51864 steps) ------------------------------------\n",
      "Max loss: 0.012550195679068565\n",
      "Min loss: 0.00444810139015317\n",
      "Mean loss: 0.008485649634773532\n",
      "Std loss: 0.002817977841673387\n",
      "Total Loss: 0.050913897808641195\n",
      "------------------------------------ epoch 8646 (51870 steps) ------------------------------------\n",
      "Max loss: 0.01711422950029373\n",
      "Min loss: 0.004498097579926252\n",
      "Mean loss: 0.01032080470273892\n",
      "Std loss: 0.004901406122093245\n",
      "Total Loss: 0.061924828216433525\n",
      "------------------------------------ epoch 8647 (51876 steps) ------------------------------------\n",
      "Max loss: 0.014490075409412384\n",
      "Min loss: 0.004125489853322506\n",
      "Mean loss: 0.00981997170795997\n",
      "Std loss: 0.004408770161286298\n",
      "Total Loss: 0.05891983024775982\n",
      "------------------------------------ epoch 8648 (51882 steps) ------------------------------------\n",
      "Max loss: 0.008251229301095009\n",
      "Min loss: 0.004038661252707243\n",
      "Mean loss: 0.0062526856393863755\n",
      "Std loss: 0.0012890504541468828\n",
      "Total Loss: 0.037516113836318254\n",
      "------------------------------------ epoch 8649 (51888 steps) ------------------------------------\n",
      "Max loss: 0.018779059872031212\n",
      "Min loss: 0.004576058592647314\n",
      "Mean loss: 0.008397925101841489\n",
      "Std loss: 0.00502647965634197\n",
      "Total Loss: 0.05038755061104894\n",
      "------------------------------------ epoch 8650 (51894 steps) ------------------------------------\n",
      "Max loss: 0.010212854482233524\n",
      "Min loss: 0.00418492779135704\n",
      "Mean loss: 0.007715761040647824\n",
      "Std loss: 0.0018715498930819427\n",
      "Total Loss: 0.04629456624388695\n",
      "------------------------------------ epoch 8651 (51900 steps) ------------------------------------\n",
      "Max loss: 0.015571292489767075\n",
      "Min loss: 0.0035745161585509777\n",
      "Mean loss: 0.0073854217771440744\n",
      "Std loss: 0.0043582582444051995\n",
      "Total Loss: 0.04431253066286445\n",
      "------------------------------------ epoch 8652 (51906 steps) ------------------------------------\n",
      "Max loss: 0.02017022669315338\n",
      "Min loss: 0.005257376469671726\n",
      "Mean loss: 0.012359221621106068\n",
      "Std loss: 0.004746275088292038\n",
      "Total Loss: 0.07415532972663641\n",
      "------------------------------------ epoch 8653 (51912 steps) ------------------------------------\n",
      "Max loss: 0.007141394075006247\n",
      "Min loss: 0.0036643273197114468\n",
      "Mean loss: 0.005466890754178166\n",
      "Std loss: 0.0012799144586887942\n",
      "Total Loss: 0.032801344525069\n",
      "------------------------------------ epoch 8654 (51918 steps) ------------------------------------\n",
      "Max loss: 0.009310190565884113\n",
      "Min loss: 0.003773271571844816\n",
      "Mean loss: 0.005475204049920042\n",
      "Std loss: 0.001941098064070109\n",
      "Total Loss: 0.032851224299520254\n",
      "------------------------------------ epoch 8655 (51924 steps) ------------------------------------\n",
      "Max loss: 0.024623682722449303\n",
      "Min loss: 0.0036094177048653364\n",
      "Mean loss: 0.0077243106594930095\n",
      "Std loss: 0.007579309614761404\n",
      "Total Loss: 0.046345863956958055\n",
      "------------------------------------ epoch 8656 (51930 steps) ------------------------------------\n",
      "Max loss: 0.008359101600944996\n",
      "Min loss: 0.00382126378826797\n",
      "Mean loss: 0.005312897691813608\n",
      "Std loss: 0.001519006655030645\n",
      "Total Loss: 0.03187738615088165\n",
      "------------------------------------ epoch 8657 (51936 steps) ------------------------------------\n",
      "Max loss: 0.008319180458784103\n",
      "Min loss: 0.003664970165118575\n",
      "Mean loss: 0.00494231815294673\n",
      "Std loss: 0.0016460078662303044\n",
      "Total Loss: 0.029653908917680383\n",
      "------------------------------------ epoch 8658 (51942 steps) ------------------------------------\n",
      "Max loss: 0.007797162979841232\n",
      "Min loss: 0.0034927409142255783\n",
      "Mean loss: 0.004977540113031864\n",
      "Std loss: 0.0014716831293316564\n",
      "Total Loss: 0.029865240678191185\n",
      "------------------------------------ epoch 8659 (51948 steps) ------------------------------------\n",
      "Max loss: 0.01015608012676239\n",
      "Min loss: 0.0032981038093566895\n",
      "Mean loss: 0.005497782801588376\n",
      "Std loss: 0.0023763436512762733\n",
      "Total Loss: 0.03298669680953026\n",
      "------------------------------------ epoch 8660 (51954 steps) ------------------------------------\n",
      "Max loss: 0.012475064024329185\n",
      "Min loss: 0.003397653577849269\n",
      "Mean loss: 0.007991182889478901\n",
      "Std loss: 0.003151784603435489\n",
      "Total Loss: 0.04794709733687341\n",
      "------------------------------------ epoch 8661 (51960 steps) ------------------------------------\n",
      "Max loss: 0.008568815886974335\n",
      "Min loss: 0.003431495279073715\n",
      "Mean loss: 0.005870119125271837\n",
      "Std loss: 0.0019683024461685647\n",
      "Total Loss: 0.03522071475163102\n",
      "------------------------------------ epoch 8662 (51966 steps) ------------------------------------\n",
      "Max loss: 0.012526179663836956\n",
      "Min loss: 0.003622564487159252\n",
      "Mean loss: 0.0068791441929837065\n",
      "Std loss: 0.0028831658690901246\n",
      "Total Loss: 0.04127486515790224\n",
      "------------------------------------ epoch 8663 (51972 steps) ------------------------------------\n",
      "Max loss: 0.006856560241430998\n",
      "Min loss: 0.0035856603644788265\n",
      "Mean loss: 0.004691676547129949\n",
      "Std loss: 0.0011215800377559167\n",
      "Total Loss: 0.028150059282779694\n",
      "------------------------------------ epoch 8664 (51978 steps) ------------------------------------\n",
      "Max loss: 0.008222493343055248\n",
      "Min loss: 0.003089283127337694\n",
      "Mean loss: 0.005191259008521835\n",
      "Std loss: 0.0020103253630356973\n",
      "Total Loss: 0.03114755405113101\n",
      "------------------------------------ epoch 8665 (51984 steps) ------------------------------------\n",
      "Max loss: 0.018115349113941193\n",
      "Min loss: 0.004057653248310089\n",
      "Mean loss: 0.009241505526006222\n",
      "Std loss: 0.005102959713493134\n",
      "Total Loss: 0.05544903315603733\n",
      "------------------------------------ epoch 8666 (51990 steps) ------------------------------------\n",
      "Max loss: 0.016090530902147293\n",
      "Min loss: 0.005845870357006788\n",
      "Mean loss: 0.010386416921392083\n",
      "Std loss: 0.0035427017828704114\n",
      "Total Loss: 0.0623185015283525\n",
      "------------------------------------ epoch 8667 (51996 steps) ------------------------------------\n",
      "Max loss: 0.0321856364607811\n",
      "Min loss: 0.00428495928645134\n",
      "Mean loss: 0.014263931661844254\n",
      "Std loss: 0.010172961287486952\n",
      "Total Loss: 0.08558358997106552\n",
      "------------------------------------ epoch 8668 (52002 steps) ------------------------------------\n",
      "Max loss: 0.012472263537347317\n",
      "Min loss: 0.004405293148010969\n",
      "Mean loss: 0.006655333408464988\n",
      "Std loss: 0.0027570753839637227\n",
      "Total Loss: 0.03993200045078993\n",
      "------------------------------------ epoch 8669 (52008 steps) ------------------------------------\n",
      "Max loss: 0.01891065575182438\n",
      "Min loss: 0.008287192322313786\n",
      "Mean loss: 0.012624190965046486\n",
      "Std loss: 0.003175977288035704\n",
      "Total Loss: 0.07574514579027891\n",
      "------------------------------------ epoch 8670 (52014 steps) ------------------------------------\n",
      "Max loss: 0.019653644412755966\n",
      "Min loss: 0.006028294563293457\n",
      "Mean loss: 0.01154345041140914\n",
      "Std loss: 0.004763047489574015\n",
      "Total Loss: 0.06926070246845484\n",
      "------------------------------------ epoch 8671 (52020 steps) ------------------------------------\n",
      "Max loss: 0.02354443073272705\n",
      "Min loss: 0.004548217635601759\n",
      "Mean loss: 0.011689925100654364\n",
      "Std loss: 0.00787148234820004\n",
      "Total Loss: 0.07013955060392618\n",
      "------------------------------------ epoch 8672 (52026 steps) ------------------------------------\n",
      "Max loss: 0.02226012945175171\n",
      "Min loss: 0.008049363270401955\n",
      "Mean loss: 0.012895153059313694\n",
      "Std loss: 0.005075223552148773\n",
      "Total Loss: 0.07737091835588217\n",
      "------------------------------------ epoch 8673 (52032 steps) ------------------------------------\n",
      "Max loss: 0.01799064874649048\n",
      "Min loss: 0.006653449963778257\n",
      "Mean loss: 0.0117541979222248\n",
      "Std loss: 0.004181510037826529\n",
      "Total Loss: 0.0705251875333488\n",
      "------------------------------------ epoch 8674 (52038 steps) ------------------------------------\n",
      "Max loss: 0.026936955749988556\n",
      "Min loss: 0.005398616194725037\n",
      "Mean loss: 0.01102582768847545\n",
      "Std loss: 0.007280292972279446\n",
      "Total Loss: 0.0661549661308527\n",
      "------------------------------------ epoch 8675 (52044 steps) ------------------------------------\n",
      "Max loss: 0.02309083566069603\n",
      "Min loss: 0.00554591603577137\n",
      "Mean loss: 0.011075683170929551\n",
      "Std loss: 0.00563707324394567\n",
      "Total Loss: 0.0664540990255773\n",
      "------------------------------------ epoch 8676 (52050 steps) ------------------------------------\n",
      "Max loss: 0.030962690711021423\n",
      "Min loss: 0.006784211844205856\n",
      "Mean loss: 0.01515632402151823\n",
      "Std loss: 0.009935156759496857\n",
      "Total Loss: 0.09093794412910938\n",
      "------------------------------------ epoch 8677 (52056 steps) ------------------------------------\n",
      "Max loss: 0.015990886837244034\n",
      "Min loss: 0.004624313209205866\n",
      "Mean loss: 0.008566169378658136\n",
      "Std loss: 0.004377038270060121\n",
      "Total Loss: 0.051397016271948814\n",
      "------------------------------------ epoch 8678 (52062 steps) ------------------------------------\n",
      "Max loss: 0.008558500558137894\n",
      "Min loss: 0.004120113328099251\n",
      "Mean loss: 0.006170791961873571\n",
      "Std loss: 0.001682537739834913\n",
      "Total Loss: 0.037024751771241426\n",
      "------------------------------------ epoch 8679 (52068 steps) ------------------------------------\n",
      "Max loss: 0.021357102319598198\n",
      "Min loss: 0.0038286137860268354\n",
      "Mean loss: 0.008866096885564426\n",
      "Std loss: 0.0059377448077888376\n",
      "Total Loss: 0.05319658131338656\n",
      "------------------------------------ epoch 8680 (52074 steps) ------------------------------------\n",
      "Max loss: 0.008381815627217293\n",
      "Min loss: 0.004528784193098545\n",
      "Mean loss: 0.006171244662255049\n",
      "Std loss: 0.0014362268253992475\n",
      "Total Loss: 0.03702746797353029\n",
      "------------------------------------ epoch 8681 (52080 steps) ------------------------------------\n",
      "Max loss: 0.01579122245311737\n",
      "Min loss: 0.0038504451513290405\n",
      "Mean loss: 0.008761007261152068\n",
      "Std loss: 0.004430001762178057\n",
      "Total Loss: 0.05256604356691241\n",
      "------------------------------------ epoch 8682 (52086 steps) ------------------------------------\n",
      "Max loss: 0.025353755801916122\n",
      "Min loss: 0.0034383833408355713\n",
      "Mean loss: 0.009184166633834442\n",
      "Std loss: 0.007792470448846904\n",
      "Total Loss: 0.05510499980300665\n",
      "------------------------------------ epoch 8683 (52092 steps) ------------------------------------\n",
      "Max loss: 0.027911726385354996\n",
      "Min loss: 0.004450333304703236\n",
      "Mean loss: 0.011178063151116172\n",
      "Std loss: 0.008698049390187857\n",
      "Total Loss: 0.06706837890669703\n",
      "------------------------------------ epoch 8684 (52098 steps) ------------------------------------\n",
      "Max loss: 0.01566953770816326\n",
      "Min loss: 0.006218868773430586\n",
      "Mean loss: 0.009488739306107163\n",
      "Std loss: 0.0031447515920463684\n",
      "Total Loss: 0.05693243583664298\n",
      "------------------------------------ epoch 8685 (52104 steps) ------------------------------------\n",
      "Max loss: 0.01472519151866436\n",
      "Min loss: 0.00447105010971427\n",
      "Mean loss: 0.009415869445850452\n",
      "Std loss: 0.0036103163508933965\n",
      "Total Loss: 0.05649521667510271\n",
      "------------------------------------ epoch 8686 (52110 steps) ------------------------------------\n",
      "Max loss: 0.01880536787211895\n",
      "Min loss: 0.003779730759561062\n",
      "Mean loss: 0.00872318958863616\n",
      "Std loss: 0.00504921424734648\n",
      "Total Loss: 0.05233913753181696\n",
      "------------------------------------ epoch 8687 (52116 steps) ------------------------------------\n",
      "Max loss: 0.0394066721200943\n",
      "Min loss: 0.003747158218175173\n",
      "Mean loss: 0.015242699533700943\n",
      "Std loss: 0.012576339945638353\n",
      "Total Loss: 0.09145619720220566\n",
      "------------------------------------ epoch 8688 (52122 steps) ------------------------------------\n",
      "Max loss: 0.010279526934027672\n",
      "Min loss: 0.006402082275599241\n",
      "Mean loss: 0.00842913806748887\n",
      "Std loss: 0.0012469688037325195\n",
      "Total Loss: 0.050574828404933214\n",
      "------------------------------------ epoch 8689 (52128 steps) ------------------------------------\n",
      "Max loss: 0.008645079098641872\n",
      "Min loss: 0.003933988511562347\n",
      "Mean loss: 0.006960214736560981\n",
      "Std loss: 0.0017800043053210674\n",
      "Total Loss: 0.04176128841936588\n",
      "------------------------------------ epoch 8690 (52134 steps) ------------------------------------\n",
      "Max loss: 0.021121039986610413\n",
      "Min loss: 0.003917335998266935\n",
      "Mean loss: 0.00964517790513734\n",
      "Std loss: 0.0054632665635020615\n",
      "Total Loss: 0.05787106743082404\n",
      "------------------------------------ epoch 8691 (52140 steps) ------------------------------------\n",
      "Max loss: 0.008990220725536346\n",
      "Min loss: 0.0050100176595151424\n",
      "Mean loss: 0.006722563256820043\n",
      "Std loss: 0.001573899507501015\n",
      "Total Loss: 0.04033537954092026\n",
      "------------------------------------ epoch 8692 (52146 steps) ------------------------------------\n",
      "Max loss: 0.012155070900917053\n",
      "Min loss: 0.004604645073413849\n",
      "Mean loss: 0.007839916118731102\n",
      "Std loss: 0.0023879028155440137\n",
      "Total Loss: 0.04703949671238661\n",
      "------------------------------------ epoch 8693 (52152 steps) ------------------------------------\n",
      "Max loss: 0.007158329244703054\n",
      "Min loss: 0.0037217908538877964\n",
      "Mean loss: 0.005140832159668207\n",
      "Std loss: 0.0013316500609734747\n",
      "Total Loss: 0.030844992958009243\n",
      "------------------------------------ epoch 8694 (52158 steps) ------------------------------------\n",
      "Max loss: 0.007705806754529476\n",
      "Min loss: 0.0030213166028261185\n",
      "Mean loss: 0.004875482991337776\n",
      "Std loss: 0.0014528304260836896\n",
      "Total Loss: 0.029252897948026657\n",
      "------------------------------------ epoch 8695 (52164 steps) ------------------------------------\n",
      "Max loss: 0.006463274359703064\n",
      "Min loss: 0.003815172705799341\n",
      "Mean loss: 0.004852857285489638\n",
      "Std loss: 0.0008147731127671952\n",
      "Total Loss: 0.029117143712937832\n",
      "------------------------------------ epoch 8696 (52170 steps) ------------------------------------\n",
      "Max loss: 0.015145352110266685\n",
      "Min loss: 0.003818147350102663\n",
      "Mean loss: 0.007958293737222752\n",
      "Std loss: 0.0037385524719247636\n",
      "Total Loss: 0.047749762423336506\n",
      "------------------------------------ epoch 8697 (52176 steps) ------------------------------------\n",
      "Max loss: 0.007774676661938429\n",
      "Min loss: 0.0033140217419713736\n",
      "Mean loss: 0.004811651500252386\n",
      "Std loss: 0.0014906423336607689\n",
      "Total Loss: 0.028869909001514316\n",
      "------------------------------------ epoch 8698 (52182 steps) ------------------------------------\n",
      "Max loss: 0.018664903938770294\n",
      "Min loss: 0.003979155328124762\n",
      "Mean loss: 0.008254553036143383\n",
      "Std loss: 0.005123256498772777\n",
      "Total Loss: 0.049527318216860294\n",
      "------------------------------------ epoch 8699 (52188 steps) ------------------------------------\n",
      "Max loss: 0.008935576304793358\n",
      "Min loss: 0.0038279867731034756\n",
      "Mean loss: 0.006509650343408187\n",
      "Std loss: 0.001791245197955673\n",
      "Total Loss: 0.03905790206044912\n",
      "------------------------------------ epoch 8700 (52194 steps) ------------------------------------\n",
      "Max loss: 0.015587328933179379\n",
      "Min loss: 0.007902422919869423\n",
      "Mean loss: 0.011404146905988455\n",
      "Std loss: 0.0023899838630247576\n",
      "Total Loss: 0.06842488143593073\n",
      "------------------------------------ epoch 8701 (52200 steps) ------------------------------------\n",
      "Max loss: 0.012639297172427177\n",
      "Min loss: 0.0037873699329793453\n",
      "Mean loss: 0.0072684074596812325\n",
      "Std loss: 0.002810024446319506\n",
      "Total Loss: 0.0436104447580874\n",
      "saved model at ./weights/model_8701.pth\n",
      "------------------------------------ epoch 8702 (52206 steps) ------------------------------------\n",
      "Max loss: 0.008945132605731487\n",
      "Min loss: 0.004477384965866804\n",
      "Mean loss: 0.006143440414840977\n",
      "Std loss: 0.0013690881122874528\n",
      "Total Loss: 0.03686064248904586\n",
      "------------------------------------ epoch 8703 (52212 steps) ------------------------------------\n",
      "Max loss: 0.01270253211259842\n",
      "Min loss: 0.003611775580793619\n",
      "Mean loss: 0.006757254634673397\n",
      "Std loss: 0.003050542560997154\n",
      "Total Loss: 0.04054352780804038\n",
      "------------------------------------ epoch 8704 (52218 steps) ------------------------------------\n",
      "Max loss: 0.01745752990245819\n",
      "Min loss: 0.0035271651577204466\n",
      "Mean loss: 0.00668341569447269\n",
      "Std loss: 0.0048564709812886075\n",
      "Total Loss: 0.04010049416683614\n",
      "------------------------------------ epoch 8705 (52224 steps) ------------------------------------\n",
      "Max loss: 0.02230466902256012\n",
      "Min loss: 0.005229815375059843\n",
      "Mean loss: 0.009429206295559803\n",
      "Std loss: 0.005895701040108201\n",
      "Total Loss: 0.05657523777335882\n",
      "------------------------------------ epoch 8706 (52230 steps) ------------------------------------\n",
      "Max loss: 0.01911504752933979\n",
      "Min loss: 0.004139332566410303\n",
      "Mean loss: 0.01016515571003159\n",
      "Std loss: 0.005390728333764196\n",
      "Total Loss: 0.06099093426018953\n",
      "------------------------------------ epoch 8707 (52236 steps) ------------------------------------\n",
      "Max loss: 0.00739774014800787\n",
      "Min loss: 0.0032080113887786865\n",
      "Mean loss: 0.0051064263097941875\n",
      "Std loss: 0.00125538692049249\n",
      "Total Loss: 0.030638557858765125\n",
      "------------------------------------ epoch 8708 (52242 steps) ------------------------------------\n",
      "Max loss: 0.025696998462080956\n",
      "Min loss: 0.004125404637306929\n",
      "Mean loss: 0.012586863478645682\n",
      "Std loss: 0.007036025197000492\n",
      "Total Loss: 0.0755211808718741\n",
      "------------------------------------ epoch 8709 (52248 steps) ------------------------------------\n",
      "Max loss: 0.020651765167713165\n",
      "Min loss: 0.006480108015239239\n",
      "Mean loss: 0.010154437739402056\n",
      "Std loss: 0.004779576375209485\n",
      "Total Loss: 0.060926626436412334\n",
      "------------------------------------ epoch 8710 (52254 steps) ------------------------------------\n",
      "Max loss: 0.010494365356862545\n",
      "Min loss: 0.005278470925986767\n",
      "Mean loss: 0.007807163211206595\n",
      "Std loss: 0.001813822578929122\n",
      "Total Loss: 0.04684297926723957\n",
      "------------------------------------ epoch 8711 (52260 steps) ------------------------------------\n",
      "Max loss: 0.013941258192062378\n",
      "Min loss: 0.003857184201478958\n",
      "Mean loss: 0.008849682752043009\n",
      "Std loss: 0.0033721409298321145\n",
      "Total Loss: 0.05309809651225805\n",
      "------------------------------------ epoch 8712 (52266 steps) ------------------------------------\n",
      "Max loss: 0.016441326588392258\n",
      "Min loss: 0.004557532258331776\n",
      "Mean loss: 0.007158322337393959\n",
      "Std loss: 0.004195965490717842\n",
      "Total Loss: 0.042949934024363756\n",
      "------------------------------------ epoch 8713 (52272 steps) ------------------------------------\n",
      "Max loss: 0.04531928151845932\n",
      "Min loss: 0.004239288624376059\n",
      "Mean loss: 0.017314369246984523\n",
      "Std loss: 0.01353961938362341\n",
      "Total Loss: 0.10388621548190713\n",
      "------------------------------------ epoch 8714 (52278 steps) ------------------------------------\n",
      "Max loss: 0.01170949824154377\n",
      "Min loss: 0.004084724001586437\n",
      "Mean loss: 0.006110788478205602\n",
      "Std loss: 0.002588817203950144\n",
      "Total Loss: 0.03666473086923361\n",
      "------------------------------------ epoch 8715 (52284 steps) ------------------------------------\n",
      "Max loss: 0.014998000115156174\n",
      "Min loss: 0.004501047544181347\n",
      "Mean loss: 0.008024903169522682\n",
      "Std loss: 0.00338482713400427\n",
      "Total Loss: 0.0481494190171361\n",
      "------------------------------------ epoch 8716 (52290 steps) ------------------------------------\n",
      "Max loss: 0.01417417824268341\n",
      "Min loss: 0.005723647773265839\n",
      "Mean loss: 0.009228955411041776\n",
      "Std loss: 0.003200459989861141\n",
      "Total Loss: 0.05537373246625066\n",
      "------------------------------------ epoch 8717 (52296 steps) ------------------------------------\n",
      "Max loss: 0.014797135256230831\n",
      "Min loss: 0.0033808895386755466\n",
      "Mean loss: 0.007887326491375765\n",
      "Std loss: 0.003882129354930461\n",
      "Total Loss: 0.047323958948254585\n",
      "------------------------------------ epoch 8718 (52302 steps) ------------------------------------\n",
      "Max loss: 0.012495673261582851\n",
      "Min loss: 0.004371723625808954\n",
      "Mean loss: 0.00654655151690046\n",
      "Std loss: 0.002713927049839839\n",
      "Total Loss: 0.03927930910140276\n",
      "------------------------------------ epoch 8719 (52308 steps) ------------------------------------\n",
      "Max loss: 0.017835168167948723\n",
      "Min loss: 0.004451259505003691\n",
      "Mean loss: 0.009537274405981103\n",
      "Std loss: 0.004273517248193462\n",
      "Total Loss: 0.05722364643588662\n",
      "------------------------------------ epoch 8720 (52314 steps) ------------------------------------\n",
      "Max loss: 0.012637069448828697\n",
      "Min loss: 0.005188673734664917\n",
      "Mean loss: 0.007715106631318728\n",
      "Std loss: 0.002901904903359219\n",
      "Total Loss: 0.04629063978791237\n",
      "------------------------------------ epoch 8721 (52320 steps) ------------------------------------\n",
      "Max loss: 0.011793883517384529\n",
      "Min loss: 0.0037877089343965054\n",
      "Mean loss: 0.006875541061162949\n",
      "Std loss: 0.0028482623177329865\n",
      "Total Loss: 0.04125324636697769\n",
      "------------------------------------ epoch 8722 (52326 steps) ------------------------------------\n",
      "Max loss: 0.008945438079535961\n",
      "Min loss: 0.004948943387717009\n",
      "Mean loss: 0.006430994796877106\n",
      "Std loss: 0.001319426469672067\n",
      "Total Loss: 0.038585968781262636\n",
      "------------------------------------ epoch 8723 (52332 steps) ------------------------------------\n",
      "Max loss: 0.01719089038670063\n",
      "Min loss: 0.003935432061553001\n",
      "Mean loss: 0.006627955629179875\n",
      "Std loss: 0.004754320772124404\n",
      "Total Loss: 0.03976773377507925\n",
      "------------------------------------ epoch 8724 (52338 steps) ------------------------------------\n",
      "Max loss: 0.0068617453798651695\n",
      "Min loss: 0.0035841045901179314\n",
      "Mean loss: 0.004948514862917364\n",
      "Std loss: 0.0011562929717049276\n",
      "Total Loss: 0.029691089177504182\n",
      "------------------------------------ epoch 8725 (52344 steps) ------------------------------------\n",
      "Max loss: 0.015390796586871147\n",
      "Min loss: 0.0032915854826569557\n",
      "Mean loss: 0.008067234962557754\n",
      "Std loss: 0.004603622275440484\n",
      "Total Loss: 0.04840340977534652\n",
      "------------------------------------ epoch 8726 (52350 steps) ------------------------------------\n",
      "Max loss: 0.012625711970031261\n",
      "Min loss: 0.003576502902433276\n",
      "Mean loss: 0.006131539625736575\n",
      "Std loss: 0.0030300423446136208\n",
      "Total Loss: 0.036789237754419446\n",
      "------------------------------------ epoch 8727 (52356 steps) ------------------------------------\n",
      "Max loss: 0.02122977003455162\n",
      "Min loss: 0.003212517127394676\n",
      "Mean loss: 0.008096995375429591\n",
      "Std loss: 0.006343707773184615\n",
      "Total Loss: 0.04858197225257754\n",
      "------------------------------------ epoch 8728 (52362 steps) ------------------------------------\n",
      "Max loss: 0.00785152893513441\n",
      "Min loss: 0.0037817431148141623\n",
      "Mean loss: 0.005820424412377179\n",
      "Std loss: 0.001836852646550597\n",
      "Total Loss: 0.03492254647426307\n",
      "------------------------------------ epoch 8729 (52368 steps) ------------------------------------\n",
      "Max loss: 0.008309651166200638\n",
      "Min loss: 0.003880611388012767\n",
      "Mean loss: 0.005755835484402875\n",
      "Std loss: 0.0014294498386674538\n",
      "Total Loss: 0.03453501290641725\n",
      "------------------------------------ epoch 8730 (52374 steps) ------------------------------------\n",
      "Max loss: 0.007797089871019125\n",
      "Min loss: 0.003496249672025442\n",
      "Mean loss: 0.00515679782256484\n",
      "Std loss: 0.0013796736219593064\n",
      "Total Loss: 0.030940786935389042\n",
      "------------------------------------ epoch 8731 (52380 steps) ------------------------------------\n",
      "Max loss: 0.028341367840766907\n",
      "Min loss: 0.003978518769145012\n",
      "Mean loss: 0.01776341302320361\n",
      "Std loss: 0.009560486330433975\n",
      "Total Loss: 0.10658047813922167\n",
      "------------------------------------ epoch 8732 (52386 steps) ------------------------------------\n",
      "Max loss: 0.014426030218601227\n",
      "Min loss: 0.005692590028047562\n",
      "Mean loss: 0.00929362140595913\n",
      "Std loss: 0.002648889767241944\n",
      "Total Loss: 0.055761728435754776\n",
      "------------------------------------ epoch 8733 (52392 steps) ------------------------------------\n",
      "Max loss: 0.026116803288459778\n",
      "Min loss: 0.0058620041236281395\n",
      "Mean loss: 0.012628016605352363\n",
      "Std loss: 0.007222648577595506\n",
      "Total Loss: 0.07576809963211417\n",
      "------------------------------------ epoch 8734 (52398 steps) ------------------------------------\n",
      "Max loss: 0.011934331618249416\n",
      "Min loss: 0.007145039737224579\n",
      "Mean loss: 0.009279359908153614\n",
      "Std loss: 0.0015517255360959275\n",
      "Total Loss: 0.05567615944892168\n",
      "------------------------------------ epoch 8735 (52404 steps) ------------------------------------\n",
      "Max loss: 0.016443654894828796\n",
      "Min loss: 0.004179530777037144\n",
      "Mean loss: 0.007586052951713403\n",
      "Std loss: 0.004084494265978711\n",
      "Total Loss: 0.04551631771028042\n",
      "------------------------------------ epoch 8736 (52410 steps) ------------------------------------\n",
      "Max loss: 0.008208101615309715\n",
      "Min loss: 0.00381285953335464\n",
      "Mean loss: 0.005133164386885862\n",
      "Std loss: 0.0014681857917803838\n",
      "Total Loss: 0.03079898632131517\n",
      "------------------------------------ epoch 8737 (52416 steps) ------------------------------------\n",
      "Max loss: 0.008191490545868874\n",
      "Min loss: 0.003105749376118183\n",
      "Mean loss: 0.005537839916845162\n",
      "Std loss: 0.0018491004436272322\n",
      "Total Loss: 0.033227039501070976\n",
      "------------------------------------ epoch 8738 (52422 steps) ------------------------------------\n",
      "Max loss: 0.014868069440126419\n",
      "Min loss: 0.0038645172026008368\n",
      "Mean loss: 0.0071979804973428445\n",
      "Std loss: 0.0035962795822090396\n",
      "Total Loss: 0.04318788298405707\n",
      "------------------------------------ epoch 8739 (52428 steps) ------------------------------------\n",
      "Max loss: 0.011483374051749706\n",
      "Min loss: 0.003376225708052516\n",
      "Mean loss: 0.008304455899633467\n",
      "Std loss: 0.0026148747518467876\n",
      "Total Loss: 0.0498267353978008\n",
      "------------------------------------ epoch 8740 (52434 steps) ------------------------------------\n",
      "Max loss: 0.015745453536510468\n",
      "Min loss: 0.003817307762801647\n",
      "Mean loss: 0.006968120035404961\n",
      "Std loss: 0.004141283544662091\n",
      "Total Loss: 0.04180872021242976\n",
      "------------------------------------ epoch 8741 (52440 steps) ------------------------------------\n",
      "Max loss: 0.006426576524972916\n",
      "Min loss: 0.003259898629039526\n",
      "Mean loss: 0.00494699296541512\n",
      "Std loss: 0.0012185146482862582\n",
      "Total Loss: 0.02968195779249072\n",
      "------------------------------------ epoch 8742 (52446 steps) ------------------------------------\n",
      "Max loss: 0.007394014857709408\n",
      "Min loss: 0.0033333715982735157\n",
      "Mean loss: 0.005515830668931206\n",
      "Std loss: 0.0014498606921437647\n",
      "Total Loss: 0.033094984013587236\n",
      "------------------------------------ epoch 8743 (52452 steps) ------------------------------------\n",
      "Max loss: 0.027221878990530968\n",
      "Min loss: 0.006043992470949888\n",
      "Mean loss: 0.01094009269339343\n",
      "Std loss: 0.0074259697920959515\n",
      "Total Loss: 0.06564055616036057\n",
      "------------------------------------ epoch 8744 (52458 steps) ------------------------------------\n",
      "Max loss: 0.014314375817775726\n",
      "Min loss: 0.005246920045465231\n",
      "Mean loss: 0.009150701885422071\n",
      "Std loss: 0.002995064115993912\n",
      "Total Loss: 0.054904211312532425\n",
      "------------------------------------ epoch 8745 (52464 steps) ------------------------------------\n",
      "Max loss: 0.005640789866447449\n",
      "Min loss: 0.003951301798224449\n",
      "Mean loss: 0.00476247351616621\n",
      "Std loss: 0.0006532180885279006\n",
      "Total Loss: 0.02857484109699726\n",
      "------------------------------------ epoch 8746 (52470 steps) ------------------------------------\n",
      "Max loss: 0.009592689573764801\n",
      "Min loss: 0.004455994348973036\n",
      "Mean loss: 0.006214766064658761\n",
      "Std loss: 0.0017028356228440396\n",
      "Total Loss: 0.037288596387952566\n",
      "------------------------------------ epoch 8747 (52476 steps) ------------------------------------\n",
      "Max loss: 0.011550381779670715\n",
      "Min loss: 0.003531645517796278\n",
      "Mean loss: 0.0057727308400596184\n",
      "Std loss: 0.0028964314574296264\n",
      "Total Loss: 0.03463638504035771\n",
      "------------------------------------ epoch 8748 (52482 steps) ------------------------------------\n",
      "Max loss: 0.008438942022621632\n",
      "Min loss: 0.003939352929592133\n",
      "Mean loss: 0.0054052600171417\n",
      "Std loss: 0.0014760905684575356\n",
      "Total Loss: 0.0324315601028502\n",
      "------------------------------------ epoch 8749 (52488 steps) ------------------------------------\n",
      "Max loss: 0.013037647120654583\n",
      "Min loss: 0.00373686570674181\n",
      "Mean loss: 0.006956507607052724\n",
      "Std loss: 0.0030471752319057694\n",
      "Total Loss: 0.04173904564231634\n",
      "------------------------------------ epoch 8750 (52494 steps) ------------------------------------\n",
      "Max loss: 0.010652998462319374\n",
      "Min loss: 0.003488060785457492\n",
      "Mean loss: 0.006339338103619714\n",
      "Std loss: 0.0021828820397374058\n",
      "Total Loss: 0.03803602862171829\n",
      "------------------------------------ epoch 8751 (52500 steps) ------------------------------------\n",
      "Max loss: 0.014965733513236046\n",
      "Min loss: 0.004116756841540337\n",
      "Mean loss: 0.009347149015714725\n",
      "Std loss: 0.0038373382697314508\n",
      "Total Loss: 0.05608289409428835\n",
      "------------------------------------ epoch 8752 (52506 steps) ------------------------------------\n",
      "Max loss: 0.016741624101996422\n",
      "Min loss: 0.0054276203736662865\n",
      "Mean loss: 0.01023307271922628\n",
      "Std loss: 0.004136721661647106\n",
      "Total Loss: 0.061398436315357685\n",
      "------------------------------------ epoch 8753 (52512 steps) ------------------------------------\n",
      "Max loss: 0.011697962880134583\n",
      "Min loss: 0.003937619272619486\n",
      "Mean loss: 0.006131537569065888\n",
      "Std loss: 0.0026666593811564866\n",
      "Total Loss: 0.03678922541439533\n",
      "------------------------------------ epoch 8754 (52518 steps) ------------------------------------\n",
      "Max loss: 0.008589251898229122\n",
      "Min loss: 0.003898543305695057\n",
      "Mean loss: 0.006371643549452226\n",
      "Std loss: 0.0016120779797055235\n",
      "Total Loss: 0.03822986129671335\n",
      "------------------------------------ epoch 8755 (52524 steps) ------------------------------------\n",
      "Max loss: 0.030592648312449455\n",
      "Min loss: 0.004493981599807739\n",
      "Mean loss: 0.010402416810393333\n",
      "Std loss: 0.009103288604495434\n",
      "Total Loss: 0.06241450086236\n",
      "------------------------------------ epoch 8756 (52530 steps) ------------------------------------\n",
      "Max loss: 0.01278773508965969\n",
      "Min loss: 0.003917842172086239\n",
      "Mean loss: 0.008022658294066787\n",
      "Std loss: 0.003294159185111105\n",
      "Total Loss: 0.04813594976440072\n",
      "------------------------------------ epoch 8757 (52536 steps) ------------------------------------\n",
      "Max loss: 0.010580820962786674\n",
      "Min loss: 0.003468131646513939\n",
      "Mean loss: 0.00656639124887685\n",
      "Std loss: 0.0026892677886577567\n",
      "Total Loss: 0.0393983474932611\n",
      "------------------------------------ epoch 8758 (52542 steps) ------------------------------------\n",
      "Max loss: 0.011496144346892834\n",
      "Min loss: 0.003825940890237689\n",
      "Mean loss: 0.007492074393667281\n",
      "Std loss: 0.0027409148280405637\n",
      "Total Loss: 0.044952446362003684\n",
      "------------------------------------ epoch 8759 (52548 steps) ------------------------------------\n",
      "Max loss: 0.008270671591162682\n",
      "Min loss: 0.0036910870112478733\n",
      "Mean loss: 0.005596236713851492\n",
      "Std loss: 0.00157694073720837\n",
      "Total Loss: 0.03357742028310895\n",
      "------------------------------------ epoch 8760 (52554 steps) ------------------------------------\n",
      "Max loss: 0.026153549551963806\n",
      "Min loss: 0.003812673967331648\n",
      "Mean loss: 0.009323935024440289\n",
      "Std loss: 0.007614673189910669\n",
      "Total Loss: 0.05594361014664173\n",
      "------------------------------------ epoch 8761 (52560 steps) ------------------------------------\n",
      "Max loss: 0.03512962907552719\n",
      "Min loss: 0.003601954085752368\n",
      "Mean loss: 0.013459327669503788\n",
      "Std loss: 0.011997841983933503\n",
      "Total Loss: 0.08075596601702273\n",
      "------------------------------------ epoch 8762 (52566 steps) ------------------------------------\n",
      "Max loss: 0.037661075592041016\n",
      "Min loss: 0.005124869290739298\n",
      "Mean loss: 0.015328071468199292\n",
      "Std loss: 0.010663526491794502\n",
      "Total Loss: 0.09196842880919576\n",
      "------------------------------------ epoch 8763 (52572 steps) ------------------------------------\n",
      "Max loss: 0.010504970327019691\n",
      "Min loss: 0.004175225738435984\n",
      "Mean loss: 0.0069554648362100124\n",
      "Std loss: 0.0022708722621830567\n",
      "Total Loss: 0.041732789017260075\n",
      "------------------------------------ epoch 8764 (52578 steps) ------------------------------------\n",
      "Max loss: 0.03214453533291817\n",
      "Min loss: 0.0041851261630654335\n",
      "Mean loss: 0.009951258078217506\n",
      "Std loss: 0.009952271694612751\n",
      "Total Loss: 0.05970754846930504\n",
      "------------------------------------ epoch 8765 (52584 steps) ------------------------------------\n",
      "Max loss: 0.010685404762625694\n",
      "Min loss: 0.004441262688487768\n",
      "Mean loss: 0.00715386588126421\n",
      "Std loss: 0.0019106025559729738\n",
      "Total Loss: 0.04292319528758526\n",
      "------------------------------------ epoch 8766 (52590 steps) ------------------------------------\n",
      "Max loss: 0.008133508265018463\n",
      "Min loss: 0.00396856851875782\n",
      "Mean loss: 0.005454443007086714\n",
      "Std loss: 0.0015359193519440888\n",
      "Total Loss: 0.032726658042520285\n",
      "------------------------------------ epoch 8767 (52596 steps) ------------------------------------\n",
      "Max loss: 0.015085185877978802\n",
      "Min loss: 0.004641822539269924\n",
      "Mean loss: 0.009068325317154327\n",
      "Std loss: 0.0033247516023998096\n",
      "Total Loss: 0.05440995190292597\n",
      "------------------------------------ epoch 8768 (52602 steps) ------------------------------------\n",
      "Max loss: 0.023271961137652397\n",
      "Min loss: 0.0040719094686210155\n",
      "Mean loss: 0.010777621297165751\n",
      "Std loss: 0.006802333189228286\n",
      "Total Loss: 0.06466572778299451\n",
      "------------------------------------ epoch 8769 (52608 steps) ------------------------------------\n",
      "Max loss: 0.03151120990514755\n",
      "Min loss: 0.0047731176018714905\n",
      "Mean loss: 0.011716959920401374\n",
      "Std loss: 0.009098169449065496\n",
      "Total Loss: 0.07030175952240825\n",
      "------------------------------------ epoch 8770 (52614 steps) ------------------------------------\n",
      "Max loss: 0.02158880978822708\n",
      "Min loss: 0.004178749397397041\n",
      "Mean loss: 0.012326767047246298\n",
      "Std loss: 0.005667947037639338\n",
      "Total Loss: 0.07396060228347778\n",
      "------------------------------------ epoch 8771 (52620 steps) ------------------------------------\n",
      "Max loss: 0.015005337074398994\n",
      "Min loss: 0.004820168484002352\n",
      "Mean loss: 0.007993798858175674\n",
      "Std loss: 0.0033767761463172094\n",
      "Total Loss: 0.04796279314905405\n",
      "------------------------------------ epoch 8772 (52626 steps) ------------------------------------\n",
      "Max loss: 0.023168187588453293\n",
      "Min loss: 0.005736319348216057\n",
      "Mean loss: 0.012390094343572855\n",
      "Std loss: 0.00576024855059635\n",
      "Total Loss: 0.07434056606143713\n",
      "------------------------------------ epoch 8773 (52632 steps) ------------------------------------\n",
      "Max loss: 0.01352023333311081\n",
      "Min loss: 0.004607907496392727\n",
      "Mean loss: 0.008714887235934535\n",
      "Std loss: 0.0035869404471461676\n",
      "Total Loss: 0.052289323415607214\n",
      "------------------------------------ epoch 8774 (52638 steps) ------------------------------------\n",
      "Max loss: 0.024885814636945724\n",
      "Min loss: 0.005018908996134996\n",
      "Mean loss: 0.009163788752630353\n",
      "Std loss: 0.007061479291617565\n",
      "Total Loss: 0.05498273251578212\n",
      "------------------------------------ epoch 8775 (52644 steps) ------------------------------------\n",
      "Max loss: 0.009150016121566296\n",
      "Min loss: 0.004204005002975464\n",
      "Mean loss: 0.005783592971662681\n",
      "Std loss: 0.0017263203552293914\n",
      "Total Loss: 0.03470155782997608\n",
      "------------------------------------ epoch 8776 (52650 steps) ------------------------------------\n",
      "Max loss: 0.013856659643352032\n",
      "Min loss: 0.003376931417733431\n",
      "Mean loss: 0.007784136105328798\n",
      "Std loss: 0.003487752399077274\n",
      "Total Loss: 0.04670481663197279\n",
      "------------------------------------ epoch 8777 (52656 steps) ------------------------------------\n",
      "Max loss: 0.013712143525481224\n",
      "Min loss: 0.004717751871794462\n",
      "Mean loss: 0.007667524972930551\n",
      "Std loss: 0.0031012177455128262\n",
      "Total Loss: 0.0460051498375833\n",
      "------------------------------------ epoch 8778 (52662 steps) ------------------------------------\n",
      "Max loss: 0.013095442205667496\n",
      "Min loss: 0.004104120656847954\n",
      "Mean loss: 0.007348221881935994\n",
      "Std loss: 0.0032609709513458937\n",
      "Total Loss: 0.04408933129161596\n",
      "------------------------------------ epoch 8779 (52668 steps) ------------------------------------\n",
      "Max loss: 0.03292407840490341\n",
      "Min loss: 0.004918075632303953\n",
      "Mean loss: 0.011969245970249176\n",
      "Std loss: 0.009612774419244869\n",
      "Total Loss: 0.07181547582149506\n",
      "------------------------------------ epoch 8780 (52674 steps) ------------------------------------\n",
      "Max loss: 0.013532789424061775\n",
      "Min loss: 0.004319524392485619\n",
      "Mean loss: 0.007084763686483105\n",
      "Std loss: 0.0031067378425761417\n",
      "Total Loss: 0.04250858211889863\n",
      "------------------------------------ epoch 8781 (52680 steps) ------------------------------------\n",
      "Max loss: 0.009840535931289196\n",
      "Min loss: 0.004082699306309223\n",
      "Mean loss: 0.007126175798475742\n",
      "Std loss: 0.0020308754912673763\n",
      "Total Loss: 0.042757054790854454\n",
      "------------------------------------ epoch 8782 (52686 steps) ------------------------------------\n",
      "Max loss: 0.008734742179512978\n",
      "Min loss: 0.004026785027235746\n",
      "Mean loss: 0.00559965994519492\n",
      "Std loss: 0.0015714370203098453\n",
      "Total Loss: 0.03359795967116952\n",
      "------------------------------------ epoch 8783 (52692 steps) ------------------------------------\n",
      "Max loss: 0.01435203105211258\n",
      "Min loss: 0.004222705028951168\n",
      "Mean loss: 0.008594050693015257\n",
      "Std loss: 0.00402402147319542\n",
      "Total Loss: 0.051564304158091545\n",
      "------------------------------------ epoch 8784 (52698 steps) ------------------------------------\n",
      "Max loss: 0.007805305998772383\n",
      "Min loss: 0.00431515509262681\n",
      "Mean loss: 0.005414984965076049\n",
      "Std loss: 0.0012111763783438116\n",
      "Total Loss: 0.032489909790456295\n",
      "------------------------------------ epoch 8785 (52704 steps) ------------------------------------\n",
      "Max loss: 0.017923712730407715\n",
      "Min loss: 0.0033973599784076214\n",
      "Mean loss: 0.008909701136872172\n",
      "Std loss: 0.0048910316913671256\n",
      "Total Loss: 0.053458206821233034\n",
      "------------------------------------ epoch 8786 (52710 steps) ------------------------------------\n",
      "Max loss: 0.009567291475832462\n",
      "Min loss: 0.0038583013229072094\n",
      "Mean loss: 0.00593444185021023\n",
      "Std loss: 0.0019114954520463355\n",
      "Total Loss: 0.03560665110126138\n",
      "------------------------------------ epoch 8787 (52716 steps) ------------------------------------\n",
      "Max loss: 0.01131222303956747\n",
      "Min loss: 0.003625822253525257\n",
      "Mean loss: 0.0066332573381563025\n",
      "Std loss: 0.002768296653758829\n",
      "Total Loss: 0.03979954402893782\n",
      "------------------------------------ epoch 8788 (52722 steps) ------------------------------------\n",
      "Max loss: 0.013084521517157555\n",
      "Min loss: 0.003730689873918891\n",
      "Mean loss: 0.0067009663907811046\n",
      "Std loss: 0.00333145757975497\n",
      "Total Loss: 0.04020579834468663\n",
      "------------------------------------ epoch 8789 (52728 steps) ------------------------------------\n",
      "Max loss: 0.013308292254805565\n",
      "Min loss: 0.006611538585275412\n",
      "Mean loss: 0.008895756443962455\n",
      "Std loss: 0.0023653932888777975\n",
      "Total Loss: 0.05337453866377473\n",
      "------------------------------------ epoch 8790 (52734 steps) ------------------------------------\n",
      "Max loss: 0.01430249959230423\n",
      "Min loss: 0.0028609824366867542\n",
      "Mean loss: 0.006254948132360975\n",
      "Std loss: 0.0038408270440269355\n",
      "Total Loss: 0.03752968879416585\n",
      "------------------------------------ epoch 8791 (52740 steps) ------------------------------------\n",
      "Max loss: 0.009536957368254662\n",
      "Min loss: 0.004060348030179739\n",
      "Mean loss: 0.006328519356126587\n",
      "Std loss: 0.0021497173856636735\n",
      "Total Loss: 0.03797111613675952\n",
      "------------------------------------ epoch 8792 (52746 steps) ------------------------------------\n",
      "Max loss: 0.015274620614945889\n",
      "Min loss: 0.005704828538000584\n",
      "Mean loss: 0.008025947337349256\n",
      "Std loss: 0.003309080603444001\n",
      "Total Loss: 0.048155684024095535\n",
      "------------------------------------ epoch 8793 (52752 steps) ------------------------------------\n",
      "Max loss: 0.011496433988213539\n",
      "Min loss: 0.0037272637709975243\n",
      "Mean loss: 0.00743559127052625\n",
      "Std loss: 0.0025384488098489787\n",
      "Total Loss: 0.0446135476231575\n",
      "------------------------------------ epoch 8794 (52758 steps) ------------------------------------\n",
      "Max loss: 0.009528840892016888\n",
      "Min loss: 0.003259045537561178\n",
      "Mean loss: 0.006585088325664401\n",
      "Std loss: 0.0021526563243719748\n",
      "Total Loss: 0.039510529953986406\n",
      "------------------------------------ epoch 8795 (52764 steps) ------------------------------------\n",
      "Max loss: 0.014481445774435997\n",
      "Min loss: 0.004197863396257162\n",
      "Mean loss: 0.007637597930928071\n",
      "Std loss: 0.00359343974105221\n",
      "Total Loss: 0.04582558758556843\n",
      "------------------------------------ epoch 8796 (52770 steps) ------------------------------------\n",
      "Max loss: 0.018358096480369568\n",
      "Min loss: 0.004458192735910416\n",
      "Mean loss: 0.007748966027672092\n",
      "Std loss: 0.004854389734344282\n",
      "Total Loss: 0.04649379616603255\n",
      "------------------------------------ epoch 8797 (52776 steps) ------------------------------------\n",
      "Max loss: 0.00863155908882618\n",
      "Min loss: 0.0044221761636435986\n",
      "Mean loss: 0.007050415733829141\n",
      "Std loss: 0.0012963243649009954\n",
      "Total Loss: 0.042302494402974844\n",
      "------------------------------------ epoch 8798 (52782 steps) ------------------------------------\n",
      "Max loss: 0.020153604447841644\n",
      "Min loss: 0.003359573893249035\n",
      "Mean loss: 0.008634104548643032\n",
      "Std loss: 0.005556612222926328\n",
      "Total Loss: 0.051804627291858196\n",
      "------------------------------------ epoch 8799 (52788 steps) ------------------------------------\n",
      "Max loss: 0.025521699339151382\n",
      "Min loss: 0.004179435782134533\n",
      "Mean loss: 0.009183686304216584\n",
      "Std loss: 0.007417717997873236\n",
      "Total Loss: 0.0551021178252995\n",
      "------------------------------------ epoch 8800 (52794 steps) ------------------------------------\n",
      "Max loss: 0.01375170610845089\n",
      "Min loss: 0.0037640980444848537\n",
      "Mean loss: 0.007293990192313989\n",
      "Std loss: 0.003827467819068421\n",
      "Total Loss: 0.043763941153883934\n",
      "------------------------------------ epoch 8801 (52800 steps) ------------------------------------\n",
      "Max loss: 0.02957010082900524\n",
      "Min loss: 0.004399319179356098\n",
      "Mean loss: 0.00926297049348553\n",
      "Std loss: 0.009107740691763209\n",
      "Total Loss: 0.05557782296091318\n",
      "saved model at ./weights/model_8801.pth\n",
      "------------------------------------ epoch 8802 (52806 steps) ------------------------------------\n",
      "Max loss: 0.019859664142131805\n",
      "Min loss: 0.00419072387740016\n",
      "Mean loss: 0.009810200969999036\n",
      "Std loss: 0.005075360311823657\n",
      "Total Loss: 0.05886120581999421\n",
      "------------------------------------ epoch 8803 (52812 steps) ------------------------------------\n",
      "Max loss: 0.01496833749115467\n",
      "Min loss: 0.007596567273139954\n",
      "Mean loss: 0.011020345923801264\n",
      "Std loss: 0.002643931670554327\n",
      "Total Loss: 0.06612207554280758\n",
      "------------------------------------ epoch 8804 (52818 steps) ------------------------------------\n",
      "Max loss: 0.013248670846223831\n",
      "Min loss: 0.004325606860220432\n",
      "Mean loss: 0.0074278435204178095\n",
      "Std loss: 0.0030711007576129526\n",
      "Total Loss: 0.04456706112250686\n",
      "------------------------------------ epoch 8805 (52824 steps) ------------------------------------\n",
      "Max loss: 0.015970978885889053\n",
      "Min loss: 0.0033177563454955816\n",
      "Mean loss: 0.009057980380021036\n",
      "Std loss: 0.0052481993752142376\n",
      "Total Loss: 0.054347882280126214\n",
      "------------------------------------ epoch 8806 (52830 steps) ------------------------------------\n",
      "Max loss: 0.015174525789916515\n",
      "Min loss: 0.00451833987608552\n",
      "Mean loss: 0.00685169097657005\n",
      "Std loss: 0.0038002126427773837\n",
      "Total Loss: 0.0411101458594203\n",
      "------------------------------------ epoch 8807 (52836 steps) ------------------------------------\n",
      "Max loss: 0.0105203278362751\n",
      "Min loss: 0.004761858843266964\n",
      "Mean loss: 0.006734977398688595\n",
      "Std loss: 0.0019625697480026246\n",
      "Total Loss: 0.04040986439213157\n",
      "------------------------------------ epoch 8808 (52842 steps) ------------------------------------\n",
      "Max loss: 0.0064102113246917725\n",
      "Min loss: 0.0030631020199507475\n",
      "Mean loss: 0.00452570461978515\n",
      "Std loss: 0.0013277988006004735\n",
      "Total Loss: 0.0271542277187109\n",
      "------------------------------------ epoch 8809 (52848 steps) ------------------------------------\n",
      "Max loss: 0.009547729045152664\n",
      "Min loss: 0.0033023010473698378\n",
      "Mean loss: 0.005655191179054479\n",
      "Std loss: 0.0024208140596007014\n",
      "Total Loss: 0.03393114707432687\n",
      "------------------------------------ epoch 8810 (52854 steps) ------------------------------------\n",
      "Max loss: 0.03712250292301178\n",
      "Min loss: 0.003106632037088275\n",
      "Mean loss: 0.010376160382293165\n",
      "Std loss: 0.012121021729504022\n",
      "Total Loss: 0.06225696229375899\n",
      "------------------------------------ epoch 8811 (52860 steps) ------------------------------------\n",
      "Max loss: 0.024628570303320885\n",
      "Min loss: 0.004274173639714718\n",
      "Mean loss: 0.011112689583872756\n",
      "Std loss: 0.006950007504012976\n",
      "Total Loss: 0.06667613750323653\n",
      "------------------------------------ epoch 8812 (52866 steps) ------------------------------------\n",
      "Max loss: 0.0171066764742136\n",
      "Min loss: 0.005119191017001867\n",
      "Mean loss: 0.011878163631384572\n",
      "Std loss: 0.004232933279242087\n",
      "Total Loss: 0.07126898178830743\n",
      "------------------------------------ epoch 8813 (52872 steps) ------------------------------------\n",
      "Max loss: 0.010000496171414852\n",
      "Min loss: 0.005468291696161032\n",
      "Mean loss: 0.008229039997483293\n",
      "Std loss: 0.0014198177615775069\n",
      "Total Loss: 0.04937423998489976\n",
      "------------------------------------ epoch 8814 (52878 steps) ------------------------------------\n",
      "Max loss: 0.010931288823485374\n",
      "Min loss: 0.004679144360125065\n",
      "Mean loss: 0.00791417807340622\n",
      "Std loss: 0.002503865716304645\n",
      "Total Loss: 0.04748506844043732\n",
      "------------------------------------ epoch 8815 (52884 steps) ------------------------------------\n",
      "Max loss: 0.016977161169052124\n",
      "Min loss: 0.005214379169046879\n",
      "Mean loss: 0.009261413787802061\n",
      "Std loss: 0.0043406960365319405\n",
      "Total Loss: 0.05556848272681236\n",
      "------------------------------------ epoch 8816 (52890 steps) ------------------------------------\n",
      "Max loss: 0.042329397052526474\n",
      "Min loss: 0.004218184854835272\n",
      "Mean loss: 0.01262799968632559\n",
      "Std loss: 0.013439617427111968\n",
      "Total Loss: 0.07576799811795354\n",
      "------------------------------------ epoch 8817 (52896 steps) ------------------------------------\n",
      "Max loss: 0.01802479848265648\n",
      "Min loss: 0.004116281401365995\n",
      "Mean loss: 0.010794872107605139\n",
      "Std loss: 0.004832493246111128\n",
      "Total Loss: 0.06476923264563084\n",
      "------------------------------------ epoch 8818 (52902 steps) ------------------------------------\n",
      "Max loss: 0.020228100940585136\n",
      "Min loss: 0.007572048809379339\n",
      "Mean loss: 0.013146678994720181\n",
      "Std loss: 0.003940200834186607\n",
      "Total Loss: 0.07888007396832108\n",
      "------------------------------------ epoch 8819 (52908 steps) ------------------------------------\n",
      "Max loss: 0.012302927672863007\n",
      "Min loss: 0.006014651618897915\n",
      "Mean loss: 0.008162704606850943\n",
      "Std loss: 0.0021875041012819904\n",
      "Total Loss: 0.04897622764110565\n",
      "------------------------------------ epoch 8820 (52914 steps) ------------------------------------\n",
      "Max loss: 0.04399855062365532\n",
      "Min loss: 0.00529064703732729\n",
      "Mean loss: 0.017483421756575506\n",
      "Std loss: 0.014789519673672489\n",
      "Total Loss: 0.10490053053945303\n",
      "------------------------------------ epoch 8821 (52920 steps) ------------------------------------\n",
      "Max loss: 0.011988609097898006\n",
      "Min loss: 0.004022414796054363\n",
      "Mean loss: 0.00782160748106738\n",
      "Std loss: 0.0026730620083900186\n",
      "Total Loss: 0.046929644886404276\n",
      "------------------------------------ epoch 8822 (52926 steps) ------------------------------------\n",
      "Max loss: 0.013232318684458733\n",
      "Min loss: 0.005694884341210127\n",
      "Mean loss: 0.00820600187095503\n",
      "Std loss: 0.002384127394108905\n",
      "Total Loss: 0.04923601122573018\n",
      "------------------------------------ epoch 8823 (52932 steps) ------------------------------------\n",
      "Max loss: 0.01435416005551815\n",
      "Min loss: 0.005440972745418549\n",
      "Mean loss: 0.007858288008719683\n",
      "Std loss: 0.003062545104433193\n",
      "Total Loss: 0.047149728052318096\n",
      "------------------------------------ epoch 8824 (52938 steps) ------------------------------------\n",
      "Max loss: 0.01116742193698883\n",
      "Min loss: 0.003731469390913844\n",
      "Mean loss: 0.007085170402812461\n",
      "Std loss: 0.0028166968728045904\n",
      "Total Loss: 0.042511022416874766\n",
      "------------------------------------ epoch 8825 (52944 steps) ------------------------------------\n",
      "Max loss: 0.012357078492641449\n",
      "Min loss: 0.0049057044088840485\n",
      "Mean loss: 0.006900767097249627\n",
      "Std loss: 0.00263153042916985\n",
      "Total Loss: 0.04140460258349776\n",
      "------------------------------------ epoch 8826 (52950 steps) ------------------------------------\n",
      "Max loss: 0.010391248390078545\n",
      "Min loss: 0.0037304391153156757\n",
      "Mean loss: 0.005614216284205516\n",
      "Std loss: 0.0023258381982981043\n",
      "Total Loss: 0.0336852977052331\n",
      "------------------------------------ epoch 8827 (52956 steps) ------------------------------------\n",
      "Max loss: 0.014671720564365387\n",
      "Min loss: 0.0037167712580412626\n",
      "Mean loss: 0.007040908521351715\n",
      "Std loss: 0.003761808593014672\n",
      "Total Loss: 0.04224545112811029\n",
      "------------------------------------ epoch 8828 (52962 steps) ------------------------------------\n",
      "Max loss: 0.028407488018274307\n",
      "Min loss: 0.003786186221987009\n",
      "Mean loss: 0.011047739535570145\n",
      "Std loss: 0.008293565275825997\n",
      "Total Loss: 0.06628643721342087\n",
      "------------------------------------ epoch 8829 (52968 steps) ------------------------------------\n",
      "Max loss: 0.010643504559993744\n",
      "Min loss: 0.004205944947898388\n",
      "Mean loss: 0.006954792033260067\n",
      "Std loss: 0.0027214554085327205\n",
      "Total Loss: 0.041728752199560404\n",
      "------------------------------------ epoch 8830 (52974 steps) ------------------------------------\n",
      "Max loss: 0.012213163077831268\n",
      "Min loss: 0.004350596573203802\n",
      "Mean loss: 0.006477950916936\n",
      "Std loss: 0.002630818683948393\n",
      "Total Loss: 0.038867705501616\n",
      "------------------------------------ epoch 8831 (52980 steps) ------------------------------------\n",
      "Max loss: 0.012236474081873894\n",
      "Min loss: 0.0034570759162306786\n",
      "Mean loss: 0.007724812875191371\n",
      "Std loss: 0.0030233090829368987\n",
      "Total Loss: 0.046348877251148224\n",
      "------------------------------------ epoch 8832 (52986 steps) ------------------------------------\n",
      "Max loss: 0.012973989360034466\n",
      "Min loss: 0.0035370416007936\n",
      "Mean loss: 0.00751158082857728\n",
      "Std loss: 0.003199334232015915\n",
      "Total Loss: 0.04506948497146368\n",
      "------------------------------------ epoch 8833 (52992 steps) ------------------------------------\n",
      "Max loss: 0.010063099674880505\n",
      "Min loss: 0.003763963235542178\n",
      "Mean loss: 0.006463495354788999\n",
      "Std loss: 0.0022253735025584806\n",
      "Total Loss: 0.03878097212873399\n",
      "------------------------------------ epoch 8834 (52998 steps) ------------------------------------\n",
      "Max loss: 0.0065346043556928635\n",
      "Min loss: 0.004575069062411785\n",
      "Mean loss: 0.0056963822183509665\n",
      "Std loss: 0.0006831913501661408\n",
      "Total Loss: 0.0341782933101058\n",
      "------------------------------------ epoch 8835 (53004 steps) ------------------------------------\n",
      "Max loss: 0.0402798131108284\n",
      "Min loss: 0.0042340452782809734\n",
      "Mean loss: 0.012613762713347873\n",
      "Std loss: 0.012665001026310945\n",
      "Total Loss: 0.07568257628008723\n",
      "------------------------------------ epoch 8836 (53010 steps) ------------------------------------\n",
      "Max loss: 0.007816496305167675\n",
      "Min loss: 0.004021181724965572\n",
      "Mean loss: 0.005940155281374852\n",
      "Std loss: 0.0014502788935537095\n",
      "Total Loss: 0.03564093168824911\n",
      "------------------------------------ epoch 8837 (53016 steps) ------------------------------------\n",
      "Max loss: 0.015435507521033287\n",
      "Min loss: 0.003960132133215666\n",
      "Mean loss: 0.007305280150224765\n",
      "Std loss: 0.0038725770293717475\n",
      "Total Loss: 0.04383168090134859\n",
      "------------------------------------ epoch 8838 (53022 steps) ------------------------------------\n",
      "Max loss: 0.018074220046401024\n",
      "Min loss: 0.0034853811375796795\n",
      "Mean loss: 0.008212941465899348\n",
      "Std loss: 0.005098098594450457\n",
      "Total Loss: 0.04927764879539609\n",
      "------------------------------------ epoch 8839 (53028 steps) ------------------------------------\n",
      "Max loss: 0.019091425463557243\n",
      "Min loss: 0.005346592515707016\n",
      "Mean loss: 0.010729032723853985\n",
      "Std loss: 0.005134606359957903\n",
      "Total Loss: 0.06437419634312391\n",
      "------------------------------------ epoch 8840 (53034 steps) ------------------------------------\n",
      "Max loss: 0.012382525019347668\n",
      "Min loss: 0.004301631823182106\n",
      "Mean loss: 0.008833216658482948\n",
      "Std loss: 0.002813097339010787\n",
      "Total Loss: 0.052999299950897694\n",
      "------------------------------------ epoch 8841 (53040 steps) ------------------------------------\n",
      "Max loss: 0.017291082069277763\n",
      "Min loss: 0.005502892192453146\n",
      "Mean loss: 0.008623665509124597\n",
      "Std loss: 0.003960843625748339\n",
      "Total Loss: 0.05174199305474758\n",
      "------------------------------------ epoch 8842 (53046 steps) ------------------------------------\n",
      "Max loss: 0.016498200595378876\n",
      "Min loss: 0.005850418470799923\n",
      "Mean loss: 0.008863268187269568\n",
      "Std loss: 0.003590119231901417\n",
      "Total Loss: 0.05317960912361741\n",
      "------------------------------------ epoch 8843 (53052 steps) ------------------------------------\n",
      "Max loss: 0.008222389966249466\n",
      "Min loss: 0.004566466435790062\n",
      "Mean loss: 0.006714557607968648\n",
      "Std loss: 0.0011711353887053403\n",
      "Total Loss: 0.04028734564781189\n",
      "------------------------------------ epoch 8844 (53058 steps) ------------------------------------\n",
      "Max loss: 0.011356617324054241\n",
      "Min loss: 0.00392744829878211\n",
      "Mean loss: 0.007125502452254295\n",
      "Std loss: 0.002464820067417762\n",
      "Total Loss: 0.04275301471352577\n",
      "------------------------------------ epoch 8845 (53064 steps) ------------------------------------\n",
      "Max loss: 0.026645131409168243\n",
      "Min loss: 0.0038286875933408737\n",
      "Mean loss: 0.011745196534320712\n",
      "Std loss: 0.008503718704346457\n",
      "Total Loss: 0.07047117920592427\n",
      "------------------------------------ epoch 8846 (53070 steps) ------------------------------------\n",
      "Max loss: 0.014679180458188057\n",
      "Min loss: 0.004751375876367092\n",
      "Mean loss: 0.008491085764641563\n",
      "Std loss: 0.004018171746488348\n",
      "Total Loss: 0.05094651458784938\n",
      "------------------------------------ epoch 8847 (53076 steps) ------------------------------------\n",
      "Max loss: 0.0233787652105093\n",
      "Min loss: 0.0038959491066634655\n",
      "Mean loss: 0.012710848978410164\n",
      "Std loss: 0.00714828238115298\n",
      "Total Loss: 0.07626509387046099\n",
      "------------------------------------ epoch 8848 (53082 steps) ------------------------------------\n",
      "Max loss: 0.011654782108962536\n",
      "Min loss: 0.003784183179959655\n",
      "Mean loss: 0.0068192330266659456\n",
      "Std loss: 0.003128898780971294\n",
      "Total Loss: 0.040915398159995675\n",
      "------------------------------------ epoch 8849 (53088 steps) ------------------------------------\n",
      "Max loss: 0.010902268812060356\n",
      "Min loss: 0.005007385276257992\n",
      "Mean loss: 0.008041662474473318\n",
      "Std loss: 0.0023506331717304697\n",
      "Total Loss: 0.048249974846839905\n",
      "------------------------------------ epoch 8850 (53094 steps) ------------------------------------\n",
      "Max loss: 0.012334853410720825\n",
      "Min loss: 0.004457988776266575\n",
      "Mean loss: 0.007582461849475901\n",
      "Std loss: 0.0027722848693282555\n",
      "Total Loss: 0.0454947710968554\n",
      "------------------------------------ epoch 8851 (53100 steps) ------------------------------------\n",
      "Max loss: 0.014919854700565338\n",
      "Min loss: 0.004704765975475311\n",
      "Mean loss: 0.008507617826883992\n",
      "Std loss: 0.003450338285198796\n",
      "Total Loss: 0.05104570696130395\n",
      "------------------------------------ epoch 8852 (53106 steps) ------------------------------------\n",
      "Max loss: 0.016176538541913033\n",
      "Min loss: 0.0032874923199415207\n",
      "Mean loss: 0.008339145065595707\n",
      "Std loss: 0.004771409195741165\n",
      "Total Loss: 0.05003487039357424\n",
      "------------------------------------ epoch 8853 (53112 steps) ------------------------------------\n",
      "Max loss: 0.013305651023983955\n",
      "Min loss: 0.003801615908741951\n",
      "Mean loss: 0.00680749708165725\n",
      "Std loss: 0.003198791612556227\n",
      "Total Loss: 0.040844982489943504\n",
      "------------------------------------ epoch 8854 (53118 steps) ------------------------------------\n",
      "Max loss: 0.007204502820968628\n",
      "Min loss: 0.004216262139379978\n",
      "Mean loss: 0.005747890022272865\n",
      "Std loss: 0.0012788615450534087\n",
      "Total Loss: 0.03448734013363719\n",
      "------------------------------------ epoch 8855 (53124 steps) ------------------------------------\n",
      "Max loss: 0.009996534325182438\n",
      "Min loss: 0.0033164597116410732\n",
      "Mean loss: 0.007489265097926061\n",
      "Std loss: 0.0025424760263245407\n",
      "Total Loss: 0.04493559058755636\n",
      "------------------------------------ epoch 8856 (53130 steps) ------------------------------------\n",
      "Max loss: 0.016244085505604744\n",
      "Min loss: 0.0034124578814953566\n",
      "Mean loss: 0.00893204694148153\n",
      "Std loss: 0.005126820004204532\n",
      "Total Loss: 0.053592281648889184\n",
      "------------------------------------ epoch 8857 (53136 steps) ------------------------------------\n",
      "Max loss: 0.01389407180249691\n",
      "Min loss: 0.003908120095729828\n",
      "Mean loss: 0.006930721069996555\n",
      "Std loss: 0.0035384502994038995\n",
      "Total Loss: 0.041584326419979334\n",
      "------------------------------------ epoch 8858 (53142 steps) ------------------------------------\n",
      "Max loss: 0.013865487650036812\n",
      "Min loss: 0.003177633974701166\n",
      "Mean loss: 0.0065147617521385355\n",
      "Std loss: 0.0038073875542861406\n",
      "Total Loss: 0.03908857051283121\n",
      "------------------------------------ epoch 8859 (53148 steps) ------------------------------------\n",
      "Max loss: 0.010316448286175728\n",
      "Min loss: 0.004003432113677263\n",
      "Mean loss: 0.006196488160640001\n",
      "Std loss: 0.002262576502419091\n",
      "Total Loss: 0.03717892896384001\n",
      "------------------------------------ epoch 8860 (53154 steps) ------------------------------------\n",
      "Max loss: 0.00977160595357418\n",
      "Min loss: 0.005294891074299812\n",
      "Mean loss: 0.00709595965842406\n",
      "Std loss: 0.0014893345929953047\n",
      "Total Loss: 0.04257575795054436\n",
      "------------------------------------ epoch 8861 (53160 steps) ------------------------------------\n",
      "Max loss: 0.011684790253639221\n",
      "Min loss: 0.0036457092501223087\n",
      "Mean loss: 0.00775968578333656\n",
      "Std loss: 0.003063865871406985\n",
      "Total Loss: 0.04655811470001936\n",
      "------------------------------------ epoch 8862 (53166 steps) ------------------------------------\n",
      "Max loss: 0.02989349327981472\n",
      "Min loss: 0.003921094816178083\n",
      "Mean loss: 0.012519465992227197\n",
      "Std loss: 0.008635700800431998\n",
      "Total Loss: 0.07511679595336318\n",
      "------------------------------------ epoch 8863 (53172 steps) ------------------------------------\n",
      "Max loss: 0.029031984508037567\n",
      "Min loss: 0.004654766991734505\n",
      "Mean loss: 0.014304417263095578\n",
      "Std loss: 0.009107988062492878\n",
      "Total Loss: 0.08582650357857347\n",
      "------------------------------------ epoch 8864 (53178 steps) ------------------------------------\n",
      "Max loss: 0.01682206243276596\n",
      "Min loss: 0.005521660204976797\n",
      "Mean loss: 0.0086169073668619\n",
      "Std loss: 0.0038137594421537068\n",
      "Total Loss: 0.0517014442011714\n",
      "------------------------------------ epoch 8865 (53184 steps) ------------------------------------\n",
      "Max loss: 0.008491076529026031\n",
      "Min loss: 0.004535237327218056\n",
      "Mean loss: 0.006641263918330272\n",
      "Std loss: 0.0013433824508092568\n",
      "Total Loss: 0.03984758350998163\n",
      "------------------------------------ epoch 8866 (53190 steps) ------------------------------------\n",
      "Max loss: 0.014214426279067993\n",
      "Min loss: 0.0037355925887823105\n",
      "Mean loss: 0.00956791111578544\n",
      "Std loss: 0.0037441691371191323\n",
      "Total Loss: 0.05740746669471264\n",
      "------------------------------------ epoch 8867 (53196 steps) ------------------------------------\n",
      "Max loss: 0.03286600857973099\n",
      "Min loss: 0.0034435465931892395\n",
      "Mean loss: 0.011483279755339026\n",
      "Std loss: 0.01057928156607434\n",
      "Total Loss: 0.06889967853203416\n",
      "------------------------------------ epoch 8868 (53202 steps) ------------------------------------\n",
      "Max loss: 0.010567592456936836\n",
      "Min loss: 0.0071044377982616425\n",
      "Mean loss: 0.00889130107437571\n",
      "Std loss: 0.0013414264131449396\n",
      "Total Loss: 0.05334780644625425\n",
      "------------------------------------ epoch 8869 (53208 steps) ------------------------------------\n",
      "Max loss: 0.011416351422667503\n",
      "Min loss: 0.0037307385355234146\n",
      "Mean loss: 0.006599769772340854\n",
      "Std loss: 0.002771279689075454\n",
      "Total Loss: 0.039598618634045124\n",
      "------------------------------------ epoch 8870 (53214 steps) ------------------------------------\n",
      "Max loss: 0.03245341777801514\n",
      "Min loss: 0.0047968411818146706\n",
      "Mean loss: 0.015017807871724168\n",
      "Std loss: 0.00908576474727604\n",
      "Total Loss: 0.09010684723034501\n",
      "------------------------------------ epoch 8871 (53220 steps) ------------------------------------\n",
      "Max loss: 0.009168047457933426\n",
      "Min loss: 0.006853480823338032\n",
      "Mean loss: 0.008261897523577014\n",
      "Std loss: 0.0008954972620684033\n",
      "Total Loss: 0.04957138514146209\n",
      "------------------------------------ epoch 8872 (53226 steps) ------------------------------------\n",
      "Max loss: 0.01602298766374588\n",
      "Min loss: 0.0037843664176762104\n",
      "Mean loss: 0.007191181803743045\n",
      "Std loss: 0.004258725733536344\n",
      "Total Loss: 0.04314709082245827\n",
      "------------------------------------ epoch 8873 (53232 steps) ------------------------------------\n",
      "Max loss: 0.031974852085113525\n",
      "Min loss: 0.005106570199131966\n",
      "Mean loss: 0.009975894043842951\n",
      "Std loss: 0.009845286303661965\n",
      "Total Loss: 0.05985536426305771\n",
      "------------------------------------ epoch 8874 (53238 steps) ------------------------------------\n",
      "Max loss: 0.017937367781996727\n",
      "Min loss: 0.004442641511559486\n",
      "Mean loss: 0.01121563200528423\n",
      "Std loss: 0.0054165551675388526\n",
      "Total Loss: 0.06729379203170538\n",
      "------------------------------------ epoch 8875 (53244 steps) ------------------------------------\n",
      "Max loss: 0.012986931949853897\n",
      "Min loss: 0.0037138424813747406\n",
      "Mean loss: 0.006831766804680228\n",
      "Std loss: 0.0033006222623446816\n",
      "Total Loss: 0.04099060082808137\n",
      "------------------------------------ epoch 8876 (53250 steps) ------------------------------------\n",
      "Max loss: 0.008293475024402142\n",
      "Min loss: 0.004002459347248077\n",
      "Mean loss: 0.005626592707509796\n",
      "Std loss: 0.001327172892358646\n",
      "Total Loss: 0.033759556245058775\n",
      "------------------------------------ epoch 8877 (53256 steps) ------------------------------------\n",
      "Max loss: 0.02145380526781082\n",
      "Min loss: 0.0037179167848080397\n",
      "Mean loss: 0.009613077738322318\n",
      "Std loss: 0.006256561456325902\n",
      "Total Loss: 0.057678466429933906\n",
      "------------------------------------ epoch 8878 (53262 steps) ------------------------------------\n",
      "Max loss: 0.015698954463005066\n",
      "Min loss: 0.003898212220519781\n",
      "Mean loss: 0.00895078550092876\n",
      "Std loss: 0.004687353706994006\n",
      "Total Loss: 0.05370471300557256\n",
      "------------------------------------ epoch 8879 (53268 steps) ------------------------------------\n",
      "Max loss: 0.014782309532165527\n",
      "Min loss: 0.003389850025996566\n",
      "Mean loss: 0.006858636431085567\n",
      "Std loss: 0.003678411762163705\n",
      "Total Loss: 0.0411518185865134\n",
      "------------------------------------ epoch 8880 (53274 steps) ------------------------------------\n",
      "Max loss: 0.016953211277723312\n",
      "Min loss: 0.0033566344063729048\n",
      "Mean loss: 0.006823982538965841\n",
      "Std loss: 0.004618768145956368\n",
      "Total Loss: 0.04094389523379505\n",
      "------------------------------------ epoch 8881 (53280 steps) ------------------------------------\n",
      "Max loss: 0.011379487812519073\n",
      "Min loss: 0.0040255216881632805\n",
      "Mean loss: 0.007329987517247598\n",
      "Std loss: 0.0023004218559140224\n",
      "Total Loss: 0.043979925103485584\n",
      "------------------------------------ epoch 8882 (53286 steps) ------------------------------------\n",
      "Max loss: 0.019005468115210533\n",
      "Min loss: 0.004887057468295097\n",
      "Mean loss: 0.008173722152908644\n",
      "Std loss: 0.004967080044389302\n",
      "Total Loss: 0.04904233291745186\n",
      "------------------------------------ epoch 8883 (53292 steps) ------------------------------------\n",
      "Max loss: 0.010356071405112743\n",
      "Min loss: 0.0034647095017135143\n",
      "Mean loss: 0.00772678564923505\n",
      "Std loss: 0.0024235262172237807\n",
      "Total Loss: 0.0463607138954103\n",
      "------------------------------------ epoch 8884 (53298 steps) ------------------------------------\n",
      "Max loss: 0.019455285742878914\n",
      "Min loss: 0.006704803556203842\n",
      "Mean loss: 0.010386239504441619\n",
      "Std loss: 0.004517390321488634\n",
      "Total Loss: 0.062317437026649714\n",
      "------------------------------------ epoch 8885 (53304 steps) ------------------------------------\n",
      "Max loss: 0.01563955284655094\n",
      "Min loss: 0.004093332216143608\n",
      "Mean loss: 0.009851325768977404\n",
      "Std loss: 0.004145517634444573\n",
      "Total Loss: 0.05910795461386442\n",
      "------------------------------------ epoch 8886 (53310 steps) ------------------------------------\n",
      "Max loss: 0.018151601776480675\n",
      "Min loss: 0.00631207600235939\n",
      "Mean loss: 0.011917056438202659\n",
      "Std loss: 0.0047086140741930535\n",
      "Total Loss: 0.07150233862921596\n",
      "------------------------------------ epoch 8887 (53316 steps) ------------------------------------\n",
      "Max loss: 0.026485193520784378\n",
      "Min loss: 0.006147424690425396\n",
      "Mean loss: 0.012647039256989956\n",
      "Std loss: 0.006797482261816214\n",
      "Total Loss: 0.07588223554193974\n",
      "------------------------------------ epoch 8888 (53322 steps) ------------------------------------\n",
      "Max loss: 0.010543104261159897\n",
      "Min loss: 0.005961202085018158\n",
      "Mean loss: 0.007830890205999216\n",
      "Std loss: 0.0018241442312602468\n",
      "Total Loss: 0.04698534123599529\n",
      "------------------------------------ epoch 8889 (53328 steps) ------------------------------------\n",
      "Max loss: 0.007116155698895454\n",
      "Min loss: 0.004515712149441242\n",
      "Mean loss: 0.005379720591008663\n",
      "Std loss: 0.000907064775222749\n",
      "Total Loss: 0.03227832354605198\n",
      "------------------------------------ epoch 8890 (53334 steps) ------------------------------------\n",
      "Max loss: 0.017729127779603004\n",
      "Min loss: 0.003525195177644491\n",
      "Mean loss: 0.009335176165526112\n",
      "Std loss: 0.00573951127423399\n",
      "Total Loss: 0.05601105699315667\n",
      "------------------------------------ epoch 8891 (53340 steps) ------------------------------------\n",
      "Max loss: 0.017980435863137245\n",
      "Min loss: 0.0040283529087901115\n",
      "Mean loss: 0.007928225522240004\n",
      "Std loss: 0.005239232046989497\n",
      "Total Loss: 0.04756935313344002\n",
      "------------------------------------ epoch 8892 (53346 steps) ------------------------------------\n",
      "Max loss: 0.02569517120718956\n",
      "Min loss: 0.003274231916293502\n",
      "Mean loss: 0.009035478384854892\n",
      "Std loss: 0.007552849579890266\n",
      "Total Loss: 0.05421287030912936\n",
      "------------------------------------ epoch 8893 (53352 steps) ------------------------------------\n",
      "Max loss: 0.008021952584385872\n",
      "Min loss: 0.00360189750790596\n",
      "Mean loss: 0.005319602244223158\n",
      "Std loss: 0.0016033683388540056\n",
      "Total Loss: 0.031917613465338945\n",
      "------------------------------------ epoch 8894 (53358 steps) ------------------------------------\n",
      "Max loss: 0.013073602691292763\n",
      "Min loss: 0.0032055596821010113\n",
      "Mean loss: 0.006213609944097698\n",
      "Std loss: 0.0036084093567155524\n",
      "Total Loss: 0.037281659664586186\n",
      "------------------------------------ epoch 8895 (53364 steps) ------------------------------------\n",
      "Max loss: 0.023512985557317734\n",
      "Min loss: 0.0031833848915994167\n",
      "Mean loss: 0.010066932959792515\n",
      "Std loss: 0.007809088613217756\n",
      "Total Loss: 0.06040159775875509\n",
      "------------------------------------ epoch 8896 (53370 steps) ------------------------------------\n",
      "Max loss: 0.018014393746852875\n",
      "Min loss: 0.0031880007591098547\n",
      "Mean loss: 0.007981718556645015\n",
      "Std loss: 0.005340402519956745\n",
      "Total Loss: 0.047890311339870095\n",
      "------------------------------------ epoch 8897 (53376 steps) ------------------------------------\n",
      "Max loss: 0.01663384586572647\n",
      "Min loss: 0.004629525821655989\n",
      "Mean loss: 0.00995013389425973\n",
      "Std loss: 0.0036336749432658018\n",
      "Total Loss: 0.059700803365558386\n",
      "------------------------------------ epoch 8898 (53382 steps) ------------------------------------\n",
      "Max loss: 0.020298728719353676\n",
      "Min loss: 0.006343531422317028\n",
      "Mean loss: 0.011330592523639401\n",
      "Std loss: 0.005333031495004678\n",
      "Total Loss: 0.0679835551418364\n",
      "------------------------------------ epoch 8899 (53388 steps) ------------------------------------\n",
      "Max loss: 0.023520328104496002\n",
      "Min loss: 0.0062566036358475685\n",
      "Mean loss: 0.010542862738172213\n",
      "Std loss: 0.006156847885660476\n",
      "Total Loss: 0.06325717642903328\n",
      "------------------------------------ epoch 8900 (53394 steps) ------------------------------------\n",
      "Max loss: 0.01722390577197075\n",
      "Min loss: 0.008312190882861614\n",
      "Mean loss: 0.01242667343467474\n",
      "Std loss: 0.0034712489082673317\n",
      "Total Loss: 0.07456004060804844\n",
      "------------------------------------ epoch 8901 (53400 steps) ------------------------------------\n",
      "Max loss: 0.04074950888752937\n",
      "Min loss: 0.005786735564470291\n",
      "Mean loss: 0.013414870171497265\n",
      "Std loss: 0.012350433584419595\n",
      "Total Loss: 0.08048922102898359\n",
      "saved model at ./weights/model_8901.pth\n",
      "------------------------------------ epoch 8902 (53406 steps) ------------------------------------\n",
      "Max loss: 0.03868555277585983\n",
      "Min loss: 0.005724500399082899\n",
      "Mean loss: 0.01562658332598706\n",
      "Std loss: 0.01125676993780897\n",
      "Total Loss: 0.09375949995592237\n",
      "------------------------------------ epoch 8903 (53412 steps) ------------------------------------\n",
      "Max loss: 0.025579160079360008\n",
      "Min loss: 0.005300236400216818\n",
      "Mean loss: 0.014254728564992547\n",
      "Std loss: 0.006748618208593721\n",
      "Total Loss: 0.08552837138995528\n",
      "------------------------------------ epoch 8904 (53418 steps) ------------------------------------\n",
      "Max loss: 0.026437833905220032\n",
      "Min loss: 0.005594324320554733\n",
      "Mean loss: 0.017380593189348776\n",
      "Std loss: 0.007176930975646892\n",
      "Total Loss: 0.10428355913609266\n",
      "------------------------------------ epoch 8905 (53424 steps) ------------------------------------\n",
      "Max loss: 0.014121850952506065\n",
      "Min loss: 0.0074733225628733635\n",
      "Mean loss: 0.009290343305716911\n",
      "Std loss: 0.0022047775729038786\n",
      "Total Loss: 0.05574205983430147\n",
      "------------------------------------ epoch 8906 (53430 steps) ------------------------------------\n",
      "Max loss: 0.012134965509176254\n",
      "Min loss: 0.005663755349814892\n",
      "Mean loss: 0.007662738285337885\n",
      "Std loss: 0.0023216728324259857\n",
      "Total Loss: 0.04597642971202731\n",
      "------------------------------------ epoch 8907 (53436 steps) ------------------------------------\n",
      "Max loss: 0.01773024909198284\n",
      "Min loss: 0.005343667697161436\n",
      "Mean loss: 0.008705791784450412\n",
      "Std loss: 0.004262642190275618\n",
      "Total Loss: 0.05223475070670247\n",
      "------------------------------------ epoch 8908 (53442 steps) ------------------------------------\n",
      "Max loss: 0.022395117208361626\n",
      "Min loss: 0.004871325101703405\n",
      "Mean loss: 0.011601568277304372\n",
      "Std loss: 0.0061536444331029075\n",
      "Total Loss: 0.06960940966382623\n",
      "------------------------------------ epoch 8909 (53448 steps) ------------------------------------\n",
      "Max loss: 0.017119161784648895\n",
      "Min loss: 0.004933767020702362\n",
      "Mean loss: 0.00999038169781367\n",
      "Std loss: 0.004262356488084613\n",
      "Total Loss: 0.05994229018688202\n",
      "------------------------------------ epoch 8910 (53454 steps) ------------------------------------\n",
      "Max loss: 0.011839417740702629\n",
      "Min loss: 0.003825242631137371\n",
      "Mean loss: 0.006790717054779331\n",
      "Std loss: 0.0027487618519711258\n",
      "Total Loss: 0.040744302328675985\n",
      "------------------------------------ epoch 8911 (53460 steps) ------------------------------------\n",
      "Max loss: 0.01775367744266987\n",
      "Min loss: 0.004485991783440113\n",
      "Mean loss: 0.007971676144128045\n",
      "Std loss: 0.0045653818182854605\n",
      "Total Loss: 0.04783005686476827\n",
      "------------------------------------ epoch 8912 (53466 steps) ------------------------------------\n",
      "Max loss: 0.0081319659948349\n",
      "Min loss: 0.004424099810421467\n",
      "Mean loss: 0.005997927238543828\n",
      "Std loss: 0.0011003776850923418\n",
      "Total Loss: 0.03598756343126297\n",
      "------------------------------------ epoch 8913 (53472 steps) ------------------------------------\n",
      "Max loss: 0.013931188732385635\n",
      "Min loss: 0.004229120910167694\n",
      "Mean loss: 0.009488028551762303\n",
      "Std loss: 0.003287352787230387\n",
      "Total Loss: 0.056928171310573816\n",
      "------------------------------------ epoch 8914 (53478 steps) ------------------------------------\n",
      "Max loss: 0.017685241997241974\n",
      "Min loss: 0.0036386577412486076\n",
      "Mean loss: 0.008849161677062511\n",
      "Std loss: 0.005342332310953177\n",
      "Total Loss: 0.05309497006237507\n",
      "------------------------------------ epoch 8915 (53484 steps) ------------------------------------\n",
      "Max loss: 0.006894064601510763\n",
      "Min loss: 0.004259604960680008\n",
      "Mean loss: 0.005466223771994312\n",
      "Std loss: 0.0008695332181285132\n",
      "Total Loss: 0.032797342631965876\n",
      "------------------------------------ epoch 8916 (53490 steps) ------------------------------------\n",
      "Max loss: 0.0127828074619174\n",
      "Min loss: 0.0048962002620100975\n",
      "Mean loss: 0.007056106425200899\n",
      "Std loss: 0.002616912093917646\n",
      "Total Loss: 0.0423366385512054\n",
      "------------------------------------ epoch 8917 (53496 steps) ------------------------------------\n",
      "Max loss: 0.013160321861505508\n",
      "Min loss: 0.004091124981641769\n",
      "Mean loss: 0.007993756327778101\n",
      "Std loss: 0.0031393422958046024\n",
      "Total Loss: 0.047962537966668606\n",
      "------------------------------------ epoch 8918 (53502 steps) ------------------------------------\n",
      "Max loss: 0.012338931672275066\n",
      "Min loss: 0.003104203147813678\n",
      "Mean loss: 0.006304593873210251\n",
      "Std loss: 0.0030640476650849874\n",
      "Total Loss: 0.03782756323926151\n",
      "------------------------------------ epoch 8919 (53508 steps) ------------------------------------\n",
      "Max loss: 0.019339971244335175\n",
      "Min loss: 0.0036531584337353706\n",
      "Mean loss: 0.008013876581874987\n",
      "Std loss: 0.005445796567891799\n",
      "Total Loss: 0.04808325949124992\n",
      "------------------------------------ epoch 8920 (53514 steps) ------------------------------------\n",
      "Max loss: 0.01676628552377224\n",
      "Min loss: 0.003474633675068617\n",
      "Mean loss: 0.007404021297891934\n",
      "Std loss: 0.004355490302562924\n",
      "Total Loss: 0.04442412778735161\n",
      "------------------------------------ epoch 8921 (53520 steps) ------------------------------------\n",
      "Max loss: 0.020377948880195618\n",
      "Min loss: 0.0033176951110363007\n",
      "Mean loss: 0.009497077243092159\n",
      "Std loss: 0.007550449509439937\n",
      "Total Loss: 0.05698246345855296\n",
      "------------------------------------ epoch 8922 (53526 steps) ------------------------------------\n",
      "Max loss: 0.00853060744702816\n",
      "Min loss: 0.004858081694692373\n",
      "Mean loss: 0.006602174835279584\n",
      "Std loss: 0.0013545674691463302\n",
      "Total Loss: 0.039613049011677504\n",
      "------------------------------------ epoch 8923 (53532 steps) ------------------------------------\n",
      "Max loss: 0.01579822413623333\n",
      "Min loss: 0.003678552806377411\n",
      "Mean loss: 0.008649369313692054\n",
      "Std loss: 0.00398219503985608\n",
      "Total Loss: 0.05189621588215232\n",
      "------------------------------------ epoch 8924 (53538 steps) ------------------------------------\n",
      "Max loss: 0.01215712632983923\n",
      "Min loss: 0.00391180906444788\n",
      "Mean loss: 0.006558160297572613\n",
      "Std loss: 0.0027931180299640146\n",
      "Total Loss: 0.03934896178543568\n",
      "------------------------------------ epoch 8925 (53544 steps) ------------------------------------\n",
      "Max loss: 0.04867713898420334\n",
      "Min loss: 0.0044500320218503475\n",
      "Mean loss: 0.01511483135012289\n",
      "Std loss: 0.015327071009473678\n",
      "Total Loss: 0.09068898810073733\n",
      "------------------------------------ epoch 8926 (53550 steps) ------------------------------------\n",
      "Max loss: 0.013850612565875053\n",
      "Min loss: 0.005029859021306038\n",
      "Mean loss: 0.0074660503305494785\n",
      "Std loss: 0.002953353179451079\n",
      "Total Loss: 0.04479630198329687\n",
      "------------------------------------ epoch 8927 (53556 steps) ------------------------------------\n",
      "Max loss: 0.04296942055225372\n",
      "Min loss: 0.006968820467591286\n",
      "Mean loss: 0.01988622002924482\n",
      "Std loss: 0.012835677977449273\n",
      "Total Loss: 0.11931732017546892\n",
      "------------------------------------ epoch 8928 (53562 steps) ------------------------------------\n",
      "Max loss: 0.04205472394824028\n",
      "Min loss: 0.004339065868407488\n",
      "Mean loss: 0.016719723663603265\n",
      "Std loss: 0.015042796863824637\n",
      "Total Loss: 0.1003183419816196\n",
      "------------------------------------ epoch 8929 (53568 steps) ------------------------------------\n",
      "Max loss: 0.039062000811100006\n",
      "Min loss: 0.00795676838606596\n",
      "Mean loss: 0.01549604907631874\n",
      "Std loss: 0.011077375498662424\n",
      "Total Loss: 0.09297629445791245\n",
      "------------------------------------ epoch 8930 (53574 steps) ------------------------------------\n",
      "Max loss: 0.020726878196001053\n",
      "Min loss: 0.004900999367237091\n",
      "Mean loss: 0.009294035534063974\n",
      "Std loss: 0.0055628352240353375\n",
      "Total Loss: 0.05576421320438385\n",
      "------------------------------------ epoch 8931 (53580 steps) ------------------------------------\n",
      "Max loss: 0.013386617414653301\n",
      "Min loss: 0.004795498680323362\n",
      "Mean loss: 0.007578487042337656\n",
      "Std loss: 0.003055001154804232\n",
      "Total Loss: 0.045470922254025936\n",
      "------------------------------------ epoch 8932 (53586 steps) ------------------------------------\n",
      "Max loss: 0.013308383524417877\n",
      "Min loss: 0.0040937503799796104\n",
      "Mean loss: 0.009315669458980361\n",
      "Std loss: 0.0033724220346207615\n",
      "Total Loss: 0.05589401675388217\n",
      "------------------------------------ epoch 8933 (53592 steps) ------------------------------------\n",
      "Max loss: 0.008749796077609062\n",
      "Min loss: 0.004040410742163658\n",
      "Mean loss: 0.006479302033161123\n",
      "Std loss: 0.001634959522092705\n",
      "Total Loss: 0.03887581219896674\n",
      "------------------------------------ epoch 8934 (53598 steps) ------------------------------------\n",
      "Max loss: 0.050558626651763916\n",
      "Min loss: 0.004588698968291283\n",
      "Mean loss: 0.014347306918352842\n",
      "Std loss: 0.016328316418435157\n",
      "Total Loss: 0.08608384151011705\n",
      "------------------------------------ epoch 8935 (53604 steps) ------------------------------------\n",
      "Max loss: 0.017402658239006996\n",
      "Min loss: 0.0037641150411218405\n",
      "Mean loss: 0.009613010838317374\n",
      "Std loss: 0.005838673610597322\n",
      "Total Loss: 0.057678065029904246\n",
      "------------------------------------ epoch 8936 (53610 steps) ------------------------------------\n",
      "Max loss: 0.015476140193641186\n",
      "Min loss: 0.004052964970469475\n",
      "Mean loss: 0.007478721983109911\n",
      "Std loss: 0.004103928683909566\n",
      "Total Loss: 0.04487233189865947\n",
      "------------------------------------ epoch 8937 (53616 steps) ------------------------------------\n",
      "Max loss: 0.011558443307876587\n",
      "Min loss: 0.004197897855192423\n",
      "Mean loss: 0.006642450345680118\n",
      "Std loss: 0.0023733415915848832\n",
      "Total Loss: 0.039854702074080706\n",
      "------------------------------------ epoch 8938 (53622 steps) ------------------------------------\n",
      "Max loss: 0.037217963486909866\n",
      "Min loss: 0.004464902449399233\n",
      "Mean loss: 0.012838883015016714\n",
      "Std loss: 0.011415286034745167\n",
      "Total Loss: 0.07703329809010029\n",
      "------------------------------------ epoch 8939 (53628 steps) ------------------------------------\n",
      "Max loss: 0.018818020820617676\n",
      "Min loss: 0.0034975565504282713\n",
      "Mean loss: 0.011396432722297808\n",
      "Std loss: 0.004769825241691054\n",
      "Total Loss: 0.06837859633378685\n",
      "------------------------------------ epoch 8940 (53634 steps) ------------------------------------\n",
      "Max loss: 0.01995762437582016\n",
      "Min loss: 0.0042556282132864\n",
      "Mean loss: 0.00854964468938609\n",
      "Std loss: 0.005361237121179436\n",
      "Total Loss: 0.05129786813631654\n",
      "------------------------------------ epoch 8941 (53640 steps) ------------------------------------\n",
      "Max loss: 0.014368566684424877\n",
      "Min loss: 0.004232702776789665\n",
      "Mean loss: 0.007671033730730414\n",
      "Std loss: 0.003565972359360355\n",
      "Total Loss: 0.046026202384382486\n",
      "------------------------------------ epoch 8942 (53646 steps) ------------------------------------\n",
      "Max loss: 0.01940326765179634\n",
      "Min loss: 0.004236340522766113\n",
      "Mean loss: 0.008565155789256096\n",
      "Std loss: 0.0051241005765583275\n",
      "Total Loss: 0.051390934735536575\n",
      "------------------------------------ epoch 8943 (53652 steps) ------------------------------------\n",
      "Max loss: 0.014980973675847054\n",
      "Min loss: 0.004576723091304302\n",
      "Mean loss: 0.008220225029314557\n",
      "Std loss: 0.00335561504086436\n",
      "Total Loss: 0.049321350175887346\n",
      "------------------------------------ epoch 8944 (53658 steps) ------------------------------------\n",
      "Max loss: 0.01085576694458723\n",
      "Min loss: 0.004183315671980381\n",
      "Mean loss: 0.007507489761337638\n",
      "Std loss: 0.002739246290039108\n",
      "Total Loss: 0.04504493856802583\n",
      "------------------------------------ epoch 8945 (53664 steps) ------------------------------------\n",
      "Max loss: 0.01718560792505741\n",
      "Min loss: 0.003527290653437376\n",
      "Mean loss: 0.008434767602011561\n",
      "Std loss: 0.004411314476516473\n",
      "Total Loss: 0.05060860561206937\n",
      "------------------------------------ epoch 8946 (53670 steps) ------------------------------------\n",
      "Max loss: 0.010421408340334892\n",
      "Min loss: 0.0039415787905454636\n",
      "Mean loss: 0.0063268868252635\n",
      "Std loss: 0.0020693290008973533\n",
      "Total Loss: 0.037961320951581\n",
      "------------------------------------ epoch 8947 (53676 steps) ------------------------------------\n",
      "Max loss: 0.01719430461525917\n",
      "Min loss: 0.005082900635898113\n",
      "Mean loss: 0.009143878938630223\n",
      "Std loss: 0.003864281826404557\n",
      "Total Loss: 0.05486327363178134\n",
      "------------------------------------ epoch 8948 (53682 steps) ------------------------------------\n",
      "Max loss: 0.01804967224597931\n",
      "Min loss: 0.003132845275104046\n",
      "Mean loss: 0.007930899931428334\n",
      "Std loss: 0.005290975379106269\n",
      "Total Loss: 0.04758539958857\n",
      "------------------------------------ epoch 8949 (53688 steps) ------------------------------------\n",
      "Max loss: 0.017286226153373718\n",
      "Min loss: 0.0035201578866690397\n",
      "Mean loss: 0.007198135795382162\n",
      "Std loss: 0.0047875769218291625\n",
      "Total Loss: 0.04318881477229297\n",
      "------------------------------------ epoch 8950 (53694 steps) ------------------------------------\n",
      "Max loss: 0.013859065249562263\n",
      "Min loss: 0.0055491384118795395\n",
      "Mean loss: 0.00828017278884848\n",
      "Std loss: 0.002667055333167217\n",
      "Total Loss: 0.04968103673309088\n",
      "------------------------------------ epoch 8951 (53700 steps) ------------------------------------\n",
      "Max loss: 0.009291348978877068\n",
      "Min loss: 0.004692022688686848\n",
      "Mean loss: 0.006930266351749499\n",
      "Std loss: 0.0017278872909216188\n",
      "Total Loss: 0.041581598110497\n",
      "------------------------------------ epoch 8952 (53706 steps) ------------------------------------\n",
      "Max loss: 0.007800394203513861\n",
      "Min loss: 0.0033940866123884916\n",
      "Mean loss: 0.0046424801694229245\n",
      "Std loss: 0.0014627123857635356\n",
      "Total Loss: 0.027854881016537547\n",
      "------------------------------------ epoch 8953 (53712 steps) ------------------------------------\n",
      "Max loss: 0.011776800267398357\n",
      "Min loss: 0.0037607294507324696\n",
      "Mean loss: 0.006606876850128174\n",
      "Std loss: 0.0025691614604382933\n",
      "Total Loss: 0.03964126110076904\n",
      "------------------------------------ epoch 8954 (53718 steps) ------------------------------------\n",
      "Max loss: 0.009899224154651165\n",
      "Min loss: 0.003814879572018981\n",
      "Mean loss: 0.006674466227802138\n",
      "Std loss: 0.002218943291451057\n",
      "Total Loss: 0.040046797366812825\n",
      "------------------------------------ epoch 8955 (53724 steps) ------------------------------------\n",
      "Max loss: 0.015282798558473587\n",
      "Min loss: 0.00500622671097517\n",
      "Mean loss: 0.008215480328847965\n",
      "Std loss: 0.0035676324342004528\n",
      "Total Loss: 0.04929288197308779\n",
      "------------------------------------ epoch 8956 (53730 steps) ------------------------------------\n",
      "Max loss: 0.005676704924553633\n",
      "Min loss: 0.004372688475996256\n",
      "Mean loss: 0.005038346629589796\n",
      "Std loss: 0.0005518761830671207\n",
      "Total Loss: 0.030230079777538776\n",
      "------------------------------------ epoch 8957 (53736 steps) ------------------------------------\n",
      "Max loss: 0.0326123908162117\n",
      "Min loss: 0.003453644923865795\n",
      "Mean loss: 0.009553868866836032\n",
      "Std loss: 0.010392080006673672\n",
      "Total Loss: 0.05732321320101619\n",
      "------------------------------------ epoch 8958 (53742 steps) ------------------------------------\n",
      "Max loss: 0.02202247455716133\n",
      "Min loss: 0.004560660570859909\n",
      "Mean loss: 0.00962355857094129\n",
      "Std loss: 0.005727437258846528\n",
      "Total Loss: 0.057741351425647736\n",
      "------------------------------------ epoch 8959 (53748 steps) ------------------------------------\n",
      "Max loss: 0.009943678043782711\n",
      "Min loss: 0.003976061008870602\n",
      "Mean loss: 0.005531100633864601\n",
      "Std loss: 0.002041494404812596\n",
      "Total Loss: 0.03318660380318761\n",
      "------------------------------------ epoch 8960 (53754 steps) ------------------------------------\n",
      "Max loss: 0.007713375147432089\n",
      "Min loss: 0.004099516663700342\n",
      "Mean loss: 0.0054153028565148515\n",
      "Std loss: 0.001477291638335684\n",
      "Total Loss: 0.03249181713908911\n",
      "------------------------------------ epoch 8961 (53760 steps) ------------------------------------\n",
      "Max loss: 0.031862225383520126\n",
      "Min loss: 0.0035391103010624647\n",
      "Mean loss: 0.01029238225116084\n",
      "Std loss: 0.009733222208444251\n",
      "Total Loss: 0.06175429350696504\n",
      "------------------------------------ epoch 8962 (53766 steps) ------------------------------------\n",
      "Max loss: 0.01765074022114277\n",
      "Min loss: 0.005272578448057175\n",
      "Mean loss: 0.009629143712421259\n",
      "Std loss: 0.003916760396300649\n",
      "Total Loss: 0.05777486227452755\n",
      "------------------------------------ epoch 8963 (53772 steps) ------------------------------------\n",
      "Max loss: 0.021843180060386658\n",
      "Min loss: 0.0038444017991423607\n",
      "Mean loss: 0.009030701825395226\n",
      "Std loss: 0.0062026371415441406\n",
      "Total Loss: 0.05418421095237136\n",
      "------------------------------------ epoch 8964 (53778 steps) ------------------------------------\n",
      "Max loss: 0.021016735583543777\n",
      "Min loss: 0.004597583785653114\n",
      "Mean loss: 0.008479228165621558\n",
      "Std loss: 0.005667073805305652\n",
      "Total Loss: 0.05087536899372935\n",
      "------------------------------------ epoch 8965 (53784 steps) ------------------------------------\n",
      "Max loss: 0.01671043038368225\n",
      "Min loss: 0.003844931721687317\n",
      "Mean loss: 0.007845762651413679\n",
      "Std loss: 0.004217736808186428\n",
      "Total Loss: 0.047074575908482075\n",
      "------------------------------------ epoch 8966 (53790 steps) ------------------------------------\n",
      "Max loss: 0.01449731644243002\n",
      "Min loss: 0.0033559135627001524\n",
      "Mean loss: 0.007793970871716738\n",
      "Std loss: 0.004764255528401012\n",
      "Total Loss: 0.046763825230300426\n",
      "------------------------------------ epoch 8967 (53796 steps) ------------------------------------\n",
      "Max loss: 0.04043792188167572\n",
      "Min loss: 0.004453001543879509\n",
      "Mean loss: 0.014170160361876091\n",
      "Std loss: 0.012824430267666317\n",
      "Total Loss: 0.08502096217125654\n",
      "------------------------------------ epoch 8968 (53802 steps) ------------------------------------\n",
      "Max loss: 0.012640334665775299\n",
      "Min loss: 0.004158385097980499\n",
      "Mean loss: 0.0074175492239495116\n",
      "Std loss: 0.003024898900228641\n",
      "Total Loss: 0.04450529534369707\n",
      "------------------------------------ epoch 8969 (53808 steps) ------------------------------------\n",
      "Max loss: 0.02391130104660988\n",
      "Min loss: 0.004297981038689613\n",
      "Mean loss: 0.01145945768803358\n",
      "Std loss: 0.008189426595236644\n",
      "Total Loss: 0.06875674612820148\n",
      "------------------------------------ epoch 8970 (53814 steps) ------------------------------------\n",
      "Max loss: 0.0108173293992877\n",
      "Min loss: 0.004443102050572634\n",
      "Mean loss: 0.007937347438807288\n",
      "Std loss: 0.0020877303768701066\n",
      "Total Loss: 0.04762408463284373\n",
      "------------------------------------ epoch 8971 (53820 steps) ------------------------------------\n",
      "Max loss: 0.015923891216516495\n",
      "Min loss: 0.005505682434886694\n",
      "Mean loss: 0.007818510057404637\n",
      "Std loss: 0.003648123049990983\n",
      "Total Loss: 0.046911060344427824\n",
      "------------------------------------ epoch 8972 (53826 steps) ------------------------------------\n",
      "Max loss: 0.005724775604903698\n",
      "Min loss: 0.003640140872448683\n",
      "Mean loss: 0.004703365654374163\n",
      "Std loss: 0.0007345025402412814\n",
      "Total Loss: 0.028220193926244974\n",
      "------------------------------------ epoch 8973 (53832 steps) ------------------------------------\n",
      "Max loss: 0.02465692162513733\n",
      "Min loss: 0.003771847113966942\n",
      "Mean loss: 0.010246179377039274\n",
      "Std loss: 0.007354003258884529\n",
      "Total Loss: 0.06147707626223564\n",
      "------------------------------------ epoch 8974 (53838 steps) ------------------------------------\n",
      "Max loss: 0.00888231210410595\n",
      "Min loss: 0.004406350664794445\n",
      "Mean loss: 0.005795075946177046\n",
      "Std loss: 0.0014506610670166125\n",
      "Total Loss: 0.03477045567706227\n",
      "------------------------------------ epoch 8975 (53844 steps) ------------------------------------\n",
      "Max loss: 0.01691584847867489\n",
      "Min loss: 0.004397115670144558\n",
      "Mean loss: 0.007615129851425688\n",
      "Std loss: 0.004247229641721306\n",
      "Total Loss: 0.045690779108554125\n",
      "------------------------------------ epoch 8976 (53850 steps) ------------------------------------\n",
      "Max loss: 0.01234606932848692\n",
      "Min loss: 0.0039567588828504086\n",
      "Mean loss: 0.008035572788988551\n",
      "Std loss: 0.002936831672681275\n",
      "Total Loss: 0.0482134367339313\n",
      "------------------------------------ epoch 8977 (53856 steps) ------------------------------------\n",
      "Max loss: 0.019046396017074585\n",
      "Min loss: 0.0035083189141005278\n",
      "Mean loss: 0.009852634626440704\n",
      "Std loss: 0.006478582972281274\n",
      "Total Loss: 0.05911580775864422\n",
      "------------------------------------ epoch 8978 (53862 steps) ------------------------------------\n",
      "Max loss: 0.01309911161661148\n",
      "Min loss: 0.0032649855129420757\n",
      "Mean loss: 0.008413223006452123\n",
      "Std loss: 0.0032491431457912965\n",
      "Total Loss: 0.05047933803871274\n",
      "------------------------------------ epoch 8979 (53868 steps) ------------------------------------\n",
      "Max loss: 0.020438795909285545\n",
      "Min loss: 0.0032942197285592556\n",
      "Mean loss: 0.008475102872277299\n",
      "Std loss: 0.005764119253821563\n",
      "Total Loss: 0.0508506172336638\n",
      "------------------------------------ epoch 8980 (53874 steps) ------------------------------------\n",
      "Max loss: 0.008721251040697098\n",
      "Min loss: 0.003626524470746517\n",
      "Mean loss: 0.00619667690868179\n",
      "Std loss: 0.0019447725027368131\n",
      "Total Loss: 0.03718006145209074\n",
      "------------------------------------ epoch 8981 (53880 steps) ------------------------------------\n",
      "Max loss: 0.01025220938026905\n",
      "Min loss: 0.0032761446200311184\n",
      "Mean loss: 0.00632140312033395\n",
      "Std loss: 0.002083828953270686\n",
      "Total Loss: 0.0379284187220037\n",
      "------------------------------------ epoch 8982 (53886 steps) ------------------------------------\n",
      "Max loss: 0.009314377792179585\n",
      "Min loss: 0.0032715462148189545\n",
      "Mean loss: 0.006017505812148253\n",
      "Std loss: 0.0019290277193952084\n",
      "Total Loss: 0.03610503487288952\n",
      "------------------------------------ epoch 8983 (53892 steps) ------------------------------------\n",
      "Max loss: 0.01845516823232174\n",
      "Min loss: 0.003091211896389723\n",
      "Mean loss: 0.007309340250988801\n",
      "Std loss: 0.005307673397977236\n",
      "Total Loss: 0.04385604150593281\n",
      "------------------------------------ epoch 8984 (53898 steps) ------------------------------------\n",
      "Max loss: 0.022734355181455612\n",
      "Min loss: 0.004908777307718992\n",
      "Mean loss: 0.012561473607396087\n",
      "Std loss: 0.006587464986024931\n",
      "Total Loss: 0.07536884164437652\n",
      "------------------------------------ epoch 8985 (53904 steps) ------------------------------------\n",
      "Max loss: 0.011853902600705624\n",
      "Min loss: 0.004156751558184624\n",
      "Mean loss: 0.006918019615113735\n",
      "Std loss: 0.002673061628938858\n",
      "Total Loss: 0.04150811769068241\n",
      "------------------------------------ epoch 8986 (53910 steps) ------------------------------------\n",
      "Max loss: 0.01278171967715025\n",
      "Min loss: 0.0046530975960195065\n",
      "Mean loss: 0.008519906162594756\n",
      "Std loss: 0.003185797686572308\n",
      "Total Loss: 0.05111943697556853\n",
      "------------------------------------ epoch 8987 (53916 steps) ------------------------------------\n",
      "Max loss: 0.012816313654184341\n",
      "Min loss: 0.004012930206954479\n",
      "Mean loss: 0.006887060046816866\n",
      "Std loss: 0.0031381255480522163\n",
      "Total Loss: 0.041322360280901194\n",
      "------------------------------------ epoch 8988 (53922 steps) ------------------------------------\n",
      "Max loss: 0.007313682697713375\n",
      "Min loss: 0.004484206438064575\n",
      "Mean loss: 0.0053648867178708315\n",
      "Std loss: 0.0009416985882538844\n",
      "Total Loss: 0.03218932030722499\n",
      "------------------------------------ epoch 8989 (53928 steps) ------------------------------------\n",
      "Max loss: 0.02847670577466488\n",
      "Min loss: 0.0036698663607239723\n",
      "Mean loss: 0.008521579167184731\n",
      "Std loss: 0.008952032528087859\n",
      "Total Loss: 0.05112947500310838\n",
      "------------------------------------ epoch 8990 (53934 steps) ------------------------------------\n",
      "Max loss: 0.016325894743204117\n",
      "Min loss: 0.004159692209213972\n",
      "Mean loss: 0.009124932733053962\n",
      "Std loss: 0.003949203204241658\n",
      "Total Loss: 0.054749596398323774\n",
      "------------------------------------ epoch 8991 (53940 steps) ------------------------------------\n",
      "Max loss: 0.02204163931310177\n",
      "Min loss: 0.006287313066422939\n",
      "Mean loss: 0.012813429425780972\n",
      "Std loss: 0.006296734155821811\n",
      "Total Loss: 0.07688057655468583\n",
      "------------------------------------ epoch 8992 (53946 steps) ------------------------------------\n",
      "Max loss: 0.008543198928236961\n",
      "Min loss: 0.003379242494702339\n",
      "Mean loss: 0.0062247774718950195\n",
      "Std loss: 0.0019438073146956759\n",
      "Total Loss: 0.037348664831370115\n",
      "------------------------------------ epoch 8993 (53952 steps) ------------------------------------\n",
      "Max loss: 0.030180146917700768\n",
      "Min loss: 0.004736036062240601\n",
      "Mean loss: 0.009506588646521172\n",
      "Std loss: 0.00927631363066596\n",
      "Total Loss: 0.057039531879127026\n",
      "------------------------------------ epoch 8994 (53958 steps) ------------------------------------\n",
      "Max loss: 0.007082601543515921\n",
      "Min loss: 0.004561058711260557\n",
      "Mean loss: 0.006168452091515064\n",
      "Std loss: 0.0009055557947151697\n",
      "Total Loss: 0.037010712549090385\n",
      "------------------------------------ epoch 8995 (53964 steps) ------------------------------------\n",
      "Max loss: 0.01336010079830885\n",
      "Min loss: 0.0037081949412822723\n",
      "Mean loss: 0.007168031530454755\n",
      "Std loss: 0.0033373462720551056\n",
      "Total Loss: 0.04300818918272853\n",
      "------------------------------------ epoch 8996 (53970 steps) ------------------------------------\n",
      "Max loss: 0.0148810725659132\n",
      "Min loss: 0.004528624936938286\n",
      "Mean loss: 0.009920377594729265\n",
      "Std loss: 0.0036852514071156963\n",
      "Total Loss: 0.05952226556837559\n",
      "------------------------------------ epoch 8997 (53976 steps) ------------------------------------\n",
      "Max loss: 0.018870312720537186\n",
      "Min loss: 0.004285528790205717\n",
      "Mean loss: 0.00760608942558368\n",
      "Std loss: 0.005147726441506268\n",
      "Total Loss: 0.04563653655350208\n",
      "------------------------------------ epoch 8998 (53982 steps) ------------------------------------\n",
      "Max loss: 0.025504469871520996\n",
      "Min loss: 0.005376146640628576\n",
      "Mean loss: 0.009704142110422254\n",
      "Std loss: 0.007128117394241136\n",
      "Total Loss: 0.05822485266253352\n",
      "------------------------------------ epoch 8999 (53988 steps) ------------------------------------\n",
      "Max loss: 0.02433985471725464\n",
      "Min loss: 0.004715799354016781\n",
      "Mean loss: 0.00953445148964723\n",
      "Std loss: 0.0068691823275934686\n",
      "Total Loss: 0.05720670893788338\n",
      "------------------------------------ epoch 9000 (53994 steps) ------------------------------------\n",
      "Max loss: 0.01578361541032791\n",
      "Min loss: 0.00415333267301321\n",
      "Mean loss: 0.009348176574955383\n",
      "Std loss: 0.0037423460208018526\n",
      "Total Loss: 0.056089059449732304\n",
      "------------------------------------ epoch 9001 (54000 steps) ------------------------------------\n",
      "Max loss: 0.012549079954624176\n",
      "Min loss: 0.004374250769615173\n",
      "Mean loss: 0.00688303355127573\n",
      "Std loss: 0.002791053985345893\n",
      "Total Loss: 0.04129820130765438\n",
      "saved model at ./weights/model_9001.pth\n",
      "------------------------------------ epoch 9002 (54006 steps) ------------------------------------\n",
      "Max loss: 0.007992135360836983\n",
      "Min loss: 0.003165390808135271\n",
      "Mean loss: 0.005774510170643528\n",
      "Std loss: 0.0020578121994015925\n",
      "Total Loss: 0.03464706102386117\n",
      "------------------------------------ epoch 9003 (54012 steps) ------------------------------------\n",
      "Max loss: 0.007586206309497356\n",
      "Min loss: 0.003614722518250346\n",
      "Mean loss: 0.00556016080857565\n",
      "Std loss: 0.0015034871804629787\n",
      "Total Loss: 0.0333609648514539\n",
      "------------------------------------ epoch 9004 (54018 steps) ------------------------------------\n",
      "Max loss: 0.015644609928131104\n",
      "Min loss: 0.0031351069919764996\n",
      "Mean loss: 0.009846540090317527\n",
      "Std loss: 0.004488871942181959\n",
      "Total Loss: 0.059079240541905165\n",
      "------------------------------------ epoch 9005 (54024 steps) ------------------------------------\n",
      "Max loss: 0.029492054134607315\n",
      "Min loss: 0.004010757431387901\n",
      "Mean loss: 0.013680137771492204\n",
      "Std loss: 0.00946448767401221\n",
      "Total Loss: 0.08208082662895322\n",
      "------------------------------------ epoch 9006 (54030 steps) ------------------------------------\n",
      "Max loss: 0.016787704080343246\n",
      "Min loss: 0.0050338273867964745\n",
      "Mean loss: 0.009544649083788196\n",
      "Std loss: 0.003959339681590636\n",
      "Total Loss: 0.05726789450272918\n",
      "------------------------------------ epoch 9007 (54036 steps) ------------------------------------\n",
      "Max loss: 0.010551943443715572\n",
      "Min loss: 0.004106508102267981\n",
      "Mean loss: 0.007353402053316434\n",
      "Std loss: 0.0021473141819485318\n",
      "Total Loss: 0.044120412319898605\n",
      "------------------------------------ epoch 9008 (54042 steps) ------------------------------------\n",
      "Max loss: 0.014913029037415981\n",
      "Min loss: 0.004755897913128138\n",
      "Mean loss: 0.008805917964006463\n",
      "Std loss: 0.003972339793459372\n",
      "Total Loss: 0.05283550778403878\n",
      "------------------------------------ epoch 9009 (54048 steps) ------------------------------------\n",
      "Max loss: 0.013515975326299667\n",
      "Min loss: 0.005291115492582321\n",
      "Mean loss: 0.008202385312567154\n",
      "Std loss: 0.002689711768019725\n",
      "Total Loss: 0.04921431187540293\n",
      "------------------------------------ epoch 9010 (54054 steps) ------------------------------------\n",
      "Max loss: 0.04397463798522949\n",
      "Min loss: 0.00523742288351059\n",
      "Mean loss: 0.013752267230302095\n",
      "Std loss: 0.013984835581549823\n",
      "Total Loss: 0.08251360338181257\n",
      "------------------------------------ epoch 9011 (54060 steps) ------------------------------------\n",
      "Max loss: 0.017416227608919144\n",
      "Min loss: 0.004802435636520386\n",
      "Mean loss: 0.009781973902136087\n",
      "Std loss: 0.005165306498538638\n",
      "Total Loss: 0.058691843412816525\n",
      "------------------------------------ epoch 9012 (54066 steps) ------------------------------------\n",
      "Max loss: 0.0076892985962331295\n",
      "Min loss: 0.0053556933999061584\n",
      "Mean loss: 0.006500904603550832\n",
      "Std loss: 0.000870097024713106\n",
      "Total Loss: 0.03900542762130499\n",
      "------------------------------------ epoch 9013 (54072 steps) ------------------------------------\n",
      "Max loss: 0.02055070549249649\n",
      "Min loss: 0.004451925866305828\n",
      "Mean loss: 0.007664945597449939\n",
      "Std loss: 0.005821939620333282\n",
      "Total Loss: 0.04598967358469963\n",
      "------------------------------------ epoch 9014 (54078 steps) ------------------------------------\n",
      "Max loss: 0.009710986167192459\n",
      "Min loss: 0.0039240578189492226\n",
      "Mean loss: 0.007410757786904772\n",
      "Std loss: 0.001911809863046548\n",
      "Total Loss: 0.04446454672142863\n",
      "------------------------------------ epoch 9015 (54084 steps) ------------------------------------\n",
      "Max loss: 0.05627422034740448\n",
      "Min loss: 0.004142827354371548\n",
      "Mean loss: 0.018035241480295856\n",
      "Std loss: 0.017931215828866653\n",
      "Total Loss: 0.10821144888177514\n",
      "------------------------------------ epoch 9016 (54090 steps) ------------------------------------\n",
      "Max loss: 0.013963252305984497\n",
      "Min loss: 0.004828800447285175\n",
      "Mean loss: 0.009965521128227314\n",
      "Std loss: 0.0036739770561072184\n",
      "Total Loss: 0.05979312676936388\n",
      "------------------------------------ epoch 9017 (54096 steps) ------------------------------------\n",
      "Max loss: 0.008663647808134556\n",
      "Min loss: 0.004840020090341568\n",
      "Mean loss: 0.006292414230604966\n",
      "Std loss: 0.001283906927171857\n",
      "Total Loss: 0.0377544853836298\n",
      "------------------------------------ epoch 9018 (54102 steps) ------------------------------------\n",
      "Max loss: 0.02188902348279953\n",
      "Min loss: 0.004091047681868076\n",
      "Mean loss: 0.010406468606864413\n",
      "Std loss: 0.0072840938652463764\n",
      "Total Loss: 0.062438811641186476\n",
      "------------------------------------ epoch 9019 (54108 steps) ------------------------------------\n",
      "Max loss: 0.010884917341172695\n",
      "Min loss: 0.004564093425869942\n",
      "Mean loss: 0.007528270361945033\n",
      "Std loss: 0.002461922488310413\n",
      "Total Loss: 0.0451696221716702\n",
      "------------------------------------ epoch 9020 (54114 steps) ------------------------------------\n",
      "Max loss: 0.02230796031653881\n",
      "Min loss: 0.0039284564554691315\n",
      "Mean loss: 0.008445650649567446\n",
      "Std loss: 0.006481301303382321\n",
      "Total Loss: 0.05067390389740467\n",
      "------------------------------------ epoch 9021 (54120 steps) ------------------------------------\n",
      "Max loss: 0.020353084430098534\n",
      "Min loss: 0.003233970608562231\n",
      "Mean loss: 0.011022978617499271\n",
      "Std loss: 0.006087965696360018\n",
      "Total Loss: 0.06613787170499563\n",
      "------------------------------------ epoch 9022 (54126 steps) ------------------------------------\n",
      "Max loss: 0.023859580978751183\n",
      "Min loss: 0.005500931292772293\n",
      "Mean loss: 0.010752575161556402\n",
      "Std loss: 0.006283538222390527\n",
      "Total Loss: 0.06451545096933842\n",
      "------------------------------------ epoch 9023 (54132 steps) ------------------------------------\n",
      "Max loss: 0.013362498953938484\n",
      "Min loss: 0.004482963588088751\n",
      "Mean loss: 0.006683106146131952\n",
      "Std loss: 0.0031998772259992105\n",
      "Total Loss: 0.040098636876791716\n",
      "------------------------------------ epoch 9024 (54138 steps) ------------------------------------\n",
      "Max loss: 0.014374757185578346\n",
      "Min loss: 0.0041274335235357285\n",
      "Mean loss: 0.008388553435603777\n",
      "Std loss: 0.0030984227591041528\n",
      "Total Loss: 0.050331320613622665\n",
      "------------------------------------ epoch 9025 (54144 steps) ------------------------------------\n",
      "Max loss: 0.01630844734609127\n",
      "Min loss: 0.004294989164918661\n",
      "Mean loss: 0.00893548682021598\n",
      "Std loss: 0.0041548517981825255\n",
      "Total Loss: 0.05361292092129588\n",
      "------------------------------------ epoch 9026 (54150 steps) ------------------------------------\n",
      "Max loss: 0.016675297170877457\n",
      "Min loss: 0.003611222840845585\n",
      "Mean loss: 0.008947235764935613\n",
      "Std loss: 0.00486353990599697\n",
      "Total Loss: 0.053683414589613676\n",
      "------------------------------------ epoch 9027 (54156 steps) ------------------------------------\n",
      "Max loss: 0.025247847661376\n",
      "Min loss: 0.006503119133412838\n",
      "Mean loss: 0.012226815801113844\n",
      "Std loss: 0.006744602905779908\n",
      "Total Loss: 0.07336089480668306\n",
      "------------------------------------ epoch 9028 (54162 steps) ------------------------------------\n",
      "Max loss: 0.03419056534767151\n",
      "Min loss: 0.003840892342850566\n",
      "Mean loss: 0.010491384154496094\n",
      "Std loss: 0.010738675399017007\n",
      "Total Loss: 0.06294830492697656\n",
      "------------------------------------ epoch 9029 (54168 steps) ------------------------------------\n",
      "Max loss: 0.014636709354817867\n",
      "Min loss: 0.005681872367858887\n",
      "Mean loss: 0.011050398151079813\n",
      "Std loss: 0.0029238804726042554\n",
      "Total Loss: 0.06630238890647888\n",
      "------------------------------------ epoch 9030 (54174 steps) ------------------------------------\n",
      "Max loss: 0.014503542333841324\n",
      "Min loss: 0.004650093149393797\n",
      "Mean loss: 0.008711809680486718\n",
      "Std loss: 0.004068663933992302\n",
      "Total Loss: 0.05227085808292031\n",
      "------------------------------------ epoch 9031 (54180 steps) ------------------------------------\n",
      "Max loss: 0.02957535907626152\n",
      "Min loss: 0.0040149083361029625\n",
      "Mean loss: 0.009268312637383739\n",
      "Std loss: 0.00912269732387512\n",
      "Total Loss: 0.055609875824302435\n",
      "------------------------------------ epoch 9032 (54186 steps) ------------------------------------\n",
      "Max loss: 0.020776715129613876\n",
      "Min loss: 0.005880522541701794\n",
      "Mean loss: 0.011708363036935529\n",
      "Std loss: 0.00615940402503106\n",
      "Total Loss: 0.07025017822161317\n",
      "------------------------------------ epoch 9033 (54192 steps) ------------------------------------\n",
      "Max loss: 0.01834866963326931\n",
      "Min loss: 0.005581425502896309\n",
      "Mean loss: 0.00992374929289023\n",
      "Std loss: 0.00500347665465884\n",
      "Total Loss: 0.059542495757341385\n",
      "------------------------------------ epoch 9034 (54198 steps) ------------------------------------\n",
      "Max loss: 0.011262593790888786\n",
      "Min loss: 0.004313308279961348\n",
      "Mean loss: 0.006408924935385585\n",
      "Std loss: 0.002663616089187506\n",
      "Total Loss: 0.03845354961231351\n",
      "------------------------------------ epoch 9035 (54204 steps) ------------------------------------\n",
      "Max loss: 0.023841261863708496\n",
      "Min loss: 0.0035339908208698034\n",
      "Mean loss: 0.012425513744043807\n",
      "Std loss: 0.007541364861852992\n",
      "Total Loss: 0.07455308246426284\n",
      "------------------------------------ epoch 9036 (54210 steps) ------------------------------------\n",
      "Max loss: 0.021386338397860527\n",
      "Min loss: 0.004342485684901476\n",
      "Mean loss: 0.01017821921656529\n",
      "Std loss: 0.007406678566740299\n",
      "Total Loss: 0.061069315299391747\n",
      "------------------------------------ epoch 9037 (54216 steps) ------------------------------------\n",
      "Max loss: 0.0331660695374012\n",
      "Min loss: 0.004258804023265839\n",
      "Mean loss: 0.011290739833687743\n",
      "Std loss: 0.009943342920076217\n",
      "Total Loss: 0.06774443900212646\n",
      "------------------------------------ epoch 9038 (54222 steps) ------------------------------------\n",
      "Max loss: 0.033756110817193985\n",
      "Min loss: 0.00555915292352438\n",
      "Mean loss: 0.015228470787405968\n",
      "Std loss: 0.0089478317840598\n",
      "Total Loss: 0.0913708247244358\n",
      "------------------------------------ epoch 9039 (54228 steps) ------------------------------------\n",
      "Max loss: 0.03455328196287155\n",
      "Min loss: 0.005920980125665665\n",
      "Mean loss: 0.01124261769776543\n",
      "Std loss: 0.010466098531961135\n",
      "Total Loss: 0.06745570618659258\n",
      "------------------------------------ epoch 9040 (54234 steps) ------------------------------------\n",
      "Max loss: 0.01054765097796917\n",
      "Min loss: 0.004042092710733414\n",
      "Mean loss: 0.007213082940628131\n",
      "Std loss: 0.0022105171649916644\n",
      "Total Loss: 0.04327849764376879\n",
      "------------------------------------ epoch 9041 (54240 steps) ------------------------------------\n",
      "Max loss: 0.020447585731744766\n",
      "Min loss: 0.005136760883033276\n",
      "Mean loss: 0.009766679257154465\n",
      "Std loss: 0.00513313486269065\n",
      "Total Loss: 0.05860007554292679\n",
      "------------------------------------ epoch 9042 (54246 steps) ------------------------------------\n",
      "Max loss: 0.027185333892703056\n",
      "Min loss: 0.005214137490838766\n",
      "Mean loss: 0.010815182933583856\n",
      "Std loss: 0.007530859501924887\n",
      "Total Loss: 0.06489109760150313\n",
      "------------------------------------ epoch 9043 (54252 steps) ------------------------------------\n",
      "Max loss: 0.013301923871040344\n",
      "Min loss: 0.0043223206885159016\n",
      "Mean loss: 0.007341657687599461\n",
      "Std loss: 0.002966755533948358\n",
      "Total Loss: 0.04404994612559676\n",
      "------------------------------------ epoch 9044 (54258 steps) ------------------------------------\n",
      "Max loss: 0.012718025594949722\n",
      "Min loss: 0.004283570684492588\n",
      "Mean loss: 0.006707902454460661\n",
      "Std loss: 0.003074582440376735\n",
      "Total Loss: 0.040247414726763964\n",
      "------------------------------------ epoch 9045 (54264 steps) ------------------------------------\n",
      "Max loss: 0.018675731495022774\n",
      "Min loss: 0.00302093755453825\n",
      "Mean loss: 0.008036518042596677\n",
      "Std loss: 0.00539999432782467\n",
      "Total Loss: 0.04821910825558007\n",
      "------------------------------------ epoch 9046 (54270 steps) ------------------------------------\n",
      "Max loss: 0.013983655720949173\n",
      "Min loss: 0.004563543945550919\n",
      "Mean loss: 0.007946812314912677\n",
      "Std loss: 0.003548443407878144\n",
      "Total Loss: 0.04768087388947606\n",
      "------------------------------------ epoch 9047 (54276 steps) ------------------------------------\n",
      "Max loss: 0.010204470716416836\n",
      "Min loss: 0.004090677015483379\n",
      "Mean loss: 0.00685567509693404\n",
      "Std loss: 0.0023525766867965765\n",
      "Total Loss: 0.04113405058160424\n",
      "------------------------------------ epoch 9048 (54282 steps) ------------------------------------\n",
      "Max loss: 0.011450680904090405\n",
      "Min loss: 0.005698682274669409\n",
      "Mean loss: 0.007705177646130323\n",
      "Std loss: 0.0020026984336346757\n",
      "Total Loss: 0.04623106587678194\n",
      "------------------------------------ epoch 9049 (54288 steps) ------------------------------------\n",
      "Max loss: 0.01819470524787903\n",
      "Min loss: 0.004739874042570591\n",
      "Mean loss: 0.008986840567861995\n",
      "Std loss: 0.004586473675827216\n",
      "Total Loss: 0.053921043407171965\n",
      "------------------------------------ epoch 9050 (54294 steps) ------------------------------------\n",
      "Max loss: 0.011480078101158142\n",
      "Min loss: 0.004391118418425322\n",
      "Mean loss: 0.0069614808696011705\n",
      "Std loss: 0.002288224318300504\n",
      "Total Loss: 0.04176888521760702\n",
      "------------------------------------ epoch 9051 (54300 steps) ------------------------------------\n",
      "Max loss: 0.010415731929242611\n",
      "Min loss: 0.004038783255964518\n",
      "Mean loss: 0.006719101297979553\n",
      "Std loss: 0.0023145691785129772\n",
      "Total Loss: 0.04031460778787732\n",
      "------------------------------------ epoch 9052 (54306 steps) ------------------------------------\n",
      "Max loss: 0.02412085235118866\n",
      "Min loss: 0.0035195667296648026\n",
      "Mean loss: 0.01003194460645318\n",
      "Std loss: 0.0077955327424121664\n",
      "Total Loss: 0.06019166763871908\n",
      "------------------------------------ epoch 9053 (54312 steps) ------------------------------------\n",
      "Max loss: 0.017449896782636642\n",
      "Min loss: 0.005307956598699093\n",
      "Mean loss: 0.008512714567283789\n",
      "Std loss: 0.0042372497053911485\n",
      "Total Loss: 0.051076287403702736\n",
      "------------------------------------ epoch 9054 (54318 steps) ------------------------------------\n",
      "Max loss: 0.01105127390474081\n",
      "Min loss: 0.004155121743679047\n",
      "Mean loss: 0.00677626533433795\n",
      "Std loss: 0.0021605207287002147\n",
      "Total Loss: 0.0406575920060277\n",
      "------------------------------------ epoch 9055 (54324 steps) ------------------------------------\n",
      "Max loss: 0.01898474432528019\n",
      "Min loss: 0.004292778205126524\n",
      "Mean loss: 0.008025694716100892\n",
      "Std loss: 0.005102312979072666\n",
      "Total Loss: 0.04815416829660535\n",
      "------------------------------------ epoch 9056 (54330 steps) ------------------------------------\n",
      "Max loss: 0.015456810593605042\n",
      "Min loss: 0.0048621101304888725\n",
      "Mean loss: 0.008118041868632039\n",
      "Std loss: 0.0035890382474228003\n",
      "Total Loss: 0.04870825121179223\n",
      "------------------------------------ epoch 9057 (54336 steps) ------------------------------------\n",
      "Max loss: 0.035934463143348694\n",
      "Min loss: 0.0035765133798122406\n",
      "Mean loss: 0.011411942696819702\n",
      "Std loss: 0.011175141266115844\n",
      "Total Loss: 0.06847165618091822\n",
      "------------------------------------ epoch 9058 (54342 steps) ------------------------------------\n",
      "Max loss: 0.014737814664840698\n",
      "Min loss: 0.00630232272669673\n",
      "Mean loss: 0.010992448854570588\n",
      "Std loss: 0.002920485613388275\n",
      "Total Loss: 0.06595469312742352\n",
      "------------------------------------ epoch 9059 (54348 steps) ------------------------------------\n",
      "Max loss: 0.02013329789042473\n",
      "Min loss: 0.004755538888275623\n",
      "Mean loss: 0.00976605728889505\n",
      "Std loss: 0.005339171281264177\n",
      "Total Loss: 0.058596343733370304\n",
      "------------------------------------ epoch 9060 (54354 steps) ------------------------------------\n",
      "Max loss: 0.02626848593354225\n",
      "Min loss: 0.003791063791140914\n",
      "Mean loss: 0.00909380711770306\n",
      "Std loss: 0.007807890839941327\n",
      "Total Loss: 0.05456284270621836\n",
      "------------------------------------ epoch 9061 (54360 steps) ------------------------------------\n",
      "Max loss: 0.020657289773225784\n",
      "Min loss: 0.004372809547930956\n",
      "Mean loss: 0.011013064145420989\n",
      "Std loss: 0.006207785733358322\n",
      "Total Loss: 0.06607838487252593\n",
      "------------------------------------ epoch 9062 (54366 steps) ------------------------------------\n",
      "Max loss: 0.01389005035161972\n",
      "Min loss: 0.005332790315151215\n",
      "Mean loss: 0.010295647118861476\n",
      "Std loss: 0.0028500841572080254\n",
      "Total Loss: 0.06177388271316886\n",
      "------------------------------------ epoch 9063 (54372 steps) ------------------------------------\n",
      "Max loss: 0.009553341194987297\n",
      "Min loss: 0.0040418971329927444\n",
      "Mean loss: 0.005854045661787192\n",
      "Std loss: 0.0018826664513497664\n",
      "Total Loss: 0.03512427397072315\n",
      "------------------------------------ epoch 9064 (54378 steps) ------------------------------------\n",
      "Max loss: 0.03326353058218956\n",
      "Min loss: 0.005808008369058371\n",
      "Mean loss: 0.013591381022706628\n",
      "Std loss: 0.009164699381507185\n",
      "Total Loss: 0.08154828613623977\n",
      "------------------------------------ epoch 9065 (54384 steps) ------------------------------------\n",
      "Max loss: 0.01249108836054802\n",
      "Min loss: 0.004566282499581575\n",
      "Mean loss: 0.008023215302576622\n",
      "Std loss: 0.003133339936109856\n",
      "Total Loss: 0.04813929181545973\n",
      "------------------------------------ epoch 9066 (54390 steps) ------------------------------------\n",
      "Max loss: 0.020853038877248764\n",
      "Min loss: 0.006498798727989197\n",
      "Mean loss: 0.010966915637254715\n",
      "Std loss: 0.004745954042027107\n",
      "Total Loss: 0.06580149382352829\n",
      "------------------------------------ epoch 9067 (54396 steps) ------------------------------------\n",
      "Max loss: 0.014399814419448376\n",
      "Min loss: 0.00575119536370039\n",
      "Mean loss: 0.010436689636359612\n",
      "Std loss: 0.003068899058433053\n",
      "Total Loss: 0.06262013781815767\n",
      "------------------------------------ epoch 9068 (54402 steps) ------------------------------------\n",
      "Max loss: 0.018888449296355247\n",
      "Min loss: 0.006779815070331097\n",
      "Mean loss: 0.011968150734901428\n",
      "Std loss: 0.004145029364403403\n",
      "Total Loss: 0.07180890440940857\n",
      "------------------------------------ epoch 9069 (54408 steps) ------------------------------------\n",
      "Max loss: 0.026390714570879936\n",
      "Min loss: 0.0057383207604289055\n",
      "Mean loss: 0.012566318114598593\n",
      "Std loss: 0.007072331991235104\n",
      "Total Loss: 0.07539790868759155\n",
      "------------------------------------ epoch 9070 (54414 steps) ------------------------------------\n",
      "Max loss: 0.022208988666534424\n",
      "Min loss: 0.00394007284194231\n",
      "Mean loss: 0.008805043219278256\n",
      "Std loss: 0.006244633309189482\n",
      "Total Loss: 0.05283025931566954\n",
      "------------------------------------ epoch 9071 (54420 steps) ------------------------------------\n",
      "Max loss: 0.012692469172179699\n",
      "Min loss: 0.0038333251141011715\n",
      "Mean loss: 0.0064616939052939415\n",
      "Std loss: 0.003014580024372811\n",
      "Total Loss: 0.03877016343176365\n",
      "------------------------------------ epoch 9072 (54426 steps) ------------------------------------\n",
      "Max loss: 0.015320226550102234\n",
      "Min loss: 0.004762531258165836\n",
      "Mean loss: 0.0076168061544497805\n",
      "Std loss: 0.003709033044318857\n",
      "Total Loss: 0.045700836926698685\n",
      "------------------------------------ epoch 9073 (54432 steps) ------------------------------------\n",
      "Max loss: 0.015689317137002945\n",
      "Min loss: 0.006573419086635113\n",
      "Mean loss: 0.009958290184537569\n",
      "Std loss: 0.002977518997676559\n",
      "Total Loss: 0.05974974110722542\n",
      "------------------------------------ epoch 9074 (54438 steps) ------------------------------------\n",
      "Max loss: 0.010210290551185608\n",
      "Min loss: 0.004076291806995869\n",
      "Mean loss: 0.006105350252861778\n",
      "Std loss: 0.002023521247958958\n",
      "Total Loss: 0.03663210151717067\n",
      "------------------------------------ epoch 9075 (54444 steps) ------------------------------------\n",
      "Max loss: 0.01463831216096878\n",
      "Min loss: 0.004181019961833954\n",
      "Mean loss: 0.008172929364567002\n",
      "Std loss: 0.003385707607453006\n",
      "Total Loss: 0.04903757618740201\n",
      "------------------------------------ epoch 9076 (54450 steps) ------------------------------------\n",
      "Max loss: 0.018787279725074768\n",
      "Min loss: 0.004054951015859842\n",
      "Mean loss: 0.01075792444559435\n",
      "Std loss: 0.005149235404349087\n",
      "Total Loss: 0.0645475466735661\n",
      "------------------------------------ epoch 9077 (54456 steps) ------------------------------------\n",
      "Max loss: 0.0315576009452343\n",
      "Min loss: 0.0038761659525334835\n",
      "Mean loss: 0.010367877393340072\n",
      "Std loss: 0.010017506141691985\n",
      "Total Loss: 0.062207264360040426\n",
      "------------------------------------ epoch 9078 (54462 steps) ------------------------------------\n",
      "Max loss: 0.011747526936233044\n",
      "Min loss: 0.004151220433413982\n",
      "Mean loss: 0.007632610310489933\n",
      "Std loss: 0.002356348390899138\n",
      "Total Loss: 0.045795661862939596\n",
      "------------------------------------ epoch 9079 (54468 steps) ------------------------------------\n",
      "Max loss: 0.013342304155230522\n",
      "Min loss: 0.004276433028280735\n",
      "Mean loss: 0.007807235621536772\n",
      "Std loss: 0.0031516861836294032\n",
      "Total Loss: 0.04684341372922063\n",
      "------------------------------------ epoch 9080 (54474 steps) ------------------------------------\n",
      "Max loss: 0.015217261388897896\n",
      "Min loss: 0.003868242260068655\n",
      "Mean loss: 0.008669321037208041\n",
      "Std loss: 0.00446798163398732\n",
      "Total Loss: 0.05201592622324824\n",
      "------------------------------------ epoch 9081 (54480 steps) ------------------------------------\n",
      "Max loss: 0.01696172170341015\n",
      "Min loss: 0.00426341500133276\n",
      "Mean loss: 0.008214343649645647\n",
      "Std loss: 0.004222312147745978\n",
      "Total Loss: 0.04928606189787388\n",
      "------------------------------------ epoch 9082 (54486 steps) ------------------------------------\n",
      "Max loss: 0.023371640592813492\n",
      "Min loss: 0.004231778904795647\n",
      "Mean loss: 0.009721513216694197\n",
      "Std loss: 0.007322418166304515\n",
      "Total Loss: 0.058329079300165176\n",
      "------------------------------------ epoch 9083 (54492 steps) ------------------------------------\n",
      "Max loss: 0.015387806110084057\n",
      "Min loss: 0.0050412616692483425\n",
      "Mean loss: 0.009331475167224804\n",
      "Std loss: 0.004032223762160672\n",
      "Total Loss: 0.05598885100334883\n",
      "------------------------------------ epoch 9084 (54498 steps) ------------------------------------\n",
      "Max loss: 0.01307068020105362\n",
      "Min loss: 0.004017950035631657\n",
      "Mean loss: 0.008261414399991432\n",
      "Std loss: 0.0034557671255358723\n",
      "Total Loss: 0.0495684863999486\n",
      "------------------------------------ epoch 9085 (54504 steps) ------------------------------------\n",
      "Max loss: 0.012207075953483582\n",
      "Min loss: 0.0037566889077425003\n",
      "Mean loss: 0.0071653027553111315\n",
      "Std loss: 0.0026004483334311776\n",
      "Total Loss: 0.04299181653186679\n",
      "------------------------------------ epoch 9086 (54510 steps) ------------------------------------\n",
      "Max loss: 0.009695962071418762\n",
      "Min loss: 0.0030828681774437428\n",
      "Mean loss: 0.007064721547067165\n",
      "Std loss: 0.002171138592417198\n",
      "Total Loss: 0.04238832928240299\n",
      "------------------------------------ epoch 9087 (54516 steps) ------------------------------------\n",
      "Max loss: 0.019705813378095627\n",
      "Min loss: 0.005336833652108908\n",
      "Mean loss: 0.009806285689895352\n",
      "Std loss: 0.00566716486317213\n",
      "Total Loss: 0.05883771413937211\n",
      "------------------------------------ epoch 9088 (54522 steps) ------------------------------------\n",
      "Max loss: 0.02133997157216072\n",
      "Min loss: 0.0038882556837052107\n",
      "Mean loss: 0.009032297452601293\n",
      "Std loss: 0.005880351031228322\n",
      "Total Loss: 0.05419378471560776\n",
      "------------------------------------ epoch 9089 (54528 steps) ------------------------------------\n",
      "Max loss: 0.009612464345991611\n",
      "Min loss: 0.005538322031497955\n",
      "Mean loss: 0.007749173479775588\n",
      "Std loss: 0.0013964889985846336\n",
      "Total Loss: 0.046495040878653526\n",
      "------------------------------------ epoch 9090 (54534 steps) ------------------------------------\n",
      "Max loss: 0.014909143559634686\n",
      "Min loss: 0.003927755169570446\n",
      "Mean loss: 0.009143028718729814\n",
      "Std loss: 0.003980746469843214\n",
      "Total Loss: 0.05485817231237888\n",
      "------------------------------------ epoch 9091 (54540 steps) ------------------------------------\n",
      "Max loss: 0.017498569563031197\n",
      "Min loss: 0.005992123391479254\n",
      "Mean loss: 0.009739226894453168\n",
      "Std loss: 0.004274092187334216\n",
      "Total Loss: 0.05843536136671901\n",
      "------------------------------------ epoch 9092 (54546 steps) ------------------------------------\n",
      "Max loss: 0.011628574691712856\n",
      "Min loss: 0.00537740346044302\n",
      "Mean loss: 0.007790944539010525\n",
      "Std loss: 0.0020885926896919167\n",
      "Total Loss: 0.04674566723406315\n",
      "------------------------------------ epoch 9093 (54552 steps) ------------------------------------\n",
      "Max loss: 0.01148578803986311\n",
      "Min loss: 0.0034688522573560476\n",
      "Mean loss: 0.006470810350341101\n",
      "Std loss: 0.0025651101590934236\n",
      "Total Loss: 0.03882486210204661\n",
      "------------------------------------ epoch 9094 (54558 steps) ------------------------------------\n",
      "Max loss: 0.017519311979413033\n",
      "Min loss: 0.0037834118120372295\n",
      "Mean loss: 0.008666620900233587\n",
      "Std loss: 0.005248292685594443\n",
      "Total Loss: 0.05199972540140152\n",
      "------------------------------------ epoch 9095 (54564 steps) ------------------------------------\n",
      "Max loss: 0.016120070591568947\n",
      "Min loss: 0.003391258418560028\n",
      "Mean loss: 0.006958819615344207\n",
      "Std loss: 0.004293736024816908\n",
      "Total Loss: 0.04175291769206524\n",
      "------------------------------------ epoch 9096 (54570 steps) ------------------------------------\n",
      "Max loss: 0.006243041716516018\n",
      "Min loss: 0.004415677394717932\n",
      "Mean loss: 0.005299595572675268\n",
      "Std loss: 0.0006552962519250397\n",
      "Total Loss: 0.03179757343605161\n",
      "------------------------------------ epoch 9097 (54576 steps) ------------------------------------\n",
      "Max loss: 0.025440936908125877\n",
      "Min loss: 0.0038510195445269346\n",
      "Mean loss: 0.010690017797363302\n",
      "Std loss: 0.007202516851409731\n",
      "Total Loss: 0.0641401067841798\n",
      "------------------------------------ epoch 9098 (54582 steps) ------------------------------------\n",
      "Max loss: 0.008380671963095665\n",
      "Min loss: 0.003713708370923996\n",
      "Mean loss: 0.005701926692078511\n",
      "Std loss: 0.0013819711097478034\n",
      "Total Loss: 0.034211560152471066\n",
      "------------------------------------ epoch 9099 (54588 steps) ------------------------------------\n",
      "Max loss: 0.010432004928588867\n",
      "Min loss: 0.0040654526092112064\n",
      "Mean loss: 0.006060548747579257\n",
      "Std loss: 0.00219280404883471\n",
      "Total Loss: 0.03636329248547554\n",
      "------------------------------------ epoch 9100 (54594 steps) ------------------------------------\n",
      "Max loss: 0.019688140600919724\n",
      "Min loss: 0.004728340078145266\n",
      "Mean loss: 0.009522981087987622\n",
      "Std loss: 0.005697624803885636\n",
      "Total Loss: 0.05713788652792573\n",
      "------------------------------------ epoch 9101 (54600 steps) ------------------------------------\n",
      "Max loss: 0.018496157601475716\n",
      "Min loss: 0.00604632031172514\n",
      "Mean loss: 0.012224073521792889\n",
      "Std loss: 0.004765901573222165\n",
      "Total Loss: 0.07334444113075733\n",
      "saved model at ./weights/model_9101.pth\n",
      "------------------------------------ epoch 9102 (54606 steps) ------------------------------------\n",
      "Max loss: 0.011079112999141216\n",
      "Min loss: 0.005683205556124449\n",
      "Mean loss: 0.007528561167418957\n",
      "Std loss: 0.0018805679529404744\n",
      "Total Loss: 0.04517136700451374\n",
      "------------------------------------ epoch 9103 (54612 steps) ------------------------------------\n",
      "Max loss: 0.012910554185509682\n",
      "Min loss: 0.004711544141173363\n",
      "Mean loss: 0.008132796036079526\n",
      "Std loss: 0.0027043858115667566\n",
      "Total Loss: 0.048796776216477156\n",
      "------------------------------------ epoch 9104 (54618 steps) ------------------------------------\n",
      "Max loss: 0.01790727488696575\n",
      "Min loss: 0.00491424510255456\n",
      "Mean loss: 0.008785355370491743\n",
      "Std loss: 0.004429694236677809\n",
      "Total Loss: 0.05271213222295046\n",
      "------------------------------------ epoch 9105 (54624 steps) ------------------------------------\n",
      "Max loss: 0.013951173983514309\n",
      "Min loss: 0.004082425497472286\n",
      "Mean loss: 0.007876947599773606\n",
      "Std loss: 0.0033181607279054335\n",
      "Total Loss: 0.047261685598641634\n",
      "------------------------------------ epoch 9106 (54630 steps) ------------------------------------\n",
      "Max loss: 0.014530371874570847\n",
      "Min loss: 0.003993285819888115\n",
      "Mean loss: 0.0065992882785697775\n",
      "Std loss: 0.003600243189224803\n",
      "Total Loss: 0.03959572967141867\n",
      "------------------------------------ epoch 9107 (54636 steps) ------------------------------------\n",
      "Max loss: 0.006769252009689808\n",
      "Min loss: 0.0038531124591827393\n",
      "Mean loss: 0.005557161367808779\n",
      "Std loss: 0.0009630990106928501\n",
      "Total Loss: 0.033342968206852674\n",
      "------------------------------------ epoch 9108 (54642 steps) ------------------------------------\n",
      "Max loss: 0.008534390479326248\n",
      "Min loss: 0.005728284828364849\n",
      "Mean loss: 0.006599447612340252\n",
      "Std loss: 0.0009041004301092901\n",
      "Total Loss: 0.03959668567404151\n",
      "------------------------------------ epoch 9109 (54648 steps) ------------------------------------\n",
      "Max loss: 0.015347602777183056\n",
      "Min loss: 0.004566389601677656\n",
      "Mean loss: 0.00859561120159924\n",
      "Std loss: 0.003643398262187665\n",
      "Total Loss: 0.05157366720959544\n",
      "------------------------------------ epoch 9110 (54654 steps) ------------------------------------\n",
      "Max loss: 0.026975084096193314\n",
      "Min loss: 0.005508034955710173\n",
      "Mean loss: 0.012230514471108714\n",
      "Std loss: 0.008488422480445996\n",
      "Total Loss: 0.07338308682665229\n",
      "------------------------------------ epoch 9111 (54660 steps) ------------------------------------\n",
      "Max loss: 0.019010093063116074\n",
      "Min loss: 0.00441246572881937\n",
      "Mean loss: 0.00921259067642192\n",
      "Std loss: 0.0050997719282011985\n",
      "Total Loss: 0.05527554405853152\n",
      "------------------------------------ epoch 9112 (54666 steps) ------------------------------------\n",
      "Max loss: 0.020070329308509827\n",
      "Min loss: 0.003992811776697636\n",
      "Mean loss: 0.009444019834821423\n",
      "Std loss: 0.005719757875031131\n",
      "Total Loss: 0.05666411900892854\n",
      "------------------------------------ epoch 9113 (54672 steps) ------------------------------------\n",
      "Max loss: 0.01943035051226616\n",
      "Min loss: 0.004707111045718193\n",
      "Mean loss: 0.008349212895457944\n",
      "Std loss: 0.00510594609230356\n",
      "Total Loss: 0.05009527737274766\n",
      "------------------------------------ epoch 9114 (54678 steps) ------------------------------------\n",
      "Max loss: 0.010984810069203377\n",
      "Min loss: 0.004856931045651436\n",
      "Mean loss: 0.006839638731131951\n",
      "Std loss: 0.0023294286863722413\n",
      "Total Loss: 0.041037832386791706\n",
      "------------------------------------ epoch 9115 (54684 steps) ------------------------------------\n",
      "Max loss: 0.030415428802371025\n",
      "Min loss: 0.003982033114880323\n",
      "Mean loss: 0.009459050682683786\n",
      "Std loss: 0.009431813179233098\n",
      "Total Loss: 0.056754304096102715\n",
      "------------------------------------ epoch 9116 (54690 steps) ------------------------------------\n",
      "Max loss: 0.02225419506430626\n",
      "Min loss: 0.0044974712654948235\n",
      "Mean loss: 0.0087241738413771\n",
      "Std loss: 0.006098899244658935\n",
      "Total Loss: 0.052345043048262596\n",
      "------------------------------------ epoch 9117 (54696 steps) ------------------------------------\n",
      "Max loss: 0.018696153536438942\n",
      "Min loss: 0.004917195998132229\n",
      "Mean loss: 0.010272106310973564\n",
      "Std loss: 0.004546782678544292\n",
      "Total Loss: 0.06163263786584139\n",
      "------------------------------------ epoch 9118 (54702 steps) ------------------------------------\n",
      "Max loss: 0.03209434449672699\n",
      "Min loss: 0.003933230880647898\n",
      "Mean loss: 0.010380039146790901\n",
      "Std loss: 0.010136653450529425\n",
      "Total Loss: 0.06228023488074541\n",
      "------------------------------------ epoch 9119 (54708 steps) ------------------------------------\n",
      "Max loss: 0.008964496664702892\n",
      "Min loss: 0.0038185883313417435\n",
      "Mean loss: 0.005218179741253455\n",
      "Std loss: 0.0017451376362934076\n",
      "Total Loss: 0.03130907844752073\n",
      "------------------------------------ epoch 9120 (54714 steps) ------------------------------------\n",
      "Max loss: 0.01942305639386177\n",
      "Min loss: 0.004538672044873238\n",
      "Mean loss: 0.007960244314745069\n",
      "Std loss: 0.005169109191359612\n",
      "Total Loss: 0.04776146588847041\n",
      "------------------------------------ epoch 9121 (54720 steps) ------------------------------------\n",
      "Max loss: 0.012583449482917786\n",
      "Min loss: 0.0061118146404623985\n",
      "Mean loss: 0.00934179670487841\n",
      "Std loss: 0.0026004362706481643\n",
      "Total Loss: 0.05605078022927046\n",
      "------------------------------------ epoch 9122 (54726 steps) ------------------------------------\n",
      "Max loss: 0.010998750105500221\n",
      "Min loss: 0.003346972167491913\n",
      "Mean loss: 0.006874589404712121\n",
      "Std loss: 0.0028304053144005874\n",
      "Total Loss: 0.041247536428272724\n",
      "------------------------------------ epoch 9123 (54732 steps) ------------------------------------\n",
      "Max loss: 0.017486587166786194\n",
      "Min loss: 0.004654875956475735\n",
      "Mean loss: 0.008112536122401556\n",
      "Std loss: 0.004408670689202029\n",
      "Total Loss: 0.04867521673440933\n",
      "------------------------------------ epoch 9124 (54738 steps) ------------------------------------\n",
      "Max loss: 0.007831377908587456\n",
      "Min loss: 0.00328958872705698\n",
      "Mean loss: 0.005084111432855328\n",
      "Std loss: 0.0015200484571049405\n",
      "Total Loss: 0.030504668597131968\n",
      "------------------------------------ epoch 9125 (54744 steps) ------------------------------------\n",
      "Max loss: 0.02771878056228161\n",
      "Min loss: 0.003942363895475864\n",
      "Mean loss: 0.009974043273056546\n",
      "Std loss: 0.00822519042872774\n",
      "Total Loss: 0.05984425963833928\n",
      "------------------------------------ epoch 9126 (54750 steps) ------------------------------------\n",
      "Max loss: 0.01578616350889206\n",
      "Min loss: 0.004494608845561743\n",
      "Mean loss: 0.007901076149816314\n",
      "Std loss: 0.0039493497972041495\n",
      "Total Loss: 0.047406456898897886\n",
      "------------------------------------ epoch 9127 (54756 steps) ------------------------------------\n",
      "Max loss: 0.014616861008107662\n",
      "Min loss: 0.003301451914012432\n",
      "Mean loss: 0.009023578682293495\n",
      "Std loss: 0.005230323549209993\n",
      "Total Loss: 0.05414147209376097\n",
      "------------------------------------ epoch 9128 (54762 steps) ------------------------------------\n",
      "Max loss: 0.0147627592086792\n",
      "Min loss: 0.003891321364790201\n",
      "Mean loss: 0.007983189852287373\n",
      "Std loss: 0.004459790558775967\n",
      "Total Loss: 0.04789913911372423\n",
      "------------------------------------ epoch 9129 (54768 steps) ------------------------------------\n",
      "Max loss: 0.028757337480783463\n",
      "Min loss: 0.004377515055239201\n",
      "Mean loss: 0.009315322308490673\n",
      "Std loss: 0.008779108280453193\n",
      "Total Loss: 0.05589193385094404\n",
      "------------------------------------ epoch 9130 (54774 steps) ------------------------------------\n",
      "Max loss: 0.014562717638909817\n",
      "Min loss: 0.004270466975867748\n",
      "Mean loss: 0.0069598727859556675\n",
      "Std loss: 0.0036022579293395265\n",
      "Total Loss: 0.041759236715734005\n",
      "------------------------------------ epoch 9131 (54780 steps) ------------------------------------\n",
      "Max loss: 0.008672231808304787\n",
      "Min loss: 0.004675482399761677\n",
      "Mean loss: 0.006265024654567242\n",
      "Std loss: 0.0014573241841386766\n",
      "Total Loss: 0.03759014792740345\n",
      "------------------------------------ epoch 9132 (54786 steps) ------------------------------------\n",
      "Max loss: 0.01695840433239937\n",
      "Min loss: 0.0035471823066473007\n",
      "Mean loss: 0.008715836408858499\n",
      "Std loss: 0.004567532700440492\n",
      "Total Loss: 0.05229501845315099\n",
      "------------------------------------ epoch 9133 (54792 steps) ------------------------------------\n",
      "Max loss: 0.01410607062280178\n",
      "Min loss: 0.003835570067167282\n",
      "Mean loss: 0.00824488140642643\n",
      "Std loss: 0.003952703162145698\n",
      "Total Loss: 0.04946928843855858\n",
      "------------------------------------ epoch 9134 (54798 steps) ------------------------------------\n",
      "Max loss: 0.014991570264101028\n",
      "Min loss: 0.003833971917629242\n",
      "Mean loss: 0.007612057418252031\n",
      "Std loss: 0.0038803726116918053\n",
      "Total Loss: 0.045672344509512186\n",
      "------------------------------------ epoch 9135 (54804 steps) ------------------------------------\n",
      "Max loss: 0.006874158047139645\n",
      "Min loss: 0.0037060945760458708\n",
      "Mean loss: 0.005368992957907419\n",
      "Std loss: 0.0009484394755042518\n",
      "Total Loss: 0.03221395774744451\n",
      "------------------------------------ epoch 9136 (54810 steps) ------------------------------------\n",
      "Max loss: 0.013816246762871742\n",
      "Min loss: 0.004544165916740894\n",
      "Mean loss: 0.007429662548626463\n",
      "Std loss: 0.003363554981212733\n",
      "Total Loss: 0.044577975291758776\n",
      "------------------------------------ epoch 9137 (54816 steps) ------------------------------------\n",
      "Max loss: 0.007862099446356297\n",
      "Min loss: 0.004876457154750824\n",
      "Mean loss: 0.0055773635394871235\n",
      "Std loss: 0.001051330069541778\n",
      "Total Loss: 0.03346418123692274\n",
      "------------------------------------ epoch 9138 (54822 steps) ------------------------------------\n",
      "Max loss: 0.00988589134067297\n",
      "Min loss: 0.003204873763024807\n",
      "Mean loss: 0.005737749394029379\n",
      "Std loss: 0.00229800192743452\n",
      "Total Loss: 0.03442649636417627\n",
      "------------------------------------ epoch 9139 (54828 steps) ------------------------------------\n",
      "Max loss: 0.03195306658744812\n",
      "Min loss: 0.0036381182726472616\n",
      "Mean loss: 0.011846834677271545\n",
      "Std loss: 0.009239757465537552\n",
      "Total Loss: 0.07108100806362927\n",
      "------------------------------------ epoch 9140 (54834 steps) ------------------------------------\n",
      "Max loss: 0.015350352972745895\n",
      "Min loss: 0.0033519472926855087\n",
      "Mean loss: 0.009400762928028902\n",
      "Std loss: 0.004333248500181458\n",
      "Total Loss: 0.05640457756817341\n",
      "------------------------------------ epoch 9141 (54840 steps) ------------------------------------\n",
      "Max loss: 0.012669559568166733\n",
      "Min loss: 0.007689201273024082\n",
      "Mean loss: 0.009910357184708118\n",
      "Std loss: 0.001723047344158294\n",
      "Total Loss: 0.05946214310824871\n",
      "------------------------------------ epoch 9142 (54846 steps) ------------------------------------\n",
      "Max loss: 0.011052457615733147\n",
      "Min loss: 0.0036952360533177853\n",
      "Mean loss: 0.0065201585336277885\n",
      "Std loss: 0.0025018071231176454\n",
      "Total Loss: 0.03912095120176673\n",
      "------------------------------------ epoch 9143 (54852 steps) ------------------------------------\n",
      "Max loss: 0.009842945262789726\n",
      "Min loss: 0.004491651430726051\n",
      "Mean loss: 0.006665418778235714\n",
      "Std loss: 0.0019957306629905165\n",
      "Total Loss: 0.03999251266941428\n",
      "------------------------------------ epoch 9144 (54858 steps) ------------------------------------\n",
      "Max loss: 0.035927094519138336\n",
      "Min loss: 0.0039035161025822163\n",
      "Mean loss: 0.012437079257021347\n",
      "Std loss: 0.011562136339984487\n",
      "Total Loss: 0.07462247554212809\n",
      "------------------------------------ epoch 9145 (54864 steps) ------------------------------------\n",
      "Max loss: 0.035412490367889404\n",
      "Min loss: 0.005856841336935759\n",
      "Mean loss: 0.013877609744668007\n",
      "Std loss: 0.010108999500390892\n",
      "Total Loss: 0.08326565846800804\n",
      "------------------------------------ epoch 9146 (54870 steps) ------------------------------------\n",
      "Max loss: 0.04042733460664749\n",
      "Min loss: 0.003801689948886633\n",
      "Mean loss: 0.013444943353533745\n",
      "Std loss: 0.012914855973052664\n",
      "Total Loss: 0.08066966012120247\n",
      "------------------------------------ epoch 9147 (54876 steps) ------------------------------------\n",
      "Max loss: 0.02804606780409813\n",
      "Min loss: 0.007298141252249479\n",
      "Mean loss: 0.013423681491985917\n",
      "Std loss: 0.007009612350237788\n",
      "Total Loss: 0.0805420889519155\n",
      "------------------------------------ epoch 9148 (54882 steps) ------------------------------------\n",
      "Max loss: 0.01931854337453842\n",
      "Min loss: 0.004124858416616917\n",
      "Mean loss: 0.008394596632570028\n",
      "Std loss: 0.005258566190162094\n",
      "Total Loss: 0.05036757979542017\n",
      "------------------------------------ epoch 9149 (54888 steps) ------------------------------------\n",
      "Max loss: 0.018489984795451164\n",
      "Min loss: 0.004349131137132645\n",
      "Mean loss: 0.011365726279715696\n",
      "Std loss: 0.005115014891798038\n",
      "Total Loss: 0.06819435767829418\n",
      "------------------------------------ epoch 9150 (54894 steps) ------------------------------------\n",
      "Max loss: 0.008749417029321194\n",
      "Min loss: 0.004103691317141056\n",
      "Mean loss: 0.005955684619645278\n",
      "Std loss: 0.0013849136875188415\n",
      "Total Loss: 0.035734107717871666\n",
      "------------------------------------ epoch 9151 (54900 steps) ------------------------------------\n",
      "Max loss: 0.02460453473031521\n",
      "Min loss: 0.00647740438580513\n",
      "Mean loss: 0.011941089062020183\n",
      "Std loss: 0.006337056479723143\n",
      "Total Loss: 0.0716465343721211\n",
      "------------------------------------ epoch 9152 (54906 steps) ------------------------------------\n",
      "Max loss: 0.019078565761446953\n",
      "Min loss: 0.004972607828676701\n",
      "Mean loss: 0.010948013591890534\n",
      "Std loss: 0.005466422376388186\n",
      "Total Loss: 0.0656880815513432\n",
      "------------------------------------ epoch 9153 (54912 steps) ------------------------------------\n",
      "Max loss: 0.02578463777899742\n",
      "Min loss: 0.005263324826955795\n",
      "Mean loss: 0.010508105314026276\n",
      "Std loss: 0.007661469716703561\n",
      "Total Loss: 0.06304863188415766\n",
      "------------------------------------ epoch 9154 (54918 steps) ------------------------------------\n",
      "Max loss: 0.01567959412932396\n",
      "Min loss: 0.0053696236573159695\n",
      "Mean loss: 0.010042617795988917\n",
      "Std loss: 0.0036117896589392654\n",
      "Total Loss: 0.060255706775933504\n",
      "------------------------------------ epoch 9155 (54924 steps) ------------------------------------\n",
      "Max loss: 0.02854544296860695\n",
      "Min loss: 0.004562731366604567\n",
      "Mean loss: 0.014391174151872596\n",
      "Std loss: 0.008713410779474313\n",
      "Total Loss: 0.08634704491123557\n",
      "------------------------------------ epoch 9156 (54930 steps) ------------------------------------\n",
      "Max loss: 0.016050100326538086\n",
      "Min loss: 0.004307636991143227\n",
      "Mean loss: 0.0073760030791163445\n",
      "Std loss: 0.00396909818123301\n",
      "Total Loss: 0.04425601847469807\n",
      "------------------------------------ epoch 9157 (54936 steps) ------------------------------------\n",
      "Max loss: 0.027913128957152367\n",
      "Min loss: 0.0059720175340771675\n",
      "Mean loss: 0.013578690510864059\n",
      "Std loss: 0.007458836723182512\n",
      "Total Loss: 0.08147214306518435\n",
      "------------------------------------ epoch 9158 (54942 steps) ------------------------------------\n",
      "Max loss: 0.008474055677652359\n",
      "Min loss: 0.004224449396133423\n",
      "Mean loss: 0.006517604536687334\n",
      "Std loss: 0.00165483641087267\n",
      "Total Loss: 0.039105627220124006\n",
      "------------------------------------ epoch 9159 (54948 steps) ------------------------------------\n",
      "Max loss: 0.01942194066941738\n",
      "Min loss: 0.005493945442140102\n",
      "Mean loss: 0.010569916106760502\n",
      "Std loss: 0.0046009588518361256\n",
      "Total Loss: 0.06341949664056301\n",
      "------------------------------------ epoch 9160 (54954 steps) ------------------------------------\n",
      "Max loss: 0.01954166404902935\n",
      "Min loss: 0.0034006978385150433\n",
      "Mean loss: 0.0078128797467798\n",
      "Std loss: 0.005494410661751936\n",
      "Total Loss: 0.0468772784806788\n",
      "------------------------------------ epoch 9161 (54960 steps) ------------------------------------\n",
      "Max loss: 0.017800740897655487\n",
      "Min loss: 0.004399253986775875\n",
      "Mean loss: 0.007724428006137411\n",
      "Std loss: 0.004895161755439404\n",
      "Total Loss: 0.046346568036824465\n",
      "------------------------------------ epoch 9162 (54966 steps) ------------------------------------\n",
      "Max loss: 0.015062178485095501\n",
      "Min loss: 0.005610370542854071\n",
      "Mean loss: 0.009176245735337337\n",
      "Std loss: 0.003867483320628519\n",
      "Total Loss: 0.05505747441202402\n",
      "------------------------------------ epoch 9163 (54972 steps) ------------------------------------\n",
      "Max loss: 0.026386506855487823\n",
      "Min loss: 0.00650613009929657\n",
      "Mean loss: 0.013780180054406324\n",
      "Std loss: 0.007586178175737848\n",
      "Total Loss: 0.08268108032643795\n",
      "------------------------------------ epoch 9164 (54978 steps) ------------------------------------\n",
      "Max loss: 0.05233711749315262\n",
      "Min loss: 0.0062379552982747555\n",
      "Mean loss: 0.017931361449882388\n",
      "Std loss: 0.015794146620536686\n",
      "Total Loss: 0.10758816869929433\n",
      "------------------------------------ epoch 9165 (54984 steps) ------------------------------------\n",
      "Max loss: 0.018970351666212082\n",
      "Min loss: 0.00644781393930316\n",
      "Mean loss: 0.009844899255161485\n",
      "Std loss: 0.004264377426468904\n",
      "Total Loss: 0.059069395530968904\n",
      "------------------------------------ epoch 9166 (54990 steps) ------------------------------------\n",
      "Max loss: 0.026256902143359184\n",
      "Min loss: 0.006068677641451359\n",
      "Mean loss: 0.012081856451307734\n",
      "Std loss: 0.006906671582439648\n",
      "Total Loss: 0.0724911387078464\n",
      "------------------------------------ epoch 9167 (54996 steps) ------------------------------------\n",
      "Max loss: 0.023825718089938164\n",
      "Min loss: 0.00832926481962204\n",
      "Mean loss: 0.015892885625362396\n",
      "Std loss: 0.005394532761678403\n",
      "Total Loss: 0.09535731375217438\n",
      "------------------------------------ epoch 9168 (55002 steps) ------------------------------------\n",
      "Max loss: 0.016435839235782623\n",
      "Min loss: 0.005832072347402573\n",
      "Mean loss: 0.011285697420438131\n",
      "Std loss: 0.004119622566024326\n",
      "Total Loss: 0.06771418452262878\n",
      "------------------------------------ epoch 9169 (55008 steps) ------------------------------------\n",
      "Max loss: 0.07270058989524841\n",
      "Min loss: 0.006577444728463888\n",
      "Mean loss: 0.023054533172398806\n",
      "Std loss: 0.022741126653423288\n",
      "Total Loss: 0.13832719903439283\n",
      "------------------------------------ epoch 9170 (55014 steps) ------------------------------------\n",
      "Max loss: 0.0457320474088192\n",
      "Min loss: 0.011356664821505547\n",
      "Mean loss: 0.020815987139940262\n",
      "Std loss: 0.011723642311551903\n",
      "Total Loss: 0.12489592283964157\n",
      "------------------------------------ epoch 9171 (55020 steps) ------------------------------------\n",
      "Max loss: 0.029276950284838676\n",
      "Min loss: 0.008439891040325165\n",
      "Mean loss: 0.01740477855006854\n",
      "Std loss: 0.006315221027974408\n",
      "Total Loss: 0.10442867130041122\n",
      "------------------------------------ epoch 9172 (55026 steps) ------------------------------------\n",
      "Max loss: 0.05772390589118004\n",
      "Min loss: 0.010708976536989212\n",
      "Mean loss: 0.026053544133901596\n",
      "Std loss: 0.015945898772991254\n",
      "Total Loss: 0.15632126480340958\n",
      "------------------------------------ epoch 9173 (55032 steps) ------------------------------------\n",
      "Max loss: 0.023324422538280487\n",
      "Min loss: 0.007162850350141525\n",
      "Mean loss: 0.012057509583731493\n",
      "Std loss: 0.005487376633175938\n",
      "Total Loss: 0.07234505750238895\n",
      "------------------------------------ epoch 9174 (55038 steps) ------------------------------------\n",
      "Max loss: 0.019057368859648705\n",
      "Min loss: 0.0055958665907382965\n",
      "Mean loss: 0.01351663563400507\n",
      "Std loss: 0.004567347994187626\n",
      "Total Loss: 0.08109981380403042\n",
      "------------------------------------ epoch 9175 (55044 steps) ------------------------------------\n",
      "Max loss: 0.014493593946099281\n",
      "Min loss: 0.007216194178909063\n",
      "Mean loss: 0.009025076016162833\n",
      "Std loss: 0.0025598395898231148\n",
      "Total Loss: 0.054150456096976995\n",
      "------------------------------------ epoch 9176 (55050 steps) ------------------------------------\n",
      "Max loss: 0.011953040026128292\n",
      "Min loss: 0.004465624690055847\n",
      "Mean loss: 0.007745089164624612\n",
      "Std loss: 0.0025323193348352666\n",
      "Total Loss: 0.04647053498774767\n",
      "------------------------------------ epoch 9177 (55056 steps) ------------------------------------\n",
      "Max loss: 0.014945249073207378\n",
      "Min loss: 0.004272493999451399\n",
      "Mean loss: 0.0085217683420827\n",
      "Std loss: 0.003288923274834584\n",
      "Total Loss: 0.051130610052496195\n",
      "------------------------------------ epoch 9178 (55062 steps) ------------------------------------\n",
      "Max loss: 0.01738855056464672\n",
      "Min loss: 0.004586654715240002\n",
      "Mean loss: 0.009412862981359163\n",
      "Std loss: 0.004221835546467888\n",
      "Total Loss: 0.056477177888154984\n",
      "------------------------------------ epoch 9179 (55068 steps) ------------------------------------\n",
      "Max loss: 0.01699911057949066\n",
      "Min loss: 0.0040673608891665936\n",
      "Mean loss: 0.0076222549347827835\n",
      "Std loss: 0.004711331031796056\n",
      "Total Loss: 0.0457335296086967\n",
      "------------------------------------ epoch 9180 (55074 steps) ------------------------------------\n",
      "Max loss: 0.005951151251792908\n",
      "Min loss: 0.004293038975447416\n",
      "Mean loss: 0.0051940154905120535\n",
      "Std loss: 0.0006149545636643927\n",
      "Total Loss: 0.03116409294307232\n",
      "------------------------------------ epoch 9181 (55080 steps) ------------------------------------\n",
      "Max loss: 0.031241320073604584\n",
      "Min loss: 0.0037498879246413708\n",
      "Mean loss: 0.010879108837495247\n",
      "Std loss: 0.009505695876341444\n",
      "Total Loss: 0.06527465302497149\n",
      "------------------------------------ epoch 9182 (55086 steps) ------------------------------------\n",
      "Max loss: 0.01916332170367241\n",
      "Min loss: 0.004926159977912903\n",
      "Mean loss: 0.00847153893361489\n",
      "Std loss: 0.004840152495121177\n",
      "Total Loss: 0.05082923360168934\n",
      "------------------------------------ epoch 9183 (55092 steps) ------------------------------------\n",
      "Max loss: 0.04361945018172264\n",
      "Min loss: 0.006910987198352814\n",
      "Mean loss: 0.01914100193729003\n",
      "Std loss: 0.015543384991154074\n",
      "Total Loss: 0.1148460116237402\n",
      "------------------------------------ epoch 9184 (55098 steps) ------------------------------------\n",
      "Max loss: 0.03431442379951477\n",
      "Min loss: 0.0062065692618489265\n",
      "Mean loss: 0.014555001010497412\n",
      "Std loss: 0.009839397936512962\n",
      "Total Loss: 0.08733000606298447\n",
      "------------------------------------ epoch 9185 (55104 steps) ------------------------------------\n",
      "Max loss: 0.030851345509290695\n",
      "Min loss: 0.006814758293330669\n",
      "Mean loss: 0.017382076010107994\n",
      "Std loss: 0.0077904675068234345\n",
      "Total Loss: 0.10429245606064796\n",
      "------------------------------------ epoch 9186 (55110 steps) ------------------------------------\n",
      "Max loss: 0.015816781669855118\n",
      "Min loss: 0.0046209851279854774\n",
      "Mean loss: 0.009657773189246655\n",
      "Std loss: 0.004310121414340241\n",
      "Total Loss: 0.05794663913547993\n",
      "------------------------------------ epoch 9187 (55116 steps) ------------------------------------\n",
      "Max loss: 0.018969476222991943\n",
      "Min loss: 0.004586975555866957\n",
      "Mean loss: 0.010644473290691773\n",
      "Std loss: 0.005295235367057769\n",
      "Total Loss: 0.06386683974415064\n",
      "------------------------------------ epoch 9188 (55122 steps) ------------------------------------\n",
      "Max loss: 0.012768394313752651\n",
      "Min loss: 0.004320764914155006\n",
      "Mean loss: 0.0069658514112234116\n",
      "Std loss: 0.002740992566395221\n",
      "Total Loss: 0.04179510846734047\n",
      "------------------------------------ epoch 9189 (55128 steps) ------------------------------------\n",
      "Max loss: 0.008741317316889763\n",
      "Min loss: 0.0048621539026498795\n",
      "Mean loss: 0.006647804674382011\n",
      "Std loss: 0.0013792161543318743\n",
      "Total Loss: 0.03988682804629207\n",
      "------------------------------------ epoch 9190 (55134 steps) ------------------------------------\n",
      "Max loss: 0.029448559507727623\n",
      "Min loss: 0.003562554717063904\n",
      "Mean loss: 0.009536627835283676\n",
      "Std loss: 0.009543407913073506\n",
      "Total Loss: 0.05721976701170206\n",
      "------------------------------------ epoch 9191 (55140 steps) ------------------------------------\n",
      "Max loss: 0.012837979011237621\n",
      "Min loss: 0.003536523785442114\n",
      "Mean loss: 0.00673030992038548\n",
      "Std loss: 0.0030341046817875207\n",
      "Total Loss: 0.04038185952231288\n",
      "------------------------------------ epoch 9192 (55146 steps) ------------------------------------\n",
      "Max loss: 0.02371564507484436\n",
      "Min loss: 0.004522849805653095\n",
      "Mean loss: 0.009645315119996667\n",
      "Std loss: 0.006442131945244958\n",
      "Total Loss: 0.05787189071998\n",
      "------------------------------------ epoch 9193 (55152 steps) ------------------------------------\n",
      "Max loss: 0.02496754750609398\n",
      "Min loss: 0.005953668616712093\n",
      "Mean loss: 0.010854661231860518\n",
      "Std loss: 0.006851262345318979\n",
      "Total Loss: 0.06512796739116311\n",
      "------------------------------------ epoch 9194 (55158 steps) ------------------------------------\n",
      "Max loss: 0.015175948850810528\n",
      "Min loss: 0.005122162867337465\n",
      "Mean loss: 0.008860648997748891\n",
      "Std loss: 0.003959052357853185\n",
      "Total Loss: 0.05316389398649335\n",
      "------------------------------------ epoch 9195 (55164 steps) ------------------------------------\n",
      "Max loss: 0.01427246630191803\n",
      "Min loss: 0.004635391756892204\n",
      "Mean loss: 0.009600045314679543\n",
      "Std loss: 0.0036356880622258575\n",
      "Total Loss: 0.05760027188807726\n",
      "------------------------------------ epoch 9196 (55170 steps) ------------------------------------\n",
      "Max loss: 0.014416041783988476\n",
      "Min loss: 0.0047592781484127045\n",
      "Mean loss: 0.007848888946076235\n",
      "Std loss: 0.003217403792894795\n",
      "Total Loss: 0.047093333676457405\n",
      "------------------------------------ epoch 9197 (55176 steps) ------------------------------------\n",
      "Max loss: 0.014400068670511246\n",
      "Min loss: 0.003839392215013504\n",
      "Mean loss: 0.007209352916106582\n",
      "Std loss: 0.003716723742995046\n",
      "Total Loss: 0.04325611749663949\n",
      "------------------------------------ epoch 9198 (55182 steps) ------------------------------------\n",
      "Max loss: 0.028886806219816208\n",
      "Min loss: 0.0038427626714110374\n",
      "Mean loss: 0.013490505826969942\n",
      "Std loss: 0.008333795837689725\n",
      "Total Loss: 0.08094303496181965\n",
      "------------------------------------ epoch 9199 (55188 steps) ------------------------------------\n",
      "Max loss: 0.023645242676138878\n",
      "Min loss: 0.005021682940423489\n",
      "Mean loss: 0.011668124391386906\n",
      "Std loss: 0.00763511366819102\n",
      "Total Loss: 0.07000874634832144\n",
      "------------------------------------ epoch 9200 (55194 steps) ------------------------------------\n",
      "Max loss: 0.01808791421353817\n",
      "Min loss: 0.0044162943959236145\n",
      "Mean loss: 0.009312117705121636\n",
      "Std loss: 0.005120664765908073\n",
      "Total Loss: 0.05587270623072982\n",
      "------------------------------------ epoch 9201 (55200 steps) ------------------------------------\n",
      "Max loss: 0.015744773671030998\n",
      "Min loss: 0.003202450927346945\n",
      "Mean loss: 0.006391953444108367\n",
      "Std loss: 0.0042292377959485\n",
      "Total Loss: 0.0383517206646502\n",
      "saved model at ./weights/model_9201.pth\n",
      "------------------------------------ epoch 9202 (55206 steps) ------------------------------------\n",
      "Max loss: 0.0099108275026083\n",
      "Min loss: 0.00457137543708086\n",
      "Mean loss: 0.006660178381328781\n",
      "Std loss: 0.001923810990282402\n",
      "Total Loss: 0.03996107028797269\n",
      "------------------------------------ epoch 9203 (55212 steps) ------------------------------------\n",
      "Max loss: 0.016288090497255325\n",
      "Min loss: 0.0039951917715370655\n",
      "Mean loss: 0.0076379467112322645\n",
      "Std loss: 0.004063830536708417\n",
      "Total Loss: 0.04582768026739359\n",
      "------------------------------------ epoch 9204 (55218 steps) ------------------------------------\n",
      "Max loss: 0.0190484169870615\n",
      "Min loss: 0.004869482479989529\n",
      "Mean loss: 0.008587504892299572\n",
      "Std loss: 0.004889637982593862\n",
      "Total Loss: 0.051525029353797436\n",
      "------------------------------------ epoch 9205 (55224 steps) ------------------------------------\n",
      "Max loss: 0.01983463019132614\n",
      "Min loss: 0.004954565316438675\n",
      "Mean loss: 0.010148196869219342\n",
      "Std loss: 0.004712013341089826\n",
      "Total Loss: 0.06088918121531606\n",
      "------------------------------------ epoch 9206 (55230 steps) ------------------------------------\n",
      "Max loss: 0.00860078539699316\n",
      "Min loss: 0.004799108952283859\n",
      "Mean loss: 0.00650994828902185\n",
      "Std loss: 0.0013248533781254146\n",
      "Total Loss: 0.0390596897341311\n",
      "------------------------------------ epoch 9207 (55236 steps) ------------------------------------\n",
      "Max loss: 0.024007778614759445\n",
      "Min loss: 0.0049562337808310986\n",
      "Mean loss: 0.010388513328507543\n",
      "Std loss: 0.006878212726221013\n",
      "Total Loss: 0.062331079971045256\n",
      "------------------------------------ epoch 9208 (55242 steps) ------------------------------------\n",
      "Max loss: 0.0075331637635827065\n",
      "Min loss: 0.004366263281553984\n",
      "Mean loss: 0.0060976347886025906\n",
      "Std loss: 0.0012558866545296827\n",
      "Total Loss: 0.03658580873161554\n",
      "------------------------------------ epoch 9209 (55248 steps) ------------------------------------\n",
      "Max loss: 0.028288941830396652\n",
      "Min loss: 0.005326141603291035\n",
      "Mean loss: 0.010969591016570726\n",
      "Std loss: 0.00788107551560547\n",
      "Total Loss: 0.06581754609942436\n",
      "------------------------------------ epoch 9210 (55254 steps) ------------------------------------\n",
      "Max loss: 0.011896839365363121\n",
      "Min loss: 0.003392869373783469\n",
      "Mean loss: 0.006808305042795837\n",
      "Std loss: 0.0030028624430432707\n",
      "Total Loss: 0.04084983025677502\n",
      "------------------------------------ epoch 9211 (55260 steps) ------------------------------------\n",
      "Max loss: 0.02008543536067009\n",
      "Min loss: 0.0034147906117141247\n",
      "Mean loss: 0.008110634400509298\n",
      "Std loss: 0.005728026101978059\n",
      "Total Loss: 0.04866380640305579\n",
      "------------------------------------ epoch 9212 (55266 steps) ------------------------------------\n",
      "Max loss: 0.03020685538649559\n",
      "Min loss: 0.0041668713092803955\n",
      "Mean loss: 0.01199735957197845\n",
      "Std loss: 0.009074851081330939\n",
      "Total Loss: 0.0719841574318707\n",
      "------------------------------------ epoch 9213 (55272 steps) ------------------------------------\n",
      "Max loss: 0.01314566470682621\n",
      "Min loss: 0.004251293838024139\n",
      "Mean loss: 0.008165036095306277\n",
      "Std loss: 0.003444185160740546\n",
      "Total Loss: 0.048990216571837664\n",
      "------------------------------------ epoch 9214 (55278 steps) ------------------------------------\n",
      "Max loss: 0.007742749061435461\n",
      "Min loss: 0.003929116763174534\n",
      "Mean loss: 0.005441862779359023\n",
      "Std loss: 0.0014449368540191965\n",
      "Total Loss: 0.03265117667615414\n",
      "------------------------------------ epoch 9215 (55284 steps) ------------------------------------\n",
      "Max loss: 0.013774057850241661\n",
      "Min loss: 0.005120646208524704\n",
      "Mean loss: 0.0087746053468436\n",
      "Std loss: 0.003588783964250008\n",
      "Total Loss: 0.0526476320810616\n",
      "------------------------------------ epoch 9216 (55290 steps) ------------------------------------\n",
      "Max loss: 0.010118593461811543\n",
      "Min loss: 0.0035104197449982166\n",
      "Mean loss: 0.005290281337996324\n",
      "Std loss: 0.00227761747316372\n",
      "Total Loss: 0.03174168802797794\n",
      "------------------------------------ epoch 9217 (55296 steps) ------------------------------------\n",
      "Max loss: 0.013103480450809002\n",
      "Min loss: 0.004647891037166119\n",
      "Mean loss: 0.007869232445955276\n",
      "Std loss: 0.002970143236555469\n",
      "Total Loss: 0.04721539467573166\n",
      "------------------------------------ epoch 9218 (55302 steps) ------------------------------------\n",
      "Max loss: 0.011682558804750443\n",
      "Min loss: 0.0029501018580049276\n",
      "Mean loss: 0.007255294709466398\n",
      "Std loss: 0.003065552441264078\n",
      "Total Loss: 0.04353176825679839\n",
      "------------------------------------ epoch 9219 (55308 steps) ------------------------------------\n",
      "Max loss: 0.014923091977834702\n",
      "Min loss: 0.004725828766822815\n",
      "Mean loss: 0.009674061089754105\n",
      "Std loss: 0.004402672521670448\n",
      "Total Loss: 0.05804436653852463\n",
      "------------------------------------ epoch 9220 (55314 steps) ------------------------------------\n",
      "Max loss: 0.010853111743927002\n",
      "Min loss: 0.004970989655703306\n",
      "Mean loss: 0.007752888913576801\n",
      "Std loss: 0.0020273837875980755\n",
      "Total Loss: 0.04651733348146081\n",
      "------------------------------------ epoch 9221 (55320 steps) ------------------------------------\n",
      "Max loss: 0.019512414932250977\n",
      "Min loss: 0.004265571013092995\n",
      "Mean loss: 0.007395241487150391\n",
      "Std loss: 0.0054436866770419924\n",
      "Total Loss: 0.044371448922902346\n",
      "------------------------------------ epoch 9222 (55326 steps) ------------------------------------\n",
      "Max loss: 0.006549284793436527\n",
      "Min loss: 0.0034077726304531097\n",
      "Mean loss: 0.0048974511834482355\n",
      "Std loss: 0.0012327403709880632\n",
      "Total Loss: 0.02938470710068941\n",
      "------------------------------------ epoch 9223 (55332 steps) ------------------------------------\n",
      "Max loss: 0.0075882053934037685\n",
      "Min loss: 0.003067533252760768\n",
      "Mean loss: 0.005576634973598023\n",
      "Std loss: 0.0013931650332834247\n",
      "Total Loss: 0.03345980984158814\n",
      "------------------------------------ epoch 9224 (55338 steps) ------------------------------------\n",
      "Max loss: 0.01480216532945633\n",
      "Min loss: 0.002850862918421626\n",
      "Mean loss: 0.005930039682425559\n",
      "Std loss: 0.00405210473776371\n",
      "Total Loss: 0.03558023809455335\n",
      "------------------------------------ epoch 9225 (55344 steps) ------------------------------------\n",
      "Max loss: 0.013833064585924149\n",
      "Min loss: 0.0036303154192864895\n",
      "Mean loss: 0.00748295239948978\n",
      "Std loss: 0.003408533087526663\n",
      "Total Loss: 0.04489771439693868\n",
      "------------------------------------ epoch 9226 (55350 steps) ------------------------------------\n",
      "Max loss: 0.02062472328543663\n",
      "Min loss: 0.004517942201346159\n",
      "Mean loss: 0.008844016818329692\n",
      "Std loss: 0.005828006076055833\n",
      "Total Loss: 0.05306410090997815\n",
      "------------------------------------ epoch 9227 (55356 steps) ------------------------------------\n",
      "Max loss: 0.01033026073127985\n",
      "Min loss: 0.003198342164978385\n",
      "Mean loss: 0.005786633701063693\n",
      "Std loss: 0.002455468169659065\n",
      "Total Loss: 0.034719802206382155\n",
      "------------------------------------ epoch 9228 (55362 steps) ------------------------------------\n",
      "Max loss: 0.0104906614869833\n",
      "Min loss: 0.003233839524909854\n",
      "Mean loss: 0.006305096787400544\n",
      "Std loss: 0.002862886715469679\n",
      "Total Loss: 0.03783058072440326\n",
      "------------------------------------ epoch 9229 (55368 steps) ------------------------------------\n",
      "Max loss: 0.009359820745885372\n",
      "Min loss: 0.00448593869805336\n",
      "Mean loss: 0.005702795460820198\n",
      "Std loss: 0.001655401162859304\n",
      "Total Loss: 0.03421677276492119\n",
      "------------------------------------ epoch 9230 (55374 steps) ------------------------------------\n",
      "Max loss: 0.004964276682585478\n",
      "Min loss: 0.003391613019630313\n",
      "Mean loss: 0.0039003293107574186\n",
      "Std loss: 0.0005069951143447999\n",
      "Total Loss: 0.02340197586454451\n",
      "------------------------------------ epoch 9231 (55380 steps) ------------------------------------\n",
      "Max loss: 0.008316459134221077\n",
      "Min loss: 0.004549088887870312\n",
      "Mean loss: 0.005957075084249179\n",
      "Std loss: 0.0012994636535117907\n",
      "Total Loss: 0.03574245050549507\n",
      "------------------------------------ epoch 9232 (55386 steps) ------------------------------------\n",
      "Max loss: 0.013052321039140224\n",
      "Min loss: 0.0030395181383937597\n",
      "Mean loss: 0.005529176987086733\n",
      "Std loss: 0.0034164151616170855\n",
      "Total Loss: 0.0331750619225204\n",
      "------------------------------------ epoch 9233 (55392 steps) ------------------------------------\n",
      "Max loss: 0.008936973288655281\n",
      "Min loss: 0.003150233533233404\n",
      "Mean loss: 0.0050005879408369465\n",
      "Std loss: 0.0019668039227691177\n",
      "Total Loss: 0.030003527645021677\n",
      "------------------------------------ epoch 9234 (55398 steps) ------------------------------------\n",
      "Max loss: 0.011454761028289795\n",
      "Min loss: 0.0030486881732940674\n",
      "Mean loss: 0.006993785655746858\n",
      "Std loss: 0.0033700579418964554\n",
      "Total Loss: 0.041962713934481144\n",
      "------------------------------------ epoch 9235 (55404 steps) ------------------------------------\n",
      "Max loss: 0.0062704081647098064\n",
      "Min loss: 0.0035527280997484922\n",
      "Mean loss: 0.004689559224061668\n",
      "Std loss: 0.0010092678408859846\n",
      "Total Loss: 0.028137355344370008\n",
      "------------------------------------ epoch 9236 (55410 steps) ------------------------------------\n",
      "Max loss: 0.020242970436811447\n",
      "Min loss: 0.0038018114864826202\n",
      "Mean loss: 0.0089276401946942\n",
      "Std loss: 0.005644357316157759\n",
      "Total Loss: 0.05356584116816521\n",
      "------------------------------------ epoch 9237 (55416 steps) ------------------------------------\n",
      "Max loss: 0.029450207948684692\n",
      "Min loss: 0.0047242771834135056\n",
      "Mean loss: 0.01041040662676096\n",
      "Std loss: 0.008909403844348292\n",
      "Total Loss: 0.06246243976056576\n",
      "------------------------------------ epoch 9238 (55422 steps) ------------------------------------\n",
      "Max loss: 0.012013046070933342\n",
      "Min loss: 0.005401815287768841\n",
      "Mean loss: 0.009232408522317806\n",
      "Std loss: 0.0024352266141554743\n",
      "Total Loss: 0.05539445113390684\n",
      "------------------------------------ epoch 9239 (55428 steps) ------------------------------------\n",
      "Max loss: 0.010423439554870129\n",
      "Min loss: 0.004387381486594677\n",
      "Mean loss: 0.006621211689586441\n",
      "Std loss: 0.002138122478800869\n",
      "Total Loss: 0.039727270137518644\n",
      "------------------------------------ epoch 9240 (55434 steps) ------------------------------------\n",
      "Max loss: 0.027638401836156845\n",
      "Min loss: 0.007796827703714371\n",
      "Mean loss: 0.012814159815510115\n",
      "Std loss: 0.0071615914193993\n",
      "Total Loss: 0.07688495889306068\n",
      "------------------------------------ epoch 9241 (55440 steps) ------------------------------------\n",
      "Max loss: 0.03596179932355881\n",
      "Min loss: 0.004097356460988522\n",
      "Mean loss: 0.014192142058163881\n",
      "Std loss: 0.010295726364544087\n",
      "Total Loss: 0.08515285234898329\n",
      "------------------------------------ epoch 9242 (55446 steps) ------------------------------------\n",
      "Max loss: 0.02050379291176796\n",
      "Min loss: 0.003579219803214073\n",
      "Mean loss: 0.00849396944977343\n",
      "Std loss: 0.006057069504597961\n",
      "Total Loss: 0.050963816698640585\n",
      "------------------------------------ epoch 9243 (55452 steps) ------------------------------------\n",
      "Max loss: 0.010531551204621792\n",
      "Min loss: 0.005996211431920528\n",
      "Mean loss: 0.007621984928846359\n",
      "Std loss: 0.0014186774723199099\n",
      "Total Loss: 0.045731909573078156\n",
      "------------------------------------ epoch 9244 (55458 steps) ------------------------------------\n",
      "Max loss: 0.013279787264764309\n",
      "Min loss: 0.003125392599031329\n",
      "Mean loss: 0.007731735589914024\n",
      "Std loss: 0.003822157885785149\n",
      "Total Loss: 0.04639041353948414\n",
      "------------------------------------ epoch 9245 (55464 steps) ------------------------------------\n",
      "Max loss: 0.02314905636012554\n",
      "Min loss: 0.004100732505321503\n",
      "Mean loss: 0.008566918317228556\n",
      "Std loss: 0.006628256855792029\n",
      "Total Loss: 0.051401509903371334\n",
      "------------------------------------ epoch 9246 (55470 steps) ------------------------------------\n",
      "Max loss: 0.018632907420396805\n",
      "Min loss: 0.003842177800834179\n",
      "Mean loss: 0.007398662700628241\n",
      "Std loss: 0.005128928765639003\n",
      "Total Loss: 0.044391976203769445\n",
      "------------------------------------ epoch 9247 (55476 steps) ------------------------------------\n",
      "Max loss: 0.014232402667403221\n",
      "Min loss: 0.004356454126536846\n",
      "Mean loss: 0.009597074706107378\n",
      "Std loss: 0.003953263561621468\n",
      "Total Loss: 0.05758244823664427\n",
      "------------------------------------ epoch 9248 (55482 steps) ------------------------------------\n",
      "Max loss: 0.0059530301950871944\n",
      "Min loss: 0.003748697228729725\n",
      "Mean loss: 0.00484974371890227\n",
      "Std loss: 0.0007974043960158293\n",
      "Total Loss: 0.02909846231341362\n",
      "------------------------------------ epoch 9249 (55488 steps) ------------------------------------\n",
      "Max loss: 0.014216735027730465\n",
      "Min loss: 0.0037369574420154095\n",
      "Mean loss: 0.008321258472278714\n",
      "Std loss: 0.0032580421201458373\n",
      "Total Loss: 0.049927550833672285\n",
      "------------------------------------ epoch 9250 (55494 steps) ------------------------------------\n",
      "Max loss: 0.022623982280492783\n",
      "Min loss: 0.004240295849740505\n",
      "Mean loss: 0.008611544190595547\n",
      "Std loss: 0.0063851365109064626\n",
      "Total Loss: 0.051669265143573284\n",
      "------------------------------------ epoch 9251 (55500 steps) ------------------------------------\n",
      "Max loss: 0.010451579466462135\n",
      "Min loss: 0.004073176067322493\n",
      "Mean loss: 0.006748320534825325\n",
      "Std loss: 0.002056171468378396\n",
      "Total Loss: 0.04048992320895195\n",
      "------------------------------------ epoch 9252 (55506 steps) ------------------------------------\n",
      "Max loss: 0.021790482103824615\n",
      "Min loss: 0.0031967409886419773\n",
      "Mean loss: 0.00994986086152494\n",
      "Std loss: 0.00716138066016157\n",
      "Total Loss: 0.05969916516914964\n",
      "------------------------------------ epoch 9253 (55512 steps) ------------------------------------\n",
      "Max loss: 0.015573214739561081\n",
      "Min loss: 0.004919571336358786\n",
      "Mean loss: 0.009228590798253814\n",
      "Std loss: 0.003697227447085351\n",
      "Total Loss: 0.055371544789522886\n",
      "------------------------------------ epoch 9254 (55518 steps) ------------------------------------\n",
      "Max loss: 0.015240106731653214\n",
      "Min loss: 0.0069158365949988365\n",
      "Mean loss: 0.009726423770189285\n",
      "Std loss: 0.0028550102418301304\n",
      "Total Loss: 0.05835854262113571\n",
      "------------------------------------ epoch 9255 (55524 steps) ------------------------------------\n",
      "Max loss: 0.010728294961154461\n",
      "Min loss: 0.003589258063584566\n",
      "Mean loss: 0.0058330561344822245\n",
      "Std loss: 0.0025376237121956286\n",
      "Total Loss: 0.03499833680689335\n",
      "------------------------------------ epoch 9256 (55530 steps) ------------------------------------\n",
      "Max loss: 0.020368240773677826\n",
      "Min loss: 0.004441972356289625\n",
      "Mean loss: 0.00876272787960867\n",
      "Std loss: 0.005473337924355279\n",
      "Total Loss: 0.052576367277652025\n",
      "------------------------------------ epoch 9257 (55536 steps) ------------------------------------\n",
      "Max loss: 0.0205994863063097\n",
      "Min loss: 0.004221037961542606\n",
      "Mean loss: 0.00788583280518651\n",
      "Std loss: 0.005739906308295305\n",
      "Total Loss: 0.04731499683111906\n",
      "------------------------------------ epoch 9258 (55542 steps) ------------------------------------\n",
      "Max loss: 0.011961582116782665\n",
      "Min loss: 0.004098941572010517\n",
      "Mean loss: 0.00901915249414742\n",
      "Std loss: 0.0027230135417472007\n",
      "Total Loss: 0.05411491496488452\n",
      "------------------------------------ epoch 9259 (55548 steps) ------------------------------------\n",
      "Max loss: 0.02094639651477337\n",
      "Min loss: 0.004959654062986374\n",
      "Mean loss: 0.012032408577700457\n",
      "Std loss: 0.006275800238703928\n",
      "Total Loss: 0.07219445146620274\n",
      "------------------------------------ epoch 9260 (55554 steps) ------------------------------------\n",
      "Max loss: 0.01265626773238182\n",
      "Min loss: 0.003819878678768873\n",
      "Mean loss: 0.007546428280572097\n",
      "Std loss: 0.003118348364235451\n",
      "Total Loss: 0.04527856968343258\n",
      "------------------------------------ epoch 9261 (55560 steps) ------------------------------------\n",
      "Max loss: 0.008027720265090466\n",
      "Min loss: 0.0039034157525748014\n",
      "Mean loss: 0.005176496575586498\n",
      "Std loss: 0.0014926352260522164\n",
      "Total Loss: 0.031058979453518987\n",
      "------------------------------------ epoch 9262 (55566 steps) ------------------------------------\n",
      "Max loss: 0.01925765722990036\n",
      "Min loss: 0.0033808290027081966\n",
      "Mean loss: 0.009379559584582845\n",
      "Std loss: 0.0050565754585011604\n",
      "Total Loss: 0.05627735750749707\n",
      "------------------------------------ epoch 9263 (55572 steps) ------------------------------------\n",
      "Max loss: 0.03642357140779495\n",
      "Min loss: 0.004939831793308258\n",
      "Mean loss: 0.012238792531813184\n",
      "Std loss: 0.011170844363151241\n",
      "Total Loss: 0.0734327551908791\n",
      "------------------------------------ epoch 9264 (55578 steps) ------------------------------------\n",
      "Max loss: 0.013417567126452923\n",
      "Min loss: 0.004902582615613937\n",
      "Mean loss: 0.00838971797687312\n",
      "Std loss: 0.003165161105507178\n",
      "Total Loss: 0.05033830786123872\n",
      "------------------------------------ epoch 9265 (55584 steps) ------------------------------------\n",
      "Max loss: 0.010354951024055481\n",
      "Min loss: 0.004661971237510443\n",
      "Mean loss: 0.007146919068569939\n",
      "Std loss: 0.002146255589406945\n",
      "Total Loss: 0.04288151441141963\n",
      "------------------------------------ epoch 9266 (55590 steps) ------------------------------------\n",
      "Max loss: 0.008695008233189583\n",
      "Min loss: 0.0038469303399324417\n",
      "Mean loss: 0.006090966829409202\n",
      "Std loss: 0.0018878933854456392\n",
      "Total Loss: 0.03654580097645521\n",
      "------------------------------------ epoch 9267 (55596 steps) ------------------------------------\n",
      "Max loss: 0.013534052297472954\n",
      "Min loss: 0.0038690499495714903\n",
      "Mean loss: 0.007623944547958672\n",
      "Std loss: 0.003587593345789186\n",
      "Total Loss: 0.04574366728775203\n",
      "------------------------------------ epoch 9268 (55602 steps) ------------------------------------\n",
      "Max loss: 0.009862685576081276\n",
      "Min loss: 0.004883142188191414\n",
      "Mean loss: 0.006921352818608284\n",
      "Std loss: 0.0015081628927832407\n",
      "Total Loss: 0.041528116911649704\n",
      "------------------------------------ epoch 9269 (55608 steps) ------------------------------------\n",
      "Max loss: 0.039917003363370895\n",
      "Min loss: 0.0035935095511376858\n",
      "Mean loss: 0.010610527358949184\n",
      "Std loss: 0.013120483845703234\n",
      "Total Loss: 0.0636631641536951\n",
      "------------------------------------ epoch 9270 (55614 steps) ------------------------------------\n",
      "Max loss: 0.010119715705513954\n",
      "Min loss: 0.004267745651304722\n",
      "Mean loss: 0.006870272569358349\n",
      "Std loss: 0.0020370374682967796\n",
      "Total Loss: 0.04122163541615009\n",
      "------------------------------------ epoch 9271 (55620 steps) ------------------------------------\n",
      "Max loss: 0.01857166923582554\n",
      "Min loss: 0.005558279808610678\n",
      "Mean loss: 0.01046076494579514\n",
      "Std loss: 0.004824243173697006\n",
      "Total Loss: 0.06276458967477083\n",
      "------------------------------------ epoch 9272 (55626 steps) ------------------------------------\n",
      "Max loss: 0.010055292397737503\n",
      "Min loss: 0.005460984073579311\n",
      "Mean loss: 0.0070541043144961195\n",
      "Std loss: 0.0015248833708638478\n",
      "Total Loss: 0.04232462588697672\n",
      "------------------------------------ epoch 9273 (55632 steps) ------------------------------------\n",
      "Max loss: 0.018193552270531654\n",
      "Min loss: 0.004718293435871601\n",
      "Mean loss: 0.009424884260321656\n",
      "Std loss: 0.004785020780989843\n",
      "Total Loss: 0.05654930556192994\n",
      "------------------------------------ epoch 9274 (55638 steps) ------------------------------------\n",
      "Max loss: 0.0526408776640892\n",
      "Min loss: 0.006737700663506985\n",
      "Mean loss: 0.01624894157672922\n",
      "Std loss: 0.016358030148278847\n",
      "Total Loss: 0.09749364946037531\n",
      "------------------------------------ epoch 9275 (55644 steps) ------------------------------------\n",
      "Max loss: 0.021302398294210434\n",
      "Min loss: 0.007625434547662735\n",
      "Mean loss: 0.011715514895816645\n",
      "Std loss: 0.005186385613754143\n",
      "Total Loss: 0.07029308937489986\n",
      "------------------------------------ epoch 9276 (55650 steps) ------------------------------------\n",
      "Max loss: 0.02312236651778221\n",
      "Min loss: 0.00654895044863224\n",
      "Mean loss: 0.012324330396950245\n",
      "Std loss: 0.006517347166790608\n",
      "Total Loss: 0.07394598238170147\n",
      "------------------------------------ epoch 9277 (55656 steps) ------------------------------------\n",
      "Max loss: 0.022603709250688553\n",
      "Min loss: 0.00471703615039587\n",
      "Mean loss: 0.009982349428658685\n",
      "Std loss: 0.005890287671139696\n",
      "Total Loss: 0.059894096571952105\n",
      "------------------------------------ epoch 9278 (55662 steps) ------------------------------------\n",
      "Max loss: 0.019445015117526054\n",
      "Min loss: 0.005533577874302864\n",
      "Mean loss: 0.011322807365407547\n",
      "Std loss: 0.0055804353088479116\n",
      "Total Loss: 0.06793684419244528\n",
      "------------------------------------ epoch 9279 (55668 steps) ------------------------------------\n",
      "Max loss: 0.02624104917049408\n",
      "Min loss: 0.0041292039677500725\n",
      "Mean loss: 0.011827363632619381\n",
      "Std loss: 0.007498323186921587\n",
      "Total Loss: 0.07096418179571629\n",
      "------------------------------------ epoch 9280 (55674 steps) ------------------------------------\n",
      "Max loss: 0.011047087609767914\n",
      "Min loss: 0.004472586791962385\n",
      "Mean loss: 0.007408417606105407\n",
      "Std loss: 0.0024855581618284453\n",
      "Total Loss: 0.04445050563663244\n",
      "------------------------------------ epoch 9281 (55680 steps) ------------------------------------\n",
      "Max loss: 0.014149765484035015\n",
      "Min loss: 0.004026350565254688\n",
      "Mean loss: 0.007306861303125818\n",
      "Std loss: 0.003265020262803253\n",
      "Total Loss: 0.04384116781875491\n",
      "------------------------------------ epoch 9282 (55686 steps) ------------------------------------\n",
      "Max loss: 0.014459215104579926\n",
      "Min loss: 0.003948762081563473\n",
      "Mean loss: 0.007246690957496564\n",
      "Std loss: 0.0038423687064821303\n",
      "Total Loss: 0.04348014574497938\n",
      "------------------------------------ epoch 9283 (55692 steps) ------------------------------------\n",
      "Max loss: 0.01888785883784294\n",
      "Min loss: 0.004017333500087261\n",
      "Mean loss: 0.009315714007243514\n",
      "Std loss: 0.005068931075202988\n",
      "Total Loss: 0.055894284043461084\n",
      "------------------------------------ epoch 9284 (55698 steps) ------------------------------------\n",
      "Max loss: 0.03380099684000015\n",
      "Min loss: 0.0046457648277282715\n",
      "Mean loss: 0.011336385582884153\n",
      "Std loss: 0.010212397685797847\n",
      "Total Loss: 0.06801831349730492\n",
      "------------------------------------ epoch 9285 (55704 steps) ------------------------------------\n",
      "Max loss: 0.014194773510098457\n",
      "Min loss: 0.0041870116256177425\n",
      "Mean loss: 0.00844991502041618\n",
      "Std loss: 0.0036514949012104248\n",
      "Total Loss: 0.05069949012249708\n",
      "------------------------------------ epoch 9286 (55710 steps) ------------------------------------\n",
      "Max loss: 0.020775113254785538\n",
      "Min loss: 0.004539117682725191\n",
      "Mean loss: 0.009178280054281155\n",
      "Std loss: 0.005643486788305256\n",
      "Total Loss: 0.05506968032568693\n",
      "------------------------------------ epoch 9287 (55716 steps) ------------------------------------\n",
      "Max loss: 0.027166977524757385\n",
      "Min loss: 0.005141901783645153\n",
      "Mean loss: 0.012459170306101441\n",
      "Std loss: 0.007033894865307298\n",
      "Total Loss: 0.07475502183660865\n",
      "------------------------------------ epoch 9288 (55722 steps) ------------------------------------\n",
      "Max loss: 0.021230194717645645\n",
      "Min loss: 0.0050589097663760185\n",
      "Mean loss: 0.01092264234709243\n",
      "Std loss: 0.0059980228002370515\n",
      "Total Loss: 0.06553585408255458\n",
      "------------------------------------ epoch 9289 (55728 steps) ------------------------------------\n",
      "Max loss: 0.025682171806693077\n",
      "Min loss: 0.004560002125799656\n",
      "Mean loss: 0.013357164183010658\n",
      "Std loss: 0.007840491255225615\n",
      "Total Loss: 0.08014298509806395\n",
      "------------------------------------ epoch 9290 (55734 steps) ------------------------------------\n",
      "Max loss: 0.04165511578321457\n",
      "Min loss: 0.004807210527360439\n",
      "Mean loss: 0.015851219029476244\n",
      "Std loss: 0.014077916814445287\n",
      "Total Loss: 0.09510731417685747\n",
      "------------------------------------ epoch 9291 (55740 steps) ------------------------------------\n",
      "Max loss: 0.009011656045913696\n",
      "Min loss: 0.004119588993489742\n",
      "Mean loss: 0.006683541772266229\n",
      "Std loss: 0.0018541601663976033\n",
      "Total Loss: 0.040101250633597374\n",
      "------------------------------------ epoch 9292 (55746 steps) ------------------------------------\n",
      "Max loss: 0.017249146476387978\n",
      "Min loss: 0.005688993260264397\n",
      "Mean loss: 0.009492762852460146\n",
      "Std loss: 0.004115314225221841\n",
      "Total Loss: 0.056956577114760876\n",
      "------------------------------------ epoch 9293 (55752 steps) ------------------------------------\n",
      "Max loss: 0.009121733717620373\n",
      "Min loss: 0.004520543850958347\n",
      "Mean loss: 0.006031749226773779\n",
      "Std loss: 0.0015348557710997467\n",
      "Total Loss: 0.03619049536064267\n",
      "------------------------------------ epoch 9294 (55758 steps) ------------------------------------\n",
      "Max loss: 0.01599753461778164\n",
      "Min loss: 0.005872204899787903\n",
      "Mean loss: 0.01037942369778951\n",
      "Std loss: 0.0037859016524962293\n",
      "Total Loss: 0.06227654218673706\n",
      "------------------------------------ epoch 9295 (55764 steps) ------------------------------------\n",
      "Max loss: 0.008102933876216412\n",
      "Min loss: 0.005715671926736832\n",
      "Mean loss: 0.006895488516117136\n",
      "Std loss: 0.0009126021451712421\n",
      "Total Loss: 0.041372931096702814\n",
      "------------------------------------ epoch 9296 (55770 steps) ------------------------------------\n",
      "Max loss: 0.01301741972565651\n",
      "Min loss: 0.0038651740178465843\n",
      "Mean loss: 0.006802669493481517\n",
      "Std loss: 0.003132183456878797\n",
      "Total Loss: 0.0408160169608891\n",
      "------------------------------------ epoch 9297 (55776 steps) ------------------------------------\n",
      "Max loss: 0.01321286242455244\n",
      "Min loss: 0.003923281095921993\n",
      "Mean loss: 0.006843347335234284\n",
      "Std loss: 0.003028107537402883\n",
      "Total Loss: 0.041060084011405706\n",
      "------------------------------------ epoch 9298 (55782 steps) ------------------------------------\n",
      "Max loss: 0.0145588219165802\n",
      "Min loss: 0.0034974829759448767\n",
      "Mean loss: 0.008671740264010927\n",
      "Std loss: 0.004220202689905038\n",
      "Total Loss: 0.052030441584065557\n",
      "------------------------------------ epoch 9299 (55788 steps) ------------------------------------\n",
      "Max loss: 0.018329285085201263\n",
      "Min loss: 0.0037964386865496635\n",
      "Mean loss: 0.006978014173607032\n",
      "Std loss: 0.005154801823092051\n",
      "Total Loss: 0.04186808504164219\n",
      "------------------------------------ epoch 9300 (55794 steps) ------------------------------------\n",
      "Max loss: 0.016279717907309532\n",
      "Min loss: 0.00351326703093946\n",
      "Mean loss: 0.008879261828648547\n",
      "Std loss: 0.005215657352623759\n",
      "Total Loss: 0.053275570971891284\n",
      "------------------------------------ epoch 9301 (55800 steps) ------------------------------------\n",
      "Max loss: 0.009907704778015614\n",
      "Min loss: 0.003548220731317997\n",
      "Mean loss: 0.006448856166874369\n",
      "Std loss: 0.0025640509714826404\n",
      "Total Loss: 0.038693137001246214\n",
      "saved model at ./weights/model_9301.pth\n",
      "------------------------------------ epoch 9302 (55806 steps) ------------------------------------\n",
      "Max loss: 0.01494662370532751\n",
      "Min loss: 0.0040113115683197975\n",
      "Mean loss: 0.006915976681436102\n",
      "Std loss: 0.0037252831666110113\n",
      "Total Loss: 0.04149586008861661\n",
      "------------------------------------ epoch 9303 (55812 steps) ------------------------------------\n",
      "Max loss: 0.01701522432267666\n",
      "Min loss: 0.00491554057225585\n",
      "Mean loss: 0.010124693857505918\n",
      "Std loss: 0.004092251230394692\n",
      "Total Loss: 0.060748163145035505\n",
      "------------------------------------ epoch 9304 (55818 steps) ------------------------------------\n",
      "Max loss: 0.015275133773684502\n",
      "Min loss: 0.0034856656566262245\n",
      "Mean loss: 0.006648045343657334\n",
      "Std loss: 0.004021910525087021\n",
      "Total Loss: 0.03988827206194401\n",
      "------------------------------------ epoch 9305 (55824 steps) ------------------------------------\n",
      "Max loss: 0.017196670174598694\n",
      "Min loss: 0.004949219524860382\n",
      "Mean loss: 0.008800430145735541\n",
      "Std loss: 0.004089282671037592\n",
      "Total Loss: 0.05280258087441325\n",
      "------------------------------------ epoch 9306 (55830 steps) ------------------------------------\n",
      "Max loss: 0.011325202882289886\n",
      "Min loss: 0.005392024293541908\n",
      "Mean loss: 0.008247874677181244\n",
      "Std loss: 0.0021705243325480347\n",
      "Total Loss: 0.04948724806308746\n",
      "------------------------------------ epoch 9307 (55836 steps) ------------------------------------\n",
      "Max loss: 0.0070788017474114895\n",
      "Min loss: 0.003752100979909301\n",
      "Mean loss: 0.005192880091878275\n",
      "Std loss: 0.0010932050839733353\n",
      "Total Loss: 0.03115728055126965\n",
      "------------------------------------ epoch 9308 (55842 steps) ------------------------------------\n",
      "Max loss: 0.008171875029802322\n",
      "Min loss: 0.003618018701672554\n",
      "Mean loss: 0.00511228844212989\n",
      "Std loss: 0.0015871316895647856\n",
      "Total Loss: 0.03067373065277934\n",
      "------------------------------------ epoch 9309 (55848 steps) ------------------------------------\n",
      "Max loss: 0.031463928520679474\n",
      "Min loss: 0.0037914745043963194\n",
      "Mean loss: 0.010893923890156051\n",
      "Std loss: 0.00941552223175601\n",
      "Total Loss: 0.0653635433409363\n",
      "------------------------------------ epoch 9310 (55854 steps) ------------------------------------\n",
      "Max loss: 0.012640908360481262\n",
      "Min loss: 0.004416896030306816\n",
      "Mean loss: 0.006291524584715565\n",
      "Std loss: 0.0028857972900982044\n",
      "Total Loss: 0.03774914750829339\n",
      "------------------------------------ epoch 9311 (55860 steps) ------------------------------------\n",
      "Max loss: 0.012789441272616386\n",
      "Min loss: 0.004775779787451029\n",
      "Mean loss: 0.006718141259625554\n",
      "Std loss: 0.002757512267177616\n",
      "Total Loss: 0.040308847557753325\n",
      "------------------------------------ epoch 9312 (55866 steps) ------------------------------------\n",
      "Max loss: 0.008088475093245506\n",
      "Min loss: 0.003123728558421135\n",
      "Mean loss: 0.004475906762915353\n",
      "Std loss: 0.0017019300808322113\n",
      "Total Loss: 0.026855440577492118\n",
      "------------------------------------ epoch 9313 (55872 steps) ------------------------------------\n",
      "Max loss: 0.037511587142944336\n",
      "Min loss: 0.004281862638890743\n",
      "Mean loss: 0.011870170555387935\n",
      "Std loss: 0.011797507539089829\n",
      "Total Loss: 0.0712210233323276\n",
      "------------------------------------ epoch 9314 (55878 steps) ------------------------------------\n",
      "Max loss: 0.011846033856272697\n",
      "Min loss: 0.003764756955206394\n",
      "Mean loss: 0.007164576323702931\n",
      "Std loss: 0.0032139036079046365\n",
      "Total Loss: 0.04298745794221759\n",
      "------------------------------------ epoch 9315 (55884 steps) ------------------------------------\n",
      "Max loss: 0.013770061545073986\n",
      "Min loss: 0.003360339906066656\n",
      "Mean loss: 0.005848239447611074\n",
      "Std loss: 0.0036521992713980144\n",
      "Total Loss: 0.03508943668566644\n",
      "------------------------------------ epoch 9316 (55890 steps) ------------------------------------\n",
      "Max loss: 0.008245831355452538\n",
      "Min loss: 0.0038812800776213408\n",
      "Mean loss: 0.005551886861212552\n",
      "Std loss: 0.0014717409459098052\n",
      "Total Loss: 0.03331132116727531\n",
      "------------------------------------ epoch 9317 (55896 steps) ------------------------------------\n",
      "Max loss: 0.00541450222954154\n",
      "Min loss: 0.0039724851958453655\n",
      "Mean loss: 0.00451163745795687\n",
      "Std loss: 0.0005015073570400721\n",
      "Total Loss: 0.027069824747741222\n",
      "------------------------------------ epoch 9318 (55902 steps) ------------------------------------\n",
      "Max loss: 0.01620340719819069\n",
      "Min loss: 0.0028625265695154667\n",
      "Mean loss: 0.006191586027853191\n",
      "Std loss: 0.004619546404738191\n",
      "Total Loss: 0.037149516167119145\n",
      "------------------------------------ epoch 9319 (55908 steps) ------------------------------------\n",
      "Max loss: 0.014811396598815918\n",
      "Min loss: 0.003412597579881549\n",
      "Mean loss: 0.007123064676610132\n",
      "Std loss: 0.003684685572408895\n",
      "Total Loss: 0.04273838805966079\n",
      "------------------------------------ epoch 9320 (55914 steps) ------------------------------------\n",
      "Max loss: 0.006230237893760204\n",
      "Min loss: 0.003866512794047594\n",
      "Mean loss: 0.005135461299990614\n",
      "Std loss: 0.0008151079503530204\n",
      "Total Loss: 0.030812767799943686\n",
      "------------------------------------ epoch 9321 (55920 steps) ------------------------------------\n",
      "Max loss: 0.009580024518072605\n",
      "Min loss: 0.005334201268851757\n",
      "Mean loss: 0.006522784863288204\n",
      "Std loss: 0.001479674609210392\n",
      "Total Loss: 0.03913670917972922\n",
      "------------------------------------ epoch 9322 (55926 steps) ------------------------------------\n",
      "Max loss: 0.008088544011116028\n",
      "Min loss: 0.003602252108976245\n",
      "Mean loss: 0.005318051475721101\n",
      "Std loss: 0.0016230253702882564\n",
      "Total Loss: 0.031908308854326606\n",
      "------------------------------------ epoch 9323 (55932 steps) ------------------------------------\n",
      "Max loss: 0.006013873033225536\n",
      "Min loss: 0.003177754580974579\n",
      "Mean loss: 0.0044867701750869555\n",
      "Std loss: 0.0010183589341732043\n",
      "Total Loss: 0.02692062105052173\n",
      "------------------------------------ epoch 9324 (55938 steps) ------------------------------------\n",
      "Max loss: 0.007323488127440214\n",
      "Min loss: 0.0028477103915065527\n",
      "Mean loss: 0.005064003130731483\n",
      "Std loss: 0.0016924292698094383\n",
      "Total Loss: 0.0303840187843889\n",
      "------------------------------------ epoch 9325 (55944 steps) ------------------------------------\n",
      "Max loss: 0.008982392959296703\n",
      "Min loss: 0.0031708122696727514\n",
      "Mean loss: 0.005546936571287612\n",
      "Std loss: 0.001822694439675745\n",
      "Total Loss: 0.03328161942772567\n",
      "------------------------------------ epoch 9326 (55950 steps) ------------------------------------\n",
      "Max loss: 0.0343346893787384\n",
      "Min loss: 0.003534151939675212\n",
      "Mean loss: 0.014651773769098023\n",
      "Std loss: 0.012294807173803556\n",
      "Total Loss: 0.08791064261458814\n",
      "------------------------------------ epoch 9327 (55956 steps) ------------------------------------\n",
      "Max loss: 0.03689466044306755\n",
      "Min loss: 0.006738957948982716\n",
      "Mean loss: 0.01659738381082813\n",
      "Std loss: 0.011993106612803655\n",
      "Total Loss: 0.09958430286496878\n",
      "------------------------------------ epoch 9328 (55962 steps) ------------------------------------\n",
      "Max loss: 0.010866465047001839\n",
      "Min loss: 0.006356445606797934\n",
      "Mean loss: 0.008924744635199508\n",
      "Std loss: 0.001706741188059604\n",
      "Total Loss: 0.05354846781119704\n",
      "------------------------------------ epoch 9329 (55968 steps) ------------------------------------\n",
      "Max loss: 0.03574170544743538\n",
      "Min loss: 0.011880321428179741\n",
      "Mean loss: 0.021748623965928953\n",
      "Std loss: 0.008140506470045496\n",
      "Total Loss: 0.1304917437955737\n",
      "------------------------------------ epoch 9330 (55974 steps) ------------------------------------\n",
      "Max loss: 0.050603918731212616\n",
      "Min loss: 0.014700964093208313\n",
      "Mean loss: 0.02370262611657381\n",
      "Std loss: 0.012385759617753827\n",
      "Total Loss: 0.14221575669944286\n",
      "------------------------------------ epoch 9331 (55980 steps) ------------------------------------\n",
      "Max loss: 0.025807475671172142\n",
      "Min loss: 0.012146813794970512\n",
      "Mean loss: 0.016805388188610475\n",
      "Std loss: 0.004682441365790026\n",
      "Total Loss: 0.10083232913166285\n",
      "------------------------------------ epoch 9332 (55986 steps) ------------------------------------\n",
      "Max loss: 0.015790730714797974\n",
      "Min loss: 0.011301109567284584\n",
      "Mean loss: 0.012840621483822664\n",
      "Std loss: 0.0015989074104627628\n",
      "Total Loss: 0.07704372890293598\n",
      "------------------------------------ epoch 9333 (55992 steps) ------------------------------------\n",
      "Max loss: 0.0126939183101058\n",
      "Min loss: 0.0077188401482999325\n",
      "Mean loss: 0.0100364217069\n",
      "Std loss: 0.00197215726649825\n",
      "Total Loss: 0.0602185302414\n",
      "------------------------------------ epoch 9334 (55998 steps) ------------------------------------\n",
      "Max loss: 0.013101156800985336\n",
      "Min loss: 0.006727853324264288\n",
      "Mean loss: 0.00983360557196041\n",
      "Std loss: 0.0018456535836892376\n",
      "Total Loss: 0.05900163343176246\n",
      "------------------------------------ epoch 9335 (56004 steps) ------------------------------------\n",
      "Max loss: 0.01058131828904152\n",
      "Min loss: 0.004851524718105793\n",
      "Mean loss: 0.006643783844386538\n",
      "Std loss: 0.0018894335925165769\n",
      "Total Loss: 0.03986270306631923\n",
      "------------------------------------ epoch 9336 (56010 steps) ------------------------------------\n",
      "Max loss: 0.019572343677282333\n",
      "Min loss: 0.004547813441604376\n",
      "Mean loss: 0.008242682398607334\n",
      "Std loss: 0.005146237461279793\n",
      "Total Loss: 0.049456094391644\n",
      "------------------------------------ epoch 9337 (56016 steps) ------------------------------------\n",
      "Max loss: 0.05026380717754364\n",
      "Min loss: 0.0057411957532167435\n",
      "Mean loss: 0.01791173266246915\n",
      "Std loss: 0.015189171021663967\n",
      "Total Loss: 0.10747039597481489\n",
      "------------------------------------ epoch 9338 (56022 steps) ------------------------------------\n",
      "Max loss: 0.015051948837935925\n",
      "Min loss: 0.005407737102359533\n",
      "Mean loss: 0.008644574865077933\n",
      "Std loss: 0.0031490765097379955\n",
      "Total Loss: 0.051867449190467596\n",
      "------------------------------------ epoch 9339 (56028 steps) ------------------------------------\n",
      "Max loss: 0.029665756970643997\n",
      "Min loss: 0.008297152817249298\n",
      "Mean loss: 0.01549004918585221\n",
      "Std loss: 0.007243505847709013\n",
      "Total Loss: 0.09294029511511326\n",
      "------------------------------------ epoch 9340 (56034 steps) ------------------------------------\n",
      "Max loss: 0.02041511982679367\n",
      "Min loss: 0.005517666228115559\n",
      "Mean loss: 0.0100158565522482\n",
      "Std loss: 0.005151869192629258\n",
      "Total Loss: 0.0600951393134892\n",
      "------------------------------------ epoch 9341 (56040 steps) ------------------------------------\n",
      "Max loss: 0.007912304252386093\n",
      "Min loss: 0.005347438156604767\n",
      "Mean loss: 0.00635196299602588\n",
      "Std loss: 0.0010105388116844625\n",
      "Total Loss: 0.03811177797615528\n",
      "------------------------------------ epoch 9342 (56046 steps) ------------------------------------\n",
      "Max loss: 0.015199593268334866\n",
      "Min loss: 0.005220065824687481\n",
      "Mean loss: 0.010542406855771938\n",
      "Std loss: 0.004236056952850894\n",
      "Total Loss: 0.06325444113463163\n",
      "------------------------------------ epoch 9343 (56052 steps) ------------------------------------\n",
      "Max loss: 0.011255303397774696\n",
      "Min loss: 0.004824455827474594\n",
      "Mean loss: 0.006822794598216812\n",
      "Std loss: 0.0020500494971347686\n",
      "Total Loss: 0.04093676758930087\n",
      "------------------------------------ epoch 9344 (56058 steps) ------------------------------------\n",
      "Max loss: 0.014389443211257458\n",
      "Min loss: 0.004040698520839214\n",
      "Mean loss: 0.008400604361668229\n",
      "Std loss: 0.003726306562678706\n",
      "Total Loss: 0.050403626170009375\n",
      "------------------------------------ epoch 9345 (56064 steps) ------------------------------------\n",
      "Max loss: 0.029280856251716614\n",
      "Min loss: 0.007362139876931906\n",
      "Mean loss: 0.012092913578574857\n",
      "Std loss: 0.0077779538342874445\n",
      "Total Loss: 0.07255748147144914\n",
      "------------------------------------ epoch 9346 (56070 steps) ------------------------------------\n",
      "Max loss: 0.026236793026328087\n",
      "Min loss: 0.0059668379835784435\n",
      "Mean loss: 0.011860387166962028\n",
      "Std loss: 0.006710113608863794\n",
      "Total Loss: 0.07116232300177217\n",
      "------------------------------------ epoch 9347 (56076 steps) ------------------------------------\n",
      "Max loss: 0.010448570363223553\n",
      "Min loss: 0.004429786466062069\n",
      "Mean loss: 0.006694980664178729\n",
      "Std loss: 0.0020933541872301727\n",
      "Total Loss: 0.040169883985072374\n",
      "------------------------------------ epoch 9348 (56082 steps) ------------------------------------\n",
      "Max loss: 0.01067012082785368\n",
      "Min loss: 0.005341401323676109\n",
      "Mean loss: 0.008094599548106393\n",
      "Std loss: 0.002117525075692663\n",
      "Total Loss: 0.04856759728863835\n",
      "------------------------------------ epoch 9349 (56088 steps) ------------------------------------\n",
      "Max loss: 0.013666912913322449\n",
      "Min loss: 0.0049713305197656155\n",
      "Mean loss: 0.008571759099140763\n",
      "Std loss: 0.002764274924679238\n",
      "Total Loss: 0.05143055459484458\n",
      "------------------------------------ epoch 9350 (56094 steps) ------------------------------------\n",
      "Max loss: 0.007686575409024954\n",
      "Min loss: 0.003661795984953642\n",
      "Mean loss: 0.005302364278274278\n",
      "Std loss: 0.0014498952919707296\n",
      "Total Loss: 0.03181418566964567\n",
      "------------------------------------ epoch 9351 (56100 steps) ------------------------------------\n",
      "Max loss: 0.03446584194898605\n",
      "Min loss: 0.005561362951993942\n",
      "Mean loss: 0.01145747140981257\n",
      "Std loss: 0.010326924449791556\n",
      "Total Loss: 0.06874482845887542\n",
      "------------------------------------ epoch 9352 (56106 steps) ------------------------------------\n",
      "Max loss: 0.011505248956382275\n",
      "Min loss: 0.0036360935773700476\n",
      "Mean loss: 0.007083805782410006\n",
      "Std loss: 0.0028854572301757133\n",
      "Total Loss: 0.042502834694460034\n",
      "------------------------------------ epoch 9353 (56112 steps) ------------------------------------\n",
      "Max loss: 0.03532254695892334\n",
      "Min loss: 0.004296552389860153\n",
      "Mean loss: 0.011719703519095978\n",
      "Std loss: 0.010708259281715846\n",
      "Total Loss: 0.07031822111457586\n",
      "------------------------------------ epoch 9354 (56118 steps) ------------------------------------\n",
      "Max loss: 0.02574949711561203\n",
      "Min loss: 0.0048449682071805\n",
      "Mean loss: 0.01066229515708983\n",
      "Std loss: 0.007649748091005711\n",
      "Total Loss: 0.06397377094253898\n",
      "------------------------------------ epoch 9355 (56124 steps) ------------------------------------\n",
      "Max loss: 0.011643423698842525\n",
      "Min loss: 0.00444770697504282\n",
      "Mean loss: 0.007330670797576507\n",
      "Std loss: 0.002408703429046605\n",
      "Total Loss: 0.04398402478545904\n",
      "------------------------------------ epoch 9356 (56130 steps) ------------------------------------\n",
      "Max loss: 0.007677064742892981\n",
      "Min loss: 0.004107086919248104\n",
      "Mean loss: 0.005305647850036621\n",
      "Std loss: 0.0011226859101823728\n",
      "Total Loss: 0.03183388710021973\n",
      "------------------------------------ epoch 9357 (56136 steps) ------------------------------------\n",
      "Max loss: 0.021139172837138176\n",
      "Min loss: 0.00457908445969224\n",
      "Mean loss: 0.009869711784025034\n",
      "Std loss: 0.005557698929475313\n",
      "Total Loss: 0.0592182707041502\n",
      "------------------------------------ epoch 9358 (56142 steps) ------------------------------------\n",
      "Max loss: 0.012302402406930923\n",
      "Min loss: 0.004527419339865446\n",
      "Mean loss: 0.006901107185209791\n",
      "Std loss: 0.0025596145171794126\n",
      "Total Loss: 0.041406643111258745\n",
      "------------------------------------ epoch 9359 (56148 steps) ------------------------------------\n",
      "Max loss: 0.01766030862927437\n",
      "Min loss: 0.004732927307486534\n",
      "Mean loss: 0.009528167080134153\n",
      "Std loss: 0.00464193169921177\n",
      "Total Loss: 0.05716900248080492\n",
      "------------------------------------ epoch 9360 (56154 steps) ------------------------------------\n",
      "Max loss: 0.012693138793110847\n",
      "Min loss: 0.004759129136800766\n",
      "Mean loss: 0.006739413210501273\n",
      "Std loss: 0.0026944830515717414\n",
      "Total Loss: 0.04043647926300764\n",
      "------------------------------------ epoch 9361 (56160 steps) ------------------------------------\n",
      "Max loss: 0.016565605998039246\n",
      "Min loss: 0.003998428117483854\n",
      "Mean loss: 0.007692019687965512\n",
      "Std loss: 0.004663264683953463\n",
      "Total Loss: 0.046152118127793074\n",
      "------------------------------------ epoch 9362 (56166 steps) ------------------------------------\n",
      "Max loss: 0.013291679322719574\n",
      "Min loss: 0.003946999087929726\n",
      "Mean loss: 0.007391316428159674\n",
      "Std loss: 0.003517588543331295\n",
      "Total Loss: 0.044347898568958044\n",
      "------------------------------------ epoch 9363 (56172 steps) ------------------------------------\n",
      "Max loss: 0.024020284414291382\n",
      "Min loss: 0.003599004354327917\n",
      "Mean loss: 0.008957608602941036\n",
      "Std loss: 0.007065412161572539\n",
      "Total Loss: 0.05374565161764622\n",
      "------------------------------------ epoch 9364 (56178 steps) ------------------------------------\n",
      "Max loss: 0.011654547415673733\n",
      "Min loss: 0.004518982954323292\n",
      "Mean loss: 0.007765558936322729\n",
      "Std loss: 0.0023043049209458704\n",
      "Total Loss: 0.04659335361793637\n",
      "------------------------------------ epoch 9365 (56184 steps) ------------------------------------\n",
      "Max loss: 0.009878282435238361\n",
      "Min loss: 0.0044001187197864056\n",
      "Mean loss: 0.007371617558722694\n",
      "Std loss: 0.0019891225876731127\n",
      "Total Loss: 0.04422970535233617\n",
      "------------------------------------ epoch 9366 (56190 steps) ------------------------------------\n",
      "Max loss: 0.012429721653461456\n",
      "Min loss: 0.00436711311340332\n",
      "Mean loss: 0.006526982178911567\n",
      "Std loss: 0.002767086967549257\n",
      "Total Loss: 0.0391618930734694\n",
      "------------------------------------ epoch 9367 (56196 steps) ------------------------------------\n",
      "Max loss: 0.022729791700839996\n",
      "Min loss: 0.004118422046303749\n",
      "Mean loss: 0.011276088965435823\n",
      "Std loss: 0.0077495697277155376\n",
      "Total Loss: 0.06765653379261494\n",
      "------------------------------------ epoch 9368 (56202 steps) ------------------------------------\n",
      "Max loss: 0.01323553454130888\n",
      "Min loss: 0.005138325970619917\n",
      "Mean loss: 0.00789192869948844\n",
      "Std loss: 0.002613229531796903\n",
      "Total Loss: 0.04735157219693065\n",
      "------------------------------------ epoch 9369 (56208 steps) ------------------------------------\n",
      "Max loss: 0.013681679964065552\n",
      "Min loss: 0.0032050306908786297\n",
      "Mean loss: 0.006829158480589588\n",
      "Std loss: 0.0037497286333700846\n",
      "Total Loss: 0.04097495088353753\n",
      "------------------------------------ epoch 9370 (56214 steps) ------------------------------------\n",
      "Max loss: 0.032113105058670044\n",
      "Min loss: 0.004347603768110275\n",
      "Mean loss: 0.011027354669446746\n",
      "Std loss: 0.009589002552907221\n",
      "Total Loss: 0.06616412801668048\n",
      "------------------------------------ epoch 9371 (56220 steps) ------------------------------------\n",
      "Max loss: 0.02728361450135708\n",
      "Min loss: 0.0038304640911519527\n",
      "Mean loss: 0.011556426606451472\n",
      "Std loss: 0.008079269564163612\n",
      "Total Loss: 0.06933855963870883\n",
      "------------------------------------ epoch 9372 (56226 steps) ------------------------------------\n",
      "Max loss: 0.019864490255713463\n",
      "Min loss: 0.005137765314429998\n",
      "Mean loss: 0.010966344193244973\n",
      "Std loss: 0.005490349781314715\n",
      "Total Loss: 0.06579806515946984\n",
      "------------------------------------ epoch 9373 (56232 steps) ------------------------------------\n",
      "Max loss: 0.010463843122124672\n",
      "Min loss: 0.004256353713572025\n",
      "Mean loss: 0.0077522943417231245\n",
      "Std loss: 0.002227632413917114\n",
      "Total Loss: 0.046513766050338745\n",
      "------------------------------------ epoch 9374 (56238 steps) ------------------------------------\n",
      "Max loss: 0.012342691421508789\n",
      "Min loss: 0.004562702961266041\n",
      "Mean loss: 0.007034999163200458\n",
      "Std loss: 0.002562656491489499\n",
      "Total Loss: 0.04220999497920275\n",
      "------------------------------------ epoch 9375 (56244 steps) ------------------------------------\n",
      "Max loss: 0.00855835061520338\n",
      "Min loss: 0.004776974208652973\n",
      "Mean loss: 0.006095927441492677\n",
      "Std loss: 0.0014389553883053625\n",
      "Total Loss: 0.03657556464895606\n",
      "------------------------------------ epoch 9376 (56250 steps) ------------------------------------\n",
      "Max loss: 0.009370565414428711\n",
      "Min loss: 0.0034989132545888424\n",
      "Mean loss: 0.005751758581027389\n",
      "Std loss: 0.0020583064121638744\n",
      "Total Loss: 0.03451055148616433\n",
      "------------------------------------ epoch 9377 (56256 steps) ------------------------------------\n",
      "Max loss: 0.02285919152200222\n",
      "Min loss: 0.005106449127197266\n",
      "Mean loss: 0.01181772816926241\n",
      "Std loss: 0.005533093268514066\n",
      "Total Loss: 0.07090636901557446\n",
      "------------------------------------ epoch 9378 (56262 steps) ------------------------------------\n",
      "Max loss: 0.011874370276927948\n",
      "Min loss: 0.003375992877408862\n",
      "Mean loss: 0.006480572163127363\n",
      "Std loss: 0.003096929598759141\n",
      "Total Loss: 0.038883432978764176\n",
      "------------------------------------ epoch 9379 (56268 steps) ------------------------------------\n",
      "Max loss: 0.012556533329188824\n",
      "Min loss: 0.004075653851032257\n",
      "Mean loss: 0.006081896911685665\n",
      "Std loss: 0.0029573265471451228\n",
      "Total Loss: 0.03649138147011399\n",
      "------------------------------------ epoch 9380 (56274 steps) ------------------------------------\n",
      "Max loss: 0.024622034281492233\n",
      "Min loss: 0.004587399773299694\n",
      "Mean loss: 0.010234758025035262\n",
      "Std loss: 0.006994385900053626\n",
      "Total Loss: 0.06140854815021157\n",
      "------------------------------------ epoch 9381 (56280 steps) ------------------------------------\n",
      "Max loss: 0.02079235576093197\n",
      "Min loss: 0.003825174644589424\n",
      "Mean loss: 0.008859674291064342\n",
      "Std loss: 0.005725738101018565\n",
      "Total Loss: 0.05315804574638605\n",
      "------------------------------------ epoch 9382 (56286 steps) ------------------------------------\n",
      "Max loss: 0.007207249291241169\n",
      "Min loss: 0.004976088181138039\n",
      "Mean loss: 0.00585400010459125\n",
      "Std loss: 0.0008259496311161606\n",
      "Total Loss: 0.0351240006275475\n",
      "------------------------------------ epoch 9383 (56292 steps) ------------------------------------\n",
      "Max loss: 0.008799852803349495\n",
      "Min loss: 0.005450235679745674\n",
      "Mean loss: 0.00709495204500854\n",
      "Std loss: 0.0012043237627051586\n",
      "Total Loss: 0.04256971227005124\n",
      "------------------------------------ epoch 9384 (56298 steps) ------------------------------------\n",
      "Max loss: 0.013668419793248177\n",
      "Min loss: 0.004376820754259825\n",
      "Mean loss: 0.0067814983582745\n",
      "Std loss: 0.0031715150572029006\n",
      "Total Loss: 0.040688990149647\n",
      "------------------------------------ epoch 9385 (56304 steps) ------------------------------------\n",
      "Max loss: 0.007637146394699812\n",
      "Min loss: 0.0037019066512584686\n",
      "Mean loss: 0.005738287155206005\n",
      "Std loss: 0.001171942530991609\n",
      "Total Loss: 0.03442972293123603\n",
      "------------------------------------ epoch 9386 (56310 steps) ------------------------------------\n",
      "Max loss: 0.00962466187775135\n",
      "Min loss: 0.004434745758771896\n",
      "Mean loss: 0.006981389907499154\n",
      "Std loss: 0.00194817350908508\n",
      "Total Loss: 0.041888339444994926\n",
      "------------------------------------ epoch 9387 (56316 steps) ------------------------------------\n",
      "Max loss: 0.010468589141964912\n",
      "Min loss: 0.0035886652767658234\n",
      "Mean loss: 0.006138576737915476\n",
      "Std loss: 0.002200539554605226\n",
      "Total Loss: 0.03683146042749286\n",
      "------------------------------------ epoch 9388 (56322 steps) ------------------------------------\n",
      "Max loss: 0.028769254684448242\n",
      "Min loss: 0.0036954861134290695\n",
      "Mean loss: 0.01235787159142395\n",
      "Std loss: 0.00914696127942885\n",
      "Total Loss: 0.07414722954854369\n",
      "------------------------------------ epoch 9389 (56328 steps) ------------------------------------\n",
      "Max loss: 0.023877963423728943\n",
      "Min loss: 0.005734728649258614\n",
      "Mean loss: 0.015013314007471005\n",
      "Std loss: 0.005962026103232651\n",
      "Total Loss: 0.09007988404482603\n",
      "------------------------------------ epoch 9390 (56334 steps) ------------------------------------\n",
      "Max loss: 0.012423115782439709\n",
      "Min loss: 0.004473015666007996\n",
      "Mean loss: 0.007568307065715392\n",
      "Std loss: 0.0029094327976764978\n",
      "Total Loss: 0.045409842394292355\n",
      "------------------------------------ epoch 9391 (56340 steps) ------------------------------------\n",
      "Max loss: 0.019418509677052498\n",
      "Min loss: 0.005707894917577505\n",
      "Mean loss: 0.00991866869541506\n",
      "Std loss: 0.0044661820273135516\n",
      "Total Loss: 0.05951201217249036\n",
      "------------------------------------ epoch 9392 (56346 steps) ------------------------------------\n",
      "Max loss: 0.008144763298332691\n",
      "Min loss: 0.004001081921160221\n",
      "Mean loss: 0.005393609016512831\n",
      "Std loss: 0.00144330802188045\n",
      "Total Loss: 0.032361654099076986\n",
      "------------------------------------ epoch 9393 (56352 steps) ------------------------------------\n",
      "Max loss: 0.014038573019206524\n",
      "Min loss: 0.005019403528422117\n",
      "Mean loss: 0.009739055608709654\n",
      "Std loss: 0.003490931736719739\n",
      "Total Loss: 0.05843433365225792\n",
      "------------------------------------ epoch 9394 (56358 steps) ------------------------------------\n",
      "Max loss: 0.0073475828394293785\n",
      "Min loss: 0.004829181358218193\n",
      "Mean loss: 0.006233302488302191\n",
      "Std loss: 0.0007421116271850173\n",
      "Total Loss: 0.03739981492981315\n",
      "------------------------------------ epoch 9395 (56364 steps) ------------------------------------\n",
      "Max loss: 0.022386154159903526\n",
      "Min loss: 0.0032293079420924187\n",
      "Mean loss: 0.00941086463474979\n",
      "Std loss: 0.007290346295522987\n",
      "Total Loss: 0.05646518780849874\n",
      "------------------------------------ epoch 9396 (56370 steps) ------------------------------------\n",
      "Max loss: 0.015624269843101501\n",
      "Min loss: 0.005397436209022999\n",
      "Mean loss: 0.009437364138041934\n",
      "Std loss: 0.0033122311256209135\n",
      "Total Loss: 0.0566241848282516\n",
      "------------------------------------ epoch 9397 (56376 steps) ------------------------------------\n",
      "Max loss: 0.02743712067604065\n",
      "Min loss: 0.004027081653475761\n",
      "Mean loss: 0.009188943387319645\n",
      "Std loss: 0.00822072518005132\n",
      "Total Loss: 0.055133660323917866\n",
      "------------------------------------ epoch 9398 (56382 steps) ------------------------------------\n",
      "Max loss: 0.02064097672700882\n",
      "Min loss: 0.0038027900736778975\n",
      "Mean loss: 0.010797842134100696\n",
      "Std loss: 0.00665015560880055\n",
      "Total Loss: 0.06478705280460417\n",
      "------------------------------------ epoch 9399 (56388 steps) ------------------------------------\n",
      "Max loss: 0.012706509791314602\n",
      "Min loss: 0.004655424505472183\n",
      "Mean loss: 0.008101233281195164\n",
      "Std loss: 0.0028698576201699735\n",
      "Total Loss: 0.04860739968717098\n",
      "------------------------------------ epoch 9400 (56394 steps) ------------------------------------\n",
      "Max loss: 0.010423026047647\n",
      "Min loss: 0.003717293031513691\n",
      "Mean loss: 0.006580892174194257\n",
      "Std loss: 0.0023817089405536325\n",
      "Total Loss: 0.03948535304516554\n",
      "------------------------------------ epoch 9401 (56400 steps) ------------------------------------\n",
      "Max loss: 0.024294286966323853\n",
      "Min loss: 0.004484384320676327\n",
      "Mean loss: 0.010247290056819717\n",
      "Std loss: 0.00675080097298032\n",
      "Total Loss: 0.0614837403409183\n",
      "saved model at ./weights/model_9401.pth\n",
      "------------------------------------ epoch 9402 (56406 steps) ------------------------------------\n",
      "Max loss: 0.012096486985683441\n",
      "Min loss: 0.003596013877540827\n",
      "Mean loss: 0.006680522890140613\n",
      "Std loss: 0.003090450168095133\n",
      "Total Loss: 0.04008313734084368\n",
      "------------------------------------ epoch 9403 (56412 steps) ------------------------------------\n",
      "Max loss: 0.020911533385515213\n",
      "Min loss: 0.0031747426837682724\n",
      "Mean loss: 0.008781315370773276\n",
      "Std loss: 0.005943530896892197\n",
      "Total Loss: 0.052687892224639654\n",
      "------------------------------------ epoch 9404 (56418 steps) ------------------------------------\n",
      "Max loss: 0.01139894314110279\n",
      "Min loss: 0.004290207289159298\n",
      "Mean loss: 0.00780045132463177\n",
      "Std loss: 0.0024894367206313326\n",
      "Total Loss: 0.04680270794779062\n",
      "------------------------------------ epoch 9405 (56424 steps) ------------------------------------\n",
      "Max loss: 0.007373981177806854\n",
      "Min loss: 0.004331748932600021\n",
      "Mean loss: 0.006079247687011957\n",
      "Std loss: 0.0011821742458169504\n",
      "Total Loss: 0.03647548612207174\n",
      "------------------------------------ epoch 9406 (56430 steps) ------------------------------------\n",
      "Max loss: 0.009302031248807907\n",
      "Min loss: 0.003617311595007777\n",
      "Mean loss: 0.006115650020850201\n",
      "Std loss: 0.0018020244301823187\n",
      "Total Loss: 0.03669390012510121\n",
      "------------------------------------ epoch 9407 (56436 steps) ------------------------------------\n",
      "Max loss: 0.010866536758840084\n",
      "Min loss: 0.003957740962505341\n",
      "Mean loss: 0.005934731646751364\n",
      "Std loss: 0.0023783182553497755\n",
      "Total Loss: 0.035608389880508184\n",
      "------------------------------------ epoch 9408 (56442 steps) ------------------------------------\n",
      "Max loss: 0.013840055093169212\n",
      "Min loss: 0.0032932478934526443\n",
      "Mean loss: 0.00787140514391164\n",
      "Std loss: 0.0032004252545349272\n",
      "Total Loss: 0.04722843086346984\n",
      "------------------------------------ epoch 9409 (56448 steps) ------------------------------------\n",
      "Max loss: 0.012249007821083069\n",
      "Min loss: 0.004636470694094896\n",
      "Mean loss: 0.007259121475120385\n",
      "Std loss: 0.0025173696550724266\n",
      "Total Loss: 0.04355472885072231\n",
      "------------------------------------ epoch 9410 (56454 steps) ------------------------------------\n",
      "Max loss: 0.0106069166213274\n",
      "Min loss: 0.005123873706907034\n",
      "Mean loss: 0.008129988486568132\n",
      "Std loss: 0.0024523518704014056\n",
      "Total Loss: 0.0487799309194088\n",
      "------------------------------------ epoch 9411 (56460 steps) ------------------------------------\n",
      "Max loss: 0.03746502846479416\n",
      "Min loss: 0.0036310702562332153\n",
      "Mean loss: 0.010332447515490154\n",
      "Std loss: 0.01218962978299358\n",
      "Total Loss: 0.06199468509294093\n",
      "------------------------------------ epoch 9412 (56466 steps) ------------------------------------\n",
      "Max loss: 0.0075262426398694515\n",
      "Min loss: 0.0031032459810376167\n",
      "Mean loss: 0.00517190876416862\n",
      "Std loss: 0.001712204914629349\n",
      "Total Loss: 0.03103145258501172\n",
      "------------------------------------ epoch 9413 (56472 steps) ------------------------------------\n",
      "Max loss: 0.02495022863149643\n",
      "Min loss: 0.005111389793455601\n",
      "Mean loss: 0.01078245866422852\n",
      "Std loss: 0.006677757363126399\n",
      "Total Loss: 0.06469475198537111\n",
      "------------------------------------ epoch 9414 (56478 steps) ------------------------------------\n",
      "Max loss: 0.009021413512527943\n",
      "Min loss: 0.0038393153809010983\n",
      "Mean loss: 0.00690827580789725\n",
      "Std loss: 0.0018039065739718588\n",
      "Total Loss: 0.0414496548473835\n",
      "------------------------------------ epoch 9415 (56484 steps) ------------------------------------\n",
      "Max loss: 0.009430802427232265\n",
      "Min loss: 0.005076723173260689\n",
      "Mean loss: 0.006854027354468902\n",
      "Std loss: 0.0015705737343181523\n",
      "Total Loss: 0.04112416412681341\n",
      "------------------------------------ epoch 9416 (56490 steps) ------------------------------------\n",
      "Max loss: 0.01361229456961155\n",
      "Min loss: 0.004581061191856861\n",
      "Mean loss: 0.006697683595120907\n",
      "Std loss: 0.003120168316103711\n",
      "Total Loss: 0.04018610157072544\n",
      "------------------------------------ epoch 9417 (56496 steps) ------------------------------------\n",
      "Max loss: 0.012317702174186707\n",
      "Min loss: 0.0032532408367842436\n",
      "Mean loss: 0.0062173667053381605\n",
      "Std loss: 0.003195253739775042\n",
      "Total Loss: 0.03730420023202896\n",
      "------------------------------------ epoch 9418 (56502 steps) ------------------------------------\n",
      "Max loss: 0.006671856623142958\n",
      "Min loss: 0.0035199932754039764\n",
      "Mean loss: 0.005324251173684995\n",
      "Std loss: 0.0012296699087283706\n",
      "Total Loss: 0.031945507042109966\n",
      "------------------------------------ epoch 9419 (56508 steps) ------------------------------------\n",
      "Max loss: 0.013798905536532402\n",
      "Min loss: 0.0040358551777899265\n",
      "Mean loss: 0.007041850282500188\n",
      "Std loss: 0.0035661791598395835\n",
      "Total Loss: 0.042251101695001125\n",
      "------------------------------------ epoch 9420 (56514 steps) ------------------------------------\n",
      "Max loss: 0.009795313701033592\n",
      "Min loss: 0.003549424931406975\n",
      "Mean loss: 0.006950705933074157\n",
      "Std loss: 0.0022357270116155825\n",
      "Total Loss: 0.04170423559844494\n",
      "------------------------------------ epoch 9421 (56520 steps) ------------------------------------\n",
      "Max loss: 0.009196832776069641\n",
      "Min loss: 0.0038949227891862392\n",
      "Mean loss: 0.00607722345739603\n",
      "Std loss: 0.001719858759523487\n",
      "Total Loss: 0.03646334074437618\n",
      "------------------------------------ epoch 9422 (56526 steps) ------------------------------------\n",
      "Max loss: 0.007060297764837742\n",
      "Min loss: 0.00332409399561584\n",
      "Mean loss: 0.00470132283711185\n",
      "Std loss: 0.0011963016216626151\n",
      "Total Loss: 0.028207937022671103\n",
      "------------------------------------ epoch 9423 (56532 steps) ------------------------------------\n",
      "Max loss: 0.009182076901197433\n",
      "Min loss: 0.0035850757267326117\n",
      "Mean loss: 0.005702673923224211\n",
      "Std loss: 0.0019416752792210219\n",
      "Total Loss: 0.034216043539345264\n",
      "------------------------------------ epoch 9424 (56538 steps) ------------------------------------\n",
      "Max loss: 0.007974205538630486\n",
      "Min loss: 0.003674635663628578\n",
      "Mean loss: 0.005010883789509535\n",
      "Std loss: 0.0013873460048590774\n",
      "Total Loss: 0.03006530273705721\n",
      "------------------------------------ epoch 9425 (56544 steps) ------------------------------------\n",
      "Max loss: 0.022079825401306152\n",
      "Min loss: 0.004810314625501633\n",
      "Mean loss: 0.010104371700435877\n",
      "Std loss: 0.005723964946214922\n",
      "Total Loss: 0.06062623020261526\n",
      "------------------------------------ epoch 9426 (56550 steps) ------------------------------------\n",
      "Max loss: 0.01027899980545044\n",
      "Min loss: 0.0033699716441333294\n",
      "Mean loss: 0.007578458171337843\n",
      "Std loss: 0.00223517952500892\n",
      "Total Loss: 0.04547074902802706\n",
      "------------------------------------ epoch 9427 (56556 steps) ------------------------------------\n",
      "Max loss: 0.025900645181536674\n",
      "Min loss: 0.003530807327479124\n",
      "Mean loss: 0.012796282845859727\n",
      "Std loss: 0.0071280382622998\n",
      "Total Loss: 0.07677769707515836\n",
      "------------------------------------ epoch 9428 (56562 steps) ------------------------------------\n",
      "Max loss: 0.03581179678440094\n",
      "Min loss: 0.007501761894673109\n",
      "Mean loss: 0.0160837157163769\n",
      "Std loss: 0.009679876894879364\n",
      "Total Loss: 0.0965022942982614\n",
      "------------------------------------ epoch 9429 (56568 steps) ------------------------------------\n",
      "Max loss: 0.03811276704072952\n",
      "Min loss: 0.006289015524089336\n",
      "Mean loss: 0.014703579867879549\n",
      "Std loss: 0.010810918033086295\n",
      "Total Loss: 0.0882214792072773\n",
      "------------------------------------ epoch 9430 (56574 steps) ------------------------------------\n",
      "Max loss: 0.03882722184062004\n",
      "Min loss: 0.006432665046304464\n",
      "Mean loss: 0.014031373662874103\n",
      "Std loss: 0.011414442818673702\n",
      "Total Loss: 0.08418824197724462\n",
      "------------------------------------ epoch 9431 (56580 steps) ------------------------------------\n",
      "Max loss: 0.03015616536140442\n",
      "Min loss: 0.008122275583446026\n",
      "Mean loss: 0.01703948884581526\n",
      "Std loss: 0.008121216462403744\n",
      "Total Loss: 0.10223693307489157\n",
      "------------------------------------ epoch 9432 (56586 steps) ------------------------------------\n",
      "Max loss: 0.02241312712430954\n",
      "Min loss: 0.006489095743745565\n",
      "Mean loss: 0.0126711738606294\n",
      "Std loss: 0.0060656186734722485\n",
      "Total Loss: 0.0760270431637764\n",
      "------------------------------------ epoch 9433 (56592 steps) ------------------------------------\n",
      "Max loss: 0.03417433053255081\n",
      "Min loss: 0.004498802125453949\n",
      "Mean loss: 0.013410990048820773\n",
      "Std loss: 0.01034362004523434\n",
      "Total Loss: 0.08046594029292464\n",
      "------------------------------------ epoch 9434 (56598 steps) ------------------------------------\n",
      "Max loss: 0.01724025048315525\n",
      "Min loss: 0.006204254925251007\n",
      "Mean loss: 0.01110914209857583\n",
      "Std loss: 0.0036755815771202347\n",
      "Total Loss: 0.06665485259145498\n",
      "------------------------------------ epoch 9435 (56604 steps) ------------------------------------\n",
      "Max loss: 0.007341807242482901\n",
      "Min loss: 0.0042214710265398026\n",
      "Mean loss: 0.005865267322709163\n",
      "Std loss: 0.0009662363142622633\n",
      "Total Loss: 0.03519160393625498\n",
      "------------------------------------ epoch 9436 (56610 steps) ------------------------------------\n",
      "Max loss: 0.012957962229847908\n",
      "Min loss: 0.004356752149760723\n",
      "Mean loss: 0.007506964107354482\n",
      "Std loss: 0.003203742131266394\n",
      "Total Loss: 0.04504178464412689\n",
      "------------------------------------ epoch 9437 (56616 steps) ------------------------------------\n",
      "Max loss: 0.017113514244556427\n",
      "Min loss: 0.004447936546057463\n",
      "Mean loss: 0.008037636910254756\n",
      "Std loss: 0.004459717225720448\n",
      "Total Loss: 0.04822582146152854\n",
      "------------------------------------ epoch 9438 (56622 steps) ------------------------------------\n",
      "Max loss: 0.007108990103006363\n",
      "Min loss: 0.004634194076061249\n",
      "Mean loss: 0.005789970668653647\n",
      "Std loss: 0.0009174572967815667\n",
      "Total Loss: 0.03473982401192188\n",
      "------------------------------------ epoch 9439 (56628 steps) ------------------------------------\n",
      "Max loss: 0.012895969673991203\n",
      "Min loss: 0.003639302682131529\n",
      "Mean loss: 0.007816768639410535\n",
      "Std loss: 0.0038663383206115646\n",
      "Total Loss: 0.04690061183646321\n",
      "------------------------------------ epoch 9440 (56634 steps) ------------------------------------\n",
      "Max loss: 0.005591218825429678\n",
      "Min loss: 0.0029906155541539192\n",
      "Mean loss: 0.004600605772187312\n",
      "Std loss: 0.0010996928344434013\n",
      "Total Loss: 0.027603634633123875\n",
      "------------------------------------ epoch 9441 (56640 steps) ------------------------------------\n",
      "Max loss: 0.008979301899671555\n",
      "Min loss: 0.0036849218886345625\n",
      "Mean loss: 0.00544520968105644\n",
      "Std loss: 0.0017693494174804687\n",
      "Total Loss: 0.03267125808633864\n",
      "------------------------------------ epoch 9442 (56646 steps) ------------------------------------\n",
      "Max loss: 0.021295320242643356\n",
      "Min loss: 0.004657221492379904\n",
      "Mean loss: 0.013275179623936614\n",
      "Std loss: 0.006310935938401869\n",
      "Total Loss: 0.07965107774361968\n",
      "------------------------------------ epoch 9443 (56652 steps) ------------------------------------\n",
      "Max loss: 0.016933362931013107\n",
      "Min loss: 0.003474579658359289\n",
      "Mean loss: 0.007191295347486933\n",
      "Std loss: 0.004550714917503099\n",
      "Total Loss: 0.0431477720849216\n",
      "------------------------------------ epoch 9444 (56658 steps) ------------------------------------\n",
      "Max loss: 0.015663117170333862\n",
      "Min loss: 0.0059760939329862595\n",
      "Mean loss: 0.008395513674865166\n",
      "Std loss: 0.003435472229139145\n",
      "Total Loss: 0.050373082049191\n",
      "------------------------------------ epoch 9445 (56664 steps) ------------------------------------\n",
      "Max loss: 0.03075559437274933\n",
      "Min loss: 0.004789452534168959\n",
      "Mean loss: 0.013447195990011096\n",
      "Std loss: 0.00817900239269106\n",
      "Total Loss: 0.08068317594006658\n",
      "------------------------------------ epoch 9446 (56670 steps) ------------------------------------\n",
      "Max loss: 0.013579653576016426\n",
      "Min loss: 0.0037838600110262632\n",
      "Mean loss: 0.007386648833441238\n",
      "Std loss: 0.003443101993135319\n",
      "Total Loss: 0.044319893000647426\n",
      "------------------------------------ epoch 9447 (56676 steps) ------------------------------------\n",
      "Max loss: 0.014072079211473465\n",
      "Min loss: 0.004523023962974548\n",
      "Mean loss: 0.008001478932177028\n",
      "Std loss: 0.003113456298882816\n",
      "Total Loss: 0.04800887359306216\n",
      "------------------------------------ epoch 9448 (56682 steps) ------------------------------------\n",
      "Max loss: 0.027073677629232407\n",
      "Min loss: 0.003524889936670661\n",
      "Mean loss: 0.010801677475683391\n",
      "Std loss: 0.008601013700446438\n",
      "Total Loss: 0.06481006485410035\n",
      "------------------------------------ epoch 9449 (56688 steps) ------------------------------------\n",
      "Max loss: 0.00977737084031105\n",
      "Min loss: 0.003489910624921322\n",
      "Mean loss: 0.006225720513612032\n",
      "Std loss: 0.0023532467335968363\n",
      "Total Loss: 0.03735432308167219\n",
      "------------------------------------ epoch 9450 (56694 steps) ------------------------------------\n",
      "Max loss: 0.018717966973781586\n",
      "Min loss: 0.003806044813245535\n",
      "Mean loss: 0.008648294412220517\n",
      "Std loss: 0.004988522671107968\n",
      "Total Loss: 0.05188976647332311\n",
      "------------------------------------ epoch 9451 (56700 steps) ------------------------------------\n",
      "Max loss: 0.015912028029561043\n",
      "Min loss: 0.005674873944371939\n",
      "Mean loss: 0.009666159749031067\n",
      "Std loss: 0.004386068345423826\n",
      "Total Loss: 0.0579969584941864\n",
      "------------------------------------ epoch 9452 (56706 steps) ------------------------------------\n",
      "Max loss: 0.01421089842915535\n",
      "Min loss: 0.004332374781370163\n",
      "Mean loss: 0.00797878128166\n",
      "Std loss: 0.0033176062801049213\n",
      "Total Loss: 0.04787268768996\n",
      "------------------------------------ epoch 9453 (56712 steps) ------------------------------------\n",
      "Max loss: 0.008083523251116276\n",
      "Min loss: 0.003983809147030115\n",
      "Mean loss: 0.005535722089310487\n",
      "Std loss: 0.0013888862999150038\n",
      "Total Loss: 0.03321433253586292\n",
      "------------------------------------ epoch 9454 (56718 steps) ------------------------------------\n",
      "Max loss: 0.01153437327593565\n",
      "Min loss: 0.003921799827367067\n",
      "Mean loss: 0.007436125073581934\n",
      "Std loss: 0.002838902404670891\n",
      "Total Loss: 0.044616750441491604\n",
      "------------------------------------ epoch 9455 (56724 steps) ------------------------------------\n",
      "Max loss: 0.012762149795889854\n",
      "Min loss: 0.003026785794645548\n",
      "Mean loss: 0.006263102404773235\n",
      "Std loss: 0.003103865341846685\n",
      "Total Loss: 0.03757861442863941\n",
      "------------------------------------ epoch 9456 (56730 steps) ------------------------------------\n",
      "Max loss: 0.01589907705783844\n",
      "Min loss: 0.004224699921905994\n",
      "Mean loss: 0.007575823304553826\n",
      "Std loss: 0.003967840262285726\n",
      "Total Loss: 0.04545493982732296\n",
      "------------------------------------ epoch 9457 (56736 steps) ------------------------------------\n",
      "Max loss: 0.008720165118575096\n",
      "Min loss: 0.0029508406296372414\n",
      "Mean loss: 0.004474014587079485\n",
      "Std loss: 0.001962040808153521\n",
      "Total Loss: 0.02684408752247691\n",
      "------------------------------------ epoch 9458 (56742 steps) ------------------------------------\n",
      "Max loss: 0.007015030365437269\n",
      "Min loss: 0.002952244598418474\n",
      "Mean loss: 0.004705637111328542\n",
      "Std loss: 0.0014171227204049534\n",
      "Total Loss: 0.028233822667971253\n",
      "------------------------------------ epoch 9459 (56748 steps) ------------------------------------\n",
      "Max loss: 0.007622342556715012\n",
      "Min loss: 0.003137598279863596\n",
      "Mean loss: 0.005442761738474171\n",
      "Std loss: 0.0013644858110318631\n",
      "Total Loss: 0.03265657043084502\n",
      "------------------------------------ epoch 9460 (56754 steps) ------------------------------------\n",
      "Max loss: 0.019821491092443466\n",
      "Min loss: 0.003016597591340542\n",
      "Mean loss: 0.008982029821102818\n",
      "Std loss: 0.006355282943022164\n",
      "Total Loss: 0.05389217892661691\n",
      "------------------------------------ epoch 9461 (56760 steps) ------------------------------------\n",
      "Max loss: 0.006700843572616577\n",
      "Min loss: 0.003163762390613556\n",
      "Mean loss: 0.005134478948699932\n",
      "Std loss: 0.0014379650358770223\n",
      "Total Loss: 0.030806873692199588\n",
      "------------------------------------ epoch 9462 (56766 steps) ------------------------------------\n",
      "Max loss: 0.03853767737746239\n",
      "Min loss: 0.004928188398480415\n",
      "Mean loss: 0.013188331543157497\n",
      "Std loss: 0.011473805261933685\n",
      "Total Loss: 0.07912998925894499\n",
      "------------------------------------ epoch 9463 (56772 steps) ------------------------------------\n",
      "Max loss: 0.007969600148499012\n",
      "Min loss: 0.0038854023441672325\n",
      "Mean loss: 0.006070465470353763\n",
      "Std loss: 0.001452292114939662\n",
      "Total Loss: 0.036422792822122574\n",
      "------------------------------------ epoch 9464 (56778 steps) ------------------------------------\n",
      "Max loss: 0.007793288677930832\n",
      "Min loss: 0.004324434325098991\n",
      "Mean loss: 0.0058275041325638694\n",
      "Std loss: 0.0011840882816255267\n",
      "Total Loss: 0.034965024795383215\n",
      "------------------------------------ epoch 9465 (56784 steps) ------------------------------------\n",
      "Max loss: 0.014726405963301659\n",
      "Min loss: 0.004352145828306675\n",
      "Mean loss: 0.008710048471887907\n",
      "Std loss: 0.0037276227702951388\n",
      "Total Loss: 0.05226029083132744\n",
      "------------------------------------ epoch 9466 (56790 steps) ------------------------------------\n",
      "Max loss: 0.015529538504779339\n",
      "Min loss: 0.0050514875911176205\n",
      "Mean loss: 0.007520850204552214\n",
      "Std loss: 0.003637140761459477\n",
      "Total Loss: 0.04512510122731328\n",
      "------------------------------------ epoch 9467 (56796 steps) ------------------------------------\n",
      "Max loss: 0.010309645906090736\n",
      "Min loss: 0.003227820387110114\n",
      "Mean loss: 0.005804681180355449\n",
      "Std loss: 0.002276298960471871\n",
      "Total Loss: 0.0348280870821327\n",
      "------------------------------------ epoch 9468 (56802 steps) ------------------------------------\n",
      "Max loss: 0.030028140172362328\n",
      "Min loss: 0.004584497306495905\n",
      "Mean loss: 0.01328488850655655\n",
      "Std loss: 0.010177921456724477\n",
      "Total Loss: 0.0797093310393393\n",
      "------------------------------------ epoch 9469 (56808 steps) ------------------------------------\n",
      "Max loss: 0.011084328405559063\n",
      "Min loss: 0.0037039457820355892\n",
      "Mean loss: 0.006791684931765\n",
      "Std loss: 0.00305904206595155\n",
      "Total Loss: 0.04075010959059\n",
      "------------------------------------ epoch 9470 (56814 steps) ------------------------------------\n",
      "Max loss: 0.015508736483752728\n",
      "Min loss: 0.004389011766761541\n",
      "Mean loss: 0.007434804380560915\n",
      "Std loss: 0.0037513893739473704\n",
      "Total Loss: 0.04460882628336549\n",
      "------------------------------------ epoch 9471 (56820 steps) ------------------------------------\n",
      "Max loss: 0.03814331442117691\n",
      "Min loss: 0.0051977504044771194\n",
      "Mean loss: 0.01861973541478316\n",
      "Std loss: 0.013875268669545507\n",
      "Total Loss: 0.11171841248869896\n",
      "------------------------------------ epoch 9472 (56826 steps) ------------------------------------\n",
      "Max loss: 0.022435016930103302\n",
      "Min loss: 0.004728068131953478\n",
      "Mean loss: 0.012800463708117604\n",
      "Std loss: 0.006247088315353683\n",
      "Total Loss: 0.07680278224870563\n",
      "------------------------------------ epoch 9473 (56832 steps) ------------------------------------\n",
      "Max loss: 0.05021577328443527\n",
      "Min loss: 0.0074247210286557674\n",
      "Mean loss: 0.01644954294897616\n",
      "Std loss: 0.015207453928826985\n",
      "Total Loss: 0.09869725769385695\n",
      "------------------------------------ epoch 9474 (56838 steps) ------------------------------------\n",
      "Max loss: 0.03248399496078491\n",
      "Min loss: 0.007338696625083685\n",
      "Mean loss: 0.017032107105478644\n",
      "Std loss: 0.00869624348945245\n",
      "Total Loss: 0.10219264263287187\n",
      "------------------------------------ epoch 9475 (56844 steps) ------------------------------------\n",
      "Max loss: 0.06204083561897278\n",
      "Min loss: 0.006737569347023964\n",
      "Mean loss: 0.018776377352575462\n",
      "Std loss: 0.019527293805431407\n",
      "Total Loss: 0.11265826411545277\n",
      "------------------------------------ epoch 9476 (56850 steps) ------------------------------------\n",
      "Max loss: 0.012477297335863113\n",
      "Min loss: 0.00738337030634284\n",
      "Mean loss: 0.010122208778435985\n",
      "Std loss: 0.0015493626515986165\n",
      "Total Loss: 0.06073325267061591\n",
      "------------------------------------ epoch 9477 (56856 steps) ------------------------------------\n",
      "Max loss: 0.014530597254633904\n",
      "Min loss: 0.0053351572714746\n",
      "Mean loss: 0.00979941252929469\n",
      "Std loss: 0.003661608769229059\n",
      "Total Loss: 0.05879647517576814\n",
      "------------------------------------ epoch 9478 (56862 steps) ------------------------------------\n",
      "Max loss: 0.013794063590466976\n",
      "Min loss: 0.005003594793379307\n",
      "Mean loss: 0.008250499842688441\n",
      "Std loss: 0.0032015933583668314\n",
      "Total Loss: 0.04950299905613065\n",
      "------------------------------------ epoch 9479 (56868 steps) ------------------------------------\n",
      "Max loss: 0.01020415686070919\n",
      "Min loss: 0.004221749491989613\n",
      "Mean loss: 0.005935132193068664\n",
      "Std loss: 0.002275633499315456\n",
      "Total Loss: 0.03561079315841198\n",
      "------------------------------------ epoch 9480 (56874 steps) ------------------------------------\n",
      "Max loss: 0.01872243732213974\n",
      "Min loss: 0.005654678680002689\n",
      "Mean loss: 0.011550205138822397\n",
      "Std loss: 0.0041692396364138755\n",
      "Total Loss: 0.06930123083293438\n",
      "------------------------------------ epoch 9481 (56880 steps) ------------------------------------\n",
      "Max loss: 0.017645064741373062\n",
      "Min loss: 0.004063454922288656\n",
      "Mean loss: 0.010821260046213865\n",
      "Std loss: 0.004926879407483516\n",
      "Total Loss: 0.06492756027728319\n",
      "------------------------------------ epoch 9482 (56886 steps) ------------------------------------\n",
      "Max loss: 0.014840829186141491\n",
      "Min loss: 0.004705332685261965\n",
      "Mean loss: 0.008219155560558042\n",
      "Std loss: 0.0033615680705225337\n",
      "Total Loss: 0.049314933363348246\n",
      "------------------------------------ epoch 9483 (56892 steps) ------------------------------------\n",
      "Max loss: 0.014565165154635906\n",
      "Min loss: 0.004606572445482016\n",
      "Mean loss: 0.007821844425052404\n",
      "Std loss: 0.003404349969720014\n",
      "Total Loss: 0.046931066550314426\n",
      "------------------------------------ epoch 9484 (56898 steps) ------------------------------------\n",
      "Max loss: 0.01084046345204115\n",
      "Min loss: 0.004966349340975285\n",
      "Mean loss: 0.007354199032609661\n",
      "Std loss: 0.0023512639736074127\n",
      "Total Loss: 0.04412519419565797\n",
      "------------------------------------ epoch 9485 (56904 steps) ------------------------------------\n",
      "Max loss: 0.01403036992996931\n",
      "Min loss: 0.0036117471754550934\n",
      "Mean loss: 0.006605865356201927\n",
      "Std loss: 0.0035530854431554235\n",
      "Total Loss: 0.03963519213721156\n",
      "------------------------------------ epoch 9486 (56910 steps) ------------------------------------\n",
      "Max loss: 0.00905225332826376\n",
      "Min loss: 0.003429112955927849\n",
      "Mean loss: 0.00504837860353291\n",
      "Std loss: 0.001881471209026655\n",
      "Total Loss: 0.030290271621197462\n",
      "------------------------------------ epoch 9487 (56916 steps) ------------------------------------\n",
      "Max loss: 0.014927392825484276\n",
      "Min loss: 0.004256141372025013\n",
      "Mean loss: 0.008285845552260676\n",
      "Std loss: 0.003286029150334299\n",
      "Total Loss: 0.04971507331356406\n",
      "------------------------------------ epoch 9488 (56922 steps) ------------------------------------\n",
      "Max loss: 0.023658156394958496\n",
      "Min loss: 0.0038674501702189445\n",
      "Mean loss: 0.008231428607056538\n",
      "Std loss: 0.0071327965748833\n",
      "Total Loss: 0.04938857164233923\n",
      "------------------------------------ epoch 9489 (56928 steps) ------------------------------------\n",
      "Max loss: 0.009941142052412033\n",
      "Min loss: 0.0036194510757923126\n",
      "Mean loss: 0.006480808059374492\n",
      "Std loss: 0.0022212246359266546\n",
      "Total Loss: 0.03888484835624695\n",
      "------------------------------------ epoch 9490 (56934 steps) ------------------------------------\n",
      "Max loss: 0.0171159990131855\n",
      "Min loss: 0.003874521702528\n",
      "Mean loss: 0.00947073164085547\n",
      "Std loss: 0.004748801643352138\n",
      "Total Loss: 0.05682438984513283\n",
      "------------------------------------ epoch 9491 (56940 steps) ------------------------------------\n",
      "Max loss: 0.017481302842497826\n",
      "Min loss: 0.005826578475534916\n",
      "Mean loss: 0.009578197418401638\n",
      "Std loss: 0.0040694286449353126\n",
      "Total Loss: 0.05746918451040983\n",
      "------------------------------------ epoch 9492 (56946 steps) ------------------------------------\n",
      "Max loss: 0.02121051400899887\n",
      "Min loss: 0.004189750179648399\n",
      "Mean loss: 0.008809846437846621\n",
      "Std loss: 0.005807005858811027\n",
      "Total Loss: 0.052859078627079725\n",
      "------------------------------------ epoch 9493 (56952 steps) ------------------------------------\n",
      "Max loss: 0.014983939938247204\n",
      "Min loss: 0.005249375011771917\n",
      "Mean loss: 0.008781641023233533\n",
      "Std loss: 0.0031352285363161793\n",
      "Total Loss: 0.0526898461394012\n",
      "------------------------------------ epoch 9494 (56958 steps) ------------------------------------\n",
      "Max loss: 0.0160784050822258\n",
      "Min loss: 0.003080726135522127\n",
      "Mean loss: 0.007272576913237572\n",
      "Std loss: 0.0052910607096009485\n",
      "Total Loss: 0.04363546147942543\n",
      "------------------------------------ epoch 9495 (56964 steps) ------------------------------------\n",
      "Max loss: 0.010002466849982738\n",
      "Min loss: 0.0033251349814236164\n",
      "Mean loss: 0.0058985198847949505\n",
      "Std loss: 0.0021290067039826522\n",
      "Total Loss: 0.0353911193087697\n",
      "------------------------------------ epoch 9496 (56970 steps) ------------------------------------\n",
      "Max loss: 0.008024759590625763\n",
      "Min loss: 0.004085075575858355\n",
      "Mean loss: 0.00634277433467408\n",
      "Std loss: 0.0016577583342681159\n",
      "Total Loss: 0.03805664600804448\n",
      "------------------------------------ epoch 9497 (56976 steps) ------------------------------------\n",
      "Max loss: 0.022227831184864044\n",
      "Min loss: 0.003683187998831272\n",
      "Mean loss: 0.007357693122078975\n",
      "Std loss: 0.00666460034752284\n",
      "Total Loss: 0.04414615873247385\n",
      "------------------------------------ epoch 9498 (56982 steps) ------------------------------------\n",
      "Max loss: 0.009253340773284435\n",
      "Min loss: 0.00463807862251997\n",
      "Mean loss: 0.0058528275694698095\n",
      "Std loss: 0.0016195052868261004\n",
      "Total Loss: 0.03511696541681886\n",
      "------------------------------------ epoch 9499 (56988 steps) ------------------------------------\n",
      "Max loss: 0.01246330700814724\n",
      "Min loss: 0.005033091641962528\n",
      "Mean loss: 0.007706822827458382\n",
      "Std loss: 0.0030085255970936724\n",
      "Total Loss: 0.04624093696475029\n",
      "------------------------------------ epoch 9500 (56994 steps) ------------------------------------\n",
      "Max loss: 0.00809427909553051\n",
      "Min loss: 0.0037626465782523155\n",
      "Mean loss: 0.00547085617048045\n",
      "Std loss: 0.0014627995636812908\n",
      "Total Loss: 0.0328251370228827\n",
      "------------------------------------ epoch 9501 (57000 steps) ------------------------------------\n",
      "Max loss: 0.015171628445386887\n",
      "Min loss: 0.003206159919500351\n",
      "Mean loss: 0.007586129087333878\n",
      "Std loss: 0.004797659012327991\n",
      "Total Loss: 0.04551677452400327\n",
      "saved model at ./weights/model_9501.pth\n",
      "------------------------------------ epoch 9502 (57006 steps) ------------------------------------\n",
      "Max loss: 0.011572238057851791\n",
      "Min loss: 0.0036416370421648026\n",
      "Mean loss: 0.006064731627702713\n",
      "Std loss: 0.0025640971188904465\n",
      "Total Loss: 0.03638838976621628\n",
      "------------------------------------ epoch 9503 (57012 steps) ------------------------------------\n",
      "Max loss: 0.016107982024550438\n",
      "Min loss: 0.0035160433035343885\n",
      "Mean loss: 0.007911934944180151\n",
      "Std loss: 0.005125462746299407\n",
      "Total Loss: 0.047471609665080905\n",
      "------------------------------------ epoch 9504 (57018 steps) ------------------------------------\n",
      "Max loss: 0.018803443759679794\n",
      "Min loss: 0.003982451744377613\n",
      "Mean loss: 0.008963426031793157\n",
      "Std loss: 0.004972135637042242\n",
      "Total Loss: 0.053780556190758944\n",
      "------------------------------------ epoch 9505 (57024 steps) ------------------------------------\n",
      "Max loss: 0.022194020450115204\n",
      "Min loss: 0.005155330523848534\n",
      "Mean loss: 0.011904878929878274\n",
      "Std loss: 0.006601839280580008\n",
      "Total Loss: 0.07142927357926965\n",
      "------------------------------------ epoch 9506 (57030 steps) ------------------------------------\n",
      "Max loss: 0.019291214644908905\n",
      "Min loss: 0.004493478685617447\n",
      "Mean loss: 0.010047720822816094\n",
      "Std loss: 0.005909537228229742\n",
      "Total Loss: 0.06028632493689656\n",
      "------------------------------------ epoch 9507 (57036 steps) ------------------------------------\n",
      "Max loss: 0.011328517459332943\n",
      "Min loss: 0.004195450805127621\n",
      "Mean loss: 0.0078044812350223465\n",
      "Std loss: 0.002466817356345802\n",
      "Total Loss: 0.04682688741013408\n",
      "------------------------------------ epoch 9508 (57042 steps) ------------------------------------\n",
      "Max loss: 0.01199079304933548\n",
      "Min loss: 0.004200879484415054\n",
      "Mean loss: 0.006286895057807366\n",
      "Std loss: 0.0027657627749959474\n",
      "Total Loss: 0.037721370346844196\n",
      "------------------------------------ epoch 9509 (57048 steps) ------------------------------------\n",
      "Max loss: 0.010717188939452171\n",
      "Min loss: 0.0044060396030545235\n",
      "Mean loss: 0.007200315672283371\n",
      "Std loss: 0.0023021015171703165\n",
      "Total Loss: 0.04320189403370023\n",
      "------------------------------------ epoch 9510 (57054 steps) ------------------------------------\n",
      "Max loss: 0.010022749193012714\n",
      "Min loss: 0.004727523773908615\n",
      "Mean loss: 0.006104878848418593\n",
      "Std loss: 0.0018661169214234717\n",
      "Total Loss: 0.03662927309051156\n",
      "------------------------------------ epoch 9511 (57060 steps) ------------------------------------\n",
      "Max loss: 0.02298123948276043\n",
      "Min loss: 0.0035586953163146973\n",
      "Mean loss: 0.009819479193538427\n",
      "Std loss: 0.006689862293090691\n",
      "Total Loss: 0.058916875161230564\n",
      "------------------------------------ epoch 9512 (57066 steps) ------------------------------------\n",
      "Max loss: 0.010626059025526047\n",
      "Min loss: 0.0037778178229928017\n",
      "Mean loss: 0.00728636234998703\n",
      "Std loss: 0.002161449742406963\n",
      "Total Loss: 0.04371817409992218\n",
      "------------------------------------ epoch 9513 (57072 steps) ------------------------------------\n",
      "Max loss: 0.013643430545926094\n",
      "Min loss: 0.0031126399990171194\n",
      "Mean loss: 0.005970933513405423\n",
      "Std loss: 0.0035225140736344397\n",
      "Total Loss: 0.035825601080432534\n",
      "------------------------------------ epoch 9514 (57078 steps) ------------------------------------\n",
      "Max loss: 0.020000699907541275\n",
      "Min loss: 0.004137836862355471\n",
      "Mean loss: 0.011464208597317338\n",
      "Std loss: 0.005019075492804421\n",
      "Total Loss: 0.06878525158390403\n",
      "------------------------------------ epoch 9515 (57084 steps) ------------------------------------\n",
      "Max loss: 0.011406870558857918\n",
      "Min loss: 0.0042504118755459785\n",
      "Mean loss: 0.007128579153989752\n",
      "Std loss: 0.002221517477238206\n",
      "Total Loss: 0.04277147492393851\n",
      "------------------------------------ epoch 9516 (57090 steps) ------------------------------------\n",
      "Max loss: 0.03526156395673752\n",
      "Min loss: 0.0035484489053487778\n",
      "Mean loss: 0.010109537979587913\n",
      "Std loss: 0.011274733416263852\n",
      "Total Loss: 0.060657227877527475\n",
      "------------------------------------ epoch 9517 (57096 steps) ------------------------------------\n",
      "Max loss: 0.009333085268735886\n",
      "Min loss: 0.005004209466278553\n",
      "Mean loss: 0.006698837736621499\n",
      "Std loss: 0.0016521557620285253\n",
      "Total Loss: 0.040193026419728994\n",
      "------------------------------------ epoch 9518 (57102 steps) ------------------------------------\n",
      "Max loss: 0.036279868334531784\n",
      "Min loss: 0.005416539963334799\n",
      "Mean loss: 0.012763635721057653\n",
      "Std loss: 0.010880768318824575\n",
      "Total Loss: 0.07658181432634592\n",
      "------------------------------------ epoch 9519 (57108 steps) ------------------------------------\n",
      "Max loss: 0.02632620930671692\n",
      "Min loss: 0.0038723768666386604\n",
      "Mean loss: 0.012242789069811503\n",
      "Std loss: 0.008406128988670276\n",
      "Total Loss: 0.07345673441886902\n",
      "------------------------------------ epoch 9520 (57114 steps) ------------------------------------\n",
      "Max loss: 0.016149315983057022\n",
      "Min loss: 0.004212804138660431\n",
      "Mean loss: 0.006979767854015033\n",
      "Std loss: 0.004143103866220702\n",
      "Total Loss: 0.041878607124090195\n",
      "------------------------------------ epoch 9521 (57120 steps) ------------------------------------\n",
      "Max loss: 0.019700150936841965\n",
      "Min loss: 0.0036242459900677204\n",
      "Mean loss: 0.007453505803520481\n",
      "Std loss: 0.0055540043599015135\n",
      "Total Loss: 0.044721034821122885\n",
      "------------------------------------ epoch 9522 (57126 steps) ------------------------------------\n",
      "Max loss: 0.018303576856851578\n",
      "Min loss: 0.004258497152477503\n",
      "Mean loss: 0.009599234443157911\n",
      "Std loss: 0.004317316828876012\n",
      "Total Loss: 0.05759540665894747\n",
      "------------------------------------ epoch 9523 (57132 steps) ------------------------------------\n",
      "Max loss: 0.01437288336455822\n",
      "Min loss: 0.0038212845101952553\n",
      "Mean loss: 0.007812354248017073\n",
      "Std loss: 0.004396632108352783\n",
      "Total Loss: 0.046874125488102436\n",
      "------------------------------------ epoch 9524 (57138 steps) ------------------------------------\n",
      "Max loss: 0.017503252252936363\n",
      "Min loss: 0.003247525542974472\n",
      "Mean loss: 0.0076137809082865715\n",
      "Std loss: 0.00501767168558171\n",
      "Total Loss: 0.04568268544971943\n",
      "------------------------------------ epoch 9525 (57144 steps) ------------------------------------\n",
      "Max loss: 0.008613168261945248\n",
      "Min loss: 0.0030425216536968946\n",
      "Mean loss: 0.006411322043277323\n",
      "Std loss: 0.0020189688730783578\n",
      "Total Loss: 0.03846793225966394\n",
      "------------------------------------ epoch 9526 (57150 steps) ------------------------------------\n",
      "Max loss: 0.012984204106032848\n",
      "Min loss: 0.0029629047494381666\n",
      "Mean loss: 0.007274065515957773\n",
      "Std loss: 0.003026382019910286\n",
      "Total Loss: 0.043644393095746636\n",
      "------------------------------------ epoch 9527 (57156 steps) ------------------------------------\n",
      "Max loss: 0.011121347546577454\n",
      "Min loss: 0.004033240955322981\n",
      "Mean loss: 0.0071513290361811714\n",
      "Std loss: 0.002904254479247725\n",
      "Total Loss: 0.04290797421708703\n",
      "------------------------------------ epoch 9528 (57162 steps) ------------------------------------\n",
      "Max loss: 0.0121956467628479\n",
      "Min loss: 0.0035993268247693777\n",
      "Mean loss: 0.00765390635933727\n",
      "Std loss: 0.003368188853979297\n",
      "Total Loss: 0.04592343815602362\n",
      "------------------------------------ epoch 9529 (57168 steps) ------------------------------------\n",
      "Max loss: 0.0061653731390833855\n",
      "Min loss: 0.0029517989605665207\n",
      "Mean loss: 0.004688850875633459\n",
      "Std loss: 0.0011760012917380784\n",
      "Total Loss: 0.02813310525380075\n",
      "------------------------------------ epoch 9530 (57174 steps) ------------------------------------\n",
      "Max loss: 0.020153356716036797\n",
      "Min loss: 0.003126736730337143\n",
      "Mean loss: 0.008661222721760472\n",
      "Std loss: 0.006005038906826097\n",
      "Total Loss: 0.05196733633056283\n",
      "------------------------------------ epoch 9531 (57180 steps) ------------------------------------\n",
      "Max loss: 0.008631905540823936\n",
      "Min loss: 0.003823427017778158\n",
      "Mean loss: 0.006269640677298109\n",
      "Std loss: 0.0015941875240963633\n",
      "Total Loss: 0.03761784406378865\n",
      "------------------------------------ epoch 9532 (57186 steps) ------------------------------------\n",
      "Max loss: 0.006858707871288061\n",
      "Min loss: 0.0035659056156873703\n",
      "Mean loss: 0.00494088251919796\n",
      "Std loss: 0.0013534974071858364\n",
      "Total Loss: 0.029645295115187764\n",
      "------------------------------------ epoch 9533 (57192 steps) ------------------------------------\n",
      "Max loss: 0.0159984789788723\n",
      "Min loss: 0.0031843711622059345\n",
      "Mean loss: 0.008671101337919632\n",
      "Std loss: 0.005075922303328791\n",
      "Total Loss: 0.052026608027517796\n",
      "------------------------------------ epoch 9534 (57198 steps) ------------------------------------\n",
      "Max loss: 0.036224380135536194\n",
      "Min loss: 0.0036887580063194036\n",
      "Mean loss: 0.010014427321342131\n",
      "Std loss: 0.011746376228721479\n",
      "Total Loss: 0.06008656392805278\n",
      "------------------------------------ epoch 9535 (57204 steps) ------------------------------------\n",
      "Max loss: 0.02180326357483864\n",
      "Min loss: 0.0036729376297444105\n",
      "Mean loss: 0.009179170747908453\n",
      "Std loss: 0.006297820070624188\n",
      "Total Loss: 0.05507502448745072\n",
      "------------------------------------ epoch 9536 (57210 steps) ------------------------------------\n",
      "Max loss: 0.011381353251636028\n",
      "Min loss: 0.005512030329555273\n",
      "Mean loss: 0.00792079969930152\n",
      "Std loss: 0.002011814534585608\n",
      "Total Loss: 0.047524798195809126\n",
      "------------------------------------ epoch 9537 (57216 steps) ------------------------------------\n",
      "Max loss: 0.010688714683055878\n",
      "Min loss: 0.005624443292617798\n",
      "Mean loss: 0.007589050180589159\n",
      "Std loss: 0.0015785659221847217\n",
      "Total Loss: 0.045534301083534956\n",
      "------------------------------------ epoch 9538 (57222 steps) ------------------------------------\n",
      "Max loss: 0.012449636124074459\n",
      "Min loss: 0.00537885632365942\n",
      "Mean loss: 0.008059201063588262\n",
      "Std loss: 0.0024654097608796353\n",
      "Total Loss: 0.04835520638152957\n",
      "------------------------------------ epoch 9539 (57228 steps) ------------------------------------\n",
      "Max loss: 0.023795945569872856\n",
      "Min loss: 0.003720611333847046\n",
      "Mean loss: 0.009812928813820085\n",
      "Std loss: 0.006544851721732159\n",
      "Total Loss: 0.058877572882920504\n",
      "------------------------------------ epoch 9540 (57234 steps) ------------------------------------\n",
      "Max loss: 0.024296274408698082\n",
      "Min loss: 0.0036794026382267475\n",
      "Mean loss: 0.009236494544893503\n",
      "Std loss: 0.007115998985372422\n",
      "Total Loss: 0.05541896726936102\n",
      "------------------------------------ epoch 9541 (57240 steps) ------------------------------------\n",
      "Max loss: 0.022180475294589996\n",
      "Min loss: 0.006011003162711859\n",
      "Mean loss: 0.012345229663575688\n",
      "Std loss: 0.006342931749293445\n",
      "Total Loss: 0.07407137798145413\n",
      "------------------------------------ epoch 9542 (57246 steps) ------------------------------------\n",
      "Max loss: 0.014160439372062683\n",
      "Min loss: 0.004175507929176092\n",
      "Mean loss: 0.00687610808139046\n",
      "Std loss: 0.003410311382517287\n",
      "Total Loss: 0.04125664848834276\n",
      "------------------------------------ epoch 9543 (57252 steps) ------------------------------------\n",
      "Max loss: 0.022329911589622498\n",
      "Min loss: 0.003891339525580406\n",
      "Mean loss: 0.01007149275392294\n",
      "Std loss: 0.006987334506267657\n",
      "Total Loss: 0.060428956523537636\n",
      "------------------------------------ epoch 9544 (57258 steps) ------------------------------------\n",
      "Max loss: 0.006637245416641235\n",
      "Min loss: 0.0045574018731713295\n",
      "Mean loss: 0.005612482627232869\n",
      "Std loss: 0.0007881147428541799\n",
      "Total Loss: 0.03367489576339722\n",
      "------------------------------------ epoch 9545 (57264 steps) ------------------------------------\n",
      "Max loss: 0.016063209623098373\n",
      "Min loss: 0.00359788304194808\n",
      "Mean loss: 0.007395874941721559\n",
      "Std loss: 0.004205755235166104\n",
      "Total Loss: 0.04437524965032935\n",
      "------------------------------------ epoch 9546 (57270 steps) ------------------------------------\n",
      "Max loss: 0.009048452600836754\n",
      "Min loss: 0.0038137086667120457\n",
      "Mean loss: 0.006410070384542148\n",
      "Std loss: 0.0019724800510309273\n",
      "Total Loss: 0.038460422307252884\n",
      "------------------------------------ epoch 9547 (57276 steps) ------------------------------------\n",
      "Max loss: 0.01786048337817192\n",
      "Min loss: 0.004457275848835707\n",
      "Mean loss: 0.008892506749058763\n",
      "Std loss: 0.004970397296127708\n",
      "Total Loss: 0.05335504049435258\n",
      "------------------------------------ epoch 9548 (57282 steps) ------------------------------------\n",
      "Max loss: 0.019134147092700005\n",
      "Min loss: 0.003400986548513174\n",
      "Mean loss: 0.009693445560211936\n",
      "Std loss: 0.005291904407840409\n",
      "Total Loss: 0.05816067336127162\n",
      "------------------------------------ epoch 9549 (57288 steps) ------------------------------------\n",
      "Max loss: 0.01883017085492611\n",
      "Min loss: 0.004029381554573774\n",
      "Mean loss: 0.00926277949474752\n",
      "Std loss: 0.004960920588979635\n",
      "Total Loss: 0.05557667696848512\n",
      "------------------------------------ epoch 9550 (57294 steps) ------------------------------------\n",
      "Max loss: 0.013575991615653038\n",
      "Min loss: 0.005125644616782665\n",
      "Mean loss: 0.00847974264373382\n",
      "Std loss: 0.002999563762267782\n",
      "Total Loss: 0.050878455862402916\n",
      "------------------------------------ epoch 9551 (57300 steps) ------------------------------------\n",
      "Max loss: 0.011544467881321907\n",
      "Min loss: 0.0035290070809423923\n",
      "Mean loss: 0.006824296433478594\n",
      "Std loss: 0.0032526173886446227\n",
      "Total Loss: 0.04094577860087156\n",
      "------------------------------------ epoch 9552 (57306 steps) ------------------------------------\n",
      "Max loss: 0.028412431478500366\n",
      "Min loss: 0.003548860549926758\n",
      "Mean loss: 0.013613403930018345\n",
      "Std loss: 0.008861409815459634\n",
      "Total Loss: 0.08168042358011007\n",
      "------------------------------------ epoch 9553 (57312 steps) ------------------------------------\n",
      "Max loss: 0.014160722494125366\n",
      "Min loss: 0.006322293542325497\n",
      "Mean loss: 0.010047045691559712\n",
      "Std loss: 0.003485746560141775\n",
      "Total Loss: 0.06028227414935827\n",
      "------------------------------------ epoch 9554 (57318 steps) ------------------------------------\n",
      "Max loss: 0.017107881605625153\n",
      "Min loss: 0.005160213448107243\n",
      "Mean loss: 0.010661376950641474\n",
      "Std loss: 0.004112893163603171\n",
      "Total Loss: 0.06396826170384884\n",
      "------------------------------------ epoch 9555 (57324 steps) ------------------------------------\n",
      "Max loss: 0.010655349120497704\n",
      "Min loss: 0.004170367494225502\n",
      "Mean loss: 0.007175161813696225\n",
      "Std loss: 0.0022171336006764093\n",
      "Total Loss: 0.04305097088217735\n",
      "------------------------------------ epoch 9556 (57330 steps) ------------------------------------\n",
      "Max loss: 0.0344850979745388\n",
      "Min loss: 0.0051507712341845036\n",
      "Mean loss: 0.012988644419237971\n",
      "Std loss: 0.009923870770058449\n",
      "Total Loss: 0.07793186651542783\n",
      "------------------------------------ epoch 9557 (57336 steps) ------------------------------------\n",
      "Max loss: 0.011232612654566765\n",
      "Min loss: 0.004002817906439304\n",
      "Mean loss: 0.007171362095202009\n",
      "Std loss: 0.0024023528149815063\n",
      "Total Loss: 0.04302817257121205\n",
      "------------------------------------ epoch 9558 (57342 steps) ------------------------------------\n",
      "Max loss: 0.007598875090479851\n",
      "Min loss: 0.00358267268165946\n",
      "Mean loss: 0.005194702185690403\n",
      "Std loss: 0.0012323666634980598\n",
      "Total Loss: 0.031168213114142418\n",
      "------------------------------------ epoch 9559 (57348 steps) ------------------------------------\n",
      "Max loss: 0.011324889957904816\n",
      "Min loss: 0.003336995840072632\n",
      "Mean loss: 0.006824360112659633\n",
      "Std loss: 0.0031494380537179143\n",
      "Total Loss: 0.0409461606759578\n",
      "------------------------------------ epoch 9560 (57354 steps) ------------------------------------\n",
      "Max loss: 0.014050820842385292\n",
      "Min loss: 0.003202032297849655\n",
      "Mean loss: 0.006566291442140937\n",
      "Std loss: 0.0036052189818483547\n",
      "Total Loss: 0.03939774865284562\n",
      "------------------------------------ epoch 9561 (57360 steps) ------------------------------------\n",
      "Max loss: 0.006258956156671047\n",
      "Min loss: 0.0032855267636477947\n",
      "Mean loss: 0.004517106339335442\n",
      "Std loss: 0.0011158889307586678\n",
      "Total Loss: 0.02710263803601265\n",
      "------------------------------------ epoch 9562 (57366 steps) ------------------------------------\n",
      "Max loss: 0.012470094487071037\n",
      "Min loss: 0.0052541326731443405\n",
      "Mean loss: 0.007552500038097302\n",
      "Std loss: 0.002335248075451948\n",
      "Total Loss: 0.04531500022858381\n",
      "------------------------------------ epoch 9563 (57372 steps) ------------------------------------\n",
      "Max loss: 0.014123925939202309\n",
      "Min loss: 0.002891378477215767\n",
      "Mean loss: 0.0067775011993944645\n",
      "Std loss: 0.003650405172806752\n",
      "Total Loss: 0.04066500719636679\n",
      "------------------------------------ epoch 9564 (57378 steps) ------------------------------------\n",
      "Max loss: 0.05470620095729828\n",
      "Min loss: 0.0057006580755114555\n",
      "Mean loss: 0.017819253417352836\n",
      "Std loss: 0.01673962981129876\n",
      "Total Loss: 0.10691552050411701\n",
      "------------------------------------ epoch 9565 (57384 steps) ------------------------------------\n",
      "Max loss: 0.01467522419989109\n",
      "Min loss: 0.005223127081990242\n",
      "Mean loss: 0.007721896981820464\n",
      "Std loss: 0.0035425711332584195\n",
      "Total Loss: 0.046331381890922785\n",
      "------------------------------------ epoch 9566 (57390 steps) ------------------------------------\n",
      "Max loss: 0.008609640412032604\n",
      "Min loss: 0.004416244570165873\n",
      "Mean loss: 0.005805199888224403\n",
      "Std loss: 0.001425074283691185\n",
      "Total Loss: 0.03483119932934642\n",
      "------------------------------------ epoch 9567 (57396 steps) ------------------------------------\n",
      "Max loss: 0.03371613845229149\n",
      "Min loss: 0.0040101539343595505\n",
      "Mean loss: 0.016610650345683098\n",
      "Std loss: 0.010228940514500672\n",
      "Total Loss: 0.09966390207409859\n",
      "------------------------------------ epoch 9568 (57402 steps) ------------------------------------\n",
      "Max loss: 0.027345141395926476\n",
      "Min loss: 0.004169972613453865\n",
      "Mean loss: 0.01117034574660162\n",
      "Std loss: 0.00775140412811191\n",
      "Total Loss: 0.06702207447960973\n",
      "------------------------------------ epoch 9569 (57408 steps) ------------------------------------\n",
      "Max loss: 0.012969104573130608\n",
      "Min loss: 0.004343181848526001\n",
      "Mean loss: 0.008648530347272754\n",
      "Std loss: 0.0035495993703041637\n",
      "Total Loss: 0.05189118208363652\n",
      "------------------------------------ epoch 9570 (57414 steps) ------------------------------------\n",
      "Max loss: 0.010803785175085068\n",
      "Min loss: 0.0038132648915052414\n",
      "Mean loss: 0.006404377364863952\n",
      "Std loss: 0.0023771848042184037\n",
      "Total Loss: 0.03842626418918371\n",
      "------------------------------------ epoch 9571 (57420 steps) ------------------------------------\n",
      "Max loss: 0.01982889324426651\n",
      "Min loss: 0.004434516187757254\n",
      "Mean loss: 0.00990371343990167\n",
      "Std loss: 0.005316153935149103\n",
      "Total Loss: 0.05942228063941002\n",
      "------------------------------------ epoch 9572 (57426 steps) ------------------------------------\n",
      "Max loss: 0.028455620631575584\n",
      "Min loss: 0.005506432615220547\n",
      "Mean loss: 0.012047147223105034\n",
      "Std loss: 0.0075309749304971075\n",
      "Total Loss: 0.0722828833386302\n",
      "------------------------------------ epoch 9573 (57432 steps) ------------------------------------\n",
      "Max loss: 0.01510457694530487\n",
      "Min loss: 0.0060402690432965755\n",
      "Mean loss: 0.009790314516673485\n",
      "Std loss: 0.0037747120575436894\n",
      "Total Loss: 0.05874188710004091\n",
      "------------------------------------ epoch 9574 (57438 steps) ------------------------------------\n",
      "Max loss: 0.05225799232721329\n",
      "Min loss: 0.008832491934299469\n",
      "Mean loss: 0.019677345640957355\n",
      "Std loss: 0.015143441266961898\n",
      "Total Loss: 0.11806407384574413\n",
      "------------------------------------ epoch 9575 (57444 steps) ------------------------------------\n",
      "Max loss: 0.018435847014188766\n",
      "Min loss: 0.007161432411521673\n",
      "Mean loss: 0.01071141908566157\n",
      "Std loss: 0.003947469191858521\n",
      "Total Loss: 0.06426851451396942\n",
      "------------------------------------ epoch 9576 (57450 steps) ------------------------------------\n",
      "Max loss: 0.01790386438369751\n",
      "Min loss: 0.00887647457420826\n",
      "Mean loss: 0.012238021319111189\n",
      "Std loss: 0.0034905644500340177\n",
      "Total Loss: 0.07342812791466713\n",
      "------------------------------------ epoch 9577 (57456 steps) ------------------------------------\n",
      "Max loss: 0.05075078830122948\n",
      "Min loss: 0.008635211735963821\n",
      "Mean loss: 0.016940000001341105\n",
      "Std loss: 0.015157616848086818\n",
      "Total Loss: 0.10164000000804663\n",
      "------------------------------------ epoch 9578 (57462 steps) ------------------------------------\n",
      "Max loss: 0.01788274198770523\n",
      "Min loss: 0.00565685098990798\n",
      "Mean loss: 0.010793834070985516\n",
      "Std loss: 0.005109906860642431\n",
      "Total Loss: 0.0647630044259131\n",
      "------------------------------------ epoch 9579 (57468 steps) ------------------------------------\n",
      "Max loss: 0.022249143570661545\n",
      "Min loss: 0.010692788287997246\n",
      "Mean loss: 0.014868256946404776\n",
      "Std loss: 0.004310255118641489\n",
      "Total Loss: 0.08920954167842865\n",
      "------------------------------------ epoch 9580 (57474 steps) ------------------------------------\n",
      "Max loss: 0.0203109048306942\n",
      "Min loss: 0.0045308503322303295\n",
      "Mean loss: 0.009544071198130647\n",
      "Std loss: 0.0051492709913034595\n",
      "Total Loss: 0.057264427188783884\n",
      "------------------------------------ epoch 9581 (57480 steps) ------------------------------------\n",
      "Max loss: 0.0599345862865448\n",
      "Min loss: 0.006069660186767578\n",
      "Mean loss: 0.017461095781375963\n",
      "Std loss: 0.019187675734089596\n",
      "Total Loss: 0.10476657468825579\n",
      "------------------------------------ epoch 9582 (57486 steps) ------------------------------------\n",
      "Max loss: 0.013425992801785469\n",
      "Min loss: 0.004666413180530071\n",
      "Mean loss: 0.009178611993168792\n",
      "Std loss: 0.0030676009336322555\n",
      "Total Loss: 0.05507167195901275\n",
      "------------------------------------ epoch 9583 (57492 steps) ------------------------------------\n",
      "Max loss: 0.03580490127205849\n",
      "Min loss: 0.004146880470216274\n",
      "Mean loss: 0.013862943199152747\n",
      "Std loss: 0.010264441382407893\n",
      "Total Loss: 0.08317765919491649\n",
      "------------------------------------ epoch 9584 (57498 steps) ------------------------------------\n",
      "Max loss: 0.00914901401847601\n",
      "Min loss: 0.0056509836576879025\n",
      "Mean loss: 0.0072147745328644914\n",
      "Std loss: 0.0012040527682567067\n",
      "Total Loss: 0.04328864719718695\n",
      "------------------------------------ epoch 9585 (57504 steps) ------------------------------------\n",
      "Max loss: 0.01126219891011715\n",
      "Min loss: 0.004449737258255482\n",
      "Mean loss: 0.007519526872783899\n",
      "Std loss: 0.0024373102997542486\n",
      "Total Loss: 0.045117161236703396\n",
      "------------------------------------ epoch 9586 (57510 steps) ------------------------------------\n",
      "Max loss: 0.009866805747151375\n",
      "Min loss: 0.0048862965777516365\n",
      "Mean loss: 0.006796530059849222\n",
      "Std loss: 0.001713357809243955\n",
      "Total Loss: 0.040779180359095335\n",
      "------------------------------------ epoch 9587 (57516 steps) ------------------------------------\n",
      "Max loss: 0.029905257746577263\n",
      "Min loss: 0.0035648103803396225\n",
      "Mean loss: 0.012144375398444632\n",
      "Std loss: 0.010269482477729835\n",
      "Total Loss: 0.0728662523906678\n",
      "------------------------------------ epoch 9588 (57522 steps) ------------------------------------\n",
      "Max loss: 0.011235084384679794\n",
      "Min loss: 0.0034468385856598616\n",
      "Mean loss: 0.005393754264029364\n",
      "Std loss: 0.0026454778777381133\n",
      "Total Loss: 0.03236252558417618\n",
      "------------------------------------ epoch 9589 (57528 steps) ------------------------------------\n",
      "Max loss: 0.01780002750456333\n",
      "Min loss: 0.003931768238544464\n",
      "Mean loss: 0.010973759771635136\n",
      "Std loss: 0.005247373118997819\n",
      "Total Loss: 0.06584255862981081\n",
      "------------------------------------ epoch 9590 (57534 steps) ------------------------------------\n",
      "Max loss: 0.012586817145347595\n",
      "Min loss: 0.0038250149227678776\n",
      "Mean loss: 0.007931762374937534\n",
      "Std loss: 0.002991020590405501\n",
      "Total Loss: 0.047590574249625206\n",
      "------------------------------------ epoch 9591 (57540 steps) ------------------------------------\n",
      "Max loss: 0.025711484253406525\n",
      "Min loss: 0.004681270569562912\n",
      "Mean loss: 0.012612164486199617\n",
      "Std loss: 0.008027089814653717\n",
      "Total Loss: 0.0756729869171977\n",
      "------------------------------------ epoch 9592 (57546 steps) ------------------------------------\n",
      "Max loss: 0.014028079807758331\n",
      "Min loss: 0.004563347902148962\n",
      "Mean loss: 0.0075703147643556195\n",
      "Std loss: 0.0031231133953534356\n",
      "Total Loss: 0.04542188858613372\n",
      "------------------------------------ epoch 9593 (57552 steps) ------------------------------------\n",
      "Max loss: 0.020887477323412895\n",
      "Min loss: 0.005079965107142925\n",
      "Mean loss: 0.009508091025054455\n",
      "Std loss: 0.005262114572303902\n",
      "Total Loss: 0.05704854615032673\n",
      "------------------------------------ epoch 9594 (57558 steps) ------------------------------------\n",
      "Max loss: 0.012254586443305016\n",
      "Min loss: 0.006258975714445114\n",
      "Mean loss: 0.008614904790495833\n",
      "Std loss: 0.0023417346396583014\n",
      "Total Loss: 0.051689428742975\n",
      "------------------------------------ epoch 9595 (57564 steps) ------------------------------------\n",
      "Max loss: 0.021946489810943604\n",
      "Min loss: 0.0072668432258069515\n",
      "Mean loss: 0.011320203154658278\n",
      "Std loss: 0.005071583435616261\n",
      "Total Loss: 0.06792121892794967\n",
      "------------------------------------ epoch 9596 (57570 steps) ------------------------------------\n",
      "Max loss: 0.00835435464978218\n",
      "Min loss: 0.0039430828765034676\n",
      "Mean loss: 0.0059631279824922485\n",
      "Std loss: 0.0014039613300438473\n",
      "Total Loss: 0.03577876789495349\n",
      "------------------------------------ epoch 9597 (57576 steps) ------------------------------------\n",
      "Max loss: 0.00868381466716528\n",
      "Min loss: 0.005167968105524778\n",
      "Mean loss: 0.006392416854699452\n",
      "Std loss: 0.0013563817586631829\n",
      "Total Loss: 0.038354501128196716\n",
      "------------------------------------ epoch 9598 (57582 steps) ------------------------------------\n",
      "Max loss: 0.020826555788517\n",
      "Min loss: 0.003754864213988185\n",
      "Mean loss: 0.008185317545818785\n",
      "Std loss: 0.006017843205033272\n",
      "Total Loss: 0.049111905274912715\n",
      "------------------------------------ epoch 9599 (57588 steps) ------------------------------------\n",
      "Max loss: 0.009371849708259106\n",
      "Min loss: 0.00316453306004405\n",
      "Mean loss: 0.0065836353072275715\n",
      "Std loss: 0.0024796332956612804\n",
      "Total Loss: 0.03950181184336543\n",
      "------------------------------------ epoch 9600 (57594 steps) ------------------------------------\n",
      "Max loss: 0.02395310066640377\n",
      "Min loss: 0.004625862464308739\n",
      "Mean loss: 0.011013758291179935\n",
      "Std loss: 0.007013808920748794\n",
      "Total Loss: 0.06608254974707961\n",
      "------------------------------------ epoch 9601 (57600 steps) ------------------------------------\n",
      "Max loss: 0.03589892014861107\n",
      "Min loss: 0.004981228616088629\n",
      "Mean loss: 0.012301323392118016\n",
      "Std loss: 0.01072785895989556\n",
      "Total Loss: 0.0738079403527081\n",
      "saved model at ./weights/model_9601.pth\n",
      "------------------------------------ epoch 9602 (57606 steps) ------------------------------------\n",
      "Max loss: 0.021999873220920563\n",
      "Min loss: 0.004485514480620623\n",
      "Mean loss: 0.010002814000472426\n",
      "Std loss: 0.006119564775135086\n",
      "Total Loss: 0.06001688400283456\n",
      "------------------------------------ epoch 9603 (57612 steps) ------------------------------------\n",
      "Max loss: 0.022040298208594322\n",
      "Min loss: 0.005175793543457985\n",
      "Mean loss: 0.008705362599963943\n",
      "Std loss: 0.006023532286910614\n",
      "Total Loss: 0.05223217559978366\n",
      "------------------------------------ epoch 9604 (57618 steps) ------------------------------------\n",
      "Max loss: 0.014985653571784496\n",
      "Min loss: 0.0060187652707099915\n",
      "Mean loss: 0.010186195839196444\n",
      "Std loss: 0.003531734409023873\n",
      "Total Loss: 0.06111717503517866\n",
      "------------------------------------ epoch 9605 (57624 steps) ------------------------------------\n",
      "Max loss: 0.008880318142473698\n",
      "Min loss: 0.003909585997462273\n",
      "Mean loss: 0.006864913739264011\n",
      "Std loss: 0.001630007906736925\n",
      "Total Loss: 0.04118948243558407\n",
      "------------------------------------ epoch 9606 (57630 steps) ------------------------------------\n",
      "Max loss: 0.012360582128167152\n",
      "Min loss: 0.0037730063777416945\n",
      "Mean loss: 0.007890905680445334\n",
      "Std loss: 0.0033178414033700793\n",
      "Total Loss: 0.047345434082672\n",
      "------------------------------------ epoch 9607 (57636 steps) ------------------------------------\n",
      "Max loss: 0.009772713296115398\n",
      "Min loss: 0.0038280149456113577\n",
      "Mean loss: 0.0057395914336666465\n",
      "Std loss: 0.001965470744004386\n",
      "Total Loss: 0.03443754860199988\n",
      "------------------------------------ epoch 9608 (57642 steps) ------------------------------------\n",
      "Max loss: 0.00665578618645668\n",
      "Min loss: 0.00450090691447258\n",
      "Mean loss: 0.005576690969367822\n",
      "Std loss: 0.0007737604985552025\n",
      "Total Loss: 0.03346014581620693\n",
      "------------------------------------ epoch 9609 (57648 steps) ------------------------------------\n",
      "Max loss: 0.010786011815071106\n",
      "Min loss: 0.003529686015099287\n",
      "Mean loss: 0.006522101427738865\n",
      "Std loss: 0.002280880946850567\n",
      "Total Loss: 0.03913260856643319\n",
      "------------------------------------ epoch 9610 (57654 steps) ------------------------------------\n",
      "Max loss: 0.017909366637468338\n",
      "Min loss: 0.0033898025285452604\n",
      "Mean loss: 0.008732636187536022\n",
      "Std loss: 0.004987264499258329\n",
      "Total Loss: 0.052395817125216126\n",
      "------------------------------------ epoch 9611 (57660 steps) ------------------------------------\n",
      "Max loss: 0.014831680804491043\n",
      "Min loss: 0.0032051182352006435\n",
      "Mean loss: 0.009451049923275908\n",
      "Std loss: 0.004625164341168699\n",
      "Total Loss: 0.05670629953965545\n",
      "------------------------------------ epoch 9612 (57666 steps) ------------------------------------\n",
      "Max loss: 0.013132356107234955\n",
      "Min loss: 0.003939132206141949\n",
      "Mean loss: 0.006331098886827628\n",
      "Std loss: 0.0031554344061306707\n",
      "Total Loss: 0.03798659332096577\n",
      "------------------------------------ epoch 9613 (57672 steps) ------------------------------------\n",
      "Max loss: 0.013909298926591873\n",
      "Min loss: 0.004791860934346914\n",
      "Mean loss: 0.008475517388433218\n",
      "Std loss: 0.003254586603930242\n",
      "Total Loss: 0.05085310433059931\n",
      "------------------------------------ epoch 9614 (57678 steps) ------------------------------------\n",
      "Max loss: 0.009639340452849865\n",
      "Min loss: 0.0031621581874787807\n",
      "Mean loss: 0.005615326653545101\n",
      "Std loss: 0.0020060827772020225\n",
      "Total Loss: 0.03369195992127061\n",
      "------------------------------------ epoch 9615 (57684 steps) ------------------------------------\n",
      "Max loss: 0.015091143548488617\n",
      "Min loss: 0.00314994715154171\n",
      "Mean loss: 0.007072964605564873\n",
      "Std loss: 0.003959727672516127\n",
      "Total Loss: 0.042437787633389235\n",
      "------------------------------------ epoch 9616 (57690 steps) ------------------------------------\n",
      "Max loss: 0.013192152604460716\n",
      "Min loss: 0.005177036393433809\n",
      "Mean loss: 0.0076861159565548105\n",
      "Std loss: 0.00274679201135055\n",
      "Total Loss: 0.04611669573932886\n",
      "------------------------------------ epoch 9617 (57696 steps) ------------------------------------\n",
      "Max loss: 0.012778349220752716\n",
      "Min loss: 0.0037640505470335484\n",
      "Mean loss: 0.008185359726970395\n",
      "Std loss: 0.0028837350696565294\n",
      "Total Loss: 0.04911215836182237\n",
      "------------------------------------ epoch 9618 (57702 steps) ------------------------------------\n",
      "Max loss: 0.01420571468770504\n",
      "Min loss: 0.0031583490781486034\n",
      "Mean loss: 0.006697362948519488\n",
      "Std loss: 0.003974219846642409\n",
      "Total Loss: 0.04018417769111693\n",
      "------------------------------------ epoch 9619 (57708 steps) ------------------------------------\n",
      "Max loss: 0.01325188111513853\n",
      "Min loss: 0.002976005431264639\n",
      "Mean loss: 0.005761363155518969\n",
      "Std loss: 0.0035557956936745523\n",
      "Total Loss: 0.03456817893311381\n",
      "------------------------------------ epoch 9620 (57714 steps) ------------------------------------\n",
      "Max loss: 0.014461448416113853\n",
      "Min loss: 0.0035628066398203373\n",
      "Mean loss: 0.00706441467627883\n",
      "Std loss: 0.0035444504053749257\n",
      "Total Loss: 0.04238648805767298\n",
      "------------------------------------ epoch 9621 (57720 steps) ------------------------------------\n",
      "Max loss: 0.01615772768855095\n",
      "Min loss: 0.0033473162911832333\n",
      "Mean loss: 0.00791694592529287\n",
      "Std loss: 0.004932977199044838\n",
      "Total Loss: 0.047501675551757216\n",
      "------------------------------------ epoch 9622 (57726 steps) ------------------------------------\n",
      "Max loss: 0.00884255301207304\n",
      "Min loss: 0.003056309884414077\n",
      "Mean loss: 0.005237577444252868\n",
      "Std loss: 0.0019190166604828162\n",
      "Total Loss: 0.03142546466551721\n",
      "------------------------------------ epoch 9623 (57732 steps) ------------------------------------\n",
      "Max loss: 0.028374165296554565\n",
      "Min loss: 0.004338445141911507\n",
      "Mean loss: 0.012462087363625566\n",
      "Std loss: 0.009183412553065129\n",
      "Total Loss: 0.0747725241817534\n",
      "------------------------------------ epoch 9624 (57738 steps) ------------------------------------\n",
      "Max loss: 0.01126161776483059\n",
      "Min loss: 0.005360608454793692\n",
      "Mean loss: 0.007378680243467291\n",
      "Std loss: 0.0020196838835169597\n",
      "Total Loss: 0.04427208146080375\n",
      "------------------------------------ epoch 9625 (57744 steps) ------------------------------------\n",
      "Max loss: 0.013895498588681221\n",
      "Min loss: 0.0049620745703577995\n",
      "Mean loss: 0.008591829255844155\n",
      "Std loss: 0.0038596799379616033\n",
      "Total Loss: 0.051550975535064936\n",
      "------------------------------------ epoch 9626 (57750 steps) ------------------------------------\n",
      "Max loss: 0.010761436074972153\n",
      "Min loss: 0.004539850167930126\n",
      "Mean loss: 0.007553154059375326\n",
      "Std loss: 0.0023567818504948586\n",
      "Total Loss: 0.045318924356251955\n",
      "------------------------------------ epoch 9627 (57756 steps) ------------------------------------\n",
      "Max loss: 0.030746085569262505\n",
      "Min loss: 0.0036672712303698063\n",
      "Mean loss: 0.011680783393482367\n",
      "Std loss: 0.009654238294799432\n",
      "Total Loss: 0.0700847003608942\n",
      "------------------------------------ epoch 9628 (57762 steps) ------------------------------------\n",
      "Max loss: 0.04588400572538376\n",
      "Min loss: 0.003519946476444602\n",
      "Mean loss: 0.013905108867523571\n",
      "Std loss: 0.014546517351195606\n",
      "Total Loss: 0.08343065320514143\n",
      "------------------------------------ epoch 9629 (57768 steps) ------------------------------------\n",
      "Max loss: 0.016922440379858017\n",
      "Min loss: 0.006309870630502701\n",
      "Mean loss: 0.009647970087826252\n",
      "Std loss: 0.0036269005978907774\n",
      "Total Loss: 0.05788782052695751\n",
      "------------------------------------ epoch 9630 (57774 steps) ------------------------------------\n",
      "Max loss: 0.01164574921131134\n",
      "Min loss: 0.004096249584108591\n",
      "Mean loss: 0.0073557990447928505\n",
      "Std loss: 0.0027366542665112083\n",
      "Total Loss: 0.044134794268757105\n",
      "------------------------------------ epoch 9631 (57780 steps) ------------------------------------\n",
      "Max loss: 0.018490293994545937\n",
      "Min loss: 0.004201917443424463\n",
      "Mean loss: 0.009150483024617037\n",
      "Std loss: 0.005086129614577369\n",
      "Total Loss: 0.05490289814770222\n",
      "------------------------------------ epoch 9632 (57786 steps) ------------------------------------\n",
      "Max loss: 0.014246384613215923\n",
      "Min loss: 0.004032758995890617\n",
      "Mean loss: 0.006308339303359389\n",
      "Std loss: 0.003621378140571966\n",
      "Total Loss: 0.037850035820156336\n",
      "------------------------------------ epoch 9633 (57792 steps) ------------------------------------\n",
      "Max loss: 0.01883089914917946\n",
      "Min loss: 0.0038761384785175323\n",
      "Mean loss: 0.007992584723979235\n",
      "Std loss: 0.00528707269989656\n",
      "Total Loss: 0.04795550834387541\n",
      "------------------------------------ epoch 9634 (57798 steps) ------------------------------------\n",
      "Max loss: 0.03283911943435669\n",
      "Min loss: 0.003898028749972582\n",
      "Mean loss: 0.010030964156612754\n",
      "Std loss: 0.010463568551516091\n",
      "Total Loss: 0.06018578493967652\n",
      "------------------------------------ epoch 9635 (57804 steps) ------------------------------------\n",
      "Max loss: 0.03617943450808525\n",
      "Min loss: 0.005789308808743954\n",
      "Mean loss: 0.014105111205329498\n",
      "Std loss: 0.010533873904854017\n",
      "Total Loss: 0.08463066723197699\n",
      "------------------------------------ epoch 9636 (57810 steps) ------------------------------------\n",
      "Max loss: 0.037629134953022\n",
      "Min loss: 0.00449276901781559\n",
      "Mean loss: 0.01707551923270027\n",
      "Std loss: 0.010692621147948304\n",
      "Total Loss: 0.10245311539620161\n",
      "------------------------------------ epoch 9637 (57816 steps) ------------------------------------\n",
      "Max loss: 0.0144009068608284\n",
      "Min loss: 0.005369998048990965\n",
      "Mean loss: 0.008252120266358057\n",
      "Std loss: 0.0031669274208003047\n",
      "Total Loss: 0.049512721598148346\n",
      "------------------------------------ epoch 9638 (57822 steps) ------------------------------------\n",
      "Max loss: 0.01791132614016533\n",
      "Min loss: 0.007625853642821312\n",
      "Mean loss: 0.01150718618494769\n",
      "Std loss: 0.003995602180611234\n",
      "Total Loss: 0.06904311710968614\n",
      "------------------------------------ epoch 9639 (57828 steps) ------------------------------------\n",
      "Max loss: 0.03038817271590233\n",
      "Min loss: 0.004346023313701153\n",
      "Mean loss: 0.012005731463432312\n",
      "Std loss: 0.008541631365329778\n",
      "Total Loss: 0.07203438878059387\n",
      "------------------------------------ epoch 9640 (57834 steps) ------------------------------------\n",
      "Max loss: 0.01452330406755209\n",
      "Min loss: 0.005519644822925329\n",
      "Mean loss: 0.010051973552132646\n",
      "Std loss: 0.003256262401468337\n",
      "Total Loss: 0.06031184131279588\n",
      "------------------------------------ epoch 9641 (57840 steps) ------------------------------------\n",
      "Max loss: 0.05314951390028\n",
      "Min loss: 0.006031674798578024\n",
      "Mean loss: 0.02093029549966256\n",
      "Std loss: 0.0193103564104615\n",
      "Total Loss: 0.12558177299797535\n",
      "------------------------------------ epoch 9642 (57846 steps) ------------------------------------\n",
      "Max loss: 0.042906828224658966\n",
      "Min loss: 0.006743370555341244\n",
      "Mean loss: 0.017916199285537004\n",
      "Std loss: 0.012323513884429075\n",
      "Total Loss: 0.10749719571322203\n",
      "------------------------------------ epoch 9643 (57852 steps) ------------------------------------\n",
      "Max loss: 0.0306691974401474\n",
      "Min loss: 0.004793467000126839\n",
      "Mean loss: 0.01400427178790172\n",
      "Std loss: 0.008631003337023982\n",
      "Total Loss: 0.08402563072741032\n",
      "------------------------------------ epoch 9644 (57858 steps) ------------------------------------\n",
      "Max loss: 0.01645059511065483\n",
      "Min loss: 0.007870901376008987\n",
      "Mean loss: 0.011686214711517096\n",
      "Std loss: 0.0027516612405269793\n",
      "Total Loss: 0.07011728826910257\n",
      "------------------------------------ epoch 9645 (57864 steps) ------------------------------------\n",
      "Max loss: 0.016028989106416702\n",
      "Min loss: 0.005152438301593065\n",
      "Mean loss: 0.009830023550118009\n",
      "Std loss: 0.004338502520338553\n",
      "Total Loss: 0.058980141300708055\n",
      "------------------------------------ epoch 9646 (57870 steps) ------------------------------------\n",
      "Max loss: 0.010298114269971848\n",
      "Min loss: 0.005400219466537237\n",
      "Mean loss: 0.006923561450093985\n",
      "Std loss: 0.0016359617320752934\n",
      "Total Loss: 0.04154136870056391\n",
      "------------------------------------ epoch 9647 (57876 steps) ------------------------------------\n",
      "Max loss: 0.013321589678525925\n",
      "Min loss: 0.005511218216270208\n",
      "Mean loss: 0.00856648269109428\n",
      "Std loss: 0.0027828462431122056\n",
      "Total Loss: 0.051398896146565676\n",
      "------------------------------------ epoch 9648 (57882 steps) ------------------------------------\n",
      "Max loss: 0.01638755574822426\n",
      "Min loss: 0.0038307958748191595\n",
      "Mean loss: 0.007071622841370602\n",
      "Std loss: 0.004349945996779097\n",
      "Total Loss: 0.042429737048223615\n",
      "------------------------------------ epoch 9649 (57888 steps) ------------------------------------\n",
      "Max loss: 0.012836023233830929\n",
      "Min loss: 0.003952648025006056\n",
      "Mean loss: 0.007286513379464547\n",
      "Std loss: 0.002969799715409117\n",
      "Total Loss: 0.04371908027678728\n",
      "------------------------------------ epoch 9650 (57894 steps) ------------------------------------\n",
      "Max loss: 0.0164988674223423\n",
      "Min loss: 0.005662709474563599\n",
      "Mean loss: 0.009631657895321647\n",
      "Std loss: 0.003647867663025682\n",
      "Total Loss: 0.057789947371929884\n",
      "------------------------------------ epoch 9651 (57900 steps) ------------------------------------\n",
      "Max loss: 0.011185089126229286\n",
      "Min loss: 0.0037930018734186888\n",
      "Mean loss: 0.0074333309118325515\n",
      "Std loss: 0.0023882537098528864\n",
      "Total Loss: 0.04459998547099531\n",
      "------------------------------------ epoch 9652 (57906 steps) ------------------------------------\n",
      "Max loss: 0.017111627385020256\n",
      "Min loss: 0.003908014856278896\n",
      "Mean loss: 0.00887548845882217\n",
      "Std loss: 0.0045560456172533\n",
      "Total Loss: 0.053252930752933025\n",
      "------------------------------------ epoch 9653 (57912 steps) ------------------------------------\n",
      "Max loss: 0.034359924495220184\n",
      "Min loss: 0.004124999977648258\n",
      "Mean loss: 0.010565921353797117\n",
      "Std loss: 0.010761961885249069\n",
      "Total Loss: 0.06339552812278271\n",
      "------------------------------------ epoch 9654 (57918 steps) ------------------------------------\n",
      "Max loss: 0.016668427735567093\n",
      "Min loss: 0.004404003731906414\n",
      "Mean loss: 0.007169215629498164\n",
      "Std loss: 0.004315883180271521\n",
      "Total Loss: 0.04301529377698898\n",
      "------------------------------------ epoch 9655 (57924 steps) ------------------------------------\n",
      "Max loss: 0.015113135799765587\n",
      "Min loss: 0.004719078075140715\n",
      "Mean loss: 0.009137567210321626\n",
      "Std loss: 0.003446830555564693\n",
      "Total Loss: 0.05482540326192975\n",
      "------------------------------------ epoch 9656 (57930 steps) ------------------------------------\n",
      "Max loss: 0.02075081132352352\n",
      "Min loss: 0.004385052714496851\n",
      "Mean loss: 0.007524074132864674\n",
      "Std loss: 0.005931194078903099\n",
      "Total Loss: 0.045144444797188044\n",
      "------------------------------------ epoch 9657 (57936 steps) ------------------------------------\n",
      "Max loss: 0.015555160120129585\n",
      "Min loss: 0.005268608685582876\n",
      "Mean loss: 0.010348909301683307\n",
      "Std loss: 0.003981325843908931\n",
      "Total Loss: 0.06209345581009984\n",
      "------------------------------------ epoch 9658 (57942 steps) ------------------------------------\n",
      "Max loss: 0.011562332510948181\n",
      "Min loss: 0.0034605753608047962\n",
      "Mean loss: 0.007591175613924861\n",
      "Std loss: 0.003104983712279131\n",
      "Total Loss: 0.045547053683549166\n",
      "------------------------------------ epoch 9659 (57948 steps) ------------------------------------\n",
      "Max loss: 0.03557303175330162\n",
      "Min loss: 0.0033045026939362288\n",
      "Mean loss: 0.013793252835360667\n",
      "Std loss: 0.01076381198729206\n",
      "Total Loss: 0.082759517012164\n",
      "------------------------------------ epoch 9660 (57954 steps) ------------------------------------\n",
      "Max loss: 0.011642410419881344\n",
      "Min loss: 0.004686309956014156\n",
      "Mean loss: 0.008156795054674149\n",
      "Std loss: 0.0025789374326902277\n",
      "Total Loss: 0.04894077032804489\n",
      "------------------------------------ epoch 9661 (57960 steps) ------------------------------------\n",
      "Max loss: 0.01537143811583519\n",
      "Min loss: 0.0034183270763605833\n",
      "Mean loss: 0.008811891893856227\n",
      "Std loss: 0.0048366388451999845\n",
      "Total Loss: 0.052871351363137364\n",
      "------------------------------------ epoch 9662 (57966 steps) ------------------------------------\n",
      "Max loss: 0.009632446803152561\n",
      "Min loss: 0.003405525116249919\n",
      "Mean loss: 0.00654691089099894\n",
      "Std loss: 0.002263972313845491\n",
      "Total Loss: 0.03928146534599364\n",
      "------------------------------------ epoch 9663 (57972 steps) ------------------------------------\n",
      "Max loss: 0.011751635000109673\n",
      "Min loss: 0.0036584879271686077\n",
      "Mean loss: 0.006334480519096057\n",
      "Std loss: 0.0027289667641542717\n",
      "Total Loss: 0.03800688311457634\n",
      "------------------------------------ epoch 9664 (57978 steps) ------------------------------------\n",
      "Max loss: 0.0061163529753685\n",
      "Min loss: 0.004041384905576706\n",
      "Mean loss: 0.00490881137860318\n",
      "Std loss: 0.0006948960276541653\n",
      "Total Loss: 0.02945286827161908\n",
      "------------------------------------ epoch 9665 (57984 steps) ------------------------------------\n",
      "Max loss: 0.012617437168955803\n",
      "Min loss: 0.0038710497319698334\n",
      "Mean loss: 0.006869517965242267\n",
      "Std loss: 0.0029761996951700427\n",
      "Total Loss: 0.0412171077914536\n",
      "------------------------------------ epoch 9666 (57990 steps) ------------------------------------\n",
      "Max loss: 0.007325795479118824\n",
      "Min loss: 0.0037393688689917326\n",
      "Mean loss: 0.005153088869216542\n",
      "Std loss: 0.001408797488780706\n",
      "Total Loss: 0.03091853321529925\n",
      "------------------------------------ epoch 9667 (57996 steps) ------------------------------------\n",
      "Max loss: 0.02181842550635338\n",
      "Min loss: 0.0036429320462048054\n",
      "Mean loss: 0.009226676697532335\n",
      "Std loss: 0.006131442348632057\n",
      "Total Loss: 0.055360060185194016\n",
      "------------------------------------ epoch 9668 (58002 steps) ------------------------------------\n",
      "Max loss: 0.008681166917085648\n",
      "Min loss: 0.00447107432410121\n",
      "Mean loss: 0.005856329808011651\n",
      "Std loss: 0.0014006621681376594\n",
      "Total Loss: 0.035137978848069906\n",
      "------------------------------------ epoch 9669 (58008 steps) ------------------------------------\n",
      "Max loss: 0.023741241544485092\n",
      "Min loss: 0.003244306892156601\n",
      "Mean loss: 0.007849013665691018\n",
      "Std loss: 0.0071912700960772595\n",
      "Total Loss: 0.04709408199414611\n",
      "------------------------------------ epoch 9670 (58014 steps) ------------------------------------\n",
      "Max loss: 0.01834472082555294\n",
      "Min loss: 0.004957263357937336\n",
      "Mean loss: 0.00849909521639347\n",
      "Std loss: 0.004559483405686327\n",
      "Total Loss: 0.050994571298360825\n",
      "------------------------------------ epoch 9671 (58020 steps) ------------------------------------\n",
      "Max loss: 0.03172992169857025\n",
      "Min loss: 0.005187137518078089\n",
      "Mean loss: 0.010995098001634082\n",
      "Std loss: 0.009374578277918955\n",
      "Total Loss: 0.06597058800980449\n",
      "------------------------------------ epoch 9672 (58026 steps) ------------------------------------\n",
      "Max loss: 0.019767694175243378\n",
      "Min loss: 0.003517014440149069\n",
      "Mean loss: 0.008331174496561289\n",
      "Std loss: 0.005479365476639573\n",
      "Total Loss: 0.04998704697936773\n",
      "------------------------------------ epoch 9673 (58032 steps) ------------------------------------\n",
      "Max loss: 0.021004019305109978\n",
      "Min loss: 0.005321194883435965\n",
      "Mean loss: 0.01155174127779901\n",
      "Std loss: 0.004968568735321596\n",
      "Total Loss: 0.06931044766679406\n",
      "------------------------------------ epoch 9674 (58038 steps) ------------------------------------\n",
      "Max loss: 0.010365001857280731\n",
      "Min loss: 0.005606553517282009\n",
      "Mean loss: 0.007956396322697401\n",
      "Std loss: 0.0018390709996968227\n",
      "Total Loss: 0.047738377936184406\n",
      "------------------------------------ epoch 9675 (58044 steps) ------------------------------------\n",
      "Max loss: 0.019478628411889076\n",
      "Min loss: 0.0039010634645819664\n",
      "Mean loss: 0.009725371453290185\n",
      "Std loss: 0.005207530646901684\n",
      "Total Loss: 0.058352228719741106\n",
      "------------------------------------ epoch 9676 (58050 steps) ------------------------------------\n",
      "Max loss: 0.014230269007384777\n",
      "Min loss: 0.0037880297750234604\n",
      "Mean loss: 0.006493033065150182\n",
      "Std loss: 0.0036015462951682026\n",
      "Total Loss: 0.03895819839090109\n",
      "------------------------------------ epoch 9677 (58056 steps) ------------------------------------\n",
      "Max loss: 0.015543943271040916\n",
      "Min loss: 0.0038176504895091057\n",
      "Mean loss: 0.009674490351850787\n",
      "Std loss: 0.0043080592003153844\n",
      "Total Loss: 0.05804694211110473\n",
      "------------------------------------ epoch 9678 (58062 steps) ------------------------------------\n",
      "Max loss: 0.02065948396921158\n",
      "Min loss: 0.00541309779509902\n",
      "Mean loss: 0.01073156848239402\n",
      "Std loss: 0.005064304037037436\n",
      "Total Loss: 0.06438941089436412\n",
      "------------------------------------ epoch 9679 (58068 steps) ------------------------------------\n",
      "Max loss: 0.03610400855541229\n",
      "Min loss: 0.0049420734867453575\n",
      "Mean loss: 0.014049700771768888\n",
      "Std loss: 0.010267574639351663\n",
      "Total Loss: 0.08429820463061333\n",
      "------------------------------------ epoch 9680 (58074 steps) ------------------------------------\n",
      "Max loss: 0.015360618941485882\n",
      "Min loss: 0.00418609706684947\n",
      "Mean loss: 0.008695272651190558\n",
      "Std loss: 0.004226223617196056\n",
      "Total Loss: 0.052171635907143354\n",
      "------------------------------------ epoch 9681 (58080 steps) ------------------------------------\n",
      "Max loss: 0.0074152289889752865\n",
      "Min loss: 0.004102295730262995\n",
      "Mean loss: 0.005711595915878813\n",
      "Std loss: 0.0012476118029156557\n",
      "Total Loss: 0.034269575495272875\n",
      "------------------------------------ epoch 9682 (58086 steps) ------------------------------------\n",
      "Max loss: 0.02261347323656082\n",
      "Min loss: 0.003634827211499214\n",
      "Mean loss: 0.009577531910811862\n",
      "Std loss: 0.00628754096578991\n",
      "Total Loss: 0.05746519146487117\n",
      "------------------------------------ epoch 9683 (58092 steps) ------------------------------------\n",
      "Max loss: 0.0066902595572173595\n",
      "Min loss: 0.004289654083549976\n",
      "Mean loss: 0.0050783557041237755\n",
      "Std loss: 0.0008236312052671543\n",
      "Total Loss: 0.03047013422474265\n",
      "------------------------------------ epoch 9684 (58098 steps) ------------------------------------\n",
      "Max loss: 0.03624306619167328\n",
      "Min loss: 0.00311707379296422\n",
      "Mean loss: 0.011304818093776703\n",
      "Std loss: 0.011371209378206501\n",
      "Total Loss: 0.06782890856266022\n",
      "------------------------------------ epoch 9685 (58104 steps) ------------------------------------\n",
      "Max loss: 0.010638008825480938\n",
      "Min loss: 0.0041526006534695625\n",
      "Mean loss: 0.006493139934415619\n",
      "Std loss: 0.002371475698499574\n",
      "Total Loss: 0.03895883960649371\n",
      "------------------------------------ epoch 9686 (58110 steps) ------------------------------------\n",
      "Max loss: 0.03845534473657608\n",
      "Min loss: 0.00361962104216218\n",
      "Mean loss: 0.011776146789391836\n",
      "Std loss: 0.012161327094656369\n",
      "Total Loss: 0.07065688073635101\n",
      "------------------------------------ epoch 9687 (58116 steps) ------------------------------------\n",
      "Max loss: 0.008170670829713345\n",
      "Min loss: 0.005073838867247105\n",
      "Mean loss: 0.006796142707268397\n",
      "Std loss: 0.0011714019807845116\n",
      "Total Loss: 0.04077685624361038\n",
      "------------------------------------ epoch 9688 (58122 steps) ------------------------------------\n",
      "Max loss: 0.015675513073801994\n",
      "Min loss: 0.004958091303706169\n",
      "Mean loss: 0.00841230247169733\n",
      "Std loss: 0.003581607173205836\n",
      "Total Loss: 0.05047381483018398\n",
      "------------------------------------ epoch 9689 (58128 steps) ------------------------------------\n",
      "Max loss: 0.01028994657099247\n",
      "Min loss: 0.00347046903334558\n",
      "Mean loss: 0.0063035231626903014\n",
      "Std loss: 0.002553634887507844\n",
      "Total Loss: 0.03782113897614181\n",
      "------------------------------------ epoch 9690 (58134 steps) ------------------------------------\n",
      "Max loss: 0.01720307767391205\n",
      "Min loss: 0.0040311263874173164\n",
      "Mean loss: 0.00880331980685393\n",
      "Std loss: 0.0043652146182372705\n",
      "Total Loss: 0.05281991884112358\n",
      "------------------------------------ epoch 9691 (58140 steps) ------------------------------------\n",
      "Max loss: 0.015170491300523281\n",
      "Min loss: 0.0032211372163146734\n",
      "Mean loss: 0.006798163405619562\n",
      "Std loss: 0.003905221242904327\n",
      "Total Loss: 0.04078898043371737\n",
      "------------------------------------ epoch 9692 (58146 steps) ------------------------------------\n",
      "Max loss: 0.014998398721218109\n",
      "Min loss: 0.004255653824657202\n",
      "Mean loss: 0.006507424482454856\n",
      "Std loss: 0.0038134985331523506\n",
      "Total Loss: 0.03904454689472914\n",
      "------------------------------------ epoch 9693 (58152 steps) ------------------------------------\n",
      "Max loss: 0.012961171567440033\n",
      "Min loss: 0.004729267209768295\n",
      "Mean loss: 0.008646102777371803\n",
      "Std loss: 0.002696474562564202\n",
      "Total Loss: 0.051876616664230824\n",
      "------------------------------------ epoch 9694 (58158 steps) ------------------------------------\n",
      "Max loss: 0.01163925975561142\n",
      "Min loss: 0.005730075761675835\n",
      "Mean loss: 0.00762197352014482\n",
      "Std loss: 0.002321610344053163\n",
      "Total Loss: 0.04573184112086892\n",
      "------------------------------------ epoch 9695 (58164 steps) ------------------------------------\n",
      "Max loss: 0.009726354852318764\n",
      "Min loss: 0.00414992542937398\n",
      "Mean loss: 0.005566687478373448\n",
      "Std loss: 0.0019366256535956437\n",
      "Total Loss: 0.03340012487024069\n",
      "------------------------------------ epoch 9696 (58170 steps) ------------------------------------\n",
      "Max loss: 0.007044260855764151\n",
      "Min loss: 0.00339135411195457\n",
      "Mean loss: 0.004648154058183233\n",
      "Std loss: 0.0012016990905639661\n",
      "Total Loss: 0.027888924349099398\n",
      "------------------------------------ epoch 9697 (58176 steps) ------------------------------------\n",
      "Max loss: 0.009560523554682732\n",
      "Min loss: 0.003024708479642868\n",
      "Mean loss: 0.005712957975144188\n",
      "Std loss: 0.0024052558109907882\n",
      "Total Loss: 0.034277747850865126\n",
      "------------------------------------ epoch 9698 (58182 steps) ------------------------------------\n",
      "Max loss: 0.008151916787028313\n",
      "Min loss: 0.00441393069922924\n",
      "Mean loss: 0.006594234301398198\n",
      "Std loss: 0.0015014469664817634\n",
      "Total Loss: 0.03956540580838919\n",
      "------------------------------------ epoch 9699 (58188 steps) ------------------------------------\n",
      "Max loss: 0.009064855054020882\n",
      "Min loss: 0.0030892398208379745\n",
      "Mean loss: 0.005530649640907844\n",
      "Std loss: 0.0024553995885588974\n",
      "Total Loss: 0.03318389784544706\n",
      "------------------------------------ epoch 9700 (58194 steps) ------------------------------------\n",
      "Max loss: 0.009540506638586521\n",
      "Min loss: 0.004014230333268642\n",
      "Mean loss: 0.0060251740117867785\n",
      "Std loss: 0.002315433508623527\n",
      "Total Loss: 0.03615104407072067\n",
      "------------------------------------ epoch 9701 (58200 steps) ------------------------------------\n",
      "Max loss: 0.019133172929286957\n",
      "Min loss: 0.0032041154336184263\n",
      "Mean loss: 0.009141083690337837\n",
      "Std loss: 0.006700710272805644\n",
      "Total Loss: 0.05484650214202702\n",
      "saved model at ./weights/model_9701.pth\n",
      "------------------------------------ epoch 9702 (58206 steps) ------------------------------------\n",
      "Max loss: 0.022848334163427353\n",
      "Min loss: 0.004164878744632006\n",
      "Mean loss: 0.008518362883478403\n",
      "Std loss: 0.0065253382385908245\n",
      "Total Loss: 0.05111017730087042\n",
      "------------------------------------ epoch 9703 (58212 steps) ------------------------------------\n",
      "Max loss: 0.010575396940112114\n",
      "Min loss: 0.004927074071019888\n",
      "Mean loss: 0.009250841336324811\n",
      "Std loss: 0.0019638537212008454\n",
      "Total Loss: 0.055505048017948866\n",
      "------------------------------------ epoch 9704 (58218 steps) ------------------------------------\n",
      "Max loss: 0.013808193616569042\n",
      "Min loss: 0.004311107099056244\n",
      "Mean loss: 0.007015840771297614\n",
      "Std loss: 0.0033084983301880973\n",
      "Total Loss: 0.04209504462778568\n",
      "------------------------------------ epoch 9705 (58224 steps) ------------------------------------\n",
      "Max loss: 0.00839299988001585\n",
      "Min loss: 0.003296496346592903\n",
      "Mean loss: 0.005792020664860805\n",
      "Std loss: 0.00178529058156163\n",
      "Total Loss: 0.03475212398916483\n",
      "------------------------------------ epoch 9706 (58230 steps) ------------------------------------\n",
      "Max loss: 0.007173354737460613\n",
      "Min loss: 0.004069225396960974\n",
      "Mean loss: 0.005773458552236359\n",
      "Std loss: 0.0011849476200969597\n",
      "Total Loss: 0.03464075131341815\n",
      "------------------------------------ epoch 9707 (58236 steps) ------------------------------------\n",
      "Max loss: 0.018960732966661453\n",
      "Min loss: 0.004784008022397757\n",
      "Mean loss: 0.008760666940361261\n",
      "Std loss: 0.005109542534866627\n",
      "Total Loss: 0.05256400164216757\n",
      "------------------------------------ epoch 9708 (58242 steps) ------------------------------------\n",
      "Max loss: 0.011796007864177227\n",
      "Min loss: 0.00420432910323143\n",
      "Mean loss: 0.00722329046887656\n",
      "Std loss: 0.002688345931093536\n",
      "Total Loss: 0.04333974281325936\n",
      "------------------------------------ epoch 9709 (58248 steps) ------------------------------------\n",
      "Max loss: 0.016607679426670074\n",
      "Min loss: 0.003931959625333548\n",
      "Mean loss: 0.007755426379541556\n",
      "Std loss: 0.004154080557637623\n",
      "Total Loss: 0.046532558277249336\n",
      "------------------------------------ epoch 9710 (58254 steps) ------------------------------------\n",
      "Max loss: 0.012230724096298218\n",
      "Min loss: 0.0039978488348424435\n",
      "Mean loss: 0.005657001476113995\n",
      "Std loss: 0.0029453880871114367\n",
      "Total Loss: 0.03394200885668397\n",
      "------------------------------------ epoch 9711 (58260 steps) ------------------------------------\n",
      "Max loss: 0.015406585298478603\n",
      "Min loss: 0.003424439113587141\n",
      "Mean loss: 0.006723932921886444\n",
      "Std loss: 0.003979204652096299\n",
      "Total Loss: 0.040343597531318665\n",
      "------------------------------------ epoch 9712 (58266 steps) ------------------------------------\n",
      "Max loss: 0.010942957364022732\n",
      "Min loss: 0.0033311485312879086\n",
      "Mean loss: 0.005904552449161808\n",
      "Std loss: 0.002504368703012471\n",
      "Total Loss: 0.035427314694970846\n",
      "------------------------------------ epoch 9713 (58272 steps) ------------------------------------\n",
      "Max loss: 0.012988012284040451\n",
      "Min loss: 0.005908723920583725\n",
      "Mean loss: 0.008532873199631771\n",
      "Std loss: 0.0024713596790603065\n",
      "Total Loss: 0.05119723919779062\n",
      "------------------------------------ epoch 9714 (58278 steps) ------------------------------------\n",
      "Max loss: 0.010129798203706741\n",
      "Min loss: 0.0034708958119153976\n",
      "Mean loss: 0.005757210465768973\n",
      "Std loss: 0.002283384992218306\n",
      "Total Loss: 0.03454326279461384\n",
      "------------------------------------ epoch 9715 (58284 steps) ------------------------------------\n",
      "Max loss: 0.01905156672000885\n",
      "Min loss: 0.003751982469111681\n",
      "Mean loss: 0.01142467511817813\n",
      "Std loss: 0.005397785500992402\n",
      "Total Loss: 0.06854805070906878\n",
      "------------------------------------ epoch 9716 (58290 steps) ------------------------------------\n",
      "Max loss: 0.012749595567584038\n",
      "Min loss: 0.003843675134703517\n",
      "Mean loss: 0.007695508538745344\n",
      "Std loss: 0.0028978522566603885\n",
      "Total Loss: 0.04617305123247206\n",
      "------------------------------------ epoch 9717 (58296 steps) ------------------------------------\n",
      "Max loss: 0.008573928847908974\n",
      "Min loss: 0.006746141240000725\n",
      "Mean loss: 0.007626199861988425\n",
      "Std loss: 0.0006644851585460759\n",
      "Total Loss: 0.04575719917193055\n",
      "------------------------------------ epoch 9718 (58302 steps) ------------------------------------\n",
      "Max loss: 0.006103327497839928\n",
      "Min loss: 0.003880514530465007\n",
      "Mean loss: 0.004757116509911914\n",
      "Std loss: 0.0007738356026345055\n",
      "Total Loss: 0.028542699059471488\n",
      "------------------------------------ epoch 9719 (58308 steps) ------------------------------------\n",
      "Max loss: 0.01535808201879263\n",
      "Min loss: 0.0029805931262671947\n",
      "Mean loss: 0.005842676929508646\n",
      "Std loss: 0.004290895652248585\n",
      "Total Loss: 0.03505606157705188\n",
      "------------------------------------ epoch 9720 (58314 steps) ------------------------------------\n",
      "Max loss: 0.027469148859381676\n",
      "Min loss: 0.0031612718012183905\n",
      "Mean loss: 0.00931540213059634\n",
      "Std loss: 0.008417437464436872\n",
      "Total Loss: 0.05589241278357804\n",
      "------------------------------------ epoch 9721 (58320 steps) ------------------------------------\n",
      "Max loss: 0.008536703884601593\n",
      "Min loss: 0.004397077020257711\n",
      "Mean loss: 0.005975595985849698\n",
      "Std loss: 0.0014878453907769037\n",
      "Total Loss: 0.03585357591509819\n",
      "------------------------------------ epoch 9722 (58326 steps) ------------------------------------\n",
      "Max loss: 0.007379166316241026\n",
      "Min loss: 0.0031515932641923428\n",
      "Mean loss: 0.005250674517204364\n",
      "Std loss: 0.0015404650019390635\n",
      "Total Loss: 0.031504047103226185\n",
      "------------------------------------ epoch 9723 (58332 steps) ------------------------------------\n",
      "Max loss: 0.02001495473086834\n",
      "Min loss: 0.004220166709274054\n",
      "Mean loss: 0.01112155142861108\n",
      "Std loss: 0.006252719241152309\n",
      "Total Loss: 0.06672930857166648\n",
      "------------------------------------ epoch 9724 (58338 steps) ------------------------------------\n",
      "Max loss: 0.012752538546919823\n",
      "Min loss: 0.004139464348554611\n",
      "Mean loss: 0.007512298567841451\n",
      "Std loss: 0.0028801369312960517\n",
      "Total Loss: 0.0450737914070487\n",
      "------------------------------------ epoch 9725 (58344 steps) ------------------------------------\n",
      "Max loss: 0.0063741737976670265\n",
      "Min loss: 0.003990019205957651\n",
      "Mean loss: 0.005117006755123536\n",
      "Std loss: 0.0007436218058003909\n",
      "Total Loss: 0.030702040530741215\n",
      "------------------------------------ epoch 9726 (58350 steps) ------------------------------------\n",
      "Max loss: 0.007671901490539312\n",
      "Min loss: 0.0037802865263074636\n",
      "Mean loss: 0.005506831570528448\n",
      "Std loss: 0.0016382984152712094\n",
      "Total Loss: 0.033040989423170686\n",
      "------------------------------------ epoch 9727 (58356 steps) ------------------------------------\n",
      "Max loss: 0.028410345315933228\n",
      "Min loss: 0.004227772355079651\n",
      "Mean loss: 0.010711178882047534\n",
      "Std loss: 0.008610515147225436\n",
      "Total Loss: 0.0642670732922852\n",
      "------------------------------------ epoch 9728 (58362 steps) ------------------------------------\n",
      "Max loss: 0.02159992977976799\n",
      "Min loss: 0.004077808000147343\n",
      "Mean loss: 0.010823880089446902\n",
      "Std loss: 0.007297888664103534\n",
      "Total Loss: 0.06494328053668141\n",
      "------------------------------------ epoch 9729 (58368 steps) ------------------------------------\n",
      "Max loss: 0.0151218231767416\n",
      "Min loss: 0.0038155503571033478\n",
      "Mean loss: 0.007110744947567582\n",
      "Std loss: 0.003778003917868736\n",
      "Total Loss: 0.04266446968540549\n",
      "------------------------------------ epoch 9730 (58374 steps) ------------------------------------\n",
      "Max loss: 0.01142494659870863\n",
      "Min loss: 0.003635009517893195\n",
      "Mean loss: 0.00760933985778441\n",
      "Std loss: 0.0034226268031971128\n",
      "Total Loss: 0.04565603914670646\n",
      "------------------------------------ epoch 9731 (58380 steps) ------------------------------------\n",
      "Max loss: 0.025584634393453598\n",
      "Min loss: 0.005010315217077732\n",
      "Mean loss: 0.013109317980706692\n",
      "Std loss: 0.008598870405258943\n",
      "Total Loss: 0.07865590788424015\n",
      "------------------------------------ epoch 9732 (58386 steps) ------------------------------------\n",
      "Max loss: 0.013354151509702206\n",
      "Min loss: 0.004190968349575996\n",
      "Mean loss: 0.010148698774476847\n",
      "Std loss: 0.0031361538450040277\n",
      "Total Loss: 0.060892192646861076\n",
      "------------------------------------ epoch 9733 (58392 steps) ------------------------------------\n",
      "Max loss: 0.011705118231475353\n",
      "Min loss: 0.005170634016394615\n",
      "Mean loss: 0.008425210990632573\n",
      "Std loss: 0.0023209422795439447\n",
      "Total Loss: 0.05055126594379544\n",
      "------------------------------------ epoch 9734 (58398 steps) ------------------------------------\n",
      "Max loss: 0.023041769862174988\n",
      "Min loss: 0.004927926696836948\n",
      "Mean loss: 0.009824848889062801\n",
      "Std loss: 0.006098615200530092\n",
      "Total Loss: 0.05894909333437681\n",
      "------------------------------------ epoch 9735 (58404 steps) ------------------------------------\n",
      "Max loss: 0.02248610556125641\n",
      "Min loss: 0.005223223473876715\n",
      "Mean loss: 0.01091325554686288\n",
      "Std loss: 0.006603391599135106\n",
      "Total Loss: 0.06547953328117728\n",
      "------------------------------------ epoch 9736 (58410 steps) ------------------------------------\n",
      "Max loss: 0.028158564120531082\n",
      "Min loss: 0.004672934766858816\n",
      "Mean loss: 0.01241131010465324\n",
      "Std loss: 0.008989082294041292\n",
      "Total Loss: 0.07446786062791944\n",
      "------------------------------------ epoch 9737 (58416 steps) ------------------------------------\n",
      "Max loss: 0.010405107401311398\n",
      "Min loss: 0.004644539672881365\n",
      "Mean loss: 0.007529927728076776\n",
      "Std loss: 0.0017362581832138933\n",
      "Total Loss: 0.045179566368460655\n",
      "------------------------------------ epoch 9738 (58422 steps) ------------------------------------\n",
      "Max loss: 0.007368759717792273\n",
      "Min loss: 0.005209214054048061\n",
      "Mean loss: 0.0061029905918985605\n",
      "Std loss: 0.0007980105078732984\n",
      "Total Loss: 0.03661794355139136\n",
      "------------------------------------ epoch 9739 (58428 steps) ------------------------------------\n",
      "Max loss: 0.017658917233347893\n",
      "Min loss: 0.003732685698196292\n",
      "Mean loss: 0.007635365744742255\n",
      "Std loss: 0.004795849088047882\n",
      "Total Loss: 0.045812194468453526\n",
      "------------------------------------ epoch 9740 (58434 steps) ------------------------------------\n",
      "Max loss: 0.015399224124848843\n",
      "Min loss: 0.004923006519675255\n",
      "Mean loss: 0.008922288194298744\n",
      "Std loss: 0.0034838865820813656\n",
      "Total Loss: 0.053533729165792465\n",
      "------------------------------------ epoch 9741 (58440 steps) ------------------------------------\n",
      "Max loss: 0.01227833703160286\n",
      "Min loss: 0.004169844090938568\n",
      "Mean loss: 0.007086924510076642\n",
      "Std loss: 0.0027446384760986702\n",
      "Total Loss: 0.04252154706045985\n",
      "------------------------------------ epoch 9742 (58446 steps) ------------------------------------\n",
      "Max loss: 0.020447909832000732\n",
      "Min loss: 0.003306856844574213\n",
      "Mean loss: 0.011182644171640277\n",
      "Std loss: 0.006069923478242046\n",
      "Total Loss: 0.06709586502984166\n",
      "------------------------------------ epoch 9743 (58452 steps) ------------------------------------\n",
      "Max loss: 0.01677456870675087\n",
      "Min loss: 0.0035838959738612175\n",
      "Mean loss: 0.007902962543691197\n",
      "Std loss: 0.004288145547697368\n",
      "Total Loss: 0.04741777526214719\n",
      "------------------------------------ epoch 9744 (58458 steps) ------------------------------------\n",
      "Max loss: 0.01045571081340313\n",
      "Min loss: 0.004299206659197807\n",
      "Mean loss: 0.0070966933853924274\n",
      "Std loss: 0.0020100985964781725\n",
      "Total Loss: 0.042580160312354565\n",
      "------------------------------------ epoch 9745 (58464 steps) ------------------------------------\n",
      "Max loss: 0.009265587665140629\n",
      "Min loss: 0.004074744414538145\n",
      "Mean loss: 0.006452384249617656\n",
      "Std loss: 0.002032716358776751\n",
      "Total Loss: 0.038714305497705936\n",
      "------------------------------------ epoch 9746 (58470 steps) ------------------------------------\n",
      "Max loss: 0.01891045831143856\n",
      "Min loss: 0.003817648394033313\n",
      "Mean loss: 0.00744196109008044\n",
      "Std loss: 0.005406115700915877\n",
      "Total Loss: 0.04465176654048264\n",
      "------------------------------------ epoch 9747 (58476 steps) ------------------------------------\n",
      "Max loss: 0.014895182102918625\n",
      "Min loss: 0.004944590386003256\n",
      "Mean loss: 0.008328986975053946\n",
      "Std loss: 0.003535521661628157\n",
      "Total Loss: 0.04997392185032368\n",
      "------------------------------------ epoch 9748 (58482 steps) ------------------------------------\n",
      "Max loss: 0.011707771569490433\n",
      "Min loss: 0.004057195037603378\n",
      "Mean loss: 0.006873131652052204\n",
      "Std loss: 0.0029857405455364274\n",
      "Total Loss: 0.04123878991231322\n",
      "------------------------------------ epoch 9749 (58488 steps) ------------------------------------\n",
      "Max loss: 0.026763511821627617\n",
      "Min loss: 0.0033098715357482433\n",
      "Mean loss: 0.009354488652509948\n",
      "Std loss: 0.008018482582811202\n",
      "Total Loss: 0.056126931915059686\n",
      "------------------------------------ epoch 9750 (58494 steps) ------------------------------------\n",
      "Max loss: 0.007690314203500748\n",
      "Min loss: 0.005213820841163397\n",
      "Mean loss: 0.006202344627430041\n",
      "Std loss: 0.001049345241035733\n",
      "Total Loss: 0.03721406776458025\n",
      "------------------------------------ epoch 9751 (58500 steps) ------------------------------------\n",
      "Max loss: 0.01727622002363205\n",
      "Min loss: 0.0041562896221876144\n",
      "Mean loss: 0.008980274510880312\n",
      "Std loss: 0.004069236759045455\n",
      "Total Loss: 0.05388164706528187\n",
      "------------------------------------ epoch 9752 (58506 steps) ------------------------------------\n",
      "Max loss: 0.014374887570738792\n",
      "Min loss: 0.0038117989897727966\n",
      "Mean loss: 0.00801426637917757\n",
      "Std loss: 0.0035230830073629364\n",
      "Total Loss: 0.04808559827506542\n",
      "------------------------------------ epoch 9753 (58512 steps) ------------------------------------\n",
      "Max loss: 0.009316219948232174\n",
      "Min loss: 0.00273501081392169\n",
      "Mean loss: 0.0061201301869004965\n",
      "Std loss: 0.0022483506433164033\n",
      "Total Loss: 0.03672078112140298\n",
      "------------------------------------ epoch 9754 (58518 steps) ------------------------------------\n",
      "Max loss: 0.01608082465827465\n",
      "Min loss: 0.003983515780419111\n",
      "Mean loss: 0.007021591222534577\n",
      "Std loss: 0.004326872557485563\n",
      "Total Loss: 0.04212954733520746\n",
      "------------------------------------ epoch 9755 (58524 steps) ------------------------------------\n",
      "Max loss: 0.017758889123797417\n",
      "Min loss: 0.003582299454137683\n",
      "Mean loss: 0.00882625242229551\n",
      "Std loss: 0.004557199371729566\n",
      "Total Loss: 0.052957514533773065\n",
      "------------------------------------ epoch 9756 (58530 steps) ------------------------------------\n",
      "Max loss: 0.013960137963294983\n",
      "Min loss: 0.0034188139252364635\n",
      "Mean loss: 0.006979043905933698\n",
      "Std loss: 0.003804167391316573\n",
      "Total Loss: 0.04187426343560219\n",
      "------------------------------------ epoch 9757 (58536 steps) ------------------------------------\n",
      "Max loss: 0.013640722259879112\n",
      "Min loss: 0.004175504669547081\n",
      "Mean loss: 0.008220511643836895\n",
      "Std loss: 0.0029466766403617546\n",
      "Total Loss: 0.049323069863021374\n",
      "------------------------------------ epoch 9758 (58542 steps) ------------------------------------\n",
      "Max loss: 0.012432279996573925\n",
      "Min loss: 0.003642051247879863\n",
      "Mean loss: 0.0063316165857637925\n",
      "Std loss: 0.0029476568918421608\n",
      "Total Loss: 0.03798969951458275\n",
      "------------------------------------ epoch 9759 (58548 steps) ------------------------------------\n",
      "Max loss: 0.012747134082019329\n",
      "Min loss: 0.004210268147289753\n",
      "Mean loss: 0.00649530425046881\n",
      "Std loss: 0.002948243106380688\n",
      "Total Loss: 0.03897182550281286\n",
      "------------------------------------ epoch 9760 (58554 steps) ------------------------------------\n",
      "Max loss: 0.006595078855752945\n",
      "Min loss: 0.003738050814718008\n",
      "Mean loss: 0.005350619515714546\n",
      "Std loss: 0.0012216324543228546\n",
      "Total Loss: 0.032103717094287276\n",
      "------------------------------------ epoch 9761 (58560 steps) ------------------------------------\n",
      "Max loss: 0.021140193566679955\n",
      "Min loss: 0.0038188397884368896\n",
      "Mean loss: 0.007868504151701927\n",
      "Std loss: 0.00601988087922317\n",
      "Total Loss: 0.04721102491021156\n",
      "------------------------------------ epoch 9762 (58566 steps) ------------------------------------\n",
      "Max loss: 0.0361766517162323\n",
      "Min loss: 0.005299709737300873\n",
      "Mean loss: 0.012959098365778724\n",
      "Std loss: 0.010669051623801952\n",
      "Total Loss: 0.07775459019467235\n",
      "------------------------------------ epoch 9763 (58572 steps) ------------------------------------\n",
      "Max loss: 0.010008632205426693\n",
      "Min loss: 0.004170364700257778\n",
      "Mean loss: 0.007441340324779351\n",
      "Std loss: 0.002240407393918937\n",
      "Total Loss: 0.04464804194867611\n",
      "------------------------------------ epoch 9764 (58578 steps) ------------------------------------\n",
      "Max loss: 0.010979287326335907\n",
      "Min loss: 0.006146018858999014\n",
      "Mean loss: 0.008166803435112039\n",
      "Std loss: 0.0016311210061863701\n",
      "Total Loss: 0.049000820610672235\n",
      "------------------------------------ epoch 9765 (58584 steps) ------------------------------------\n",
      "Max loss: 0.01036978792399168\n",
      "Min loss: 0.005548725835978985\n",
      "Mean loss: 0.008559243598332008\n",
      "Std loss: 0.0015333890163652917\n",
      "Total Loss: 0.051355461589992046\n",
      "------------------------------------ epoch 9766 (58590 steps) ------------------------------------\n",
      "Max loss: 0.013628704473376274\n",
      "Min loss: 0.0035573006607592106\n",
      "Mean loss: 0.006493553441638748\n",
      "Std loss: 0.0033754404837981885\n",
      "Total Loss: 0.03896132064983249\n",
      "------------------------------------ epoch 9767 (58596 steps) ------------------------------------\n",
      "Max loss: 0.02291267178952694\n",
      "Min loss: 0.00406917417421937\n",
      "Mean loss: 0.008391579069818059\n",
      "Std loss: 0.006650660061512039\n",
      "Total Loss: 0.05034947441890836\n",
      "------------------------------------ epoch 9768 (58602 steps) ------------------------------------\n",
      "Max loss: 0.01176078524440527\n",
      "Min loss: 0.0035311593674123287\n",
      "Mean loss: 0.006358359707519412\n",
      "Std loss: 0.002736103501176396\n",
      "Total Loss: 0.03815015824511647\n",
      "------------------------------------ epoch 9769 (58608 steps) ------------------------------------\n",
      "Max loss: 0.0186544768512249\n",
      "Min loss: 0.0036526373587548733\n",
      "Mean loss: 0.009189145484318336\n",
      "Std loss: 0.006664453961798135\n",
      "Total Loss: 0.055134872905910015\n",
      "------------------------------------ epoch 9770 (58614 steps) ------------------------------------\n",
      "Max loss: 0.011196712031960487\n",
      "Min loss: 0.004120211116969585\n",
      "Mean loss: 0.006996469261745612\n",
      "Std loss: 0.0023862159550716187\n",
      "Total Loss: 0.04197881557047367\n",
      "------------------------------------ epoch 9771 (58620 steps) ------------------------------------\n",
      "Max loss: 0.0323123037815094\n",
      "Min loss: 0.003628596430644393\n",
      "Mean loss: 0.010632851587918898\n",
      "Std loss: 0.009990675028783402\n",
      "Total Loss: 0.06379710952751338\n",
      "------------------------------------ epoch 9772 (58626 steps) ------------------------------------\n",
      "Max loss: 0.023784521967172623\n",
      "Min loss: 0.004992018919438124\n",
      "Mean loss: 0.012479194129506746\n",
      "Std loss: 0.007414979035438369\n",
      "Total Loss: 0.07487516477704048\n",
      "------------------------------------ epoch 9773 (58632 steps) ------------------------------------\n",
      "Max loss: 0.0160263292491436\n",
      "Min loss: 0.0037300949916243553\n",
      "Mean loss: 0.008350730097542206\n",
      "Std loss: 0.003972767110262129\n",
      "Total Loss: 0.05010438058525324\n",
      "------------------------------------ epoch 9774 (58638 steps) ------------------------------------\n",
      "Max loss: 0.023068837821483612\n",
      "Min loss: 0.004633541218936443\n",
      "Mean loss: 0.010152547465016445\n",
      "Std loss: 0.00616315950706391\n",
      "Total Loss: 0.06091528479009867\n",
      "------------------------------------ epoch 9775 (58644 steps) ------------------------------------\n",
      "Max loss: 0.008896155282855034\n",
      "Min loss: 0.003283384721726179\n",
      "Mean loss: 0.005897479752699534\n",
      "Std loss: 0.0019570670943029945\n",
      "Total Loss: 0.035384878516197205\n",
      "------------------------------------ epoch 9776 (58650 steps) ------------------------------------\n",
      "Max loss: 0.02023567445576191\n",
      "Min loss: 0.003985692746937275\n",
      "Mean loss: 0.008919970675682029\n",
      "Std loss: 0.005546083931763828\n",
      "Total Loss: 0.05351982405409217\n",
      "------------------------------------ epoch 9777 (58656 steps) ------------------------------------\n",
      "Max loss: 0.03226695582270622\n",
      "Min loss: 0.005100991576910019\n",
      "Mean loss: 0.01319025456905365\n",
      "Std loss: 0.009129693793239402\n",
      "Total Loss: 0.0791415274143219\n",
      "------------------------------------ epoch 9778 (58662 steps) ------------------------------------\n",
      "Max loss: 0.036582451313734055\n",
      "Min loss: 0.0057311332784593105\n",
      "Mean loss: 0.013711261407782635\n",
      "Std loss: 0.010858332587875398\n",
      "Total Loss: 0.0822675684466958\n",
      "------------------------------------ epoch 9779 (58668 steps) ------------------------------------\n",
      "Max loss: 0.01711306720972061\n",
      "Min loss: 0.00536715891212225\n",
      "Mean loss: 0.011507311059782902\n",
      "Std loss: 0.004460130314382015\n",
      "Total Loss: 0.06904386635869741\n",
      "------------------------------------ epoch 9780 (58674 steps) ------------------------------------\n",
      "Max loss: 0.026607077568769455\n",
      "Min loss: 0.0051745157688856125\n",
      "Mean loss: 0.010845368339990577\n",
      "Std loss: 0.007330811845741264\n",
      "Total Loss: 0.06507221003994346\n",
      "------------------------------------ epoch 9781 (58680 steps) ------------------------------------\n",
      "Max loss: 0.015822257846593857\n",
      "Min loss: 0.0038351304829120636\n",
      "Mean loss: 0.010614197235554457\n",
      "Std loss: 0.004478666316889294\n",
      "Total Loss: 0.06368518341332674\n",
      "------------------------------------ epoch 9782 (58686 steps) ------------------------------------\n",
      "Max loss: 0.020105410367250443\n",
      "Min loss: 0.00563769368454814\n",
      "Mean loss: 0.011476148152723908\n",
      "Std loss: 0.004607457024049543\n",
      "Total Loss: 0.06885688891634345\n",
      "------------------------------------ epoch 9783 (58692 steps) ------------------------------------\n",
      "Max loss: 0.023000169545412064\n",
      "Min loss: 0.0035195189993828535\n",
      "Mean loss: 0.009067988217187425\n",
      "Std loss: 0.006645641631286209\n",
      "Total Loss: 0.05440792930312455\n",
      "------------------------------------ epoch 9784 (58698 steps) ------------------------------------\n",
      "Max loss: 0.025948908179998398\n",
      "Min loss: 0.0048537952825427055\n",
      "Mean loss: 0.00932495117497941\n",
      "Std loss: 0.007568649758812499\n",
      "Total Loss: 0.05594970704987645\n",
      "------------------------------------ epoch 9785 (58704 steps) ------------------------------------\n",
      "Max loss: 0.007072748150676489\n",
      "Min loss: 0.003510451642796397\n",
      "Mean loss: 0.00555879083306839\n",
      "Std loss: 0.0012438972272229832\n",
      "Total Loss: 0.033352744998410344\n",
      "------------------------------------ epoch 9786 (58710 steps) ------------------------------------\n",
      "Max loss: 0.018365083262324333\n",
      "Min loss: 0.004102017730474472\n",
      "Mean loss: 0.008162383222952485\n",
      "Std loss: 0.004804422719800971\n",
      "Total Loss: 0.04897429933771491\n",
      "------------------------------------ epoch 9787 (58716 steps) ------------------------------------\n",
      "Max loss: 0.009071292355656624\n",
      "Min loss: 0.0034977789036929607\n",
      "Mean loss: 0.004916027343521516\n",
      "Std loss: 0.00189196717193045\n",
      "Total Loss: 0.029496164061129093\n",
      "------------------------------------ epoch 9788 (58722 steps) ------------------------------------\n",
      "Max loss: 0.009921034798026085\n",
      "Min loss: 0.004607385024428368\n",
      "Mean loss: 0.006073784704009692\n",
      "Std loss: 0.002017297014583486\n",
      "Total Loss: 0.03644270822405815\n",
      "------------------------------------ epoch 9789 (58728 steps) ------------------------------------\n",
      "Max loss: 0.027134351432323456\n",
      "Min loss: 0.004141835495829582\n",
      "Mean loss: 0.01084347030458351\n",
      "Std loss: 0.007573480435146076\n",
      "Total Loss: 0.06506082182750106\n",
      "------------------------------------ epoch 9790 (58734 steps) ------------------------------------\n",
      "Max loss: 0.017132597044110298\n",
      "Min loss: 0.0034585068933665752\n",
      "Mean loss: 0.00887503094660739\n",
      "Std loss: 0.005401895490898308\n",
      "Total Loss: 0.053250185679644346\n",
      "------------------------------------ epoch 9791 (58740 steps) ------------------------------------\n",
      "Max loss: 0.013279969803988934\n",
      "Min loss: 0.004433480557054281\n",
      "Mean loss: 0.009310260182246566\n",
      "Std loss: 0.003276939538986988\n",
      "Total Loss: 0.055861561093479395\n",
      "------------------------------------ epoch 9792 (58746 steps) ------------------------------------\n",
      "Max loss: 0.017800040543079376\n",
      "Min loss: 0.004066071473062038\n",
      "Mean loss: 0.008922061882913113\n",
      "Std loss: 0.0045046645917896245\n",
      "Total Loss: 0.053532371297478676\n",
      "------------------------------------ epoch 9793 (58752 steps) ------------------------------------\n",
      "Max loss: 0.018139634281396866\n",
      "Min loss: 0.005926688201725483\n",
      "Mean loss: 0.010915752810736498\n",
      "Std loss: 0.00451676651588963\n",
      "Total Loss: 0.06549451686441898\n",
      "------------------------------------ epoch 9794 (58758 steps) ------------------------------------\n",
      "Max loss: 0.023590480908751488\n",
      "Min loss: 0.004171144217252731\n",
      "Mean loss: 0.011896705875794092\n",
      "Std loss: 0.006359731282014505\n",
      "Total Loss: 0.07138023525476456\n",
      "------------------------------------ epoch 9795 (58764 steps) ------------------------------------\n",
      "Max loss: 0.010375872254371643\n",
      "Min loss: 0.004408182110637426\n",
      "Mean loss: 0.0067018193658441305\n",
      "Std loss: 0.0020799572731741356\n",
      "Total Loss: 0.04021091619506478\n",
      "------------------------------------ epoch 9796 (58770 steps) ------------------------------------\n",
      "Max loss: 0.009311352856457233\n",
      "Min loss: 0.004836891312152147\n",
      "Mean loss: 0.006987513819088538\n",
      "Std loss: 0.0016208544589665585\n",
      "Total Loss: 0.04192508291453123\n",
      "------------------------------------ epoch 9797 (58776 steps) ------------------------------------\n",
      "Max loss: 0.01509813778102398\n",
      "Min loss: 0.004291946068406105\n",
      "Mean loss: 0.009041159025703868\n",
      "Std loss: 0.003930181129451322\n",
      "Total Loss: 0.054246954154223204\n",
      "------------------------------------ epoch 9798 (58782 steps) ------------------------------------\n",
      "Max loss: 0.015236472710967064\n",
      "Min loss: 0.0034457079600542784\n",
      "Mean loss: 0.007804877318752308\n",
      "Std loss: 0.0040225991684944016\n",
      "Total Loss: 0.04682926391251385\n",
      "------------------------------------ epoch 9799 (58788 steps) ------------------------------------\n",
      "Max loss: 0.00865139625966549\n",
      "Min loss: 0.005318988114595413\n",
      "Mean loss: 0.006644978653639555\n",
      "Std loss: 0.0013778850449836135\n",
      "Total Loss: 0.03986987192183733\n",
      "------------------------------------ epoch 9800 (58794 steps) ------------------------------------\n",
      "Max loss: 0.008851012215018272\n",
      "Min loss: 0.0034917877055704594\n",
      "Mean loss: 0.006515155313536525\n",
      "Std loss: 0.0021957797412565615\n",
      "Total Loss: 0.03909093188121915\n",
      "------------------------------------ epoch 9801 (58800 steps) ------------------------------------\n",
      "Max loss: 0.02105105295777321\n",
      "Min loss: 0.00385903287678957\n",
      "Mean loss: 0.01041744245837132\n",
      "Std loss: 0.006076206580088716\n",
      "Total Loss: 0.06250465475022793\n",
      "saved model at ./weights/model_9801.pth\n",
      "------------------------------------ epoch 9802 (58806 steps) ------------------------------------\n",
      "Max loss: 0.01452048122882843\n",
      "Min loss: 0.005944827105849981\n",
      "Mean loss: 0.007910858762140075\n",
      "Std loss: 0.003017383938758164\n",
      "Total Loss: 0.04746515257284045\n",
      "------------------------------------ epoch 9803 (58812 steps) ------------------------------------\n",
      "Max loss: 0.011861853301525116\n",
      "Min loss: 0.0039388565346598625\n",
      "Mean loss: 0.007377014495432377\n",
      "Std loss: 0.0026018553109918106\n",
      "Total Loss: 0.04426208697259426\n",
      "------------------------------------ epoch 9804 (58818 steps) ------------------------------------\n",
      "Max loss: 0.014645013026893139\n",
      "Min loss: 0.0032614641822874546\n",
      "Mean loss: 0.0072167527784282965\n",
      "Std loss: 0.003853038438765473\n",
      "Total Loss: 0.04330051667056978\n",
      "------------------------------------ epoch 9805 (58824 steps) ------------------------------------\n",
      "Max loss: 0.011032797396183014\n",
      "Min loss: 0.004719369113445282\n",
      "Mean loss: 0.008240773109719157\n",
      "Std loss: 0.002381919396647378\n",
      "Total Loss: 0.04944463865831494\n",
      "------------------------------------ epoch 9806 (58830 steps) ------------------------------------\n",
      "Max loss: 0.013038728386163712\n",
      "Min loss: 0.0032177059911191463\n",
      "Mean loss: 0.008261078347762426\n",
      "Std loss: 0.003581604947930521\n",
      "Total Loss: 0.049566470086574554\n",
      "------------------------------------ epoch 9807 (58836 steps) ------------------------------------\n",
      "Max loss: 0.009895873256027699\n",
      "Min loss: 0.003506819484755397\n",
      "Mean loss: 0.007253384799696505\n",
      "Std loss: 0.0022619944291386238\n",
      "Total Loss: 0.04352030879817903\n",
      "------------------------------------ epoch 9808 (58842 steps) ------------------------------------\n",
      "Max loss: 0.006748733576387167\n",
      "Min loss: 0.0031244070269167423\n",
      "Mean loss: 0.00465008014968286\n",
      "Std loss: 0.001339666584393987\n",
      "Total Loss: 0.027900480898097157\n",
      "------------------------------------ epoch 9809 (58848 steps) ------------------------------------\n",
      "Max loss: 0.015983736142516136\n",
      "Min loss: 0.004118896089494228\n",
      "Mean loss: 0.006982063641771674\n",
      "Std loss: 0.004077811275173932\n",
      "Total Loss: 0.041892381850630045\n",
      "------------------------------------ epoch 9810 (58854 steps) ------------------------------------\n",
      "Max loss: 0.015006291680037975\n",
      "Min loss: 0.004858570173382759\n",
      "Mean loss: 0.01050682032170395\n",
      "Std loss: 0.003965274195457654\n",
      "Total Loss: 0.0630409219302237\n",
      "------------------------------------ epoch 9811 (58860 steps) ------------------------------------\n",
      "Max loss: 0.0082512516528368\n",
      "Min loss: 0.002843202091753483\n",
      "Mean loss: 0.004989847308024764\n",
      "Std loss: 0.001712144920576576\n",
      "Total Loss: 0.029939083848148584\n",
      "------------------------------------ epoch 9812 (58866 steps) ------------------------------------\n",
      "Max loss: 0.007898314855992794\n",
      "Min loss: 0.0041841245256364346\n",
      "Mean loss: 0.005657294454673926\n",
      "Std loss: 0.0013468318690048934\n",
      "Total Loss: 0.033943766728043556\n",
      "------------------------------------ epoch 9813 (58872 steps) ------------------------------------\n",
      "Max loss: 0.008587822318077087\n",
      "Min loss: 0.0035149320028722286\n",
      "Mean loss: 0.004901001423907776\n",
      "Std loss: 0.0017387227461299376\n",
      "Total Loss: 0.02940600854344666\n",
      "------------------------------------ epoch 9814 (58878 steps) ------------------------------------\n",
      "Max loss: 0.009920329786837101\n",
      "Min loss: 0.0025516909081488848\n",
      "Mean loss: 0.004642648001511891\n",
      "Std loss: 0.0024640136884734242\n",
      "Total Loss: 0.02785588800907135\n",
      "------------------------------------ epoch 9815 (58884 steps) ------------------------------------\n",
      "Max loss: 0.009350861422717571\n",
      "Min loss: 0.003821568563580513\n",
      "Mean loss: 0.006540405486399929\n",
      "Std loss: 0.002268368048718172\n",
      "Total Loss: 0.03924243291839957\n",
      "------------------------------------ epoch 9816 (58890 steps) ------------------------------------\n",
      "Max loss: 0.037589289247989655\n",
      "Min loss: 0.004438983742147684\n",
      "Mean loss: 0.01112496410496533\n",
      "Std loss: 0.011914353779142066\n",
      "Total Loss: 0.06674978462979198\n",
      "------------------------------------ epoch 9817 (58896 steps) ------------------------------------\n",
      "Max loss: 0.02329542301595211\n",
      "Min loss: 0.006491511128842831\n",
      "Mean loss: 0.012127988816549381\n",
      "Std loss: 0.005598131831235977\n",
      "Total Loss: 0.07276793289929628\n",
      "------------------------------------ epoch 9818 (58902 steps) ------------------------------------\n",
      "Max loss: 0.006527330260723829\n",
      "Min loss: 0.0037494662683457136\n",
      "Mean loss: 0.0052406591130420566\n",
      "Std loss: 0.0009316010173489907\n",
      "Total Loss: 0.03144395467825234\n",
      "------------------------------------ epoch 9819 (58908 steps) ------------------------------------\n",
      "Max loss: 0.01067369058728218\n",
      "Min loss: 0.003927735146135092\n",
      "Mean loss: 0.0065390379168093204\n",
      "Std loss: 0.0023117529873153447\n",
      "Total Loss: 0.03923422750085592\n",
      "------------------------------------ epoch 9820 (58914 steps) ------------------------------------\n",
      "Max loss: 0.011837245896458626\n",
      "Min loss: 0.004752044565975666\n",
      "Mean loss: 0.007733478055646022\n",
      "Std loss: 0.002951585219928093\n",
      "Total Loss: 0.04640086833387613\n",
      "------------------------------------ epoch 9821 (58920 steps) ------------------------------------\n",
      "Max loss: 0.0166785828769207\n",
      "Min loss: 0.004337883088737726\n",
      "Mean loss: 0.008424614168082675\n",
      "Std loss: 0.004144458553034524\n",
      "Total Loss: 0.050547685008496046\n",
      "------------------------------------ epoch 9822 (58926 steps) ------------------------------------\n",
      "Max loss: 0.011563783511519432\n",
      "Min loss: 0.0036396514624357224\n",
      "Mean loss: 0.006720593043913444\n",
      "Std loss: 0.0028752509015918977\n",
      "Total Loss: 0.04032355826348066\n",
      "------------------------------------ epoch 9823 (58932 steps) ------------------------------------\n",
      "Max loss: 0.010610463097691536\n",
      "Min loss: 0.003353019244968891\n",
      "Mean loss: 0.006507309231286247\n",
      "Std loss: 0.0029057675840605084\n",
      "Total Loss: 0.039043855387717485\n",
      "------------------------------------ epoch 9824 (58938 steps) ------------------------------------\n",
      "Max loss: 0.014975379221141338\n",
      "Min loss: 0.006145143881440163\n",
      "Mean loss: 0.008723721606656909\n",
      "Std loss: 0.003215331126175362\n",
      "Total Loss: 0.052342329639941454\n",
      "------------------------------------ epoch 9825 (58944 steps) ------------------------------------\n",
      "Max loss: 0.01582752913236618\n",
      "Min loss: 0.004013495519757271\n",
      "Mean loss: 0.00756744450579087\n",
      "Std loss: 0.004195940463764108\n",
      "Total Loss: 0.045404667034745216\n",
      "------------------------------------ epoch 9826 (58950 steps) ------------------------------------\n",
      "Max loss: 0.029100850224494934\n",
      "Min loss: 0.0037390533834695816\n",
      "Mean loss: 0.010195797154059013\n",
      "Std loss: 0.008693837829258685\n",
      "Total Loss: 0.061174782924354076\n",
      "------------------------------------ epoch 9827 (58956 steps) ------------------------------------\n",
      "Max loss: 0.019015295431017876\n",
      "Min loss: 0.0031312608625739813\n",
      "Mean loss: 0.007119601553616424\n",
      "Std loss: 0.005407974851204421\n",
      "Total Loss: 0.042717609321698546\n",
      "------------------------------------ epoch 9828 (58962 steps) ------------------------------------\n",
      "Max loss: 0.008845599368214607\n",
      "Min loss: 0.005180752836167812\n",
      "Mean loss: 0.0065722841924677295\n",
      "Std loss: 0.0011907821261563415\n",
      "Total Loss: 0.039433705154806376\n",
      "------------------------------------ epoch 9829 (58968 steps) ------------------------------------\n",
      "Max loss: 0.011229360476136208\n",
      "Min loss: 0.00414382666349411\n",
      "Mean loss: 0.007498122286051512\n",
      "Std loss: 0.002264503200300718\n",
      "Total Loss: 0.04498873371630907\n",
      "------------------------------------ epoch 9830 (58974 steps) ------------------------------------\n",
      "Max loss: 0.03923889249563217\n",
      "Min loss: 0.004074113443493843\n",
      "Mean loss: 0.010769265393416086\n",
      "Std loss: 0.012749664982379965\n",
      "Total Loss: 0.06461559236049652\n",
      "------------------------------------ epoch 9831 (58980 steps) ------------------------------------\n",
      "Max loss: 0.008851509541273117\n",
      "Min loss: 0.002992359921336174\n",
      "Mean loss: 0.005697625262352328\n",
      "Std loss: 0.0020015256686497997\n",
      "Total Loss: 0.034185751574113965\n",
      "------------------------------------ epoch 9832 (58986 steps) ------------------------------------\n",
      "Max loss: 0.008918981067836285\n",
      "Min loss: 0.004009847063571215\n",
      "Mean loss: 0.006572790288676818\n",
      "Std loss: 0.0018401854634524735\n",
      "Total Loss: 0.03943674173206091\n",
      "------------------------------------ epoch 9833 (58992 steps) ------------------------------------\n",
      "Max loss: 0.013613605871796608\n",
      "Min loss: 0.003609706414863467\n",
      "Mean loss: 0.007406346150673926\n",
      "Std loss: 0.0034569075398740564\n",
      "Total Loss: 0.044438076904043555\n",
      "------------------------------------ epoch 9834 (58998 steps) ------------------------------------\n",
      "Max loss: 0.03103378415107727\n",
      "Min loss: 0.004342592321336269\n",
      "Mean loss: 0.014441048881659905\n",
      "Std loss: 0.010915092896324136\n",
      "Total Loss: 0.08664629328995943\n",
      "------------------------------------ epoch 9835 (59004 steps) ------------------------------------\n",
      "Max loss: 0.015649866312742233\n",
      "Min loss: 0.005831573158502579\n",
      "Mean loss: 0.009462357964366674\n",
      "Std loss: 0.0036657760017101385\n",
      "Total Loss: 0.05677414778620005\n",
      "------------------------------------ epoch 9836 (59010 steps) ------------------------------------\n",
      "Max loss: 0.008083309978246689\n",
      "Min loss: 0.005524768959730864\n",
      "Mean loss: 0.0064585839087764425\n",
      "Std loss: 0.0009417509291595245\n",
      "Total Loss: 0.03875150345265865\n",
      "------------------------------------ epoch 9837 (59016 steps) ------------------------------------\n",
      "Max loss: 0.020716551691293716\n",
      "Min loss: 0.0035778526216745377\n",
      "Mean loss: 0.008668064527834455\n",
      "Std loss: 0.005650539449119808\n",
      "Total Loss: 0.05200838716700673\n",
      "------------------------------------ epoch 9838 (59022 steps) ------------------------------------\n",
      "Max loss: 0.011417290195822716\n",
      "Min loss: 0.004231530707329512\n",
      "Mean loss: 0.006949389974276225\n",
      "Std loss: 0.0025653262440034383\n",
      "Total Loss: 0.04169633984565735\n",
      "------------------------------------ epoch 9839 (59028 steps) ------------------------------------\n",
      "Max loss: 0.028381584212183952\n",
      "Min loss: 0.004223231691867113\n",
      "Mean loss: 0.01215430535376072\n",
      "Std loss: 0.007925569036425564\n",
      "Total Loss: 0.07292583212256432\n",
      "------------------------------------ epoch 9840 (59034 steps) ------------------------------------\n",
      "Max loss: 0.012850714847445488\n",
      "Min loss: 0.0038638259284198284\n",
      "Mean loss: 0.007044398148233692\n",
      "Std loss: 0.00357315896724091\n",
      "Total Loss: 0.04226638888940215\n",
      "------------------------------------ epoch 9841 (59040 steps) ------------------------------------\n",
      "Max loss: 0.018701991066336632\n",
      "Min loss: 0.004450555890798569\n",
      "Mean loss: 0.009141933871433139\n",
      "Std loss: 0.00541200065453855\n",
      "Total Loss: 0.05485160322859883\n",
      "------------------------------------ epoch 9842 (59046 steps) ------------------------------------\n",
      "Max loss: 0.008753053843975067\n",
      "Min loss: 0.004134545102715492\n",
      "Mean loss: 0.0061425170085082454\n",
      "Std loss: 0.0017182825995574176\n",
      "Total Loss: 0.03685510205104947\n",
      "------------------------------------ epoch 9843 (59052 steps) ------------------------------------\n",
      "Max loss: 0.011676664464175701\n",
      "Min loss: 0.005477710627019405\n",
      "Mean loss: 0.00823196085790793\n",
      "Std loss: 0.0023161988074200616\n",
      "Total Loss: 0.049391765147447586\n",
      "------------------------------------ epoch 9844 (59058 steps) ------------------------------------\n",
      "Max loss: 0.012446139007806778\n",
      "Min loss: 0.004251997917890549\n",
      "Mean loss: 0.008017578084642688\n",
      "Std loss: 0.0029886716317469755\n",
      "Total Loss: 0.04810546850785613\n",
      "------------------------------------ epoch 9845 (59064 steps) ------------------------------------\n",
      "Max loss: 0.006986872758716345\n",
      "Min loss: 0.003594062989577651\n",
      "Mean loss: 0.005004171165637672\n",
      "Std loss: 0.0012803957658356982\n",
      "Total Loss: 0.03002502699382603\n",
      "------------------------------------ epoch 9846 (59070 steps) ------------------------------------\n",
      "Max loss: 0.017897337675094604\n",
      "Min loss: 0.0036252369172871113\n",
      "Mean loss: 0.008739538956433535\n",
      "Std loss: 0.006227524939707744\n",
      "Total Loss: 0.05243723373860121\n",
      "------------------------------------ epoch 9847 (59076 steps) ------------------------------------\n",
      "Max loss: 0.01357725914567709\n",
      "Min loss: 0.0038672720547765493\n",
      "Mean loss: 0.006459296486961345\n",
      "Std loss: 0.0032615584396647503\n",
      "Total Loss: 0.03875577892176807\n",
      "------------------------------------ epoch 9848 (59082 steps) ------------------------------------\n",
      "Max loss: 0.009457042440772057\n",
      "Min loss: 0.003862564219161868\n",
      "Mean loss: 0.006816624624965091\n",
      "Std loss: 0.0023453201313754553\n",
      "Total Loss: 0.04089974774979055\n",
      "------------------------------------ epoch 9849 (59088 steps) ------------------------------------\n",
      "Max loss: 0.01698857545852661\n",
      "Min loss: 0.003316723508760333\n",
      "Mean loss: 0.007052787463180721\n",
      "Std loss: 0.0046762947876961055\n",
      "Total Loss: 0.042316724779084325\n",
      "------------------------------------ epoch 9850 (59094 steps) ------------------------------------\n",
      "Max loss: 0.015645168721675873\n",
      "Min loss: 0.005159034859389067\n",
      "Mean loss: 0.00861796224489808\n",
      "Std loss: 0.0039541342720715925\n",
      "Total Loss: 0.051707773469388485\n",
      "------------------------------------ epoch 9851 (59100 steps) ------------------------------------\n",
      "Max loss: 0.02146466076374054\n",
      "Min loss: 0.006323697976768017\n",
      "Mean loss: 0.01089373572419087\n",
      "Std loss: 0.005253618759397779\n",
      "Total Loss: 0.06536241434514523\n",
      "------------------------------------ epoch 9852 (59106 steps) ------------------------------------\n",
      "Max loss: 0.033736806362867355\n",
      "Min loss: 0.005671870894730091\n",
      "Mean loss: 0.01157523055250446\n",
      "Std loss: 0.010017840153180682\n",
      "Total Loss: 0.06945138331502676\n",
      "------------------------------------ epoch 9853 (59112 steps) ------------------------------------\n",
      "Max loss: 0.011647811159491539\n",
      "Min loss: 0.00419571902602911\n",
      "Mean loss: 0.007029432182510694\n",
      "Std loss: 0.002497731992189845\n",
      "Total Loss: 0.04217659309506416\n",
      "------------------------------------ epoch 9854 (59118 steps) ------------------------------------\n",
      "Max loss: 0.017689846456050873\n",
      "Min loss: 0.0039078895933926105\n",
      "Mean loss: 0.008863439705843726\n",
      "Std loss: 0.005703300115862024\n",
      "Total Loss: 0.05318063823506236\n",
      "------------------------------------ epoch 9855 (59124 steps) ------------------------------------\n",
      "Max loss: 0.012476921081542969\n",
      "Min loss: 0.0030780720990151167\n",
      "Mean loss: 0.0073055209359154105\n",
      "Std loss: 0.003411634049609107\n",
      "Total Loss: 0.04383312561549246\n",
      "------------------------------------ epoch 9856 (59130 steps) ------------------------------------\n",
      "Max loss: 0.024255499243736267\n",
      "Min loss: 0.0035315947607159615\n",
      "Mean loss: 0.010832805574561158\n",
      "Std loss: 0.0068753059272201366\n",
      "Total Loss: 0.06499683344736695\n",
      "------------------------------------ epoch 9857 (59136 steps) ------------------------------------\n",
      "Max loss: 0.01446094922721386\n",
      "Min loss: 0.0036694975569844246\n",
      "Mean loss: 0.008074430283159018\n",
      "Std loss: 0.0034375853532285167\n",
      "Total Loss: 0.048446581698954105\n",
      "------------------------------------ epoch 9858 (59142 steps) ------------------------------------\n",
      "Max loss: 0.01041349396109581\n",
      "Min loss: 0.004528434947133064\n",
      "Mean loss: 0.007903425100569924\n",
      "Std loss: 0.002535785054805893\n",
      "Total Loss: 0.04742055060341954\n",
      "------------------------------------ epoch 9859 (59148 steps) ------------------------------------\n",
      "Max loss: 0.027231410145759583\n",
      "Min loss: 0.004527500830590725\n",
      "Mean loss: 0.010834891737128297\n",
      "Std loss: 0.007799961047484434\n",
      "Total Loss: 0.06500935042276978\n",
      "------------------------------------ epoch 9860 (59154 steps) ------------------------------------\n",
      "Max loss: 0.026901759207248688\n",
      "Min loss: 0.005073779262602329\n",
      "Mean loss: 0.011883432123189172\n",
      "Std loss: 0.008786818103418274\n",
      "Total Loss: 0.07130059273913503\n",
      "------------------------------------ epoch 9861 (59160 steps) ------------------------------------\n",
      "Max loss: 0.006767192389816046\n",
      "Min loss: 0.0036990910302847624\n",
      "Mean loss: 0.005521123801978926\n",
      "Std loss: 0.0009221024416314903\n",
      "Total Loss: 0.033126742811873555\n",
      "------------------------------------ epoch 9862 (59166 steps) ------------------------------------\n",
      "Max loss: 0.008774751797318459\n",
      "Min loss: 0.003337445203214884\n",
      "Mean loss: 0.005604923702776432\n",
      "Std loss: 0.0017738668151435313\n",
      "Total Loss: 0.03362954221665859\n",
      "------------------------------------ epoch 9863 (59172 steps) ------------------------------------\n",
      "Max loss: 0.011257022619247437\n",
      "Min loss: 0.003729267744347453\n",
      "Mean loss: 0.007318273147878547\n",
      "Std loss: 0.0032571305825938013\n",
      "Total Loss: 0.043909638887271285\n",
      "------------------------------------ epoch 9864 (59178 steps) ------------------------------------\n",
      "Max loss: 0.00789092481136322\n",
      "Min loss: 0.003837634576484561\n",
      "Mean loss: 0.006141651693421106\n",
      "Std loss: 0.0013785700241454575\n",
      "Total Loss: 0.03684991016052663\n",
      "------------------------------------ epoch 9865 (59184 steps) ------------------------------------\n",
      "Max loss: 0.01654515415430069\n",
      "Min loss: 0.004091514274477959\n",
      "Mean loss: 0.0070424693791816635\n",
      "Std loss: 0.0043854171230876164\n",
      "Total Loss: 0.04225481627508998\n",
      "------------------------------------ epoch 9866 (59190 steps) ------------------------------------\n",
      "Max loss: 0.013278115540742874\n",
      "Min loss: 0.003580160904675722\n",
      "Mean loss: 0.008843738120049238\n",
      "Std loss: 0.0033238571139483147\n",
      "Total Loss: 0.05306242872029543\n",
      "------------------------------------ epoch 9867 (59196 steps) ------------------------------------\n",
      "Max loss: 0.0252985917031765\n",
      "Min loss: 0.004740127827972174\n",
      "Mean loss: 0.009868232533335686\n",
      "Std loss: 0.007227780115459525\n",
      "Total Loss: 0.059209395200014114\n",
      "------------------------------------ epoch 9868 (59202 steps) ------------------------------------\n",
      "Max loss: 0.017090335488319397\n",
      "Min loss: 0.003513829782605171\n",
      "Mean loss: 0.008360246662050486\n",
      "Std loss: 0.004721517205277978\n",
      "Total Loss: 0.050161479972302914\n",
      "------------------------------------ epoch 9869 (59208 steps) ------------------------------------\n",
      "Max loss: 0.017101123929023743\n",
      "Min loss: 0.004215807653963566\n",
      "Mean loss: 0.007569897066180904\n",
      "Std loss: 0.004846277439193868\n",
      "Total Loss: 0.04541938239708543\n",
      "------------------------------------ epoch 9870 (59214 steps) ------------------------------------\n",
      "Max loss: 0.008140046149492264\n",
      "Min loss: 0.0036975927650928497\n",
      "Mean loss: 0.005603668745607138\n",
      "Std loss: 0.001661438160128076\n",
      "Total Loss: 0.033622012473642826\n",
      "------------------------------------ epoch 9871 (59220 steps) ------------------------------------\n",
      "Max loss: 0.023112233728170395\n",
      "Min loss: 0.0043391818180680275\n",
      "Mean loss: 0.00981878605671227\n",
      "Std loss: 0.006177447762965195\n",
      "Total Loss: 0.05891271634027362\n",
      "------------------------------------ epoch 9872 (59226 steps) ------------------------------------\n",
      "Max loss: 0.02052486315369606\n",
      "Min loss: 0.0048216660507023335\n",
      "Mean loss: 0.011304966639727354\n",
      "Std loss: 0.006281678338293228\n",
      "Total Loss: 0.06782979983836412\n",
      "------------------------------------ epoch 9873 (59232 steps) ------------------------------------\n",
      "Max loss: 0.023549353703856468\n",
      "Min loss: 0.004644508473575115\n",
      "Mean loss: 0.009572722871477405\n",
      "Std loss: 0.006540518924639303\n",
      "Total Loss: 0.05743633722886443\n",
      "------------------------------------ epoch 9874 (59238 steps) ------------------------------------\n",
      "Max loss: 0.02529977634549141\n",
      "Min loss: 0.004144879058003426\n",
      "Mean loss: 0.009896736514444152\n",
      "Std loss: 0.007115191325473553\n",
      "Total Loss: 0.059380419086664915\n",
      "------------------------------------ epoch 9875 (59244 steps) ------------------------------------\n",
      "Max loss: 0.00878471415489912\n",
      "Min loss: 0.003852595342323184\n",
      "Mean loss: 0.006304745678789914\n",
      "Std loss: 0.001984828652055068\n",
      "Total Loss: 0.03782847407273948\n",
      "------------------------------------ epoch 9876 (59250 steps) ------------------------------------\n",
      "Max loss: 0.007358967326581478\n",
      "Min loss: 0.004003520589321852\n",
      "Mean loss: 0.005307239713147283\n",
      "Std loss: 0.0011444170634851453\n",
      "Total Loss: 0.031843438278883696\n",
      "------------------------------------ epoch 9877 (59256 steps) ------------------------------------\n",
      "Max loss: 0.013477666303515434\n",
      "Min loss: 0.004621119238436222\n",
      "Mean loss: 0.008250400656834245\n",
      "Std loss: 0.0032783707802855186\n",
      "Total Loss: 0.04950240394100547\n",
      "------------------------------------ epoch 9878 (59262 steps) ------------------------------------\n",
      "Max loss: 0.01051925029605627\n",
      "Min loss: 0.003791281022131443\n",
      "Mean loss: 0.006114170808965961\n",
      "Std loss: 0.002446236218855135\n",
      "Total Loss: 0.03668502485379577\n",
      "------------------------------------ epoch 9879 (59268 steps) ------------------------------------\n",
      "Max loss: 0.021163325756788254\n",
      "Min loss: 0.0034357807599008083\n",
      "Mean loss: 0.010519567023341855\n",
      "Std loss: 0.006422110954872007\n",
      "Total Loss: 0.06311740214005113\n",
      "------------------------------------ epoch 9880 (59274 steps) ------------------------------------\n",
      "Max loss: 0.01954910159111023\n",
      "Min loss: 0.004894475918263197\n",
      "Mean loss: 0.008454672371347746\n",
      "Std loss: 0.0050513519625223\n",
      "Total Loss: 0.05072803422808647\n",
      "------------------------------------ epoch 9881 (59280 steps) ------------------------------------\n",
      "Max loss: 0.015819955617189407\n",
      "Min loss: 0.004204803146421909\n",
      "Mean loss: 0.007066943605119984\n",
      "Std loss: 0.003956661220329859\n",
      "Total Loss: 0.0424016616307199\n",
      "------------------------------------ epoch 9882 (59286 steps) ------------------------------------\n",
      "Max loss: 0.013092613779008389\n",
      "Min loss: 0.0044649699702858925\n",
      "Mean loss: 0.008305236774807176\n",
      "Std loss: 0.0033818993088314824\n",
      "Total Loss: 0.04983142064884305\n",
      "------------------------------------ epoch 9883 (59292 steps) ------------------------------------\n",
      "Max loss: 0.01502313744276762\n",
      "Min loss: 0.004420486278831959\n",
      "Mean loss: 0.009418121073395014\n",
      "Std loss: 0.0038004974145872937\n",
      "Total Loss: 0.05650872644037008\n",
      "------------------------------------ epoch 9884 (59298 steps) ------------------------------------\n",
      "Max loss: 0.012631271034479141\n",
      "Min loss: 0.004544232506304979\n",
      "Mean loss: 0.008887817151844501\n",
      "Std loss: 0.003364107066551322\n",
      "Total Loss: 0.05332690291106701\n",
      "------------------------------------ epoch 9885 (59304 steps) ------------------------------------\n",
      "Max loss: 0.04145797714591026\n",
      "Min loss: 0.006449056323617697\n",
      "Mean loss: 0.01745472856176396\n",
      "Std loss: 0.012494673711749237\n",
      "Total Loss: 0.10472837137058377\n",
      "------------------------------------ epoch 9886 (59310 steps) ------------------------------------\n",
      "Max loss: 0.01968003623187542\n",
      "Min loss: 0.005526610650122166\n",
      "Mean loss: 0.009837615924576918\n",
      "Std loss: 0.004667382682753157\n",
      "Total Loss: 0.05902569554746151\n",
      "------------------------------------ epoch 9887 (59316 steps) ------------------------------------\n",
      "Max loss: 0.033987537026405334\n",
      "Min loss: 0.00428765220567584\n",
      "Mean loss: 0.0124132145040979\n",
      "Std loss: 0.010442655885496008\n",
      "Total Loss: 0.07447928702458739\n",
      "------------------------------------ epoch 9888 (59322 steps) ------------------------------------\n",
      "Max loss: 0.013731056824326515\n",
      "Min loss: 0.006162151228636503\n",
      "Mean loss: 0.008127053811525306\n",
      "Std loss: 0.002600436513343404\n",
      "Total Loss: 0.04876232286915183\n",
      "------------------------------------ epoch 9889 (59328 steps) ------------------------------------\n",
      "Max loss: 0.01891113817691803\n",
      "Min loss: 0.004372994415462017\n",
      "Mean loss: 0.008622492430731654\n",
      "Std loss: 0.004971399057674293\n",
      "Total Loss: 0.051734954584389925\n",
      "------------------------------------ epoch 9890 (59334 steps) ------------------------------------\n",
      "Max loss: 0.017297878861427307\n",
      "Min loss: 0.005349736660718918\n",
      "Mean loss: 0.010070835938677192\n",
      "Std loss: 0.004852558055905657\n",
      "Total Loss: 0.06042501563206315\n",
      "------------------------------------ epoch 9891 (59340 steps) ------------------------------------\n",
      "Max loss: 0.020727992057800293\n",
      "Min loss: 0.003844413673505187\n",
      "Mean loss: 0.010009879906040927\n",
      "Std loss: 0.005753708541810393\n",
      "Total Loss: 0.06005927943624556\n",
      "------------------------------------ epoch 9892 (59346 steps) ------------------------------------\n",
      "Max loss: 0.03240673243999481\n",
      "Min loss: 0.0030699726194143295\n",
      "Mean loss: 0.01018679370948424\n",
      "Std loss: 0.010127848903726122\n",
      "Total Loss: 0.061120762256905437\n",
      "------------------------------------ epoch 9893 (59352 steps) ------------------------------------\n",
      "Max loss: 0.017851199954748154\n",
      "Min loss: 0.004185964353382587\n",
      "Mean loss: 0.011046554582814375\n",
      "Std loss: 0.005734672421771657\n",
      "Total Loss: 0.06627932749688625\n",
      "------------------------------------ epoch 9894 (59358 steps) ------------------------------------\n",
      "Max loss: 0.031270213425159454\n",
      "Min loss: 0.0054184310138225555\n",
      "Mean loss: 0.011266273989652595\n",
      "Std loss: 0.009126189931049739\n",
      "Total Loss: 0.06759764393791556\n",
      "------------------------------------ epoch 9895 (59364 steps) ------------------------------------\n",
      "Max loss: 0.010908718220889568\n",
      "Min loss: 0.005164473783224821\n",
      "Mean loss: 0.006956954253837466\n",
      "Std loss: 0.002026580481403916\n",
      "Total Loss: 0.0417417255230248\n",
      "------------------------------------ epoch 9896 (59370 steps) ------------------------------------\n",
      "Max loss: 0.03331980109214783\n",
      "Min loss: 0.004834527149796486\n",
      "Mean loss: 0.010814283431197206\n",
      "Std loss: 0.010114397949147244\n",
      "Total Loss: 0.06488570058718324\n",
      "------------------------------------ epoch 9897 (59376 steps) ------------------------------------\n",
      "Max loss: 0.016310088336467743\n",
      "Min loss: 0.004519199021160603\n",
      "Mean loss: 0.010263090021908283\n",
      "Std loss: 0.004327813207988116\n",
      "Total Loss: 0.0615785401314497\n",
      "------------------------------------ epoch 9898 (59382 steps) ------------------------------------\n",
      "Max loss: 0.009974537417292595\n",
      "Min loss: 0.004498432856053114\n",
      "Mean loss: 0.006643065872291724\n",
      "Std loss: 0.0017403953289366993\n",
      "Total Loss: 0.03985839523375034\n",
      "------------------------------------ epoch 9899 (59388 steps) ------------------------------------\n",
      "Max loss: 0.02611699514091015\n",
      "Min loss: 0.005252254661172628\n",
      "Mean loss: 0.013189524877816439\n",
      "Std loss: 0.00856532552074349\n",
      "Total Loss: 0.07913714926689863\n",
      "------------------------------------ epoch 9900 (59394 steps) ------------------------------------\n",
      "Max loss: 0.01630789041519165\n",
      "Min loss: 0.004087679088115692\n",
      "Mean loss: 0.008201058798780044\n",
      "Std loss: 0.004524892427708761\n",
      "Total Loss: 0.049206352792680264\n",
      "------------------------------------ epoch 9901 (59400 steps) ------------------------------------\n",
      "Max loss: 0.011638818308711052\n",
      "Min loss: 0.004048969596624374\n",
      "Mean loss: 0.00637237261980772\n",
      "Std loss: 0.002575233551478237\n",
      "Total Loss: 0.03823423571884632\n",
      "saved model at ./weights/model_9901.pth\n",
      "------------------------------------ epoch 9902 (59406 steps) ------------------------------------\n",
      "Max loss: 0.013731135055422783\n",
      "Min loss: 0.004179303999990225\n",
      "Mean loss: 0.007308890965456764\n",
      "Std loss: 0.003423276637968513\n",
      "Total Loss: 0.04385334579274058\n",
      "------------------------------------ epoch 9903 (59412 steps) ------------------------------------\n",
      "Max loss: 0.008672382682561874\n",
      "Min loss: 0.0036697841715067625\n",
      "Mean loss: 0.005540255651188393\n",
      "Std loss: 0.0019159936126022003\n",
      "Total Loss: 0.03324153390713036\n",
      "------------------------------------ epoch 9904 (59418 steps) ------------------------------------\n",
      "Max loss: 0.014677895233035088\n",
      "Min loss: 0.00406345259398222\n",
      "Mean loss: 0.009224592552830776\n",
      "Std loss: 0.003865990201656158\n",
      "Total Loss: 0.055347555316984653\n",
      "------------------------------------ epoch 9905 (59424 steps) ------------------------------------\n",
      "Max loss: 0.009609820321202278\n",
      "Min loss: 0.003877870738506317\n",
      "Mean loss: 0.006362667307257652\n",
      "Std loss: 0.0017806163622095044\n",
      "Total Loss: 0.038176003843545914\n",
      "------------------------------------ epoch 9906 (59430 steps) ------------------------------------\n",
      "Max loss: 0.010050114244222641\n",
      "Min loss: 0.00371354422532022\n",
      "Mean loss: 0.005912630391928057\n",
      "Std loss: 0.002128191998051408\n",
      "Total Loss: 0.03547578235156834\n",
      "------------------------------------ epoch 9907 (59436 steps) ------------------------------------\n",
      "Max loss: 0.007432667072862387\n",
      "Min loss: 0.003758114529773593\n",
      "Mean loss: 0.005391538647624354\n",
      "Std loss: 0.001238802868953226\n",
      "Total Loss: 0.03234923188574612\n",
      "------------------------------------ epoch 9908 (59442 steps) ------------------------------------\n",
      "Max loss: 0.009879868477582932\n",
      "Min loss: 0.0027469375636428595\n",
      "Mean loss: 0.006045533926226199\n",
      "Std loss: 0.0022062910494997346\n",
      "Total Loss: 0.03627320355735719\n",
      "------------------------------------ epoch 9909 (59448 steps) ------------------------------------\n",
      "Max loss: 0.005542558617889881\n",
      "Min loss: 0.002895767567679286\n",
      "Mean loss: 0.003702969988808036\n",
      "Std loss: 0.0008791732114911357\n",
      "Total Loss: 0.022217819932848215\n",
      "------------------------------------ epoch 9910 (59454 steps) ------------------------------------\n",
      "Max loss: 0.006150907836854458\n",
      "Min loss: 0.0032310744281858206\n",
      "Mean loss: 0.004580709190728764\n",
      "Std loss: 0.001169036615364589\n",
      "Total Loss: 0.027484255144372582\n",
      "------------------------------------ epoch 9911 (59460 steps) ------------------------------------\n",
      "Max loss: 0.02489231899380684\n",
      "Min loss: 0.004628220107406378\n",
      "Mean loss: 0.00981370941735804\n",
      "Std loss: 0.007053264089268288\n",
      "Total Loss: 0.058882256504148245\n",
      "------------------------------------ epoch 9912 (59466 steps) ------------------------------------\n",
      "Max loss: 0.02674020081758499\n",
      "Min loss: 0.003509724047034979\n",
      "Mean loss: 0.009797547206593057\n",
      "Std loss: 0.007939249354166893\n",
      "Total Loss: 0.05878528323955834\n",
      "------------------------------------ epoch 9913 (59472 steps) ------------------------------------\n",
      "Max loss: 0.016356442123651505\n",
      "Min loss: 0.0052444953471422195\n",
      "Mean loss: 0.0084896394982934\n",
      "Std loss: 0.0038650655378361957\n",
      "Total Loss: 0.0509378369897604\n",
      "------------------------------------ epoch 9914 (59478 steps) ------------------------------------\n",
      "Max loss: 0.00965193659067154\n",
      "Min loss: 0.00443599559366703\n",
      "Mean loss: 0.006838209073369701\n",
      "Std loss: 0.0019667513787644583\n",
      "Total Loss: 0.04102925444021821\n",
      "------------------------------------ epoch 9915 (59484 steps) ------------------------------------\n",
      "Max loss: 0.0180705264210701\n",
      "Min loss: 0.004124504514038563\n",
      "Mean loss: 0.008968784241005778\n",
      "Std loss: 0.004640459630069057\n",
      "Total Loss: 0.05381270544603467\n",
      "------------------------------------ epoch 9916 (59490 steps) ------------------------------------\n",
      "Max loss: 0.009246358647942543\n",
      "Min loss: 0.005316923838108778\n",
      "Mean loss: 0.007299364389230807\n",
      "Std loss: 0.0013332465354378292\n",
      "Total Loss: 0.043796186335384846\n",
      "------------------------------------ epoch 9917 (59496 steps) ------------------------------------\n",
      "Max loss: 0.020695140585303307\n",
      "Min loss: 0.006421311758458614\n",
      "Mean loss: 0.01220801364009579\n",
      "Std loss: 0.00478658868897063\n",
      "Total Loss: 0.07324808184057474\n",
      "------------------------------------ epoch 9918 (59502 steps) ------------------------------------\n",
      "Max loss: 0.010736360214650631\n",
      "Min loss: 0.0039016432128846645\n",
      "Mean loss: 0.006943986052647233\n",
      "Std loss: 0.002616674504103305\n",
      "Total Loss: 0.0416639163158834\n",
      "------------------------------------ epoch 9919 (59508 steps) ------------------------------------\n",
      "Max loss: 0.019361983984708786\n",
      "Min loss: 0.0034425246994942427\n",
      "Mean loss: 0.010855927558926245\n",
      "Std loss: 0.005091752825212323\n",
      "Total Loss: 0.06513556535355747\n",
      "------------------------------------ epoch 9920 (59514 steps) ------------------------------------\n",
      "Max loss: 0.01224991213530302\n",
      "Min loss: 0.0039077578112483025\n",
      "Mean loss: 0.006993300747126341\n",
      "Std loss: 0.0030871286718116473\n",
      "Total Loss: 0.041959804482758045\n",
      "------------------------------------ epoch 9921 (59520 steps) ------------------------------------\n",
      "Max loss: 0.018829263746738434\n",
      "Min loss: 0.005344653036445379\n",
      "Mean loss: 0.010246514109894633\n",
      "Std loss: 0.00445343520800228\n",
      "Total Loss: 0.0614790846593678\n",
      "------------------------------------ epoch 9922 (59526 steps) ------------------------------------\n",
      "Max loss: 0.03461141139268875\n",
      "Min loss: 0.00505297165364027\n",
      "Mean loss: 0.012320848026623329\n",
      "Std loss: 0.010167316840889597\n",
      "Total Loss: 0.07392508815973997\n",
      "------------------------------------ epoch 9923 (59532 steps) ------------------------------------\n",
      "Max loss: 0.02709147334098816\n",
      "Min loss: 0.0032649885397404432\n",
      "Mean loss: 0.010452246875502169\n",
      "Std loss: 0.008632059457810365\n",
      "Total Loss: 0.06271348125301301\n",
      "------------------------------------ epoch 9924 (59538 steps) ------------------------------------\n",
      "Max loss: 0.024637870490550995\n",
      "Min loss: 0.005865426268428564\n",
      "Mean loss: 0.012659229570999742\n",
      "Std loss: 0.006435190959741939\n",
      "Total Loss: 0.07595537742599845\n",
      "------------------------------------ epoch 9925 (59544 steps) ------------------------------------\n",
      "Max loss: 0.02391268126666546\n",
      "Min loss: 0.005932394415140152\n",
      "Mean loss: 0.01153085520491004\n",
      "Std loss: 0.005753262308848403\n",
      "Total Loss: 0.06918513122946024\n",
      "------------------------------------ epoch 9926 (59550 steps) ------------------------------------\n",
      "Max loss: 0.019577739760279655\n",
      "Min loss: 0.005466642789542675\n",
      "Mean loss: 0.011388972556839386\n",
      "Std loss: 0.004709922743695461\n",
      "Total Loss: 0.06833383534103632\n",
      "------------------------------------ epoch 9927 (59556 steps) ------------------------------------\n",
      "Max loss: 0.012834727764129639\n",
      "Min loss: 0.006161133758723736\n",
      "Mean loss: 0.008394936410089334\n",
      "Std loss: 0.002389832298484816\n",
      "Total Loss: 0.050369618460536\n",
      "------------------------------------ epoch 9928 (59562 steps) ------------------------------------\n",
      "Max loss: 0.0334085151553154\n",
      "Min loss: 0.004627124406397343\n",
      "Mean loss: 0.01374941955630978\n",
      "Std loss: 0.010459819089187947\n",
      "Total Loss: 0.08249651733785868\n",
      "------------------------------------ epoch 9929 (59568 steps) ------------------------------------\n",
      "Max loss: 0.012097418308258057\n",
      "Min loss: 0.0059941909275949\n",
      "Mean loss: 0.00781754101626575\n",
      "Std loss: 0.002187966644614385\n",
      "Total Loss: 0.0469052460975945\n",
      "------------------------------------ epoch 9930 (59574 steps) ------------------------------------\n",
      "Max loss: 0.02903396636247635\n",
      "Min loss: 0.005876810755580664\n",
      "Mean loss: 0.01340122458835443\n",
      "Std loss: 0.0081534119246503\n",
      "Total Loss: 0.08040734753012657\n",
      "------------------------------------ epoch 9931 (59580 steps) ------------------------------------\n",
      "Max loss: 0.016069848090410233\n",
      "Min loss: 0.0044447630643844604\n",
      "Mean loss: 0.008169726856673757\n",
      "Std loss: 0.003762475748770942\n",
      "Total Loss: 0.04901836114004254\n",
      "------------------------------------ epoch 9932 (59586 steps) ------------------------------------\n",
      "Max loss: 0.03816838935017586\n",
      "Min loss: 0.005051391199231148\n",
      "Mean loss: 0.012122537940740585\n",
      "Std loss: 0.01174974935239638\n",
      "Total Loss: 0.07273522764444351\n",
      "------------------------------------ epoch 9933 (59592 steps) ------------------------------------\n",
      "Max loss: 0.04064536467194557\n",
      "Min loss: 0.005686032585799694\n",
      "Mean loss: 0.013951479302098354\n",
      "Std loss: 0.01279373967802804\n",
      "Total Loss: 0.08370887581259012\n",
      "------------------------------------ epoch 9934 (59598 steps) ------------------------------------\n",
      "Max loss: 0.01176054310053587\n",
      "Min loss: 0.005126349627971649\n",
      "Mean loss: 0.0080917962671568\n",
      "Std loss: 0.0023870613436601574\n",
      "Total Loss: 0.0485507776029408\n",
      "------------------------------------ epoch 9935 (59604 steps) ------------------------------------\n",
      "Max loss: 0.013603372499346733\n",
      "Min loss: 0.003889109008014202\n",
      "Mean loss: 0.007112310733646154\n",
      "Std loss: 0.003370746528784623\n",
      "Total Loss: 0.042673864401876926\n",
      "------------------------------------ epoch 9936 (59610 steps) ------------------------------------\n",
      "Max loss: 0.02795167826116085\n",
      "Min loss: 0.004296367987990379\n",
      "Mean loss: 0.0129637874973317\n",
      "Std loss: 0.008032064014971425\n",
      "Total Loss: 0.07778272498399019\n",
      "------------------------------------ epoch 9937 (59616 steps) ------------------------------------\n",
      "Max loss: 0.025751788169145584\n",
      "Min loss: 0.004509043879806995\n",
      "Mean loss: 0.009638586624835929\n",
      "Std loss: 0.007357762116382386\n",
      "Total Loss: 0.05783151974901557\n",
      "------------------------------------ epoch 9938 (59622 steps) ------------------------------------\n",
      "Max loss: 0.0158996619284153\n",
      "Min loss: 0.004733181558549404\n",
      "Mean loss: 0.00790134770795703\n",
      "Std loss: 0.0038973785936083573\n",
      "Total Loss: 0.047408086247742176\n",
      "------------------------------------ epoch 9939 (59628 steps) ------------------------------------\n",
      "Max loss: 0.008513562381267548\n",
      "Min loss: 0.0036902353167533875\n",
      "Mean loss: 0.005697295535355806\n",
      "Std loss: 0.0017113304408937478\n",
      "Total Loss: 0.03418377321213484\n",
      "------------------------------------ epoch 9940 (59634 steps) ------------------------------------\n",
      "Max loss: 0.007256691809743643\n",
      "Min loss: 0.0033891533967107534\n",
      "Mean loss: 0.005511739174835384\n",
      "Std loss: 0.0012251029143851174\n",
      "Total Loss: 0.0330704350490123\n",
      "------------------------------------ epoch 9941 (59640 steps) ------------------------------------\n",
      "Max loss: 0.01395314559340477\n",
      "Min loss: 0.003404401009902358\n",
      "Mean loss: 0.006712395659027\n",
      "Std loss: 0.003568962247117972\n",
      "Total Loss: 0.040274373954162\n",
      "------------------------------------ epoch 9942 (59646 steps) ------------------------------------\n",
      "Max loss: 0.03635169565677643\n",
      "Min loss: 0.0037130245473235846\n",
      "Mean loss: 0.015593528165481985\n",
      "Std loss: 0.014211251752795728\n",
      "Total Loss: 0.09356116899289191\n",
      "------------------------------------ epoch 9943 (59652 steps) ------------------------------------\n",
      "Max loss: 0.01718027889728546\n",
      "Min loss: 0.005003677681088448\n",
      "Mean loss: 0.008270337479189038\n",
      "Std loss: 0.004246239330151264\n",
      "Total Loss: 0.04962202487513423\n",
      "------------------------------------ epoch 9944 (59658 steps) ------------------------------------\n",
      "Max loss: 0.011373749934136868\n",
      "Min loss: 0.004271703772246838\n",
      "Mean loss: 0.006541269288087885\n",
      "Std loss: 0.0022790555496505563\n",
      "Total Loss: 0.03924761572852731\n",
      "------------------------------------ epoch 9945 (59664 steps) ------------------------------------\n",
      "Max loss: 0.013485996052622795\n",
      "Min loss: 0.004246063064783812\n",
      "Mean loss: 0.007727212971076369\n",
      "Std loss: 0.00376930343347632\n",
      "Total Loss: 0.046363277826458216\n",
      "------------------------------------ epoch 9946 (59670 steps) ------------------------------------\n",
      "Max loss: 0.016063867136836052\n",
      "Min loss: 0.003782059997320175\n",
      "Mean loss: 0.008609655934075514\n",
      "Std loss: 0.005207244007087011\n",
      "Total Loss: 0.05165793560445309\n",
      "------------------------------------ epoch 9947 (59676 steps) ------------------------------------\n",
      "Max loss: 0.010474877431988716\n",
      "Min loss: 0.004538442008197308\n",
      "Mean loss: 0.006617107816661398\n",
      "Std loss: 0.0022959688111833334\n",
      "Total Loss: 0.039702646899968386\n",
      "------------------------------------ epoch 9948 (59682 steps) ------------------------------------\n",
      "Max loss: 0.012956053018569946\n",
      "Min loss: 0.00772175332531333\n",
      "Mean loss: 0.010233945601309339\n",
      "Std loss: 0.0019638171709011948\n",
      "Total Loss: 0.061403673607856035\n",
      "------------------------------------ epoch 9949 (59688 steps) ------------------------------------\n",
      "Max loss: 0.021699439734220505\n",
      "Min loss: 0.004245561081916094\n",
      "Mean loss: 0.008838271566977104\n",
      "Std loss: 0.006191774483189534\n",
      "Total Loss: 0.05302962940186262\n",
      "------------------------------------ epoch 9950 (59694 steps) ------------------------------------\n",
      "Max loss: 0.0057003675028681755\n",
      "Min loss: 0.0039670951664447784\n",
      "Mean loss: 0.004951501730829477\n",
      "Std loss: 0.0006190059818434229\n",
      "Total Loss: 0.029709010384976864\n",
      "------------------------------------ epoch 9951 (59700 steps) ------------------------------------\n",
      "Max loss: 0.019412187859416008\n",
      "Min loss: 0.003767686430364847\n",
      "Mean loss: 0.008462866302579641\n",
      "Std loss: 0.00549742338755627\n",
      "Total Loss: 0.05077719781547785\n",
      "------------------------------------ epoch 9952 (59706 steps) ------------------------------------\n",
      "Max loss: 0.00844587292522192\n",
      "Min loss: 0.0037430718075484037\n",
      "Mean loss: 0.005691337981261313\n",
      "Std loss: 0.0014855046558918418\n",
      "Total Loss: 0.03414802788756788\n",
      "------------------------------------ epoch 9953 (59712 steps) ------------------------------------\n",
      "Max loss: 0.01479807123541832\n",
      "Min loss: 0.003296088194474578\n",
      "Mean loss: 0.006319045826482276\n",
      "Std loss: 0.0038793527126188224\n",
      "Total Loss: 0.03791427495889366\n",
      "------------------------------------ epoch 9954 (59718 steps) ------------------------------------\n",
      "Max loss: 0.006366541609168053\n",
      "Min loss: 0.0032892494928091764\n",
      "Mean loss: 0.004810352615701656\n",
      "Std loss: 0.0012373455705120071\n",
      "Total Loss: 0.028862115694209933\n",
      "------------------------------------ epoch 9955 (59724 steps) ------------------------------------\n",
      "Max loss: 0.009481461718678474\n",
      "Min loss: 0.00316315283998847\n",
      "Mean loss: 0.004994229801620047\n",
      "Std loss: 0.002118943134960475\n",
      "Total Loss: 0.029965378809720278\n",
      "------------------------------------ epoch 9956 (59730 steps) ------------------------------------\n",
      "Max loss: 0.008343836292624474\n",
      "Min loss: 0.003780543338507414\n",
      "Mean loss: 0.005759989532331626\n",
      "Std loss: 0.0014859537026911903\n",
      "Total Loss: 0.034559937193989754\n",
      "------------------------------------ epoch 9957 (59736 steps) ------------------------------------\n",
      "Max loss: 0.013941822573542595\n",
      "Min loss: 0.003739622188732028\n",
      "Mean loss: 0.007547914787816505\n",
      "Std loss: 0.0033826304375264\n",
      "Total Loss: 0.04528748872689903\n",
      "------------------------------------ epoch 9958 (59742 steps) ------------------------------------\n",
      "Max loss: 0.01107962429523468\n",
      "Min loss: 0.0031125161331146955\n",
      "Mean loss: 0.005764554759177069\n",
      "Std loss: 0.0026834768389967525\n",
      "Total Loss: 0.03458732855506241\n",
      "------------------------------------ epoch 9959 (59748 steps) ------------------------------------\n",
      "Max loss: 0.032073408365249634\n",
      "Min loss: 0.0035724497865885496\n",
      "Mean loss: 0.008849400755328437\n",
      "Std loss: 0.010399631557064934\n",
      "Total Loss: 0.05309640453197062\n",
      "------------------------------------ epoch 9960 (59754 steps) ------------------------------------\n",
      "Max loss: 0.011880971491336823\n",
      "Min loss: 0.0034284768626093864\n",
      "Mean loss: 0.006424241000786424\n",
      "Std loss: 0.0028452896815190903\n",
      "Total Loss: 0.03854544600471854\n",
      "------------------------------------ epoch 9961 (59760 steps) ------------------------------------\n",
      "Max loss: 0.007934311404824257\n",
      "Min loss: 0.003512223018333316\n",
      "Mean loss: 0.00553186156321317\n",
      "Std loss: 0.0015178915635748566\n",
      "Total Loss: 0.03319116937927902\n",
      "------------------------------------ epoch 9962 (59766 steps) ------------------------------------\n",
      "Max loss: 0.02424224279820919\n",
      "Min loss: 0.003398847533389926\n",
      "Mean loss: 0.008044870609107116\n",
      "Std loss: 0.007301672831234354\n",
      "Total Loss: 0.0482692236546427\n",
      "------------------------------------ epoch 9963 (59772 steps) ------------------------------------\n",
      "Max loss: 0.01310642622411251\n",
      "Min loss: 0.0034701782278716564\n",
      "Mean loss: 0.006892687485863765\n",
      "Std loss: 0.003800415112119908\n",
      "Total Loss: 0.04135612491518259\n",
      "------------------------------------ epoch 9964 (59778 steps) ------------------------------------\n",
      "Max loss: 0.0353805236518383\n",
      "Min loss: 0.004575697705149651\n",
      "Mean loss: 0.012005502202858528\n",
      "Std loss: 0.011089381450523943\n",
      "Total Loss: 0.07203301321715117\n",
      "------------------------------------ epoch 9965 (59784 steps) ------------------------------------\n",
      "Max loss: 0.04594577103853226\n",
      "Min loss: 0.004577782936394215\n",
      "Mean loss: 0.013363574165850878\n",
      "Std loss: 0.014650151528476313\n",
      "Total Loss: 0.08018144499510527\n",
      "------------------------------------ epoch 9966 (59790 steps) ------------------------------------\n",
      "Max loss: 0.03317577764391899\n",
      "Min loss: 0.005045224912464619\n",
      "Mean loss: 0.011700472639252743\n",
      "Std loss: 0.009680725964854933\n",
      "Total Loss: 0.07020283583551645\n",
      "------------------------------------ epoch 9967 (59796 steps) ------------------------------------\n",
      "Max loss: 0.013870865106582642\n",
      "Min loss: 0.005030788481235504\n",
      "Mean loss: 0.00835464553286632\n",
      "Std loss: 0.0028326149441727892\n",
      "Total Loss: 0.050127873197197914\n",
      "------------------------------------ epoch 9968 (59802 steps) ------------------------------------\n",
      "Max loss: 0.012628825381398201\n",
      "Min loss: 0.0032103750854730606\n",
      "Mean loss: 0.007315630481267969\n",
      "Std loss: 0.003529043177823009\n",
      "Total Loss: 0.04389378288760781\n",
      "------------------------------------ epoch 9969 (59808 steps) ------------------------------------\n",
      "Max loss: 0.012115167453885078\n",
      "Min loss: 0.005044094752520323\n",
      "Mean loss: 0.007396230474114418\n",
      "Std loss: 0.0024676198642401555\n",
      "Total Loss: 0.04437738284468651\n",
      "------------------------------------ epoch 9970 (59814 steps) ------------------------------------\n",
      "Max loss: 0.02057574689388275\n",
      "Min loss: 0.0037788390181958675\n",
      "Mean loss: 0.00812601593012611\n",
      "Std loss: 0.0057447515007375645\n",
      "Total Loss: 0.048756095580756664\n",
      "------------------------------------ epoch 9971 (59820 steps) ------------------------------------\n",
      "Max loss: 0.010847858153283596\n",
      "Min loss: 0.0030097197741270065\n",
      "Mean loss: 0.005957029449443023\n",
      "Std loss: 0.0024623985192165337\n",
      "Total Loss: 0.035742176696658134\n",
      "------------------------------------ epoch 9972 (59826 steps) ------------------------------------\n",
      "Max loss: 0.006582745350897312\n",
      "Min loss: 0.0029625981114804745\n",
      "Mean loss: 0.004594178055413067\n",
      "Std loss: 0.001299877957154486\n",
      "Total Loss: 0.027565068332478404\n",
      "------------------------------------ epoch 9973 (59832 steps) ------------------------------------\n",
      "Max loss: 0.015193919651210308\n",
      "Min loss: 0.0040959627367556095\n",
      "Mean loss: 0.0067871916107833385\n",
      "Std loss: 0.0038555674346837194\n",
      "Total Loss: 0.04072314966470003\n",
      "------------------------------------ epoch 9974 (59838 steps) ------------------------------------\n",
      "Max loss: 0.008054373785853386\n",
      "Min loss: 0.004152338020503521\n",
      "Mean loss: 0.005827935878187418\n",
      "Std loss: 0.001413630941482852\n",
      "Total Loss: 0.03496761526912451\n",
      "------------------------------------ epoch 9975 (59844 steps) ------------------------------------\n",
      "Max loss: 0.013610420748591423\n",
      "Min loss: 0.0036494387313723564\n",
      "Mean loss: 0.0071703309658914804\n",
      "Std loss: 0.0035629335214577493\n",
      "Total Loss: 0.04302198579534888\n",
      "------------------------------------ epoch 9976 (59850 steps) ------------------------------------\n",
      "Max loss: 0.007079216651618481\n",
      "Min loss: 0.003687445539981127\n",
      "Mean loss: 0.005207458122943838\n",
      "Std loss: 0.00113792631131658\n",
      "Total Loss: 0.03124474873766303\n",
      "------------------------------------ epoch 9977 (59856 steps) ------------------------------------\n",
      "Max loss: 0.006193326786160469\n",
      "Min loss: 0.0029801074415445328\n",
      "Mean loss: 0.004157715865100424\n",
      "Std loss: 0.0012577041091775874\n",
      "Total Loss: 0.02494629519060254\n",
      "------------------------------------ epoch 9978 (59862 steps) ------------------------------------\n",
      "Max loss: 0.019900137558579445\n",
      "Min loss: 0.0029258958529680967\n",
      "Mean loss: 0.006717340865482886\n",
      "Std loss: 0.005990746851129597\n",
      "Total Loss: 0.04030404519289732\n",
      "------------------------------------ epoch 9979 (59868 steps) ------------------------------------\n",
      "Max loss: 0.009776544757187366\n",
      "Min loss: 0.004051164258271456\n",
      "Mean loss: 0.0057520749202619\n",
      "Std loss: 0.0021987732607672782\n",
      "Total Loss: 0.0345124495215714\n",
      "------------------------------------ epoch 9980 (59874 steps) ------------------------------------\n",
      "Max loss: 0.006308894604444504\n",
      "Min loss: 0.003929550293833017\n",
      "Mean loss: 0.00525532787044843\n",
      "Std loss: 0.0007761851732700842\n",
      "Total Loss: 0.03153196722269058\n",
      "------------------------------------ epoch 9981 (59880 steps) ------------------------------------\n",
      "Max loss: 0.01295040175318718\n",
      "Min loss: 0.004718230105936527\n",
      "Mean loss: 0.007802862363557021\n",
      "Std loss: 0.003010935411466416\n",
      "Total Loss: 0.046817174181342125\n",
      "------------------------------------ epoch 9982 (59886 steps) ------------------------------------\n",
      "Max loss: 0.011030985973775387\n",
      "Min loss: 0.0027715133037418127\n",
      "Mean loss: 0.005307081194284062\n",
      "Std loss: 0.0026541829452820607\n",
      "Total Loss: 0.03184248716570437\n",
      "------------------------------------ epoch 9983 (59892 steps) ------------------------------------\n",
      "Max loss: 0.025966372340917587\n",
      "Min loss: 0.0028401557356119156\n",
      "Mean loss: 0.01186968475424995\n",
      "Std loss: 0.009153056134712389\n",
      "Total Loss: 0.0712181085254997\n",
      "------------------------------------ epoch 9984 (59898 steps) ------------------------------------\n",
      "Max loss: 0.013290631584823132\n",
      "Min loss: 0.004072530660778284\n",
      "Mean loss: 0.007388180820271373\n",
      "Std loss: 0.003174256828794318\n",
      "Total Loss: 0.04432908492162824\n",
      "------------------------------------ epoch 9985 (59904 steps) ------------------------------------\n",
      "Max loss: 0.01523353811353445\n",
      "Min loss: 0.004316187463700771\n",
      "Mean loss: 0.0074562861894567805\n",
      "Std loss: 0.0036663211634722047\n",
      "Total Loss: 0.044737717136740685\n",
      "------------------------------------ epoch 9986 (59910 steps) ------------------------------------\n",
      "Max loss: 0.01906861923635006\n",
      "Min loss: 0.003639624221250415\n",
      "Mean loss: 0.007778282083260517\n",
      "Std loss: 0.005174504617835836\n",
      "Total Loss: 0.0466696924995631\n",
      "------------------------------------ epoch 9987 (59916 steps) ------------------------------------\n",
      "Max loss: 0.018022902309894562\n",
      "Min loss: 0.004424469545483589\n",
      "Mean loss: 0.009294564680506786\n",
      "Std loss: 0.004807880219466676\n",
      "Total Loss: 0.055767388083040714\n",
      "------------------------------------ epoch 9988 (59922 steps) ------------------------------------\n",
      "Max loss: 0.008444862440228462\n",
      "Min loss: 0.004951019771397114\n",
      "Mean loss: 0.006219467691456278\n",
      "Std loss: 0.0013363996623253023\n",
      "Total Loss: 0.03731680614873767\n",
      "------------------------------------ epoch 9989 (59928 steps) ------------------------------------\n",
      "Max loss: 0.02625105530023575\n",
      "Min loss: 0.004309182520955801\n",
      "Mean loss: 0.011879614631955823\n",
      "Std loss: 0.007240305043119641\n",
      "Total Loss: 0.07127768779173493\n",
      "------------------------------------ epoch 9990 (59934 steps) ------------------------------------\n",
      "Max loss: 0.009069466963410378\n",
      "Min loss: 0.005921884439885616\n",
      "Mean loss: 0.007508203930531939\n",
      "Std loss: 0.001154266585979626\n",
      "Total Loss: 0.04504922358319163\n",
      "------------------------------------ epoch 9991 (59940 steps) ------------------------------------\n",
      "Max loss: 0.013733195140957832\n",
      "Min loss: 0.004556872881948948\n",
      "Mean loss: 0.008377615207185348\n",
      "Std loss: 0.003675184507664992\n",
      "Total Loss: 0.05026569124311209\n",
      "------------------------------------ epoch 9992 (59946 steps) ------------------------------------\n",
      "Max loss: 0.014561694115400314\n",
      "Min loss: 0.006272312253713608\n",
      "Mean loss: 0.009538102196529508\n",
      "Std loss: 0.003145789475086635\n",
      "Total Loss: 0.057228613179177046\n",
      "------------------------------------ epoch 9993 (59952 steps) ------------------------------------\n",
      "Max loss: 0.025986989960074425\n",
      "Min loss: 0.00373774953186512\n",
      "Mean loss: 0.0103699144286414\n",
      "Std loss: 0.007746731412724654\n",
      "Total Loss: 0.06221948657184839\n",
      "------------------------------------ epoch 9994 (59958 steps) ------------------------------------\n",
      "Max loss: 0.014499999582767487\n",
      "Min loss: 0.003374591004103422\n",
      "Mean loss: 0.00751892919652164\n",
      "Std loss: 0.0037064522614996954\n",
      "Total Loss: 0.04511357517912984\n",
      "------------------------------------ epoch 9995 (59964 steps) ------------------------------------\n",
      "Max loss: 0.008852208033204079\n",
      "Min loss: 0.0029584344010800123\n",
      "Mean loss: 0.005630271470484634\n",
      "Std loss: 0.0023668560740495977\n",
      "Total Loss: 0.033781628822907805\n",
      "------------------------------------ epoch 9996 (59970 steps) ------------------------------------\n",
      "Max loss: 0.013254819437861443\n",
      "Min loss: 0.003799639642238617\n",
      "Mean loss: 0.007641675726821025\n",
      "Std loss: 0.0032342128014518404\n",
      "Total Loss: 0.04585005436092615\n",
      "------------------------------------ epoch 9997 (59976 steps) ------------------------------------\n",
      "Max loss: 0.02398858591914177\n",
      "Min loss: 0.0037558830808848143\n",
      "Mean loss: 0.008733875079390904\n",
      "Std loss: 0.006963556963747263\n",
      "Total Loss: 0.05240325047634542\n",
      "------------------------------------ epoch 9998 (59982 steps) ------------------------------------\n",
      "Max loss: 0.015818782150745392\n",
      "Min loss: 0.004009084310382605\n",
      "Mean loss: 0.008646990405395627\n",
      "Std loss: 0.004270818104104769\n",
      "Total Loss: 0.05188194243237376\n",
      "------------------------------------ epoch 9999 (59988 steps) ------------------------------------\n",
      "Max loss: 0.021204551681876183\n",
      "Min loss: 0.005025965627282858\n",
      "Mean loss: 0.010227512257794539\n",
      "Std loss: 0.00612464240674306\n",
      "Total Loss: 0.061365073546767235\n",
      "------------------------------------ epoch 10000 (59994 steps) ------------------------------------\n",
      "Max loss: 0.017954951152205467\n",
      "Min loss: 0.005030492786318064\n",
      "Mean loss: 0.013193895497048894\n",
      "Std loss: 0.004416034970265286\n",
      "Total Loss: 0.07916337298229337\n",
      "saved model at ./weights/model_10000.pth\n",
      "Fin del entrenamiento\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGzCAYAAABzfl4TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKUUlEQVR4nO3dd3xT5eIG8CdJ23QPugsFyt4bKvPCjyVLUUT0oiJOEBTUC/dyXTgQxH1RcYuiTBVQRKCMguyyd9m0tpTZvZu8vz9KDglt06Q5WafP9/PJp+3Jycmbk9OcJ+86KiGEABEREZEM1M4uABERESkHgwURERHJhsGCiIiIZMNgQURERLJhsCAiIiLZMFgQERGRbBgsiIiISDYMFkRERCQbBgsiIiKSDYMFkYtKTEyESqVCYmKis4tCMnj00Ufh7+/v7GIQ2R2DBZERlUpl0c2Sk/3bb7+NlStX2r3MCxYsgEqlwt69e+3+XK7s0UcfrfL98vb2dnbxiGoND2cXgMiVLFy40OTvH374AQkJCRWWt2zZstptvf3227jvvvswcuRIOYtIZmi1Wnz99dcVlms0GieUhqh2YrAgMvLQQw+Z/L1r1y4kJCRUWE6uycPDg+8VkZOxKYTISvn5+XjxxRcRGxsLrVaL5s2b47333oPxhYJVKhXy8/Px/fffS9Xxjz76KADg4sWLeOaZZ9C8eXP4+PggNDQUo0ePxoULF+xa7gMHDmDIkCEIDAyEv78/+vfvj127dpmsU1paitdffx1NmzaFt7c3QkND0atXLyQkJEjrZGRkYPz48ahXrx60Wi2io6Nx9913my3/e++9B5VKhYsXL1a4b8aMGfDy8kJmZiYA4PTp0xg1ahSioqLg7e2NevXq4YEHHkB2drYs+8HQdLR161Y8/fTTCA0NRWBgIB555BGpDMY+++wztG7dGlqtFjExMZg0aRKysrIqrLd7924MHToUISEh8PPzQ7t27fDxxx9XWC8tLQ0jR46Ev78/wsPD8a9//Qs6nU6W10bkClhjQWQFIQTuuusubN68GY8//jg6dOiAdevWYdq0aUhLS8OHH34IoLxJ5YknnkC3bt3w1FNPAQAaN24MAEhKSsKOHTvwwAMPoF69erhw4QLmz5+Pvn374vjx4/D19ZW93MeOHUPv3r0RGBiI6dOnw9PTE1988QX69u2LLVu2ID4+HgAwc+ZMzJ49Wyp7Tk4O9u7di/3792PgwIEAgFGjRuHYsWN49tln0bBhQ1y5cgUJCQlISUlBw4YNK33++++/H9OnT8eyZcswbdo0k/uWLVuGQYMGISQkBCUlJRg8eDCKi4vx7LPPIioqCmlpaVi9ejWysrIQFBRU7Wu9du1ahWVeXl4IDAw0WTZ58mQEBwdj5syZSE5Oxvz583Hx4kWp06xhf7z++usYMGAAJk6cKK2XlJSE7du3w9PTEwCQkJCA4cOHIzo6GlOmTEFUVBROnDiB1atXY8qUKdJz6nQ6DB48GPHx8XjvvfewYcMGvP/++2jcuDEmTpxY7WsjcguCiKo0adIkYfxvsnLlSgFAvPXWWybr3XfffUKlUokzZ85Iy/z8/MS4ceMqbLOgoKDCsp07dwoA4ocffpCWbd68WQAQmzdvNlvG7777TgAQSUlJVa4zcuRI4eXlJc6ePSstS09PFwEBAaJPnz7Ssvbt24thw4ZVuZ3MzEwBQLz77rtmy1SZ7t27i86dO5ss27Nnj8nrPnDggAAgli9fbvX2x40bJwBUehs8eLC0nmF/de7cWZSUlEjL586dKwCIVatWCSGEuHLlivDy8hKDBg0SOp1OWu+TTz4RAMS3334rhBCirKxMxMXFiQYNGojMzEyTMun1+grle+ONN0zW6dixY4X9QuTO2BRCZIU1a9ZAo9HgueeeM1n+4osvQgiBP//8s9pt+Pj4SL+Xlpbi+vXraNKkCYKDg7F//37Zy6zT6bB+/XqMHDkSjRo1kpZHR0fjn//8J7Zt24acnBwAQHBwMI4dO4bTp09XWXYvLy8kJiZW2mxgzpgxY7Bv3z6cPXtWWrZ06VJotVrcfffdACDVSKxbtw4FBQVWbR8AvL29kZCQUOE2Z86cCus+9dRTUo0DAEycOBEeHh5Ys2YNAGDDhg0oKSnB1KlToVbf+qh88sknERgYiD/++ANAeRPT+fPnMXXqVAQHB5s8h6Hmw9iECRNM/u7duzfOnTtn9WslclUMFkRWuHjxImJiYhAQEGCy3DBKpLI+BLcrLCzEq6++KvXRCAsLQ3h4OLKysmTrR2Ds6tWrKCgoQPPmzSvc17JlS+j1eqSmpgIA3njjDWRlZaFZs2Zo27Ytpk2bhsOHD0vra7VavPPOO/jzzz8RGRmJPn36YO7cucjIyKi2HKNHj4ZarcbSpUsBlDcrLV++XOr3AQBxcXF44YUX8PXXXyMsLAyDBw/Gp59+avF+0Wg0GDBgQIVbhw4dKqzbtGlTk7/9/f0RHR0t9RUxvJe37zcvLy80atRIut8QlNq0aVNt+by9vREeHm6yLCQkxOqQRuTKGCyIHOzZZ5/FrFmzcP/992PZsmVYv349EhISEBoaCr1e79Sy9enTB2fPnsW3336LNm3a4Ouvv0anTp1MhnBOnToVp06dwuzZs+Ht7Y1XXnkFLVu2xIEDB8xuOyYmBr1798ayZcsAlI+4SUlJwZgxY0zWe//993H48GH897//RWFhIZ577jm0bt0af//9t/wv2ME47JVqAwYLIis0aNAA6enpyM3NNVl+8uRJ6X6DyqrBAeDnn3/GuHHj8P777+O+++7DwIED0atXr0pHGsghPDwcvr6+SE5OrnDfyZMnoVarERsbKy2rU6cOxo8fj8WLFyM1NRXt2rXDzJkzTR7XuHFjvPjii1i/fj2OHj2KkpISvP/++9WWZcyYMTh06BCSk5OxdOlS+Pr6YsSIERXWa9u2LV5++WVs3boVf/31F9LS0vD5559b/+LNuL25Jy8vD5cuXZI6oBrey9v3W0lJCc6fPy/db+iUe/ToUVnLR+SuGCyIrDB06FDodDp88sknJss//PBDqFQqDBkyRFrm5+dXaVjQaDQmQ1MBYN68eXYbcqjRaDBo0CCsWrXKZEjo5cuXsWjRIvTq1Utqirh+/brJY/39/dGkSRMUFxcDAAoKClBUVGSyTuPGjREQECCtY86oUaOg0WiwePFiLF++HMOHD4efn590f05ODsrKykwe07ZtW6jVaou2b40vv/wSpaWl0t/z589HWVmZ9B4OGDAAXl5e+N///mfyfn3zzTfIzs7GsGHDAACdOnVCXFwcPvroowrv9+3vM1FtwOGmRFYYMWIE+vXrh5deegkXLlxA+/btsX79eqxatQpTp06Vvr0CQOfOnbFhwwZ88MEHiImJQVxcHOLj4zF8+HAsXLgQQUFBaNWqFXbu3IkNGzYgNDTUprJ9++23WLt2bYXlU6ZMwVtvvYWEhAT06tULzzzzDDw8PPDFF1+guLgYc+fOldZt1aoV+vbti86dO6NOnTrYu3cvfv75Z0yePBkAcOrUKfTv3x/3338/WrVqBQ8PD6xYsQKXL1/GAw88UG0ZIyIi0K9fP3zwwQfIzc2t0AyyadMmTJ48GaNHj0azZs1QVlaGhQsXQqPRYNSoUdVuv6ysDD/++GOl991zzz0mIaakpER6LcnJyfjss8/Qq1cv3HXXXQDKa3pmzJiB119/HXfeeSfuuusuab2uXbtKE3Gp1WrMnz8fI0aMQIcOHTB+/HhER0fj5MmTOHbsGNatW1dtuYkUxaljUohc3O3DTYUQIjc3Vzz//PMiJiZGeHp6iqZNm4p3333XZGihEEKcPHlS9OnTR/j4+AgA0tDTzMxMMX78eBEWFib8/f3F4MGDxcmTJ0WDBg1MhqdaO9y0qltqaqoQQoj9+/eLwYMHC39/f+Hr6yv69esnduzYYbKtt956S3Tr1k0EBwcLHx8f0aJFCzFr1ixpWOa1a9fEpEmTRIsWLYSfn58ICgoS8fHxYtmyZRbv06+++koAEAEBAaKwsNDkvnPnzonHHntMNG7cWHh7e4s6deqIfv36iQ0bNlS7XXPDTQGI8+fPm+yvLVu2iKeeekqEhIQIf39/MXbsWHH9+vUK2/3kk09EixYthKenp4iMjBQTJ06sMKxUCCG2bdsmBg4cKAICAoSfn59o166dmDdvnkn5/Pz8Kjzutddeq3CMEbkzlRCsqyOi2mPBggUYP348kpKS0KVLF2cXh0hx2MeCiIiIZMNgQURERLJhsCAiIiLZsI8FERERyYY1FkRERCQbBgsiIiKSjcMnyNLr9UhPT0dAQECVUx4TERGRaxFCIDc3FzExMSZX/L2dw4NFenq6yXUJiIiIyH2kpqaiXr16Vd7v8GBhuNx0amqqdH0CIiIicm05OTmIjY2VzuNVcXiwMDR/BAYGMlgQERG5meq6MbDzJhEREcmGwYKIiIhkw2BBREREsmGwICIiItkwWBAREZFsGCyIiIhINgwWREREJBsGCyIiIpINgwURERHJhsGCiIiIZMNgQURERLJhsCAiIiLZMFhUYcfZa1iWlOrsYhAREbkVh1/d1F3886vdAICW0YFoWy/IyaUhIiJyD6yxqEZaVoGzi0BEROQ2GCyqoRfOLgEREZH7YLCohl4wWRAREVmKwaIazBVERESWY7CoBmssiIiILMdgUQ3mCiIiIssxWFRDgMmCiIjIUgwW1dDrnV0CIiIi98FgUQ32sSAiIrIcg0U1mCuIiIgsx2BRDfaxICIishyDRTU48yYREZHlGCyqwaYQIiIiyzFYVEPHZEFERGQxBotq6NkWQkREZDGrg0VaWhoeeughhIaGwsfHB23btsXevXvtUTaXUKrjRBZERESW8rBm5czMTPTs2RP9+vXDn3/+ifDwcJw+fRohISH2Kp9FhBCYvOgA/s4swFePdEFEoLds29axxoKIiMhiVgWLd955B7Gxsfjuu++kZXFxcbIXyloqlQp/HLkEADhzNU/WYFFcxhoLIiIiS1nVFPLbb7+hS5cuGD16NCIiItCxY0d89dVXZh9TXFyMnJwck5s9/bQrRdbtfZBwStbtERERKZlVweLcuXOYP38+mjZtinXr1mHixIl47rnn8P3331f5mNmzZyMoKEi6xcbG2lzoyrSrFwQAqFfHxy7bJyIioupZFSz0ej06deqEt99+Gx07dsRTTz2FJ598Ep9//nmVj5kxYways7OlW2pqqs2Frkz3xqEAgDId+0QQERE5i1XBIjo6Gq1atTJZ1rJlS6SkVN38oNVqERgYaHKzB41KBYCdLYmIiJzJqmDRs2dPJCcnmyw7deoUGjRoIGuhasJDXR4s5L4a6c28QkRERBawKlg8//zz2LVrF95++22cOXMGixYtwpdffolJkybZq3wWU98MFmUy11gwVxAREVnOqmDRtWtXrFixAosXL0abNm3w5ptv4qOPPsLYsWPtVT6LGZpC5J4pky0rRERElrNqHgsAGD58OIYPH26PsthEo2EfCyIiImdTzLVCpM6bvGgYERGR0ygnWKhZY0FERORsDBZEREQkG8UFC7mHmxIREZHlFBMs1Jwgi4iIyOkUFyxYYUFEROQ8CgoW5T9ZYUFEROQ8CgoWhhoLJgsiIiJnUUywgFRjwWBBRETkLIoJFlKNhZPLQUREVJspKFiU/2QfCyIiIudRTLAwXN6cfSyIiIicRzHBgsNNiYiInE8xwUKl4sybREREzqacYHHzJ4MFERGR8ygmWLAphIiIyPkUFCzKfzJYEBEROY9iggX7WBARETmfgoJF+U8GCyIiIudRTLDgzJtERETOp6BgUf6TM28SERE5j2KCBWfeJCIicj4FBQsONyUiInI2xQQLNUeFEBEROZ1igsWtmTedWgwiIqJaTTHB4tbMm0wWREREzqKgYFH+k7mCiIjIeRQTLDjzJhERkfMpKFiU/2SwICIich7FBAvOvElEROR8CgoW5T9ZYUFEROQ8igkWbAohIiJyPgUFC868SURE5GyKCRb2nHmTc2MQERFZRjHBwjDzJjMAERGR8ygmWNhz5k2GFSIiIssoJljc6rwp/7aZK4iIiCyjmGDBPhZERETOp5hgYc8aCyIiIrKMYoKFocbCHg0XzCpERESWUVCwKP9plz4WTBZEREQWUUywsOfMm4J1FkRERBaxKljMnDkTKpXK5NaiRQt7lc0qnHmTiIjI+TysfUDr1q2xYcOGWxvwsHoTdmHfUSGyb5KIiEiRrE4FHh4eiIqKskdZbMKZN4mIiJzP6j4Wp0+fRkxMDBo1aoSxY8ciJSXF7PrFxcXIyckxudkDZ94kIiJyPquCRXx8PBYsWIC1a9di/vz5OH/+PHr37o3c3NwqHzN79mwEBQVJt9jYWJsLXRm557GQRq8SERGRxawKFkOGDMHo0aPRrl07DB48GGvWrEFWVhaWLVtW5WNmzJiB7Oxs6ZaammpzoSujVtuxjwVHhRAREVnEpp6XwcHBaNasGc6cOVPlOlqtFlqt1pansYjcfSxUuDUxFptCiIiILGPTPBZ5eXk4e/YsoqOj5SpPjUl9LGSqXVCxLYSIiMhqVgWLf/3rX9iyZQsuXLiAHTt24J577oFGo8GDDz5or/JZzK4zb8q/SSIiIkWyqink77//xoMPPojr168jPDwcvXr1wq5duxAeHm6v8lnOnjNvsi2EiIjIIlYFiyVLltirHDZTyzzzpnFDCGMFLd6TgvmJZ/H9Y90QF+bn7OIQEbksxVwrRG3UJ0KOGgZ2sSBjM349gpQbBXhpxRFnF4WIyKUpJlgY5wA5+lmoYBxUbN8eKUOpTu/sIhARuTTFBAu5ayxMMFgQERFZRDHBQmX0SmQZGWJUBcIJssiAtVdEROYpJlgY11jIMTKEXSyIiIisp5hgYTKKQ44+FsY1FvyWSkREZBHFBAuTPhYyN10wV5ABjwUiIvMUEyyMaxjkHxXC0wkREZElFBosOI8FERGRMygmWJgON7V9e5x5kyrD2isiIvMUGixs//A33gLPJURERJZRTLCQe+ZNIiIisp5ygoXMfSyMcYIsMlCx8w0RkVkKChYqKVzIful05gq6iX0siIjMU0ywAACNzJdON+CphIiIyDKKChaGDpyy11gQERGRRRQVLG41hci7XeYUMuChQERknqKChVRjIXOyYOdNIiIiyygqWGjU9mkKYY0FERGRZRQVLOzVFEJkwJBJRGSeooKFvTpv8lxCRERkGYUFi/Kfsvex4NdUIiIiiygqWNzqY2H7toyzBHMFGXDiTSIi8xQVLFScx4LsjIcWEZF5igoWantN6U1EREQWUViwMMxjIe92mVOIiIgso8xgwaubkp3wSCAiMk9ZweLmq2FTCBERkXMoK1jYq8aCOYWIiMgiCg0W8m6XuYIkTJlERGYpLFiU/5R7giwiIiKyjMKChZ1qLPgtlYiIyCIKDRa8VggREZEzKCtY8LLpRERETqWsYHGzj4WOfSyIiIicQlHBwnARMjlqGEwnxWJQISIisoSigoXhImRy11iwKYQMeCgQEZmnqGChsdNFyHgyISIisoyigoW9RoUQERGRZZQVLNT2msdC3u2R++KxQERknrKChZ1GhfDqpkRERJZRVLDQcB4LIiIip7IpWMyZMwcqlQpTp06VqTi2YR8LsjfWXhERmVfjYJGUlIQvvvgC7dq1k7M8NpGChV7e7TKnEBERWaZGwSIvLw9jx47FV199hZCQELnLVGNSHwvZh5syWVA5FVTOLgIRkUurUbCYNGkShg0bhgEDBlS7bnFxMXJyckxu9nJr5k0GAbIPhkwiIvM8rH3AkiVLsH//fiQlJVm0/uzZs/H6669bXbCauDXzprzbZU4hIiKyjFU1FqmpqZgyZQp++ukneHt7W/SYGTNmIDs7W7qlpqbWqKCW0LDzJhERkVNZVWOxb98+XLlyBZ06dZKW6XQ6bN26FZ988gmKi4uh0WhMHqPVaqHVauUpbTXUN2OSHMHCeBPMKURERJaxKlj0798fR44cMVk2fvx4tGjRAv/+978rhApHU9vpImREBgyZRETmWRUsAgIC0KZNG5Nlfn5+CA0NrbDcGW7NYyHvdtlhj4iIyDLKnHmTl00nIiJyCqtHhdwuMTFRhmLIQ8XLppOdMWQSEZmnrBoLQx8LfvoTERE5haKChaGPhdy5ghNuERERWUZZwUJtn1EhjBVERESWUVawsFcfCyYLIiIiiygqWNhrVAgRERFZRlHBwl7zWLAxhAx4JBARmafIYCH7ZdN5NiEiIrKIwoJF+U9ehIyIiMg5FBUs5OxjIar4nYiIiKqmqGChste1Qpgs6CbOaUJEZJ6igoXm5quRfR4LnkyIiIgsoqhgcWvmTQYBIiIiZ1BksJB9VIisWyMiIlIuRQYL9rEgIiJyDkUFC0MfC7ln3hSssyAiIrKIooKFYVSI3J03iYiIyDKKChbSPBZy5wrmFCIiIosoKljY7eqmsm6NiIhIuRQWLAw1FrxWCNkHjwUiIvMUGSzYx4KIiMg5FBUsDH0s5P5WyVEhREREllFUsDDMuHn2ap4MGzPeru2bI2VgyCQiMk9RweLXA2kAgJMZubJu15GnkqNp2UiWufxERESO4uHsAsjp8N/Zzi6CTXKKSjF83jYAwNm3h0pNO0RERO5CUTUW9uKoi5pdzyuRfi/T6x3ynGQdFRj2iIjMUVSwCPC2TwWMM1rV2a/DNbGPBRGReYoKFq/f1drZRbAJvwsTEZG7U1SwiK3jCwCIC/OTd8MO+pKqYrIgIiI3p6hgYa8JshxV/W3cfs+mENfE94WIyDyFBYvyn0qY0ptt+URE5I4UFSykq5u66ZTebAohIiJ3p6hgcesiZPJu1yk1Fu6ZjRSPbwsRkXmKDBY6BVw2nScwIiJyR4oKFnI2hRj3cXDUBFlsCiEiInenqGBhr86bjqJSGY8Kcc/XQEREtZuygoXaXsNNHYMVFkRE5O4UFSw0N7/xy/1ln5UHREREllFUsLBX501ndKVklnFNbKIiIjJPWcHi5quRuynEUdh5k4iI3J2ygoWCmkL4xZiIiNyRooKFYbipu85jYXytELaFEBGRO1JUsLg186Z7npXZFOL63PPIIiJyHKuCxfz589GuXTsEBgYiMDAQ3bt3x59//mmvslnNMI+FEPJ2suNFyIiIiCxjVbCoV68e5syZg3379mHv3r34v//7P9x99904duyYvcpXY+nZRbJty3GXTSciInJvHtasPGLECJO/Z82ahfnz52PXrl1o3bq1rAWrCePRIPnFZbJtl503ScL3hYjILKuChTGdTofly5cjPz8f3bt3r3K94uJiFBcXS3/n5OTU9CmrFeLnJf3u46mx2/PYDassiIjIzVndefPIkSPw9/eHVqvFhAkTsGLFCrRq1arK9WfPno2goCDpFhsba1OBzfHU3Ho5tnbgNH44v6SShOGPiMgsq4NF8+bNcfDgQezevRsTJ07EuHHjcPz48SrXnzFjBrKzs6VbamqqTQWuTqB3eSVMmYyTZDns6qY8a7k+pkwiIrOsbgrx8vJCkyZNAACdO3dGUlISPv74Y3zxxReVrq/VaqHVam0rpRUMtRZlOvc+A7h36YmIqLayeR4LvV5v0ofC2Tw05d/6S3V6J5eEiIio9rGqxmLGjBkYMmQI6tevj9zcXCxatAiJiYlYt26dvcpnNY+bFwyRtylEtk1Z8Zyss3BFfFeIiMyzKlhcuXIFjzzyCC5duoSgoCC0a9cO69atw8CBA+1VPqsZaix0evlqLBw2jwW7WBARkZuzKlh888039iqHbDzUhqYQN6+xcPxTEhER2UxR1woBgKLS8pqKkjL362PBCgsiInJ3igsWaVmFAICFuy7Ktk3OvEkG7PtCRGSe4oKFwYGUTNm2xVMJERGRZRQXLEJvTut9fxf7zfDpCLy6KRERuSPFBYuBrSIBAL5e8l0rhNXfREREllFcsPDyKH9JcnbeZKwgIiKyjPKCxc0pvYttnHlTVPkHERERVUV5wcIONRaOwjDj+vi2EBGZp7hgobs5lffB1CzZtsmOlERERJZRXLD4Zf/fAIADKVmybZMzbxIREVlGccFibHwD2bfJkzwZcIAQEZF5igsWuUVlzi6CLHgCIyIid6S4YLHz3HXZt8mTPBERkWUUFywMVzeVkzM6b7LDqGvi+0JEZJ7igkWDUF/p97NX82TZJmssiIiILKO4YDG+Z5z0e1ZBiRNLYhuGGdek4sXtiYjMUlyw8NTc+uBXqeQ5CfAcT0REZBnFBQu9UQq4cC1fno06ofqAYcY1sY8FEZF5igsWZUbXCHlvXbITS0JERFT7KC5YFBtdI8SWphDjS6U74zsqL9VORETuSHHBIj6ujvS7XqaTs6PO8cwSro/vERGReYoLFh6aWy+pqFQnyzadUXvAExgREbkjxQULY5kFpc4uAhERUa2i6GAByNOBk5UHZMCaJCIi8xQZLJpF+ku/f7L5jM3b48mEqCKdXiAz330noSMi+1BksDh1WZ6pvA2cMyrECU9KZIWHv9mNjm8m4Hh6jrOLQkQuRJHBIsxf6+wiECnejrPlVxJekpTi5JIQkStRZLD4aEwHWbfnlFEh7NlBboK1a0RkTJHBol6Ij7OLQFRryDVfDBEpgyKDRVSQt6zbc8bnJj+ryV3wUCUiY4oMFt6eGmcXgajW4PTzRGRMkcHidjq99R98wuR3Xt2UqCp6ffXrEFHtodhg8c6ottLvV3KLbNqWw64VwjhBbojHLREZU2ywMG4OWXkg3aZt8eqmZMD3paIaVAgSkYIpNlj0aRou/a5jXS2R3TBrEZExxQaLED8v6Xd/rYdN23LKqBDHPyVRjbAphIiMKTZYGNPZ+LnHD04iM/jvQWbkF5fhYGoWmxFrkVoRLH7addGmx3MeC6KqcYIsMue+z3di5Kfb8ev+NGcXhRykVgSLO9tE2fR4vVN6p/HDmtwDj1Qy58Sl8ovU/bL/byeXhBxF0cGie6NQAEDDMD+btsNe72TAQ6Ei/n8QkTFFBwt/7/JOm2U2drLQOeMiZPywJjfBphAiMqboYOGpUQEAymwcbspOR0RERJaxKljMnj0bXbt2RUBAACIiIjBy5EgkJyfbq2w281CXv7xSW2ssnFDXyyjjmpgxiWqG/zu1h1XBYsuWLZg0aRJ27dqFhIQElJaWYtCgQcjPz7dX+WziYaix0NlWY8E2ZCIiIstYNXPU2rVrTf5esGABIiIisG/fPvTp00fWgsnB82aNRVlNLkImjH9nHwuiKvFYJSIjNk1JmZ2dDQCoU6dOlesUFxejuLhY+jsnJ8eWp7SKocaipMy2GguHNYXwA5rcECeQI0uoVM4uATlKjTtv6vV6TJ06FT179kSbNm2qXG/27NkICgqSbrGxsTV9Sqt5agw1Fu7XFMIPa9fE94WIyLwaB4tJkybh6NGjWLJkidn1ZsyYgezsbOmWmppa06e0mofa0MfCtpMBh9MRVY3/HkRkrEZNIZMnT8bq1auxdetW1KtXz+y6Wq0WWq22RoWzlYdGnlEhzggW/LB2TSqwPpeoJviZVntYFSyEEHj22WexYsUKJCYmIi4uzl7lkoWXTPNYsMaCDNgUUhH/PYjImFXBYtKkSVi0aBFWrVqFgIAAZGRkAACCgoLg4+NjlwLaQq4aCxtHq9YIP6yJiMgdWdXHYv78+cjOzkbfvn0RHR0t3ZYuXWqv8tlErnksOPMmERGRZaxuCnEnntLMm+7XFMIqd9fkZv8CREQOp+hrhRhqLEptHC/qjKYQInfBEExExhQeLG7OY+GGTSH8ZkzugscqERlTdLDwlGkeC2dcNp1cE48EIiLzFB0spFEhNjaF8CJkRFXjvwcRGVN0sPCU7eqmjvno5Ac0ERG5O0UHCw/D1U1tnXnTCVUWbH0hIiU5mp7t7CKQgyg7WBiubuomNRZEREqVW1Tm7CKQgyg6WBiaQmy97LlTZt5kwwi5CeZuIjKm6GDhIdMEWe42MRiRY/H/g4huUXawkKnGglc3JQO+L0RE5ik7WBg6b9raFMKTCVGVGLaIyJiyg4VhSm83bArhZzUREbkjRQcLw0XI3LEphFwVj4XbcY8QkTFFBwuN2lBjYeuoEGf0seDHNRERuR9FB4tbw01tncdCjtIQEREpn6KDxa2rm7rhzJsOf0ayBCuSiIjMU3awMDSF2Fxj4aBrhfCkRW6IzXZEZEzZwaKG81jc/kHpjKYQfla7JpXK2SVwPTxUiciYsoOFNPOmsOlbFUeFkAEPBSIi8xQeLG59vbRlZIdzggXPYOQeGLaIyJiyg4XmVrCwZfZNZ1yEjIiIyB0pOlh4am69PFuChVNm3uS3QHITPFSJyJiig4XGuCnEhiGn7GNBBjwSiIjMU3SwMO5jYcuQU6fMvOnwZyQiIrKdooOFSqWSwoUtk2SxwoKIiMgyig4WwK3mkDJbaizYx4JuYrNYRZwgi4iMKT5YeHtqAACFJboab4MnEzLgoUBEZJ7ig4XWo/wlltgwZtTGGcFrhN8CXRPfFyIi8xQfLNQ352C2JRw4o8aCpy/XxPelImYtIjKm+GBh6GNhSzhw2EXIjE5bbH5xTXxbiIjMU3ywMFw0ypoT9e2r8iJkZMDAR0RknuKDhSw1Fs6Yx4LnL5fE94WIyDzFBwupj4UNJwRbOn7WFL8Zuya+LxUJ9jwhIiOKDxaGXvy21DoUlzFYUDm+LRVxnxCRMcUHiwvXCwAAG09eqfE2nHIRMoc/I1mCgY+IyDzFBwuDL7eeq/FjndN5kycwV8R3pSIeqkRkrNYEC1s44yJkzpiUi6rHGgsiIvMYLCzk6BoEnr5cE3NFRey8SUTGGCws5OhKC34zJiIid8RgYSFHN4cwVxARkTtisLCQo2sQ2HmTiIjcUa0JFgHeHjY93hHneePncMZIFKKaYAYmImNWB4utW7dixIgRiImJgUqlwsqVK+1QLPl0bxQKAHiwW32LH1PZ56TDayzYIY7cBI9UIjJmdbDIz89H+/bt8emnn9qjPLJrUzcQAKCycTs6BwcL1lgQEZE7srp9YMiQIRgyZIg9ymIXGnV5dirV2XamFg6eV4J9LMht8FAlIiO2dTywQHFxMYqLi6W/c3Jy7P2UJjw15XUVOhtnnHJ8502HPh0REZEs7N55c/bs2QgKCpJusbGx9n5KE4bLppfZ2LZw6nKuHMWxGOexICIid2T3YDFjxgxkZ2dLt9TUVHs/pQkPtaHGwrYT9blr+XIUx2LsY0FERO7I7k0hWq0WWq3W3k9TJcMX/8N/Z9u0nYahfjKUxnLsY0HugiOYiMiY4uexWLQnBQBw/FLN+naobg4n8fHSyFUkizBXEBGRO7K6xiIvLw9nzpyR/j5//jwOHjyIOnXqoH59y+eKcJRL2UU2PT5A64GcojKHT+nNPhbkLnioEpExq4PF3r170a9fP+nvF154AQAwbtw4LFiwQLaCuQpD50/HT5BF5B54rBKRMauDRd++fWtV+7/6ZluInjUWRERE1VJ8HwtbqQ2jSjjzJlGlatMXDSKqnuKDxcBWkTY9/mpu+eRe2QWlchTHLJOPZ35YExGRG1J8sKgb7GP1Yyr7Bvbqb8fkKI7FWGNBRETuSPHBoqCkTJbtGGou7Mk40LB6mYiI3JHig4WHxvqX6AqndNZYkLvgoUpExhQfLDQqWy+Y7jjGlRQcFULugocqERlTfLCoX8fX2UUgIhd1I7+EzY5EMlN8sLi7Q4z0e26RZSM7XOFzhjUW5C7c9UhddTANnd5MwOw/Tzq7KESKovhg4WnUx+LZxQdQWKJzYmksxz4WRPb15urjAIAvt55zckmIlEXxwcJDc6uPRWLyVTy2IKnax7jC1RpZYUFuw00PVpUb9b8icifKDxZq05e489z1Gm/L3m2x7LxJ5DiMFUT2ofhgYbiImDWqOqe//vtxG0tjTRkYLIjsSc0aCyK7UHyw8KhBsKjKgh0XZNtWZYybYJgrXAvPQcrD95TIPhQfLNSVBIuNJy5b/PggH085i2Mxdt50Lfx2WzV3PVT5jhLZh+KDRWUe/34vLmUXWrTumyPb2Lk0lVNCH4u5a0/i3s+2o6jUPUbimCNjxZfiuOuhys6bRPZRK4MFYP7aH8YflE3C/R1QmorPa8/P6tyiUpy6nGvHZyj3WeJZ7E/Jwm+H0u3+XPam4vdbxWGuILKPWhss0rMKobegvaF+aMWZOx3RsdKSstVU33cTMejDrdh74YbdnsNYSZneIc9jTzwJVc0VhmfXBN9TIvuotcFiwo/7MePXI5WGBOMPyturwO//Yie6ztqAjOwi2ctkXJJSvf1OxtfzSwAACcct72tiC/c87ZhiH4uquW1TCGuhiOyiVgSLbg3rVLp86d5UxM1Yg4b/+QPpWZb1udhz/gau5ZXg081n5CxiBaVlbvppXQklDJ1lrqiau3Y0Zr8Zx/LyqBWnG0ItCRZD2kZVu06POZtwICUTh1Kz8MLSQw4oVUXGJ+AyO9ZYSM9n92coZ89mHUdxp6vkOpq7Bkd23nQsH0+Ns4tADuLh7AI4wqM9Glo0udU9n+2osEwFFbw81BX6CQgILNx1EZEBWgxqXX1wsVapzv37JRi452nHlEbDk1BV3HUEE99Rx3LXAErWqxU1FiqVCk0jaj66o7K5LH7clYJXVh7FUwv3IfNmn4XE5CtIvVFQ4+cx/rdbf8wx/R8cQQmfJ6yxqJq7vr98S4nso1YECwD49tGuNXqcSgW8e187s+t0fDMBTf67Bo9+l4TeczfX6HluZ+hgaU+O+gbhrt9ojdVkavjawl3fXzaFOJabHiZUA7UmWMTWqThs1FKNLZjLoqwG/QiEEE4diumof3QlfKDIOTW80rjr28t31LHcNYCS9WpNsHCk6T8fQm5RKT5MOIWVB9Kk5WW39ZsYvyAJ7V5fh+yCUgBAYYnpDJVLk1LsX1gHUMIHinEfCyV0RpWVm+4OVlg4Fv9tao9a0XnToEVUAE5mWD/jZHiA1qr1l+39G9mFpVh3s59EeIAWuUVlmPDjPnSqH4xfJvbAz/v+RmLyVQDAumMZiG9UB8PnbTPZzr9/OYL7OsfarRreUf/oNanNcTUe6lsZ/MzVPDSLDHBiaVyLuwZHzk3iWO56nJD1alWNxfePdavR47xrMExqnVHny7Ff78aEH/cBAPanZCFuxhpM+/mwdP+x9Gz8493ESrdz72fbKyzLLSrFsqRUZBXY1g/DEUNaAUCngGBhnO3OXMlzXkFckPu/u+QIPE5qj1oVLCIDvZH4r75WPcbwpSbM30v+At30/c6LVd536O9sfP3XOeQXl+G/K45g8Z4UTP/5MKb/chhP/rDXpud11JBWpdVYKCEoycldv4my86ZjsQmx9qhVTSEA0DDMD6dnDcF328/j7TUnLX5cvRBfXMuz/0iNyrz1xwm89ceJCsuTLmTi67/O4VpeCWKCvSEEMK5HQwDAwdQsfLzhFP47tCWaVlFtv3hPKl4Z3gq+XvY9DHQOqhmxJ+PmKHc9kdqLu7697I/rWPy/qT1qXbAAAE+NGk/1aWxRsPC8+U31fw90RP8PElGqc61/jtsDR3SQN9KzCjHz5oRgm5Ov4qWhLfFgfH34ayu+3a1eXYfzs4ea/fY2edF+rD58CYdnDkKg9605Pb7fcQEnM3Lw1si2ZvuBKKHGwvj1lbnYMeBs7jrxESssHMs9jxKqiVrVFHK75/6vCQDgH83Cq1xHffOEUj/UF6dnDcXnD3V2SNlq6qmF+6RQYTBrzQlMXrS/ysfEzVhjtr/G6sOXAAB3frjVZPlrvx3D4j2pWH8sw2yZCop1Zu93B8YnIX7zMlXipkGLFyFzLP7b1B61O1j0b4rFT96Bzx/qjHs71rXoMXe2icLg1pF2Lpn8EpOvotWra6u8f/BHW/HeumQcTcvG0wv3IjH5SoV10qu4omvmzeGyVVm46yLeX59sXYFdWPMojggxdi2v2NlFqBFXqbHYeuoqki7ccHYxiGRTq4OFh0aN7o1D4eOlQY8mYRY/7ouHu6Bf86prOVxVQYkO8xPPVnrf5ZxifLL5DIbP24Z1xy7j0e+SKp2e/PsdF3A5pwh/Z966T2fBV5F5myy/GmxRqQ5rj2Ygr7jM4sc40oIdF3A0LdvZxSAbuULnzet5xXjk2z0Y/flOt21SIrpdrQ4Wxu7tWBcfjemALdP6YsML/wBQ+TVCDO7vEiv9/vEDHexdPNm8s9byDqu9527G6sPpJste++0Y4t/eiF7v3Jq63Li398oDafh085lKQ0lRqWmTiOFxl3OKTEaovP77MUz4cZ/Z5htjJWV6HE3LtrrXeZlOj/SsQpNlloz4+HV/GobP24ZzVzns1J05P1YAN4ym7ldAVyQiALW082Zl1GoVRho1hyQ83wdh/lVPjHVnmyh8cH97tK0bhKaRAfgw4RQuXK/5Bchu9/UjXfCEjcNJ5TB50YFq13ntt2MI8vHEtJ8PSZ1bF1YyhPaJ7/di25lrePPu1hjRPgYd3kiQ7utYPxgrnumJ4jIdFu9JBQBpArHbCSHw6qpjaBbpj4e7N8RTC/ciMfkqXh7WEk/0bgSgfK4PvQC8NGr4eFU+D8kTP5Q/btET8ejRJAw7z17HE98n4bURrXF/11iTdXOLKtaeHEnLRiMLpnt3FUkXbkAFoEvDOtKyi9fzkZFdhPhGoc4rmJO4QIWFSa1JqU4PjVrZlxbX6UWlHb3PX8tHvRAfeGr4XVcJGCyqUNUQTQOVSoV7O9WT/l781B1YfegSGoT6Ii7MD4E3azvm/HkSK4ym9bbES0NbYkAr9+rHMXXpQZO/M3Iq9sfYduYaAOCVVcfwyqpjJvcdSMnCG78fx7fbz5ss7/deIhY+3g2/7k/DjfwSzLyrNVYcSMPCXeXBZUzX+lIAmZ94Fo90b4gPEk7h8y3lTT6eGhVOzxqKrIIS3Dt/B8p0AvXr+OLTf3aSHvft9vPo0SQMjy1IQmGpDtN/OSwFiys5RXhx+SGcv5Zf4fWsOpiOuzuY75uTmV+CIB9PqRPw7fR6UeV9t/vfxtMI8fPCw3c0sGj9vOIyFJXqEOavRWGJDqM/3wkAOPnmndKkb4aJ2dY/36fK2USLy3TQepg/4e08ex1dG4bAw0VODEII6IX5i8e5QK4wGfK6/cw19G/pXv/3lvBQq6SRYfklZSYjywDgzyOXMPGn/ejZJBQ/PXGHM4pIMmOwkEl0kA+e7NOowvIPx3TAh2M6IK+4DBeu5UOnF2geFYBtp6/ht0PpaBLhj3ohPrinY10UlOhwNC1b+kY578GOeHnlUQT5eCLFhsuxu4vbQwVQ/k3GuNllRPsYzF17qyNos5f/lH6/nl9i8jcAlOoEhBCYn3gW566Wh4OUGwX4cMMpaZ0NJ67gt0PpKDRqqinT6eGhUeON1cfx1+lrlZZ308kruPuTbXimXxN0jA3G74cv4b5O9RDkW/7BeeTvbIz4ZBv6NQ/Hd+PLZ30VQkClUkEIgUe+3YML1/Oxbmof+Hp54MutZ7Hl1FV8M65rhdlez1/LxwcJ5WW2NFi0eW0dAODQa4NMLnZXWKKrsP3j6TmVBosxX+zE7vM38FjPOLw6opW0fMavR0zWe/CrXXh+QDNMGdDUorIBwI4z1xAd7IO4MD9pWVVBq7BEV2nN00cbTsFTo8bEfzQ2edyEH/fhaFoO1j/fB36VDLMG5J3SO6+4DHohKpw0q+PlofyJ1zw1apTpy/+31h7NMGlGBoDvd14AAGw/c93RRbNJYYkO769PxuA2UehqVAtIgEo4uMdQTk4OgoKCkJ2djcDAQEc+tVsynIgMGv7nDyeWhqrTr3k4xvVoiKlLDyLLaLTMmVlD8PTCfdh48goWPt4Nm05ewXfbLwAAmkT445eJPdD+9fUAypuF3hnVDjq9wKurjiLpQqbJc7w3uj38tRqoVSr0aBIm1UoA5U0bRaV6NI8KkI6VX5/pgdgQX3SdtQEAMOfetnigW30At46nj8Z0wMiOdZFdWIpFu1MwrkcD+Hp5mBxvF+YMk36v6jgMD9Bizr1t0a95BI6kZaNZZACu5xfjy63nML5nnBQijGunPhvbCZ3qhyAx+Qpm/XECCx7ris4Nbn1QL01Kwb9/OYIP7m8v1RKeuZKLX/anSZ2RPTUq/PvOFlJTmKF8KhVwdtZQ6G/WYHhqVLiaW4wHvtolBc3bX1t6ViFWHEjDP7vVR4hf9TPu5hSVot3M8vdu2uDmeOiOBvhs8xnc3aEuWsUEoqRMj2Yv/4n2scFY+UwPlOmFVOWfeqMAveeWB+efJ3RHXJgfvtt+AaO71EODUL8qn/N2mfklyCwocXjTnBACJTq92RqttjPXmTQlGteYAcADX+7ErnPlo2KM3wdX959fDmNJUnmz7e3lLirV4eFvdqNH4zA8P7CZVdu9/TPflVh6/mawcDNZBSWY9vNhDGwVielG1xuh2m3Rk/GYuuQgruSWD/38a3o/6YTVt3l4hf4qHz/QAQUlugo1D8ZOvTXEpAboxBt3wsdLg/ziMrS+WRtSlSYR/hWuqVLHzwueGhUu55gfnhrs64lHujfE0LZRaBEVWGm4qSrYXJgzDDq9QOP/rpGW+XhqpNqo8AAtruZWfP6eTULxxt1tkFVQgucWH0RaViH6t4jAN492NVtWANh44jIe/77y/lAP39FAarYDgAEtI3EgJRO/P9sLH204hQ6xIfjvivL3YNET8fgs8Sy2nbmG8AAtkl4agKJSHS5eL0CIryciAr3x26F0CCFQphPYevoqRnasi5/3/o0/jpTPNbPxxX+gXogPdp+7gW5xdaQT+MsrjyDlRiEWPNrVpGYnPasQD3+zGxP+0Riju8SioKQMGdlFiAn2waaTV9CzSZhJJ/bPEs/gcnYRZt7VGiqVCk98n4TE5KvY9d/+CPPX4kpOEWb+fgwP3dEAPRqH4eL1/Eqvg/Tc/zXBC4OaAzAfLDKyi7Bsbyoe7Fbf7MUgz1/Lx76Lmbi3Y12zTYvVnbSzC0oxZ+1JjOpUV6o5LiqtWMMHmB6Dt08yuGxvqvT5fPtrSs8qRMLxy3jojgbQqMtrL+dtOoO2dYMQ36gO7vzoL3RpEIIPxnSospzOwmBRCyzceQGvrDqGf8bXxxt3tZbat7MLSzHzt2PoEBsMvRD4Z3x9XM0tNmlSIKqJQa0isf745epXtJNW0YFoHOGP3w+lV3r/+dlD0f+DLSa1EbZ4Z1RbtIoOQtNIf1y8XoDtZ65hePtonL2Sj7Ff78Lvz/ZCRnZRlcFCTpWFtcq0jgnEsfQceKhVeGdUO/hpNZjwY/kIKy8PNZqE+0PrqcaBlCyTxw1uHSldPDHA2wO5RWXo1rAOHunRAP2aR8DXS4O4GeWB7dtHu+C/vx6V+lK9NqIVujY0vULzyTfvRItXqp4756WhLfFkn0ZScxtQfnx1bVgHPl4ajI2vLz1ft4Z1sGxCd5PHX80tRh0/L2QVlKDzW+W1cXNHtcPAVpHYevoqBreOgodaBbVKheOXcvBhwilsPHkF93aqi+cHNEN4gBZjvtyFno1D8fzAZjialo17Ptshbf/CnGHYefY6HvxqF+7tVBfv3tfepM+OcbCYO6od7u8aizNX8vBZ4hnkFJZiw4nyuYAOvjoQKpUKqw+n46UVR6XHjO5cD++Obo8hH/+FE5dyAAD/GtQM760/JT2/OSVleoz9ehc61Q/B8wOb4VBqFjo3sG8/JwYLqqC4TIek85no0jAEey9kIuF4RpUXQOvXPBybqxiVUZvd1T4Gv1VxUiMi+5nUrzF6Nw1Hyo0CWWpr+7eIwMaTFScCNAj29TRpzgSAJ3vHoXvjUHh7aPDPr3eb3HdHozpSzYuxiACtVJN4u9n3tjVba7jw8W44eSkXqZkF+GHnRQxuHYnh7WLw7OID6NawDvZUMrFay+hA9G4ahqf7NEKomZGNNcFgQdUqKdPj90PpaB4VgJbRgdh17joW7ryIN+5ujYhAb6naMDO/BB3fLB8a2rZuEI6kZeOR7g2w5/wNnMzIrfZ52tcLwtj4Bpj+i/s33VT3LYyIyBXseak/IgK8Zd2mXYPFp59+infffRcZGRlo37495s2bh27duslaMHItRaU6eGnUVbZfXs0txrW8YjSLDMDJjBws2ZOKKQOaVpgLxLiz2q/P9EBUoDc2J19BuL8Wf52+JrVHr5vaB0WlOtz96Xb7vjArXZgzDNfzipFyo8Ck2pSIyJWcfXuo2eHWNWG3YLF06VI88sgj+PzzzxEfH4+PPvoIy5cvR3JyMiIiImQrGCnX0bRsFJfpTHr+G1TWyzw9qxAXruejR2PTadcLS3Tw1KigEwLZBaWICPRGdmEpzl/Lx+G/s/BQfANsPX0VP+1OQcLNfgFdGoRg8v81QYuoQPR7LxEfjumAUp0efZqGIy2rEP5aDxxJy8YPOy9I7b4DW0VicOuoKjuGbTp5Gb/sS8OI9jHILy7Di8sPybm7XN6BVwZKNVpEVdn8r77o916is4tRa9hjhI3dgkV8fDy6du2KTz75BACg1+sRGxuLZ599Fv/5z39kKxiR0uj0AmpV+eRh0UE+JvdlF5ZWOYW8Xi9w5moeCkt0aB8bbHLfumMZiAr0Rly4nzSHghACZ6/moW6wL7w91cgpLMPCXRcwon2MNISxsEQHtbp8ZlJDb/Yb+SU4fTkXDcP84KFWQeupgb/WA19sKZ8DpKBUh2YR/vj9cDri40Lx0B0NEBfmJ83FYOg9X1Kmx5kredKcLZ4aFYrL9Ji36TSOp+eY9N358uHOCA/QorBUh2AfLzy/9CCSL99qXgvw9sDmf/VFQbEOT/6wF2lZhcgrLkNkoBbD2sbg2+3n0SE2GBev5yMy0Nukae7IzEGY+dtxCAhsOH4ZXRvWQWZBCQK8PXEyIwevDm+N/i0jkHnzyr7dZ28CALw1sg1eXnmrk52lRnWqh1/2/212nVA/LxSX6StcB+fXZ3ogOsgbyRm5uJ5XUqNw+kSvOHy9reJcMK6gdUwg/niuN4QQ2H7mOh76Znf1D6Iai4+rg6VPd69+RSvZJViUlJTA19cXP//8M0aOHCktHzduHLKysrBq1aoKjykuLkZx8a2OKzk5OYiNjWWwICK3lVNUigCth8vON1CZzPwSqNUqBPl4WjXj6+30eoG8kjL4e3lUuQ0hBI5fykGr6MAa7SPjYaFFpToIcWsyMRVuTcd+4XoBgnw8sfnkFQxpGwVfLw8IIXAjvwRnr+Yjp7AUDcP8UDfYB4Wl5TWcN/JLUL+OL0p0eqRlFprM/WF43vSsQuj0AnWDfSq8RiGENBeJEAJClE+6t/pwOh66owGCfb1w/lo+/jp9FQ/FN4BKVT5Tc05RKS5nFyGnqBStooPg7anGpewi+HppkFlQirgwP+lCdEWleuiFQGZBCbw9NVKTcm5RKU5dzkPbukEAyudl2Xr6Gny9NGgdE4gtyVcRHeyDDrd9AZGLXYJFeno66tatix07dqB791tpaPr06diyZQt2766YQmfOnInXX3+9wnIGCyIiIvdhabCw+8T+M2bMQHZ2tnRLTU2191MSERGRk1h1rZCwsDBoNBpcvmw6Qc7ly5cRFRVV6WO0Wi20WnnH0hIREZFrsqrGwsvLC507d8bGjRulZXq9Hhs3bjRpGiEiIqLayeqrm77wwgsYN24cunTpgm7duuGjjz5Cfn4+xo8fb4/yERERkRuxOliMGTMGV69exauvvoqMjAx06NABa9euRWRkpD3KR0RERG6EU3oTERFRtVxmVAgRERHVHgwWREREJBsGCyIiIpINgwURERHJhsGCiIiIZMNgQURERLJhsCAiIiLZWD1Blq0M02bk5OQ4+qmJiIiohgzn7eqmv3J4sMjNzQUAxMbGOvqpiYiIyEa5ubkICgqq8n6Hz7yp1+uRnp6OgIAAqFQq2babk5OD2NhYpKamckZPO+J+dhzua8fgfnYM7mfHsOd+FkIgNzcXMTExUKur7knh8BoLtVqNevXq2W37gYGBPGgdgPvZcbivHYP72TG4nx3DXvvZXE2FATtvEhERkWwYLIiIiEg2igkWWq0Wr732GrRarbOLomjcz47Dfe0Y3M+Owf3sGK6wnx3eeZOIiIiUSzE1FkREROR8DBZEREQkGwYLIiIikg2DBREREcmGwYKIiIhko5hg8emnn6Jhw4bw9vZGfHw89uzZ4+wiuazZs2eja9euCAgIQEREBEaOHInk5GSTdYqKijBp0iSEhobC398fo0aNwuXLl03WSUlJwbBhw+Dr64uIiAhMmzYNZWVlJuskJiaiU6dO0Gq1aNKkCRYsWGDvl+ey5syZA5VKhalTp0rLuJ/lkZaWhoceegihoaHw8fFB27ZtsXfvXul+IQReffVVREdHw8fHBwMGDMDp06dNtnHjxg2MHTsWgYGBCA4OxuOPP468vDyTdQ4fPozevXvD29sbsbGxmDt3rkNenyvQ6XR45ZVXEBcXBx8fHzRu3BhvvvmmyQWpuJ9rZuvWrRgxYgRiYmKgUqmwcuVKk/sduV+XL1+OFi1awNvbG23btsWaNWusf0FCAZYsWSK8vLzEt99+K44dOyaefPJJERwcLC5fvuzsormkwYMHi++++04cPXpUHDx4UAwdOlTUr19f5OXlSetMmDBBxMbGio0bN4q9e/eKO+64Q/To0UO6v6ysTLRp00YMGDBAHDhwQKxZs0aEhYWJGTNmSOucO3dO+Pr6ihdeeEEcP35czJs3T2g0GrF27VqHvl5XsGfPHtGwYUPRrl07MWXKFGk597Ptbty4IRo0aCAeffRRsXv3bnHu3Dmxbt06cebMGWmdOXPmiKCgILFy5Upx6NAhcdddd4m4uDhRWFgorXPnnXeK9u3bi127dom//vpLNGnSRDz44IPS/dnZ2SIyMlKMHTtWHD16VCxevFj4+PiIL774wqGv11lmzZolQkNDxerVq8X58+fF8uXLhb+/v/j444+ldbifa2bNmjXipZdeEr/++qsAIFasWGFyv6P26/bt24VGoxFz584Vx48fFy+//LLw9PQUR44cser1KCJYdOvWTUyaNEn6W6fTiZiYGDF79mwnlsp9XLlyRQAQW7ZsEUIIkZWVJTw9PcXy5culdU6cOCEAiJ07dwohyv8R1Gq1yMjIkNaZP3++CAwMFMXFxUIIIaZPny5at25t8lxjxowRgwcPtvdLcim5ubmiadOmIiEhQfzjH/+QggX3szz+/e9/i169elV5v16vF1FRUeLdd9+VlmVlZQmtVisWL14shBDi+PHjAoBISkqS1vnzzz+FSqUSaWlpQgghPvvsMxESEiLtd8NzN2/eXO6X5JKGDRsmHnvsMZNl9957rxg7dqwQgvtZLrcHC0fu1/vvv18MGzbMpDzx8fHi6aeftuo1uH1TSElJCfbt24cBAwZIy9RqNQYMGICdO3c6sWTuIzs7GwBQp04dAMC+fftQWlpqsk9btGiB+vXrS/t0586daNu2LSIjI6V1Bg8ejJycHBw7dkxax3gbhnVq2/syadIkDBs2rMK+4H6Wx2+//YYuXbpg9OjRiIiIQMeOHfHVV19J958/fx4ZGRkm+ygoKAjx8fEm+zk4OBhdunSR1hkwYADUajV2794trdOnTx94eXlJ6wwePBjJycnIzMy098t0uh49emDjxo04deoUAODQoUPYtm0bhgwZAoD72V4cuV/l+ixx+2Bx7do16HQ6kw9eAIiMjERGRoaTSuU+9Ho9pk6dip49e6JNmzYAgIyMDHh5eSE4ONhkXeN9mpGRUek+N9xnbp2cnBwUFhba4+W4nCVLlmD//v2YPXt2hfu4n+Vx7tw5zJ8/H02bNsW6deswceJEPPfcc/j+++8B3NpP5j4jMjIyEBERYXK/h4cH6tSpY9V7oWT/+c9/8MADD6BFixbw9PREx44dMXXqVIwdOxYA97O9OHK/VrWOtfvd4ZdNJ9cyadIkHD16FNu2bXN2URQnNTUVU6ZMQUJCAry9vZ1dHMXS6/Xo0qUL3n77bQBAx44dcfToUXz++ecYN26ck0unHMuWLcNPP/2ERYsWoXXr1jh48CCmTp2KmJgY7mcy4fY1FmFhYdBoNBV60l++fBlRUVFOKpV7mDx5MlavXo3NmzejXr160vKoqCiUlJQgKyvLZH3jfRoVFVXpPjfcZ26dwMBA+Pj4yP1yXM6+fftw5coVdOrUCR4eHvDw8MCWLVvwv//9Dx4eHoiMjOR+lkF0dDRatWplsqxly5ZISUkBcGs/mfuMiIqKwpUrV0zuLysrw40bN6x6L5Rs2rRpUq1F27Zt8fDDD+P555+XauO4n+3Dkfu1qnWs3e9uHyy8vLzQuXNnbNy4UVqm1+uxceNGdO/e3Yklc11CCEyePBkrVqzApk2bEBcXZ3J/586d4enpabJPk5OTkZKSIu3T7t2748iRIyYHc0JCAgIDA6UP+e7du5tsw7BObXlf+vfvjyNHjuDgwYPSrUuXLhg7dqz0O/ez7Xr27FlhuPSpU6fQoEEDAEBcXByioqJM9lFOTg52795tsp+zsrKwb98+aZ1NmzZBr9cjPj5eWmfr1q0oLS2V1klISEDz5s0REhJit9fnKgoKCqBWm54yNBoN9Ho9AO5ne3HkfpXts8Sqrp4uasmSJUKr1YoFCxaI48ePi6eeekoEBweb9KSnWyZOnCiCgoJEYmKiuHTpknQrKCiQ1pkwYYKoX7++2LRpk9i7d6/o3r276N69u3S/YRjkoEGDxMGDB8XatWtFeHh4pcMgp02bJk6cOCE+/fTTWjUMsjLGo0KE4H6Ww549e4SHh4eYNWuWOH36tPjpp5+Er6+v+PHHH6V15syZI4KDg8WqVavE4cOHxd13313pcL2OHTuK3bt3i23btommTZuaDNfLysoSkZGR4uGHHxZHjx4VS5YsEb6+vooeBmls3Lhxom7dutJw019//VWEhYWJ6dOnS+twP9dMbm6uOHDggDhw4IAAID744ANx4MABcfHiRSGE4/br9u3bhYeHh3jvvffEiRMnxGuvvVZ7h5sKIcS8efNE/fr1hZeXl+jWrZvYtWuXs4vksgBUevvuu++kdQoLC8UzzzwjQkJChK+vr7jnnnvEpUuXTLZz4cIFMWTIEOHj4yPCwsLEiy++KEpLS03W2bx5s+jQoYPw8vISjRo1MnmO2uj2YMH9LI/ff/9dtGnTRmi1WtGiRQvx5Zdfmtyv1+vFK6+8IiIjI4VWqxX9+/cXycnJJutcv35dPPjgg8Lf318EBgaK8ePHi9zcXJN1Dh06JHr16iW0Wq2oW7eumDNnjt1fm6vIyckRU6ZMEfXr1xfe3t6iUaNG4qWXXjIZvsj9XDObN2+u9DN53LhxQgjH7tdly5aJZs2aCS8vL9G6dWvxxx9/WP16VEIYTZtGREREZAO372NBREREroPBgoiIiGTDYEFERESyYbAgIiIi2TBYEBERkWwYLIiIiEg2DBZEREQkGwYLIiIikg2DBREREcmGwYKIiIhkw2BBREREsvl/5f3dDLR34+YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.unet import Unet\n",
    "from model.training import *\n",
    "from model.diffusionModel import DiffusionModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from model.t5 import t5_encode_text\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "log_file='training_progress.txt'\n",
    "def show_msg(msg, file=log_file):\n",
    "    if file is not None:\n",
    "        with open(file, 'a') as f:\n",
    "            f.write(msg+\"\\n\")\n",
    "    print(msg)\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 16\n",
    "n_epoch = 10000\n",
    "learning_rate = 1e-3\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "n_feat = 512 # hidden dimension feature\n",
    "max_text_len = 128 # word vector max size\n",
    "height = 64 # 64x64 image\n",
    "save_dir = './weights/'\n",
    "# diffusion hyperparameters\n",
    "timesteps = 5000\n",
    "\n",
    "dataset_data_path = './dataset_conceptual_captions_lite.npy'\n",
    "# load dataset\n",
    "dataset = CustomDataset(dataset_data_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True)\n",
    "\n",
    "df = DiffusionModel(timesteps, height)\n",
    "model = Unet(in_channels=3, n_feat=n_feat, max_text_len=max_text_len, height=height, device=device).to(device)\n",
    "#model.load_state_dict(torch.load(save_dir+\"model_901.pth\", map_location=device))\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "open(log_file, 'w').close()\n",
    "list_total_loss = []\n",
    "training_steps = 0\n",
    "model.train()\n",
    "for epoch in range(n_epoch):\n",
    "    show_msg(\"------------------------------------ epoch {:03d} ({} steps) ------------------------------------\".format(epoch + 1, training_steps))\n",
    "    total_loss = 0\n",
    "    loss_list = []\n",
    "    # linearly decay of learning rate\n",
    "    #optimizer.param_groups[0]['lr'] = learning_rate*(1-(epoch/n_epoch))\n",
    "    for x_0, labels in dataloader:   # x_0: images\n",
    "        optimizer.zero_grad()\n",
    "        x_0 = x_0.to(device)\n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x_0).to(device)\n",
    "        t = torch.randint(1, timesteps + 1, (x_0.shape[0],)).to(device)\n",
    "        x_t = df.noise_image(x_0, t, noise).to(device)\n",
    "        t_emb, t_mask = t5_encode_text(labels)\n",
    "        # use network to recover noise\n",
    "        pred_noise = model(x_t, t/timesteps, t_emb=t_emb, t_mask=t_mask)\n",
    "\n",
    "        # loss is measures the element-wise mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_steps+=1\n",
    "        #if (training_steps%100) == 0:\n",
    "            #print(\"Total train step: {}, Loss: {}\".format(training_steps,loss))\n",
    "            \n",
    "    loss_list = np.array(loss_list)\n",
    "    show_msg(\"Max loss: {}\".format(loss_list.max()))\n",
    "    show_msg(\"Min loss: {}\".format(loss_list.min()))\n",
    "    total_loss = loss_list.sum()\n",
    "    show_msg(\"Mean loss: {}\".format(loss_list.mean()))\n",
    "    show_msg(\"Std loss: {}\".format(loss_list.std()))\n",
    "    show_msg(\"Total Loss: {}\".format(total_loss))\n",
    "    list_total_loss.append(total_loss)\n",
    "  # save model periodically\n",
    "    if epoch%100==0 or epoch == int(n_epoch-1):\n",
    "        torch.save(model.state_dict(), save_dir + \"model_{:03d}.pth\".format(epoch+1))\n",
    "        show_msg('saved model at ' + save_dir + \"model_{:03d}.pth\".format(epoch+1))\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(list_total_loss)\n",
    "plt.title(\"Total Loss vs Epoch\")\n",
    "plt.savefig('train.png')\n",
    "\n",
    "show_msg(\"Fin del entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfc7bd9-97f1-4314-98b9-5789ed812e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from model.t5 import t5_encode_text\n",
    "\n",
    "class DiffusionModel():\n",
    "  # return beta_sqrt, alpha, alpha_sqrt, gamma, gamma_sqrt for noise and denoise image\n",
    "  def __init__(self, timesteps, height, beta1=0.0001, beta2=0.02):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "    scale = 1000 / timesteps\n",
    "    beta1 = scale * beta1\n",
    "    beta2 = scale * beta2\n",
    "    beta = torch.linspace(beta1**0.5, beta2**0.5, timesteps + 1, device=self.device)**2\n",
    "    alpha = 1 - beta\n",
    "    one_by_sqrt_alpha = 1./alpha.sqrt()\n",
    "    gamma = torch.cumprod(alpha, axis=0)\n",
    "    gamma[0] = 1\n",
    "    sqrt_one_minus_gamma = (1. - gamma).sqrt()\n",
    "    beta_by_sqrt_one_minus_gamma = beta/sqrt_one_minus_gamma\n",
    "    self.noise_schedule_dict = {'alpha':alpha, 'sqrt_alpha':alpha.sqrt(),\n",
    "                   'beta':beta, 'sqrt_beta':beta.sqrt(),\n",
    "                   'gamma':gamma, 'sqrt_gamma':gamma.sqrt(),\n",
    "                   'one_by_sqrt_alpha':one_by_sqrt_alpha,\n",
    "                   'sqrt_one_minus_gamma':sqrt_one_minus_gamma,\n",
    "                   'beta_by_sqrt_one_minus_gamma':beta_by_sqrt_one_minus_gamma}\n",
    "    self.timesteps = timesteps\n",
    "    self.height = height\n",
    "\n",
    "  def show_noise_schedule(self):\n",
    "    rows = 5\n",
    "    cols = 2\n",
    "    plt.figure(figsize=(16,16))\n",
    "    for i, schedule in enumerate(self.noise_schedule_dict):\n",
    "      plt.subplot(rows, cols, i + 1)\n",
    "      plt.title(schedule)\n",
    "      curr_schedule = self.noise_schedule_dict[schedule]\n",
    "      plt.plot(curr_schedule.to('cpu'))\n",
    "        \n",
    "df = DiffusionModel(5000,128)\n",
    "df.show_noise_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34d91ec-514c-448b-a87a-1fc4aa8f7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x10800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from model.training import CustomDataset\n",
    "from model.diffusionModel import DiffusionModel\n",
    "\n",
    "dataset_name = './dataset_conceptual_captions_lite.npy'\n",
    "dataset = CustomDataset(dataset_name)\n",
    "\n",
    "label = 'beautiful dog with glasses portrait - isolated over a white background'\n",
    "\n",
    "rows = len(dataset)\n",
    "print(rows)\n",
    "plt.figure(figsize=(16,rows))\n",
    "for i in range(len(dataset)):\n",
    "    img, curr_label = dataset[i]\n",
    "    if curr_label == label:\n",
    "        plt.subplot(rows, 1, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title(curr_label)\n",
    "        plt.imshow(img.permute(1,2,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envDM)",
   "language": "python",
   "name": "envdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
